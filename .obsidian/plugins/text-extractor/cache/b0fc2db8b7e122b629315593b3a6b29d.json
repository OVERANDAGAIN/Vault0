{"path":"Papers/Mastering the game of Go with deep  neural networks and tree search/attchments/Pasted image 20241211171349.png","text":"Backup (Fig. 3d). At each in-tree step t <L of the simulation, the rollout statistics are updated as if it has lost ny; games, Ni(sy, ar) < Ni(sp, ar) + nvi; Wilsy, ar) < Wilsr, ag) —nyg EEis virtual loss™ discourages other threads from simultaneously explor- ing the identical variation. At the end of the simulation,%he rollout statistics are uEdated in a backward pass througl_l each stept<L, reElacing the virtual losses bz the outcome, N,%. ag) < Ni(sp, ar) —nyi+ 1; Wilsy, ag) < Wilse, ap) + nyi + zt. Asynchronously, a separate backward pass is initiated when the evaluation of the leaf position s; completes. The output of the value network vg(s) is used to update value statistics in a second backward pass through each step <L, Ny(sp, ar) < Ny(sp, ar) + 1, Wi (sp, ag) < W (sp, ag) +vg(sg). The overall evaluation of each state action is a weighted average of the Monte Carlo estimates, Q(s,a)=(1— /\\)% + )\\% 5 that¥hixes together the value network and rollout evaluations with weighting parameter A. All updates are performed lock-free®.","libVersion":"0.3.2","langs":"eng"}