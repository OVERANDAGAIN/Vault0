---

---


对于这一页PPT，给出演讲文稿：
Slide 1. 标题页

    论文题目、作者信息

    开场引导：用几句话概括背景问题，引出主题

















我们关注的任务是**目标条件强化学习（goal-conditioned RL）**。



我们从第一个部分开始——学习“目标达成技能”（goal reaching skills）。我将从问题定义开始介绍，也就是所谓的“目标条件强化学习”（goal-conditioned reinforcement learning）问题。

假设我们有一个机器人，我们希望它把绿色的物体捡起来，并放进蓝色的箱子里。传统做法是，我们可以写出那个之前看到的庞大且复杂的奖励函数。

但这一次，我们不这么做，而是让用户直接提供一张图像，来表示他们希望机器人完成的结果，也就是“期望结果”。
具体来说，我们希望学习一个策略，它以当前图像和目标图像为输入，并采取一系列动作，最终使系统到达目标图像所对应的状态。

例如，在这个例子中，我们希望机器人往下移动、抓起绿色物体，然后移动到蓝色箱子上方，把它放进去。

这种“目标条件强化学习”其实是一个非常老的问题，早期的一些AI研究，包括NSH（可能指代某位研究者或团队）的工作，就曾探讨过如何学习目标导向的行为。

而我们研究的是一个特定场景：我们拥有一个数据集，数据来自一些视频，每帧带有动作标签。要注意的是，这些视频中的动作**并不一定是最优的**，动作也不一定是“好”的。但我们要从这些数据中学习，尽可能快速地达成目标的技能。
我们使用一个大型视频数据集来训练这个策略，数据中标注了每一帧的动作。但与标准强化学习中的轨迹不同，这些数据**并没有奖励标注**。

我们定义的奖励函数是：**如果在下一个时间步到达目标状态，则获得奖励**。这种定义完全基于物理过程，不需要任何奖励工程。

我们优化的目标是这些奖励的总和，Q函数就是在当前状态和动作条件下的这个目标函数。

我们方法的核心思想是：**使用对比学习，构建整个强化学习算法**。这个算法不需要额外的表示学习、奖励学习或模型学习。





这个问题为何有用？在实际中，它让用户更容易指定任务；而在数学层面，它让我们可以将方法应用到更一般的环境中，包括**连续状态空间、连续动作空间、随机动态系统**，同时还能保留某些理论性质，因为这个目标是一个良定义的（well-defined）优化目标。

以下是对三类 baseline 的简明总结，每类重点突出，便于展示或演讲时快速传达：

---

### 1️ 目标条件强化学习方法（Goal-Conditioned RL）

* **HER（Hindsight Experience Replay）**：
  通过事后重标记目标状态，使无奖励的轨迹变得可学习，搭配 TD3 构建强大的策略学习器。

* **GCBC（Goal-Conditioned Behavioral Cloning）**：
  直接对到达目标的轨迹进行行为克隆，方法简单但效果强，使用和本文方法相同的 (s, a, s_f)输入。

* **模型方法（Model-Based）**：
  学习未来状态分布 p(s\_f|s, a)，然后训练策略最大化到达目标状态的概率，适合低维问题。

---

### 2️ 表示学习方法（Representation Learning）

* **数据增强（DrQ 系列）**：
  使用图像增强来提高策略的鲁棒性，代表性方法为 DrQ。

* **辅助目标**：

  * **AutoEncoder（AE）**：重建状态作为训练目标。
  * **CURL**：使用数据增强构造正样本对，基于对比学习优化状态表示。

---

### 3️ 对比强化学习探索（Contrastive RL Variants）

* **Contrastive RL (CPC)**：
  采用 mutual information 下界（infoNCE）作为目标，学习状态-动作与未来状态之间的关联。

* **C-learning**：
  以 TD 目标为核心构建的对比学习方法，已有文献提出。

* **Contrastive RL (NCE + C-learning)**：
  本文提出的融合方法，结合对比表示学习和 C-learning 的优势，兼顾表达能力与稳定性。

---

如需，我还可以为每一类画一张对比图表。
