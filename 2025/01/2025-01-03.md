---
created: 2025-01-03T15:21
updated: 2025-02-17-22.
---
以下是图片中的文字内容：

---

Our opponent modeling consists of two components: subgoal inference model and subgoal selector.

The subgoal inference model employs the historical trajectory to predict the opponent’s subgoal, serving as input for the policy during the interaction phase. The subgoal selector scrutinizes the entire historical trajectory using a value-based heuristic to choose the appropriate subgoal for training the inference model during the update phase.

Subgoal inference model. The subgoal $g$ represents a feature embedding of a future state. Specifically, for a trajectory $\{s_0, a_0, a_0^{-i}, \ldots, s_t, a_t, a_t^{-i}, \ldots, s_T\}$, the state corresponding to subgoal $g_t$ at $s_t$ is one of future states $\mathcal{N}_t = \{s_{t+1}, s_{t+2}, \ldots, s_T\}$, denoted as $s_t^g$ and determined by the subgoal selector.

The objective of the subgoal inference model is to infer $g_t$ from the historical trajectory $\tau_t = \{s_0, a_0, a_0^{-i}, \ldots, s_{t-1}, a_{t-1}, a_{t-1}^{-i}\}$. This aligns with the intuitive hypothesis that the opponent’s intention can often be inferred after just a few initial actions.

Here, we introduce variational inference and employ a conditional variational auto-encoder (CVAE) as the subgoal inference model. In this model, we represent the subgoal posterior probability as $q_\phi(\hat{g}_t|\tau_t, s_t)$ and the likelihood estimate as $p_\theta(s_t|\hat{g}_t, \tau_t)$ with $\phi$ and $\theta$ respectively denoting the network parameters. The subgoal prior model, denoted as $p_\psi(\hat{g}_t|s_t^g)$, is a pre-trained variational autoencoder (VAE) using the states previously collected in the environment, and produces the subgoal prior $p_\psi(\hat{g}_t|s_t^g)$ given the subgoal state $s_t^g$ chosen by the subgoal selector.

Further details about the network architecture are provided in Figure 2. The optimization objective of the subgoal inference model is:

$$\langle \hat{\theta}, \hat{\phi} \rangle = \arg\max_{\theta, \phi} \mathbb{E}_{q_\phi(\hat{g}_t|\tau_t, s_t)} \left[ \log p_\theta(s_t|\hat{g}_t, \tau_t) \right] - \text{KL} \left( q_\phi(\hat{g}_t|\tau_t, s_t) \parallel p_\psi(\hat{g}_t|s_t^g) \right).$$

---

如需进一步解释其中的内容，请告诉我！





Subgoal posterior probability subgoal prior model




以下是图片中的文字内容转换为文本的结果：

---

The choice of the subgoal state plays a significant role in shaping the agent’s behavior and leaning towards either optimism or conservatism. This is especially critical when dealing with cooperative games and general-sum games, where the dynamics of interactions are complex and multifaceted. In these contexts, we provide two distinct manners for the subgoal selection:

$$\bar{g}_t = \arg \max_{s_i \in \mathcal{N}_t^H} \mathbb{E}_{g \sim p_\psi(\cdot | s_i)} V(s_t, g)$$

$$\bar{g}_t = \arg \min_{s_i \in \mathcal{N}_t^H} \mathbb{E}_{g \sim p_\psi(\cdot | s_i)} V(s_t, g)$$

where $V(s, g) = \mathbb{E}_a Q(s, g, a)$, $\mathcal{N}_t^H$ is the set of future states $\{s_{t+1}, \dots, s_{t+H}\}$. As discussed in Section 4.1, the quantity of $(s, g)$ pairs is crucial. Selecting candidate subgoal states is pivotal in this regard. Thus, we use states within the next $H$ timesteps instead of all future states. The choice of $H$ gives a tradeoff between the agent’s generalization to diverse opponents induced by the fact that the subgoals of different trajectory fragments have combinatorial properties and the learning difficulty incurred by the increased opponent subgoals.

As indicated in Equation (6), we pinpoint the subgoal within an $H$-horizon that maximizes the V-value. The agent incorporates this to optimize the Q-function, thus adopting an optimistic strategy akin to the maximax strategy [6], which applies to cooperative games. Conversely, if we choose the subgoal as in Equation (7), it corresponds to the subgoal yielding the lowest value. The agent then employs this for learning Q-function, leading to a conservative strategy similar to the minimax strategy, which is commonly used in general-sum games.

---

如需进一步分析或解释，可以随时提出！



以下是图片中的文字内容转换为文本的结果：

---

The subgoal inference model is unstable at the beginning, which may disturb the updating of the Q-network. Therefore, we use the following combination of the prior subgoal $\bar{g}$ and the inferred subgoal $\hat{g}$ as the input of Q-network,

$$g_t = \hat{g}_t \mathbb{I}(\eta > \epsilon) + \bar{g}_t \mathbb{I}(\eta \leq \epsilon), \quad \eta \sim U[0,1],$$

where $\epsilon$ is a hyperparameter that decreases to zero over training. We will further empirically study this in Section 5.4.

---

如需进一步分析，可以随时提问！