---
created: 2025-01-06T10:15
updated: 2025-02-13-13.
---


从图片中提取的文字如下：

---

In this section, we present the proposed method PTGM, utilizing the pre-trained goal-conditioned policy $P_{\phi}(a_t|s_t, s^g)$ to provide temporal abstractions for RL in downstream tasks. In RL, we train a high-level policy $\pi_{\theta}(s^g|s_t)$ which outputs a goal state to guide the low-level goal-conditioned policy $P_{\phi}$ to act in the environment for $k$ steps. To enhance the sample efficiency and stability of RL, we propose a goal clustering method and a pre-trained goal prior model. Figure 1 gives an overview of PTGM.

---

如果需要其他操作，请告诉我！

从图片中提取的文字如下：

---

A goal refers to the final state of a sub-trajectory in the dataset, representing the outcome of the short-term behavior. The goal-conditioned model $P_{\phi}$ uses the state space $\mathcal{S}$ to be the space $\mathcal{Z}$ and predicts the action for one step:

$$P(a_{t:t+k} | s_t, s^g) = \int_{s_{t+1}, \ldots, s_{t+k}} d_{s_{t+1}} \ldots d_{s_{t+k}} \prod_{i=t}^{t+k} P_{\phi}(a_i | s_i, s^g) P(s_{i+1} | s_i, a_i),$$

where $s^g \in \mathcal{S}$ is the goal state. Starting from $s_t$, $P_{\phi}$ aims at reaching the goal state $s^g$ after executing the actions sampled from it for $k$ steps.

To learn $P_{\phi}$ from data, we use a variant of hindsight relabeling (Andrychowicz et al., 2017) to label each state-action sample in the dataset with a future state as the goal state. For a $k$-step subsequence $\tau = (s_t, a_t, \cdots, s_{t+k}, a_{t+k})$ in an episode, we label each sample $(s_i, a_i), t \leq i \leq t+k$ with the goal state $s^g = s_{t+k}$. We train $P_{\phi}$ with behavior cloning, minimizing the negative log-likelihood of action prediction:

$$\mathcal{L}(\phi) = \mathbb{E}_{\mathcal{D}} \big[ -\log P_{\phi}(a_i | s_i, s^g) \big].$$

In practice, to enable the model to reach goals after different time steps, we randomly sample $k$ within a range. When states are compact vectors representing the physical state of objects in the environment, we use the raw state as the goal. When the environment is a POMDP with image observations, we use the embedding from a pre-trained image encoder as the goal. For instance, Steve-1 (Lifshitz et al., 2023) uses the embedding of $o_{t+k-16:t+k}$ from the MineCLIP’s vision encoder (Fan et al., 2022) in Minecraft.

---

如果需要其他操作，请告诉我！

从图片中提取的文字如下：

---

**4.1 Clustering in the Goal Space**

The goals $s^g \in \mathcal{S}$ from the high-dimensional state space introduce a high-dimensional continuous action space for the high-level policy, making RL sample-inefficient. To tackle this challenge, we propose to cluster the states in the dataset to discretize the goal space, constructing a discrete action space for the high-level policy. We sample a large set of states from $D$, apply t-SNE (Maaten & Hinton, 2008) to reduce the dimension of states, and apply a clustering algorithm such as K-Means (Lloyd, 1982) to group similar goal states together and output $N$ clusters. The discretized goal space is represented with $G = \{i : s^g_i\}_{i=1}^N$, where $s^g_i$ is the goal state of the $i$-th cluster center. This converts the action space of the high-level policy into a discrete action space $A^h = [N]$ and constrains the high-level policy to output goals in the cluster centers.

We observe that compressing goal states into the discrete goal space does not significantly decrease the agent’s model capacity to perform various behaviors. The reasons come from two aspects. Firstly, the clustering algorithm groups similar goals together. The cluster center can represent goals in its cluster that correspond to similar agent behaviors. Secondly, the goal-conditioned model pre-trained on the large dataset as the low-level policy can elicit generalizability on goals. The model can extract behavior information in the goal state, thereby generating correct behaviors even when the provided goal is distant from the current environment state. Given the same goal, the model can exhibit diverse behaviors in different states. These claims are substantiated in our experimental results in Section 5.4. Therefore, using the discrete goal space, the low-level policy still has the capacity to cover various behaviors in the dataset.

---

如果需要进一步操作，请告诉我！

从图片中提取的文字如下：

---

**4.2 Pre-Training the Goal Prior Model**

In RL, the agent is able to perform smooth and reasonable behaviors within $k$ consecutive steps given a goal. However, the high-level policy lacks prior knowledge to provide reasonable goals, thereby should uniformly explore the goal space to learn the task. We propose to learn this prior knowledge from the dataset by pre-training a goal prior model, improving the sample efficiency and stability of training the high-level policy.

The goal prior model $\pi_{\psi}^p(a^h|s)$ has the same structure as the high-level policy, where $a^h \in A^h$ is the index of the goal cluster centers. This model is trained to predict the distribution of future goals given the current state, using the clustered goal space $G$. Similar to training the goal-conditioned model, we sample states and subsequent goal states $(s_t, s^g)$ from the dataset. In the discretized goal space $G$, we match the goal that is closest to $s^g$ based on cosine similarity:

$$a^h = \arg\max_{i \in [N]} \left( \frac{s_i^g \cdot s^g}{\|s_i^g\| \cdot \|s^g\|} \right).$$

The training objective for the goal prior model is to minimize the negative log-likelihood of goal prediction:

$$\mathcal{L}(\psi) = \mathbb{E}_{\mathcal{D}} \big[ -\log \pi_{\psi}^p(a^h|s_t) \big].$$

The pre-trained goal prior model acts as a regularizer for the high-level policy during RL, providing intrinsic rewards that guide the agent’s exploration towards possible goals in the dataset.

---

如果需要进一步操作，请告诉我！


从图片中提取的文字如下：

---

**4.3 Reinforcement Learning with PTGM**

Given the goal clusters $G$, the pre-trained low-level policy $P_{\phi}$, and the goal prior model $\pi_{\psi}^p$, we proceed with training the high-level policy using RL for downstream tasks. At each time step, the high-level policy $\pi_{\theta}(a^h|s)$ selects an index of the goal $s^g_{a^h}$ in the clustered goal space. The fixed low-level policy acts in the environment for $k$ steps conditioned on $s^g_{a^h}$. The high-level policy is updated based on the environment rewards and the intrinsic rewards from the goal prior model.

The overall objective for training the high-level policy is to maximize the expected return:

$$J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{\infty} \gamma^t \left( \sum_{i=kt}^{(k+1)t} R(s_i, a_i) - \alpha D_{KL} \left( \pi_{\psi}^p(a^h|s_{kt}) \| \pi_{\theta}(a^h|s_{kt}) \right) \right) \right], \tag{4}$$

where $t$ represents the number of steps for the high-level policy and $\alpha$ is a hyperparameter balancing the environmental rewards and the intrinsic rewards. By optimizing this objective, the high-level policy learns to select goals that lead to task success and align with the behaviors in the dataset, achieving sample-efficient RL with a discrete action space. In principle, any online RL algorithm can be used to train the high-level policy in downstream tasks.

---

如果需要进一步处理或解释，请告诉我！


从图片中提取的文字如下：

---

**SPiRL** (Pertsch et al., 2021a). This work pre-trains a sequential VAE (Zhu et al., 2020) for $k$-step action sequence $q(z|s_t, a_{t:t+k})$, $p(a_{t:t+k}|s_t, z)$ along with a skill prior $p_a(z|s_t)$, modeling skills with a continuous latent variable $z$. In RL, the high-level policy outputs a skill $z$, and then the action decoder $p$ decodes the action sequence and executes for $k$ steps. For Kitchen, we run the released code where $k=10$. For Minecraft, there is a trade-off for selecting $k$. If we set $k=100$ which is the same as PTGM, the model fails to reconstruct the long action sequences accurately. Otherwise, if we keep $k=10$, the downstream RL can be less sample-efficient since less temporal abstractions are provided. Therefore, we run experiments on both settings, present the best result for each task in the paper, and leave the results of both settings in Appendix D.

**TACO** (Rosete-Beas et al., 2023). This work pre-trains a low-level policy $\pi(a_t|s_t, z)$ conditioned on the continuous latent variable $z$ from a learned skill posterior $q(z|\tau)$, regularizes $q$ with KL-divergence to a skill prior $p(z|s_t, s_T)$, and proposes offline RL methods to train the high-level policy. We take an online variant of TACO, where the skill prior $p$ is used to initialize and regularize the high-level policy for online RL. For each downstream task, we manually provide the task goal $s_T$.

**VPT-finetune.** VPT (Baker et al., 2022) is a foundation model for Minecraft that trains a behavior-cloning (BC) policy $\pi(a_t|o_{0:t})$ on a game playing dataset of 70K hours from the Internet. For Minecraft tasks, we train RL to either finetune the full model (Baker et al., 2022) or finetune the transformer adapters (Nottingham et al., 2023). We report the better results for each task in the paper and leave the results of both methods in Appendix D. Note that PPO from scratch fails on all tasks.
从图片中提取的文字如下：

---

**Steve-1 (Lifshitz et al., 2023).** This work builds an instruction-following agent in Minecraft. It first trains a goal-conditioned policy $\pi(a_t|o_{0:t}, g)$ on the contractor dataset, which is the same as the low-level policy in PTGM. Then, Steve-1 adopts a language-labeled dataset to map instructions to goals. We test the zero-shot performance of Steve-1 in our tasks by providing task instructions.

---

如果需要进一步处理或比较，请告诉我！
---

### 概括算法思想（每个算法一句话描述）：

1. **SPiRL**：通过预训练顺序VAE以生成技能的连续潜变量 $z$，高层策略选择技能 $z$，低层策略基于技能解码并执行 $k$-步动作。
2. **TACO**：利用离线RL预训练低层策略，基于连续潜变量 $z$ 的技能后验进行正则化，并通过KL散度调整技能先验以优化高层策略。
3. **VPT-finetune**：基于互联网的行为克隆数据集训练基础模型，通过微调完整模型或适配器实现下游任务。 

如果需要进一步解释或修改，请告诉我！
---

### Algorithm Summaries (One Sentence per Algorithm):

1. **SPiRL**: Pre-trains a sequential VAE to generate continuous skill latent variables $z$, with the high-level policy selecting skills $z$ and the low-level decoder executing $k$-step actions based on those skills.

2. **TACO**: Uses offline RL to pre-train a low-level policy conditioned on skill latent variables $z$, regularized by KL divergence to a learned skill prior, enabling the high-level policy to be trained for online RL.

3. **VPT-finetune**: Trains a foundation model on a large-scale behavior cloning dataset and fine-tunes the full model or transformer adapters to adapt to downstream tasks.

If you need more details or further clarifications, feel free to ask!