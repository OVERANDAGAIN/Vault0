

从图片中提取的文字如下：

---

In this section, we present the proposed method PTGM, utilizing the pre-trained goal-conditioned policy $P_{\phi}(a_t|s_t, s^g)$ to provide temporal abstractions for RL in downstream tasks. In RL, we train a high-level policy $\pi_{\theta}(s^g|s_t)$ which outputs a goal state to guide the low-level goal-conditioned policy $P_{\phi}$ to act in the environment for $k$ steps. To enhance the sample efficiency and stability of RL, we propose a goal clustering method and a pre-trained goal prior model. Figure 1 gives an overview of PTGM.

---

如果需要其他操作，请告诉我！

从图片中提取的文字如下：

---

A goal refers to the final state of a sub-trajectory in the dataset, representing the outcome of the short-term behavior. The goal-conditioned model $P_{\phi}$ uses the state space $\mathcal{S}$ to be the space $\mathcal{Z}$ and predicts the action for one step:

$$P(a_{t:t+k} | s_t, s^g) = \int_{s_{t+1}, \ldots, s_{t+k}} d_{s_{t+1}} \ldots d_{s_{t+k}} \prod_{i=t}^{t+k} P_{\phi}(a_i | s_i, s^g) P(s_{i+1} | s_i, a_i),$$

where $s^g \in \mathcal{S}$ is the goal state. Starting from $s_t$, $P_{\phi}$ aims at reaching the goal state $s^g$ after executing the actions sampled from it for $k$ steps.

To learn $P_{\phi}$ from data, we use a variant of hindsight relabeling (Andrychowicz et al., 2017) to label each state-action sample in the dataset with a future state as the goal state. For a $k$-step subsequence $\tau = (s_t, a_t, \cdots, s_{t+k}, a_{t+k})$ in an episode, we label each sample $(s_i, a_i), t \leq i \leq t+k$ with the goal state $s^g = s_{t+k}$. We train $P_{\phi}$ with behavior cloning, minimizing the negative log-likelihood of action prediction:

$$\mathcal{L}(\phi) = \mathbb{E}_{\mathcal{D}} \big[ -\log P_{\phi}(a_i | s_i, s^g) \big].$$

In practice, to enable the model to reach goals after different time steps, we randomly sample $k$ within a range. When states are compact vectors representing the physical state of objects in the environment, we use the raw state as the goal. When the environment is a POMDP with image observations, we use the embedding from a pre-trained image encoder as the goal. For instance, Steve-1 (Lifshitz et al., 2023) uses the embedding of $o_{t+k-16:t+k}$ from the MineCLIP’s vision encoder (Fan et al., 2022) in Minecraft.

---

如果需要其他操作，请告诉我！

