---
创建时间: 2025-一月-12日  星期日, 3:48:54 下午
---
lola:
在一群持续学习的、追求自我利益最大化的智能体中如何使其涌现互利行为一直是一个难题。博弈论研究在包含协作与竞争因素的博弈中的学习结果历史悠久。一个经典的博弈问题就是重复囚徒困境（iterated prisoners' dilemma，IPD）。在这个博弈问题中，只追求自我利益的最大化将导致一个对于所有参与者都很差的结果，而合作则可以实现整体利益最大化（评价指标是所有智能体的奖励之和）。

文章发现，对于目前的深度强化学习算法（这里指2017年及以前的算法），在 IPD 问题及类似问题中最后收敛到的策略都是只追求自我利益最大化，即拒绝合作。这表明目前的算法即使在很简单的博弈问题中也会引导智能体朝着自私行为的方向进行学习。而造成这种现象的一个很重要的原因便是目前的算法在学习的过程中很少去考虑其余智能体的学习过程的影响，仅仅是将它们当作环境中一个静态的组成部分。

为了显式地在 social setting 中考虑其余智能体的学习行为，文章提出了 Learning with Opponent Learning Awareness (LOLA) 算法。LOLA 算法在参数更新过程中通过引入额外的修正项，来考量智能体的策略对于其余智能体学习过程的影响。文章表明，引入该项后，智能体在 IPD 及类似问题中能够产生互利及协作行为

---

具体来说，文章首先将当前的多智能体强化学习（MARL）算法建模为 **Naive Learner (NL)**，并假设 NL 可以得到智能体期望累计奖励关于策略参数的 **exact gradients** 以及 **Hessians**。假定智能体 $a$ 的策略 $\pi^a$ 被 $\theta^a$ 参数化，$V^a(\theta^1, \theta^2)$ 表示智能体 $a$ 的期望累积折扣奖励，是关于所有智能体策略参数 $(\theta^1, \theta^2)$ 的函数。NL 通过以下过程不断更新两个智能体的参数：

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i);$$
$$\theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

在强化学习的设定中，智能体没有办法得到所有参数组合下对应的期望累积折扣奖励 $\{V^1, V^2\}$。因此，文章假设智能体仅能够得到特定智能体参数组合下 $\{V^1, V^2\}$ 的函数值以及梯度。根据以上假设，NL 通过以下梯度上升规则 $f^{nl}$ 来更新策略参数：

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

其中，$\delta$ 代表更新步长。我们把 $f^{nl}_1$ 称为 **naive learning rule**。

---

LOLA 的改进点在于，它引入了一种“one step look-ahead”的机制，并基于期望累积折扣奖励对其进行优化。具体来说，LOLA 并不直接在当前智能体参数下对 $V^1(\theta^1, \theta^2)$ 进行优化，而是首先考虑一次 Naive Learning 更新后的参数变化，记为 $\Delta \theta^2$。然后在更新后的参数基础上对 $V^1(\theta^1, \theta^2 + \Delta \theta^2)$ 进行优化。为了简化计算，对函数进行泰勒展开，得出以下近似关系：

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2)$$

从中可以看出，通过优化左边项，智能体的学习行为实际上通过 $\Delta \theta^2$ 的变化对智能体 $1$ 的参数优化产生了间接影响。考虑到 $\Delta \theta^2$ 是基于 Naive Learning Rule 的结果，可以表示为：

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

将此代入上式，并结合泰勒展开的结果，最终得到 LOLA 的更新规则：

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

其中，$f^{lola}_1(\theta^1, \theta^2)$ 包含了额外的修正项，用以捕捉对手学习行为对智能体自身策略更新的影响。

---

---

LOLA 的学习规则引入了相对于 Naive Learning Rule 的改进。通过以下公式表示 LOLA 的更新规则：

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

其中，$\delta$ 和 $\eta$ 分别代表一阶更新步长与二阶更新的步长。

相比于 Naive Learning Rule，LOLA 学习规则中加入了额外的二阶修正项，用于捕捉对手学习过程对智能体自身策略更新的影响。然而，这些修正项需要计算精确的梯度和 Hessian 矩阵。研究者将这种方法记为 NL-Ex 和 LOLA-Ex。

---

由于强化学习的特性，我们无法获得真实的期望累积折扣奖励，因此需要对这些梯度和 Hessian 进行估计。论文基于策略梯度的方法提出了 LOLA 的实现方式。以每个 episode 的轨迹为 $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$，智能体 $a$ 在某一时刻的累积折扣奖励为：

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

其中，$\gamma$ 为折扣因子。

基于策略 $\pi^1, \pi^2$，期望的 episodic 累积折扣奖励可以定义为 $ER_0^1(\tau)$ 和 $ER_0^2(\tau)$。对于梯度和 Hessian，我们有以下公式：

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)],$$

通过对轨迹进行采样估计，可以进一步计算策略梯度，进而实现 LOLA 学习规则。

---

---

### Naive Learner（基于策略梯度的 NL-PG 规则）
在引入基于策略梯度的 Naive Learner（NL-PG）规则时，通过对累积折扣奖励的梯度计算，得到以下公式：

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

其中，$\delta$ 是更新步长。

---

### LOLA-PG 的改进
LOLA-PG 的修正项基于 NL-PG 的形式，引入了一阶和二阶修正项，具体推导如下：

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

基于此，LOLA 的学习规则（LOLA-PG）表达式为：

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

其中，$\delta$ 和 $\eta$ 分别是不同修正项的更新步长。

---

### Opponent Modeling（对手建模）
在 LOLA 的推导过程中，假设智能体 $a$ 能够得到其对手智能体的策略参数，但在许多现实场景下，这一假设难以满足。为了解决此问题，LOLA 提出了通过对手建模（Opponent Modeling）来估计对手策略参数的方法：

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$

对手建模通过历史数据拟合其对手的策略，从而在无需明确对手参数的情况下，仍然能够应用 LOLA 的学习规则。


# LOLA-dice
### DiCE for Multi-Agent RL 具体实现方法

DiCE 在多智能体强化学习中的应用旨在通过准确估计高阶梯度，捕捉多智能体之间的复杂相互影响，优化策略更新。以下是该方法的详细实现步骤和伪代码的逻辑解析。

---

### **目标**

1. **背景问题**：
   - 在多智能体环境中，每个智能体的策略更新不仅依赖于自身的奖励，还受到对手策略更新的影响。
   - 传统策略梯度方法（Policy Gradient）假设对手策略固定，无法捕捉动态交互的影响，尤其是高阶梯度依赖。

2. **核心改进**：
   - DiCE 利用 **MAGICBOX 运算符** 构造统一目标函数，解决随机计算图（SCG）中的依赖捕捉问题。
   - 在 LOLA（Learning with Opponent-Learning Awareness）的框架下，DiCE 提供了更准确的高阶梯度估计工具，用于指导智能体的策略优化。

---

### **方法流程**

#### **Step 1: 初始化**
- 将对手的策略参数 $\theta_2$ 初始化为 $\theta_2'$，即：
  $$\theta_2' \gets \theta_2$$
- 这一步为后续的对手策略前瞻式更新（lookahead updates）做准备。

---

#### **Step 2: 对手策略前瞻式更新（Lookahead Updates）**
前瞻式更新的目的是模拟对手的学习过程，并将对手的策略变化纳入智能体自身的优化目标中。

1. **内循环模拟更新**：
   - 进行 $K$ 次内循环，模拟对手策略的学习更新。

2. **每轮更新的具体步骤**：
   - **轨迹采样**：
     - 在当前智能体策略 $\pi_{\theta_1}$ 和对手策略 $\pi_{\theta_2'}$ 下生成轨迹 $\tau_k$：
       $$\tau_k \sim (\pi_{\theta_1}, \pi_{\theta_2'})$$
   - **对手策略更新**：
     - 基于对手的 DiCE 目标函数 $\mathcal{L}_2^{\text{DiCE}}$，计算对手的策略更新：
       $$\theta_2' \gets \theta_2' + \alpha_2 \nabla_{\theta_2'} \mathcal{L}_2^{\text{DiCE}}(\theta_1, \theta_2')$$
     - 其中：
       $$\mathcal{L}_2^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^2 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right]$$
       - $\mathcal{W}_t$：表示影响奖励 $r_t^2$ 的所有随机节点集合。
       - MAGICBOX 操作符捕捉了所有高阶依赖。

---

#### **Step 3: 外循环更新智能体策略**
完成对手的前瞻式更新后，智能体通过以下步骤更新自身策略：

1. **轨迹采样**：
   - 在智能体策略 $\pi_{\theta_1}$ 和更新后的对手策略 $\pi_{\theta_2'}$ 下生成轨迹 $\tau$：
     $$\tau \sim (\pi_{\theta_1}, \pi_{\theta_2'})$$

2. **智能体策略更新**：
   - 基于智能体的 DiCE 目标函数 $\mathcal{L}_1^{\text{DiCE}}$，更新智能体的策略参数：
     $$\theta_1 \gets \theta_1 + \alpha_1 \nabla_{\theta_1} \mathcal{L}_1^{\text{DiCE}}(\theta_1, \theta_2')$$
   - 其中：
     $$\mathcal{L}_1^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^1 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right]$$
   - 此更新显式考虑了对手策略变化对自身奖励的影响。

---
感谢提供图片。以下是有关 **DiCE（Infinitely Differentiable Monte Carlo Estimator）** 的详细内容总结，基于所提供的图片：

---

### **1. DiCE 的背景与问题**
DiCE 是一种设计用于随机计算图（Stochastic Computation Graph, SCG）的梯度估计方法。SCG 中的目标是通过梯度更新优化某个目标函数，而传统梯度估计方法在以下两个方面存在问题：
1. **缺乏通用性**：直接定义梯度往往无法作为目标函数输入到标准深度学习框架中。
2. **违背自动微分规则**：传统梯度估计器无法支持嵌套式微分（higher-order derivatives），这使得高阶梯度计算变得困难。

DiCE 引入了 MAGICBOX 操作符来解决这些问题，允许在 SCG 中自动生成任意阶梯度估计，从而兼容自动微分框架。

---

### **2. DiCE 的数学基础**

#### **DiCE 的目标函数**
DiCE 提出的统一目标函数是：
$$\mathcal{L}_\mathcal{Q} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c,$$
其中：
- $\mathcal{C}$ 是所有随机节点的集合，表示与目标函数相关的代价节点（cost nodes）。
- $\mathcal{W}_c$ 是影响代价 $c$ 的所有随机节点。
- $\text{MAGICBOX}(\mathcal{W}_c)$ 是 DiCE 中的核心操作符，用于捕捉随机依赖关系。

#### **MAGICBOX 的定义**
MAGICBOX 运算符定义如下：
$$\text{MAGICBOX}(\mathcal{W}) = \exp\left( \sum_{w \in \mathcal{W}} \log p(w; \theta) - \text{stop\_gradient}\left(\sum_{w \in \mathcal{W}} \log p(w; \theta)\right) \right),$$
其中：
- $\sum_{w \in \mathcal{W}} \log p(w; \theta)$：表示所有随机节点的概率对数之和。
- $\text{stop\_gradient}()$：防止该项参与梯度的正向传播，仅在反向传播中起作用。

#### **MAGICBOX 的性质**
1. $\text{MAGICBOX}(\mathcal{W}) = 1$：在不依赖于其他随机节点时，返回值为 1。
2. $\nabla_\theta \text{MAGICBOX}(\mathcal{W}) = \text{MAGICBOX}(\mathcal{W}) \cdot \sum_{w \in \mathcal{W}} \nabla_\theta \log p(w; \theta)$：能够自动捕捉随机依赖，并生成一阶或高阶梯度。

#### **高阶梯度的通用性**
DiCE 的目标是通过 MAGICBOX 操作符，确保 SCG 中的所有随机节点的依赖关系都被正确捕捉，无论是一阶梯度还是高阶梯度。公式推导证明了 MAGICBOX 的值可以递归生成高阶梯度估计：

1. $c^{n+1}$ 的递推公式：
   $$c^{n+1} = \nabla_\theta c^n + \sum_{w \in \mathcal{W}_n} \nabla_\theta \log p(w; \theta) \cdot c^n.$$

2. 高阶梯度的闭式表达式：
   $$\nabla_\theta c^{n}_{dice} = c^{n+1}_{dice},$$
   其中 $c^{(n)}$ 是 $c$ 的第 $n$ 阶导数。

---

### **3. DiCE 的实现细节**

#### **MAGICBOX 的实现**
MAGICBOX 可以通过以下公式实现：
$$\text{MAGICBOX}(\mathcal{W}) = \exp(\tau - \text{stop\_gradient}(\tau)),$$
其中：
- $\tau = \sum_{w \in \mathcal{W}} \log p(w; \theta)$。

#### **梯度计算的实现**
使用 MAGICBOX 操作符后，目标函数 $\mathcal{L}_\mathcal{Q}$ 的梯度和高阶梯度可以通过以下方式计算：
1. **一阶梯度**：
   $$\nabla_\theta \mathcal{L}_\mathcal{Q} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot \nabla_\theta c.$$
2. **高阶梯度**：
   通过递归公式计算：
   $$c^{n+1} = \nabla_\theta c^n + \sum_{w \in \mathcal{W}_n} \nabla_\theta \log p(w; \theta) \cdot c^n.$$

#### **深度学习框架中的实现**
MAGICBOX 运算符可以直接通过深度学习框架实现。例如：
- 在 TensorFlow 中使用 `stop_gradient()` 运算符。
- 在 PyTorch 中使用 `detach()` 方法。

---

### **4. DiCE 的优势**

1. **捕捉完整依赖关系**：
   - MAGICBOX 确保了所有随机节点的依赖关系被正确捕捉，从而消除了遗漏高阶梯度的风险。

2. **支持高阶梯度**：
   - 自动生成任意阶梯度估计，适用于复杂的多智能体交互优化。

3. **兼容性强**：
   - 与自动微分框架无缝集成，无需手动推导高阶梯度。

4. **通用性**：
   - 适用于任意 SCG，无论目标函数的复杂程度如何。

---

### **5. DiCE 在 Multi-Agent RL 中的应用**

在多智能体强化学习中，DiCE 通过 MAGICBOX 操作符构建每个智能体的目标函数，同时捕捉智能体间的高阶依赖关系，优化多智能体交互。

#### **多智能体目标函数**
每个智能体的目标函数为：
$$\mathcal{L}_i^{\text{DiCE}} = \mathbb{E}\left[\sum_{t=0}^T \gamma^t r_t^i \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right].$$
通过 MAGICBOX，捕捉所有高阶依赖关系，确保梯度估计的准确性。

---
````ad-note
### 整理后的 LOLA-DiCE 内容：清晰的逻辑与上下文关系

---

### **1. 背景与问题**

#### **1.1 多智能体环境中的核心挑战**
- **复杂的相互影响**：在多智能体强化学习（RL）中，每个智能体的策略更新不仅受自身的奖励影响，还会受到其他智能体策略变化的间接影响。
- **高阶梯度问题**：
  - 传统方法（如策略梯度 Policy Gradient）假设其他智能体的策略固定，忽略了策略间复杂的交叉依赖。
  - 在 LOLA 框架中，通过泰勒展开引入高阶梯度，但由于对高阶项的简化和手动推导，容易忽略依赖关系，影响准确性。

#### **1.2 DiCE 的引入**
- DiCE（Infinitely Differentiable Monte Carlo Estimator）通过 **MAGICBOX 操作符**，为随机计算图（SCG）提供了通用的目标函数框架。
- DiCE 能捕捉所有随机依赖，包括高阶梯度，适配自动微分框架，解决了高阶梯度估计的复杂性和准确性问题。

---

### **2. LOLA 的问题与 DiCE 的改进**

#### **2.1 LOLA 的更新公式及其问题**

LOLA 的更新公式包括两部分：
1. **常规梯度更新**：
   $$\nabla_{\theta_1} V^1(\theta_1, \theta_2),$$
   捕捉智能体 1 对自身奖励的直接影响。

2. **高阶梯度更新**：
   $$(\nabla_{\theta_2} V^1)^T \nabla_{\theta_1} \nabla_{\theta_2} V^2,$$
   用于建模对手策略更新对智能体 1 奖励的间接影响。

- **泰勒展开的简化问题**：
  在泰勒展开中，LOLA 忽略了二阶和更高阶非线性项，如：
  - $(\Delta \theta_2)^T \nabla_{\theta_2}^2 V^1$：对手策略更新的非线性影响。
  - $\nabla_{\theta_1} \Delta \theta_2$：策略间复杂的交叉反馈。

- **问题总结**：
  - 手动推导高阶梯度复杂且容易遗漏，尤其在多智能体和复杂环境中。
  - 高阶项的忽略导致梯度更新不完整，影响策略优化的准确性。

---

#### **2.2 DiCE 的改进**

DiCE 通过 **MAGICBOX 运算符** 自动捕捉高阶梯度，优化了 LOLA 中的更新过程。

1. **提供完整目标函数**：
   - DiCE 的目标函数为：
     $$\mathcal{L}_\mathcal{Q} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c,$$
     其中 MAGICBOX 自动捕捉所有依赖关系，适配自动微分框架。

2. **捕捉高阶梯度**：
   - MAGICBOX 运算符定义：
     $$\text{MAGICBOX}(\mathcal{W}) = \exp\left( \sum_{w \in \mathcal{W}} \log p(w; \theta) - \text{stop\_gradient}(\sum_{w \in \mathcal{W}} \log p(w; \theta)) \right),$$
     - 自动捕捉随机节点的高阶依赖，无需手动推导。
     - 在多智能体场景中，能够准确捕捉对手策略变化对自身策略的间接影响。

3. **提高计算效率**：
   - MAGICBOX 将随机节点的依赖关系结构化表达，避免冗余计算，提高高阶梯度的计算效率。

---

### **3. LOLA-DiCE 的具体实现方法**

#### **3.1 方法目标**
- 在 LOLA 框架中引入 DiCE，通过高阶梯度捕捉对手策略的动态变化，改进策略优化。
- 使用 DiCE 的目标函数解决 LOLA 中手动推导高阶梯度的复杂性问题。

#### **3.2 方法流程**

##### **Step 1: 初始化**
- 对手策略参数初始化：
  $$\theta_2' \gets \theta_2,$$
  为后续的对手策略前瞻式更新做准备。

---

##### **Step 2: 对手策略前瞻式更新（Lookahead Updates）**
模拟对手的学习过程，将对手策略的变化纳入自身策略优化中。

1. **轨迹采样**：
   在当前智能体策略 $\pi_{\theta_1}$ 和对手策略 $\pi_{\theta_2'}$ 下采样轨迹：
   $$\tau_k \sim (\pi_{\theta_1}, \pi_{\theta_2'}).$$

2. **对手策略更新**：
   使用对手的 DiCE 目标函数 $\mathcal{L}_2^{\text{DiCE}}$，更新对手策略：
   $$\theta_2' \gets \theta_2' + \alpha_2 \nabla_{\theta_2'} \mathcal{L}_2^{\text{DiCE}}(\theta_1, \theta_2').$$
   其中：
   $$\mathcal{L}_2^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^2 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right].$$

---

##### **Step 3: 智能体策略更新**
完成对手前瞻式更新后，更新智能体的策略。

1. **轨迹采样**：
   在智能体策略 $\pi_{\theta_1}$ 和更新后的对手策略 $\pi_{\theta_2'}$ 下采样轨迹：
   $$\tau \sim (\pi_{\theta_1}, \pi_{\theta_2'}).$$

2. **策略更新**：
   使用智能体的 DiCE 目标函数 $\mathcal{L}_1^{\text{DiCE}}$，更新策略参数：
   $$\theta_1 \gets \theta_1 + \alpha_1 \nabla_{\theta_1} \mathcal{L}_1^{\text{DiCE}}(\theta_1, \theta_2').$$
   其中：
   $$\mathcal{L}_1^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^1 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right].$$

---

### **4. 总结：LOLA-DiCE 的优势**

1. **解决高阶梯度捕捉问题**：
   - DiCE 自动捕捉随机计算图中的所有依赖，弥补了 LOLA 中泰勒展开忽略高阶项的问题。

2. **简化实现**：
   - MAGICBOX 运算符直接适配深度学习框架，无需手动推导高阶梯度公式。

3. **提高效率与准确性**：
   - 通过结构化依赖捕捉减少冗余计算，确保策略更新的准确性。

4. **通用性**：
   - DiCE 适用于任意多智能体环境，无需为不同任务单独调整，显著提升 LOLA 的扩展性和通用性。
````




# MFOS

### 背景

#### **LOLA（Learning with Opponent-Learning Awareness）**
- **核心动机**：LOLA 的动机是引入 **心智理论**，通过理解对手行为来最大化全体的奖励。
- **假设**：LOLA 假设对手是 **NL（Naive Learner）**，即对手只关注自身的最大收益，将对手视为环境的一部分。
- **更新方式**：
  - NL 的更新公式：
    $$\phi_{t+1}^i = \phi_t^i + \alpha \nabla_{\phi_t^i} R^i(\phi_t^i, \phi_t^{-i}),$$
    其中 $\phi_t^i$ 为第 $i$ 个智能体的策略参数。
  - LOLA 假设对手使用 NL 更新方式，在此基础上通过一阶导数更新智能体参数：
    $$\phi_{t+1}^i = \phi_t^i + \alpha \nabla_{\phi_t^i} R^i(\phi_t^i, \phi_t^{-i} + \Delta \phi_t^{-i}),$$
    其中：
    $$\Delta \phi_t^{-i} = \alpha \nabla_{\phi_t^{-i}} R^{-i}(\phi_t^i, \phi_t^{-i}).$$

---

- **进一步扩展**：在假设对手是 LOLA 的前提下，可以通过更高阶的 LOLA 更新方式，但计算复杂度随之增加。
- **实验观察**：
  - 在 **IPD（Iterated Prisoner’s Dilemma）** 中，LOLA 发现了 **TFT（Tit-For-Tat）** 策略，即重复对手的动作，这是 IPD 中收益极高的合作策略。

---

#### **LOLA 的不足**
1. **短视**：只关注对手的下一步更新，忽略长时间尺度的策略影响。
2. **非对称性**：两个 LOLA 智能体相互假设对手是 NL，会带来次优解。
3. **高阶计算复杂度**：需要对对手底层更新方法的依赖，不能无模型化。

---

#### **过渡到 F-MOS**
针对上述缺陷，提出了 **F-MOS（Model-Free Opponent Shaping）**：
- 通过长时间的无模型优化解决短视问题。
- 摆脱对高阶梯度计算的依赖，采用元学习算法来实现更新策略的塑造。



### 算法架构与流程

#### 核心思想
M-FOS 的核心思想是通过构造一个元游戏 $M_n$，学习一个元策略 $\pi_\theta$。元游戏的形式为 POMDP，其中：
- **状态 $s_t$**：由全部智能体的策略参数 $(\phi_t^i, \phi_t^{-i})$ 构成。
- **动作 $a_t$**：通过策略 $\pi_\theta(\cdot \mid o_t)$ 采样，表示策略更新方向。
- **状态转移函数** $P(\cdot \mid s_t, a_t)$：决定下一状态 $s_{t+1}$。

一轮元游戏的累积回报为：
$$\bar{r}_t = \sum_{k=0}^K r_k^i(\phi_t^i, \phi_t^{-i}),$$
期望回报 $J = \sum_{t=0}^T r_t^i(\phi_t^i, \phi_t^{-i})$ 是参数 $\theta$ 的优化目标。

---

#### 流程
通过元游戏得到元策略 $\pi_\theta$ 后，计算自身策略的流程如下：
1. **优化元策略**：
   $$\max_\theta \mathbb{E}_{p(\phi_t^i, \phi_t^{-i})} \left[\sum_{t=0}^T R^i(\phi_t^i, \phi_t^{-i})\right].$$

2. **策略更新**：
   - 智能体 $i$：
     $$\phi_{t+1}^i \sim \pi_\theta(\cdot \mid \phi_t^i, \phi_t^{-i}).$$
   - 对手智能体：
     $$\phi_{t+1}^{-i} = f(\phi_t^i, \phi_t^{-i}),$$
     其中 $f(\cdot)$ 是对手的更新规则。

---

#### 优势
1. **避免自博弈矛盾**：
   - M-FOS 不依赖对手底层更新方式的假设，因此避免了自博弈过程中可能出现的矛盾。

2. **非零和博弈中的平衡**：
   - 提供一种从 NL 到 M-FOS 的渐进调整方式，通过逐渐接近自博弈中的均衡，保证智能体对分布的均匀响应接近最优。


### M-MAML（Multiagent Model-Agnostic Meta-Learning）

#### **核心思想**
M-MAML 是一种多智能体元学习算法，采用类似元策略的方法，但其目标是优化初始参数以加速智能体的学习。通过调整初始策略参数，使得智能体在接下来的学习中能够快速适应环境。

---

#### **流程**

1. **优化目标**：
   - 最大化初始参数的期望累积回报：
     $$\max_{\phi_0^i} \mathbb{E}_{p(\phi_0^i)} \left[\sum_{t=0}^T R^i(\phi_t^i, \phi_t^{-i})\right],$$
     其中 $R^i$ 为智能体 $i$ 在时间 $t$ 的即时奖励。

2. **智能体更新**：
   - 智能体 $i$ 的参数更新：
     $$\phi_{t+1}^i = \phi_t^i + \alpha^i \nabla_{\phi_t^i} R^i(\phi_t^i, \phi_t^{-i}),$$
     其中 $\alpha^i$ 是学习率。
   - 对手智能体的参数更新：
     $$\phi_{t+1}^{-i} = \phi_t^{-i} + \alpha^{-i} \nabla_{\phi_t^{-i}} R^{-i}(\phi_t^i, \phi_t^{-i}).$$

---

#### **区别于 M-FOS**
- **目标不同**：
  - M-FOS 直接优化策略参数以塑造对手行为。
  - M-MAML 优化初始参数，使智能体在后续学习中快速适应。
- **更新方式**：
  - M-FOS 通过元策略决定每一步更新。
  - M-MAML 仅在初始参数上进行优化。