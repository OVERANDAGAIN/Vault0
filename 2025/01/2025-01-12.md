---
创建时间: 2025-一月-12日  星期日, 3:48:54 下午
---
lola:
在一群持续学习的、追求自我利益最大化的智能体中如何使其涌现互利行为一直是一个难题。博弈论研究在包含协作与竞争因素的博弈中的学习结果历史悠久。一个经典的博弈问题就是重复囚徒困境（iterated prisoners' dilemma，IPD）。在这个博弈问题中，只追求自我利益的最大化将导致一个对于所有参与者都很差的结果，而合作则可以实现整体利益最大化（评价指标是所有智能体的奖励之和）。

文章发现，对于目前的深度强化学习算法（这里指2017年及以前的算法），在 IPD 问题及类似问题中最后收敛到的策略都是只追求自我利益最大化，即拒绝合作。这表明目前的算法即使在很简单的博弈问题中也会引导智能体朝着自私行为的方向进行学习。而造成这种现象的一个很重要的原因便是目前的算法在学习的过程中很少去考虑其余智能体的学习过程的影响，仅仅是将它们当作环境中一个静态的组成部分。

为了显式地在 social setting 中考虑其余智能体的学习行为，文章提出了 Learning with Opponent Learning Awareness (LOLA) 算法。LOLA 算法在参数更新过程中通过引入额外的修正项，来考量智能体的策略对于其余智能体学习过程的影响。文章表明，引入该项后，智能体在 IPD 及类似问题中能够产生互利及协作行为

---

具体来说，文章首先将当前的多智能体强化学习（MARL）算法建模为 **Naive Learner (NL)**，并假设 NL 可以得到智能体期望累计奖励关于策略参数的 **exact gradients** 以及 **Hessians**。假定智能体 $a$ 的策略 $\pi^a$ 被 $\theta^a$ 参数化，$V^a(\theta^1, \theta^2)$ 表示智能体 $a$ 的期望累积折扣奖励，是关于所有智能体策略参数 $(\theta^1, \theta^2)$ 的函数。NL 通过以下过程不断更新两个智能体的参数：

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i);$$
$$\theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

在强化学习的设定中，智能体没有办法得到所有参数组合下对应的期望累积折扣奖励 $\{V^1, V^2\}$。因此，文章假设智能体仅能够得到特定智能体参数组合下 $\{V^1, V^2\}$ 的函数值以及梯度。根据以上假设，NL 通过以下梯度上升规则 $f^{nl}$ 来更新策略参数：

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

其中，$\delta$ 代表更新步长。我们把 $f^{nl}_1$ 称为 **naive learning rule**。

---

LOLA 的改进点在于，它引入了一种“one step look-ahead”的机制，并基于期望累积折扣奖励对其进行优化。具体来说，LOLA 并不直接在当前智能体参数下对 $V^1(\theta^1, \theta^2)$ 进行优化，而是首先考虑一次 Naive Learning 更新后的参数变化，记为 $\Delta \theta^2$。然后在更新后的参数基础上对 $V^1(\theta^1, \theta^2 + \Delta \theta^2)$ 进行优化。为了简化计算，对函数进行泰勒展开，得出以下近似关系：

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2)$$

从中可以看出，通过优化左边项，智能体的学习行为实际上通过 $\Delta \theta^2$ 的变化对智能体 $1$ 的参数优化产生了间接影响。考虑到 $\Delta \theta^2$ 是基于 Naive Learning Rule 的结果，可以表示为：

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

将此代入上式，并结合泰勒展开的结果，最终得到 LOLA 的更新规则：

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

其中，$f^{lola}_1(\theta^1, \theta^2)$ 包含了额外的修正项，用以捕捉对手学习行为对智能体自身策略更新的影响。

---

---

LOLA 的学习规则引入了相对于 Naive Learning Rule 的改进。通过以下公式表示 LOLA 的更新规则：

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

其中，$\delta$ 和 $\eta$ 分别代表一阶更新步长与二阶更新的步长。

相比于 Naive Learning Rule，LOLA 学习规则中加入了额外的二阶修正项，用于捕捉对手学习过程对智能体自身策略更新的影响。然而，这些修正项需要计算精确的梯度和 Hessian 矩阵。研究者将这种方法记为 NL-Ex 和 LOLA-Ex。

---

由于强化学习的特性，我们无法获得真实的期望累积折扣奖励，因此需要对这些梯度和 Hessian 进行估计。论文基于策略梯度的方法提出了 LOLA 的实现方式。以每个 episode 的轨迹为 $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$，智能体 $a$ 在某一时刻的累积折扣奖励为：

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

其中，$\gamma$ 为折扣因子。

基于策略 $\pi^1, \pi^2$，期望的 episodic 累积折扣奖励可以定义为 $ER_0^1(\tau)$ 和 $ER_0^2(\tau)$。对于梯度和 Hessian，我们有以下公式：

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)],$$

通过对轨迹进行采样估计，可以进一步计算策略梯度，进而实现 LOLA 学习规则。

---

---

### Naive Learner（基于策略梯度的 NL-PG 规则）
在引入基于策略梯度的 Naive Learner（NL-PG）规则时，通过对累积折扣奖励的梯度计算，得到以下公式：

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

其中，$\delta$ 是更新步长。

---

### LOLA-PG 的改进
LOLA-PG 的修正项基于 NL-PG 的形式，引入了一阶和二阶修正项，具体推导如下：

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

基于此，LOLA 的学习规则（LOLA-PG）表达式为：

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

其中，$\delta$ 和 $\eta$ 分别是不同修正项的更新步长。

---

### Opponent Modeling（对手建模）
在 LOLA 的推导过程中，假设智能体 $a$ 能够得到其对手智能体的策略参数，但在许多现实场景下，这一假设难以满足。为了解决此问题，LOLA 提出了通过对手建模（Opponent Modeling）来估计对手策略参数的方法：

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$

对手建模通过历史数据拟合其对手的策略，从而在无需明确对手参数的情况下，仍然能够应用 LOLA 的学习规则。


# LOLA-dice
### DiCE for Multi-Agent RL 具体实现方法

DiCE 在多智能体强化学习中的应用旨在通过准确估计高阶梯度，捕捉多智能体之间的复杂相互影响，优化策略更新。以下是该方法的详细实现步骤和伪代码的逻辑解析。

---

### **目标**

1. **背景问题**：
   - 在多智能体环境中，每个智能体的策略更新不仅依赖于自身的奖励，还受到对手策略更新的影响。
   - 传统策略梯度方法（Policy Gradient）假设对手策略固定，无法捕捉动态交互的影响，尤其是高阶梯度依赖。

2. **核心改进**：
   - DiCE 利用 **MAGICBOX 运算符** 构造统一目标函数，解决随机计算图（SCG）中的依赖捕捉问题。
   - 在 LOLA（Learning with Opponent-Learning Awareness）的框架下，DiCE 提供了更准确的高阶梯度估计工具，用于指导智能体的策略优化。

---

### **方法流程**

#### **Step 1: 初始化**
- 将对手的策略参数 $\theta_2$ 初始化为 $\theta_2'$，即：
  $$\theta_2' \gets \theta_2$$
- 这一步为后续的对手策略前瞻式更新（lookahead updates）做准备。

---

#### **Step 2: 对手策略前瞻式更新（Lookahead Updates）**
前瞻式更新的目的是模拟对手的学习过程，并将对手的策略变化纳入智能体自身的优化目标中。

1. **内循环模拟更新**：
   - 进行 $K$ 次内循环，模拟对手策略的学习更新。

2. **每轮更新的具体步骤**：
   - **轨迹采样**：
     - 在当前智能体策略 $\pi_{\theta_1}$ 和对手策略 $\pi_{\theta_2'}$ 下生成轨迹 $\tau_k$：
       $$\tau_k \sim (\pi_{\theta_1}, \pi_{\theta_2'})$$
   - **对手策略更新**：
     - 基于对手的 DiCE 目标函数 $\mathcal{L}_2^{\text{DiCE}}$，计算对手的策略更新：
       $$\theta_2' \gets \theta_2' + \alpha_2 \nabla_{\theta_2'} \mathcal{L}_2^{\text{DiCE}}(\theta_1, \theta_2')$$
     - 其中：
       $$\mathcal{L}_2^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^2 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right]$$
       - $\mathcal{W}_t$：表示影响奖励 $r_t^2$ 的所有随机节点集合。
       - MAGICBOX 操作符捕捉了所有高阶依赖。

---

#### **Step 3: 外循环更新智能体策略**
完成对手的前瞻式更新后，智能体通过以下步骤更新自身策略：

1. **轨迹采样**：
   - 在智能体策略 $\pi_{\theta_1}$ 和更新后的对手策略 $\pi_{\theta_2'}$ 下生成轨迹 $\tau$：
     $$\tau \sim (\pi_{\theta_1}, \pi_{\theta_2'})$$

2. **智能体策略更新**：
   - 基于智能体的 DiCE 目标函数 $\mathcal{L}_1^{\text{DiCE}}$，更新智能体的策略参数：
     $$\theta_1 \gets \theta_1 + \alpha_1 \nabla_{\theta_1} \mathcal{L}_1^{\text{DiCE}}(\theta_1, \theta_2')$$
   - 其中：
     $$\mathcal{L}_1^{\text{DiCE}} = \mathbb{E}\left[\sum_t \gamma^t r_t^1 \prod_{w \in \mathcal{W}_t} \text{MAGICBOX}(w)\right]$$
   - 此更新显式考虑了对手策略变化对自身奖励的影响。

---
