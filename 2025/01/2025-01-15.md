conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia



以下是图片中关于两个 Q 函数的叙述内容的文字描述：

---

1. **Independent Q-learning (公式 2)：**
   > If an agent treats other agents as part of the environment and ignores the non-stationarity posed by the change of other agents' policies as independent Q-learning [43, 44]. Its Q-function $Q(s_t, a_t)$ is updated by:
   >
   > $$> Q(s_t, a_t) = \mathbb{E}_{\mathcal{P}(s_{t+1} | s_t, a_t^{-i}, a_i)} \left[ r + \gamma \max_a Q(s_{t+1}, a) \right].
   >$$

   在这种情况下，智能体将其他智能体的策略变化视为环境的一部分，完全忽略了由此引起的非平稳性问题。

2. **Opponent Modeling (公式 3)：**
   > Opponent modeling typically predicts the actions of other agents to address the non-stationary problem. The opponent model uses historical trajectory as input to predict $\tilde{a}^{-i} \sim \pi(\cdot | \tau)$, where $\tilde{a}^{-i}$ is the estimate of $a^{-i}$. Then, its Q-function is updated as:
   >
   > $$> Q(s_t, \tilde{a}^{-i}, a_t) = \mathbb{E}_{\mathcal{P}(s_{t+1} | s_t, \tilde{a}^{-i}, a_t)} \left[ r + \gamma \max_a Q(s_{t+1}, a) \right].
   >$$

   在这种情况下，通过对历史轨迹进行建模，智能体能够预测其他智能体的行为策略（$\tilde{a}^{-i}$），从而解决非平稳性问题。

以下是图片中关于两个 Q 函数的叙述内容转为文字的描述：

---

**Independent Q-learning (公式 2)：**

If an agent treats other agents as part of the environment and ignores the non-stationarity posed by the change of other agents' policies as independent Q-learning [43, 44]. Its Q-function $Q(s_t, a_t)$ is updated by:

$$Q(s_t, a_t) = \mathbb{E}_{\mathcal{P}(s_{t+1} | s_t, a_t^{-i}, a_i)} \left[ r + \gamma \max_a Q(s_{t+1}, a) \right].$$

在这种情况下，智能体将其他智能体的策略变化视为环境的一部分，完全忽略了由此引起的非平稳性问题。

---

**Opponent Modeling (公式 3)：**

Opponent modeling typically predicts the actions of other agents to address the non-stationary problem. The opponent model uses historical trajectory as input to predict $\tilde{a}^{-i} \sim \pi(\cdot | \tau)$, where $\tilde{a}^{-i}$ is the estimate of $a^{-i}$. Then, its Q-function is updated as:

$$Q(s_t, \tilde{a}^{-i}, a_t) = \mathbb{E}_{\mathcal{P}(s_{t+1} | s_t, \tilde{a}^{-i}, a_t)} \left[ r + \gamma \max_a Q(s_{t+1}, a) \right].$$

在这种情况下，通过对历史轨迹进行建模，智能体能够预测其他智能体的行为策略（$\tilde{a}^{-i}$），从而解决非平稳性问题。

---

**总结：**
公式 (2) 表示传统的 stationary Q-learning，即假设其他智能体的策略固定不变。  
公式 (3) 表示通过 opponent modeling 解决 non-stationarity 的 Q 函数。