---
创建时间: 2025-一月-9日  星期四, 9:44:23 上午
---
2025-1-13 周一组会分享：


lola相关算法
mfos
opponent **shaping** ： shaps opponent's actions in the future.



[Learning to Incentivize Others](https://zhuanlan.zhihu.com/p/150688960)

1. “Learning with Opponent-Learning Awareness (LOLA)”2018
   >as a method for controlling the learning dynamics of opponents in a game.
   >A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes $V^1(\theta^1, \theta^2 + \Delta\theta^2)$ where $\Delta\theta^2$ is a naive learning step in the direction that maximizes the opponent’s value function $V^2(\theta^1, \theta^2)$. 
   
2. Stable Opponent Shaping (SOS) (2018)
Variations of LOLA have been introduced to have formal stability guarantees
Letcher, A., Foerster, J., Balduzzi, D., Rockta ̈schel, T., and Whiteson, S. (2021). Stable opponent shaping in differentiable games.

3. Consistent Opponent-Learning Awareness (COLA) (2022)

Willi, T., Letcher, A., Treutlein, J., and Foerster, J. (2022). Cola: Consistent learning with opponent- learning awareness.

4. More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). 

Aghajohari, M., Cooijmans, T., Duque, J. A., Akatsuka, S., and Courville, A. (2024a). Best response shaping.

5. LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.
Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. (2024b). Loqa: Learning with opponent q-learning awareness.


4. Model-Free Opponent Shaping (MFOS) (2022)
   
   Lu, C., Willi, T., de Witt, C. S., and Foerster, J. (2022). Model-free opponent shaping.

5. Meta-value learning: a general framework for learning with learning awareness.
Cooijmans, T., Aghajohari, M., and Courville, A. (2023). Meta-value learning: a general framework for learning with learning awareness.
More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. 




7. Scaling Opponent Shaping to High Dimensional Games (Shaper) (2024)
   
Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information

  8.  A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021.
   
1. Advantage Alignment Algorithms (2024a)
LOQA: “This new approach to opponent shaping offers significant computational advantages over previous methods and lays the foundation for our work.” 


9. COALA-PG (2023)

---
---

# LOLA
### Converted to English (Condensed for PPT)

The paper models current MARL algorithms as **Naive Learners (NL)**
assuming they can compute **exact gradients** and **Hessians**
$V^a(\theta^1, \theta^2)$ represents the expected cumulative reward based on parameters $(\theta^1, \theta^2)$. 
NL updates parameters iteratively:

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i), \quad \theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

In practice, agents lack access to rewards for all parameter combinations. Instead, NL assumes availability of gradients and updates parameters using:

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

where $\delta$ is the step size. This is referred to as the **naive learning rule**.

---

LOLA improves upon NL by introducing a **one-step look-ahead** mechanism
Instead of directly optimizing $V^1(\theta^1, \theta^2)$, LOLA optimizes $V^1(\theta^1, \theta^2 + \Delta \theta^2)$, approximated via a Taylor expansion:

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2).$$

Here, $\Delta \theta^2$ is derived from the Naive Learning rule as:

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

where $\eta$ is the step size. Combining these, the LOLA update rule becomes:

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates, respectively. 
These corrections require precise calculations of gradients and Hessians, referred to as NL-Ex and LOLA-Ex.

---

In reinforcement learning, exact cumulative rewards are typically unavailable. 
Therefore, these gradients and Hessians are estimated using policy gradient methods. 
For each episode trajectory $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$, the cumulative reward at time $t$ for agent $a$ is:

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

The expected episodic rewards for policies $\pi^1$ and $\pi^2$ are defined as $ER_0^1(\tau)$ and $ER_0^2(\tau)$, with the gradient calculated as:

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)].$$

By sampling trajectories, these gradients can be estimated to implement the LOLA learning rule.


#### Naive Learner (Policy Gradient-based NL-PG Rule)  
The Naive Learner (NL-PG) uses policy gradient to update parameters based on the cumulative reward gradient:

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

where $\delta$ is the step size.

---

#### LOLA-PG Improvements  
LOLA-PG improves NL-PG by introducing first- and second-order correction terms. The second-order term is derived as:

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

The LOLA-PG update rule is given by:

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates.

---

#### Opponent Modeling  
In LOLA, it is often assumed that agent $a$ knows its opponent's policy parameters. 
To address cases where this assumption is unrealistic, LOLA proposes **Opponent Modeling** to estimate the opponent’s parameters:

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$



---

# LOLA-DICE

### DiCE 方法的详细步骤与思想解析

根据图片内容，DiCE 提出的核心思想与步骤可以分为以下几个方面进行详细说明：

---

### **1. 问题背景**
在随机计算图（Stochastic Computation Graph, SCG）中，目标是优化代价节点（Cost Nodes）关于随机节点的参数。对于高阶梯度估计，传统方法存在以下两大缺陷：
1. **无法在标准深度学习库中使用**：传统方法直接定义梯度目标，但无法构造统一的目标函数，导致与自动微分框架（auto-diff）不兼容。
2. **违反自动微分范式**：高阶梯度需要多次重复微分，直接定义梯度目标导致推导复杂且容易遗漏。

---

### **2. DiCE 方法的提出**
为了克服上述缺点，DiCE 提出了一个统一的框架，核心是构造一个可以支持任意阶梯度计算的目标函数。主要技术包括：
- **MAGICBOX 操作符**：核心工具，用于捕捉随机节点的依赖关系并正确生成梯度。
- **统一目标函数**：通过 MAGICBOX，将随机目标转化为一个可微分的目标函数，确保高阶梯度计算的正确性和高效性。

---

### **3. 具体步骤**

#### **Step 1: 构造目标函数**
目标函数 $\mathcal{L}_{\text{DiCE}}$ 的表达式为：
$$\mathcal{L}_{\text{DiCE}} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c,$$
其中：
- $\mathcal{C}$：所有代价节点的集合。
- $\mathcal{W}_c$：随机计算图中影响代价节点 $c$ 的随机节点集合。
- **MAGICBOX** 操作符定义如下：
  $$\text{MAGICBOX}(\mathcal{W}) = \exp\left(\sum_{w \in \mathcal{W}} \log p(w; \theta) - \text{stop\_gradient}\left(\sum_{w \in \mathcal{W}} \log p(w; \theta)\right)\right),$$
  其中：
  - $p(w; \theta)$：随机变量 $w$ 的概率分布；
  - $\text{stop\_gradient}$：防止正向传播影响 MAGICBOX 的值，但在反向传播中捕捉随机变量对目标的依赖。

#### **Step 2: 验证梯度正确性**
DiCE 通过理论证明，其目标函数能够正确生成所有高阶梯度：
1. **一阶梯度**：
   $$\nabla_{\theta} \mathcal{L}_{\text{DiCE}} = \mathbb{E}_{p(x; \theta)} \left[ \nabla_{\theta} \log p(x; \theta) \cdot c \right],$$
   与策略梯度方法一致。
2. **高阶梯度**：
   $$\nabla_{\theta_1} \nabla_{\theta_2} \mathcal{L}_{\text{DiCE}} = \text{MAGICBOX} \cdot \nabla_{\theta_1} \nabla_{\theta_2} c + \text{cross terms},$$
   通过 MAGICBOX 自动捕捉交叉项，确保高阶梯度的完整性。

#### **Step 3: 减少梯度估计的方差**
引入基线（Baseline）项以降低梯度估计的方差：
$$\mathcal{L}_{\text{DiCE}} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c + \sum_{w \in \mathcal{S}} \left(1 - \text{MAGICBOX}(\{w\})\right) b_w,$$
其中基线 $b_w$ 不影响梯度估计的偏差，但可以有效降低方差。

#### **Step 4: 高效实现高阶梯度**
在实际实现中，DiCE 直接利用自动微分工具，通过 MAGICBOX 操作符生成所有高阶梯度，无需手动推导复杂公式。例如：
- **高阶梯度生成公式**：
  $$c^{n+1} = \nabla_{\theta} c^n + \sum_{w \in \mathcal{W}} \nabla_{\theta} \log p(w; \theta),$$
  其中 $c^{n+1}$ 表示第 $n+1$ 阶梯度。

---

### **4. DiCE 在强化学习中的应用**
在强化学习中，DiCE 被用来优化策略梯度的高阶梯度估计。具体应用如下：
- **代价节点（Cost Nodes）**：奖励 $r_t$ 作为代价节点。
- **随机节点（Stochastic Nodes）**：策略参数化的动作 $a_t \sim \pi(a_t | s_t; \theta)$。
- **DiCE 构造的目标函数**：
  $$\mathcal{L}_{\text{DiCE}} = \sum_t \text{MAGICBOX}(\mathcal{W}_t) \cdot r_t,$$
  其中 $\mathcal{W}_t$ 包括所有影响 $r_t$ 的随机节点。

通过自动微分工具，DiCE 可高效生成策略梯度和高阶梯度，适用于复杂强化学习任务。
