---
创建时间: 2025-一月-9日  星期四, 9:44:23 上午
---
2025-1-13 周一组会分享：


lola相关算法
mfos
opponent **shaping** ： shaps opponent's actions in the future.



[Learning to Incentivize Others](https://zhuanlan.zhihu.com/p/150688960)

1. “Learning with Opponent-Learning Awareness (LOLA)”2018
   >as a method for controlling the learning dynamics of opponents in a game.
   >A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes $V^1(\theta^1, \theta^2 + \Delta\theta^2)$ where $\Delta\theta^2$ is a naive learning step in the direction that maximizes the opponent’s value function $V^2(\theta^1, \theta^2)$. 
   
2. Stable Opponent Shaping (SOS) (2018)
Variations of LOLA have been introduced to have formal stability guarantees
Letcher, A., Foerster, J., Balduzzi, D., Rockta ̈schel, T., and Whiteson, S. (2021). Stable opponent shaping in differentiable games.

3. Consistent Opponent-Learning Awareness (COLA) (2022)

Willi, T., Letcher, A., Treutlein, J., and Foerster, J. (2022). Cola: Consistent learning with opponent- learning awareness.

4. More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). 

Aghajohari, M., Cooijmans, T., Duque, J. A., Akatsuka, S., and Courville, A. (2024a). Best response shaping.

5. LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.
Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. (2024b). Loqa: Learning with opponent q-learning awareness.


4. Model-Free Opponent Shaping (MFOS) (2022)
   
   Lu, C., Willi, T., de Witt, C. S., and Foerster, J. (2022). Model-free opponent shaping.

5. Meta-value learning: a general framework for learning with learning awareness.
Cooijmans, T., Aghajohari, M., and Courville, A. (2023). Meta-value learning: a general framework for learning with learning awareness.
More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. 




7. Scaling Opponent Shaping to High Dimensional Games (Shaper) (2024)
   
Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information

  8.  A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021.
   
1. Advantage Alignment Algorithms (2024a)
LOQA: “This new approach to opponent shaping offers significant computational advantages over previous methods and lays the foundation for our work.” 


9. COALA-PG (2023)

---
---

The paper first models the current MARL algorithm as **Naive Learner (NL)**, assuming NL can obtain the **exact gradients** and **Hessians** of expected cumulative rewards with respect to strategy parameters. 
For agent $a$, whose policy $\pi^a$ is parameterized by $\theta^a$, 
$V^a(\theta^1, \theta^2)$ represents the cumulative discounted rewards, which is a function of all agents' parameters $(\theta^1, \theta^2)$. 
NL updates the agents' parameters iteratively:

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i), \quad \theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

However, in reinforcement learning, agents cannot access the cumulative rewards for all parameter combinations $\{V^1, V^2\}$. Instead, they can only obtain function values and gradients for specific parameter combinations. Based on this, NL updates the parameters using the gradient ascent rule $f^{nl}$:

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

where $\delta$ is the learning rate. The $f^{nl}_1$ term is referred to as the **naive learning rule**.

---

LOLA improves upon NL by introducing a "one step look-ahead" mechanism to optimize expected cumulative rewards. Instead of directly optimizing $V^1(\theta^1, \theta^2)$ at the current parameters, LOLA considers the effect of an NL update on $\theta^2$, denoted as $\Delta \theta^2$, and optimizes $V^1(\theta^1, \theta^2 + \Delta \theta^2)$. Using a Taylor expansion, this can be approximated as:

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2).$$

The term $\Delta \theta^2$, derived from the Naive Learning Rule, is expressed as:

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

where $\eta$ is the step size. Substituting this back into the Taylor expansion, LOLA's parameter update rule becomes:

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

where $f^{lola}_1$ introduces additional correction terms that account for the influence of the opponent's learning on the agent's own updates.

--- 
Certainly, here’s the continuation with more details translated and adapted for your PPT:

---

The LOLA learning rule introduces a significant improvement over the naive approach by incorporating second-order correction terms. The complete update rule is:

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are the step sizes for first- and second-order updates, respectively. These additional correction terms explicitly account for the impact of an opponent's learning behavior on the agent's policy optimization.

In practice, calculating exact gradients and Hessians required in the above formula is often infeasible. To address this, the paper uses **policy gradient methods** to estimate these terms. For an episode trajectory $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$, the cumulative discounted reward for agent $a$ is defined as:

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

where $\gamma$ is the discount factor. The expected episodic cumulative reward for agents' policies $\pi^1$ and $\pi^2$ is denoted as $ER^1_0(\tau)$ and $ER^2_0(\tau)$, and the gradients are computed as:

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)].$$

Using this gradient, the LOLA update rule can be effectively applied by estimating the required terms through trajectory sampling.

---

In many real-world scenarios, agents do not have direct access to their opponents' policy parameters. To overcome this limitation, the paper introduces **Opponent Modeling** to infer the opponent's strategy from observed data. This is achieved by maximizing the log-likelihood of the opponent's actions:

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$

This approach allows LOLA to adapt to environments where opponent policies are unknown while still leveraging the look-ahead optimization mechanism.

---

Let me know if you'd like this further refined or expanded for additional clarity or emphasis!





