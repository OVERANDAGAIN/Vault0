---
创建时间: 2025-一月-9日  星期四, 9:44:23 上午
---
2025-1-13 周一组会分享：


lola相关算法
mfos
opponent **shaping** ： shaps opponent's actions in the future.



[Learning to Incentivize Others](https://zhuanlan.zhihu.com/p/150688960)

1. “Learning with Opponent-Learning Awareness (LOLA)”2018
   >as a method for controlling the learning dynamics of opponents in a game.
   >A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes $V^1(\theta^1, \theta^2 + \Delta\theta^2)$ where $\Delta\theta^2$ is a naive learning step in the direction that maximizes the opponent’s value function $V^2(\theta^1, \theta^2)$. 
   
2. Stable Opponent Shaping (SOS) (2018)
Variations of LOLA have been introduced to have formal stability guarantees
Letcher, A., Foerster, J., Balduzzi, D., Rockta ̈schel, T., and Whiteson, S. (2021). Stable opponent shaping in differentiable games.

3. Consistent Opponent-Learning Awareness (COLA) (2022)

Willi, T., Letcher, A., Treutlein, J., and Foerster, J. (2022). Cola: Consistent learning with opponent- learning awareness.

4. More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). 

Aghajohari, M., Cooijmans, T., Duque, J. A., Akatsuka, S., and Courville, A. (2024a). Best response shaping.

5. LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.
Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. (2024b). Loqa: Learning with opponent q-learning awareness.


4. Model-Free Opponent Shaping (MFOS) (2022)
   
   Lu, C., Willi, T., de Witt, C. S., and Foerster, J. (2022). Model-free opponent shaping.

5. Meta-value learning: a general framework for learning with learning awareness.
Cooijmans, T., Aghajohari, M., and Courville, A. (2023). Meta-value learning: a general framework for learning with learning awareness.
More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. 




7. Scaling Opponent Shaping to High Dimensional Games (Shaper) (2024)
   
Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information

  8.  A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021.
   
1. Advantage Alignment Algorithms (2024a)
LOQA: “This new approach to opponent shaping offers significant computational advantages over previous methods and lays the foundation for our work.” 


9. COALA-PG (2023)

---
---

# LOLA
### Converted to English (Condensed for PPT)

The paper models current MARL algorithms as **Naive Learners (NL)**
assuming they can compute **exact gradients** and **Hessians**
$V^a(\theta^1, \theta^2)$ represents the expected cumulative reward based on parameters $(\theta^1, \theta^2)$. 
NL updates parameters iteratively:

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i), \quad \theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

In practice, agents lack access to rewards for all parameter combinations. Instead, NL assumes availability of gradients and updates parameters using:

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

where $\delta$ is the step size. This is referred to as the **naive learning rule**.

---

LOLA improves upon NL by introducing a **one-step look-ahead** mechanism
Instead of directly optimizing $V^1(\theta^1, \theta^2)$, LOLA optimizes $V^1(\theta^1, \theta^2 + \Delta \theta^2)$, approximated via a Taylor expansion:

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2).$$

Here, $\Delta \theta^2$ is derived from the Naive Learning rule as:

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

where $\eta$ is the step size. Combining these, the LOLA update rule becomes:

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates, respectively. 
These corrections require precise calculations of gradients and Hessians, referred to as NL-Ex and LOLA-Ex.

---

In reinforcement learning, exact cumulative rewards are typically unavailable. 
Therefore, these gradients and Hessians are estimated using policy gradient methods. 
For each episode trajectory $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$, the cumulative reward at time $t$ for agent $a$ is:

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

The expected episodic rewards for policies $\pi^1$ and $\pi^2$ are defined as $ER_0^1(\tau)$ and $ER_0^2(\tau)$, with the gradient calculated as:

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)].$$

By sampling trajectories, these gradients can be estimated to implement the LOLA learning rule.


#### Naive Learner (Policy Gradient-based NL-PG Rule)  
The Naive Learner (NL-PG) uses policy gradient to update parameters based on the cumulative reward gradient:

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

where $\delta$ is the step size.

---

#### LOLA-PG Improvements  
LOLA-PG improves NL-PG by introducing first- and second-order correction terms. The second-order term is derived as:

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

The LOLA-PG update rule is given by:

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates.

---

#### Opponent Modeling  
In LOLA, it is often assumed that agent $a$ knows its opponent's policy parameters. 
To address cases where this assumption is unrealistic, LOLA proposes **Opponent Modeling** to estimate the opponent’s parameters:

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$



---

# LOLA-DICE
### **LOLA-DiCE Overview**

---

#### **1. Background and Challenges**
- **Multi-Agent Interaction**: In multi-agent reinforcement learning (RL), each agent's policy update depends not only on its own rewards but also on the dynamic updates of opponents' policies.
- **LOLA’s Limitation**:
  - Uses Taylor expansion for opponent learning, but simplifies and ignores higher-order terms (e.g., $(\Delta \theta_1)^T \nabla_{\theta_2} V^1$), leading to incomplete gradient updates.
  - High-order gradient derivation is complex and error-prone in stochastic environments.

---

#### **2. DiCE’s Contribution**
- **MAGICBOX Operator**:
  - Automatically captures all dependencies in stochastic computation graphs (SCG).
  - Generates accurate higher-order gradients, solving LOLA's gradient truncation issues.
- **Unified Objective**:
  $$\mathcal{L}_\mathcal{Q} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c$$
  - Ensures compatibility with auto-differentiation frameworks, eliminating manual derivation.

---

#### **3. LOLA-DiCE Implementation**

##### **Step 1: Opponent Lookahead Updates**
- Simulate the opponent's learning process through inner-loop updates:
  - Sample trajectories $\tau_k \sim (\pi_{\theta_1}, \pi_{\theta_2'})$.
  - Update opponent's policy:
    $$\theta_2' \gets \theta_2' + \alpha_2 \nabla_{\theta_2'} \mathcal{L}_2^{\text{DiCE}}(\theta_1, \theta_2').$$

##### **Step 2: Agent Policy Update**
- After opponent updates, optimize the agent’s policy:
  - Sample trajectories $\tau \sim (\pi_{\theta_1}, \pi_{\theta_2'})$.
  - Update the agent’s policy:
    $$\theta_1 \gets \theta_1 + \alpha_1 \nabla_{\theta_1} \mathcal{L}_1^{\text{DiCE}}(\theta_1, \theta_2').$$

---

#### **4. Advantages of LOLA-DiCE**
1. **Complete Gradients**:
   - Accurately captures higher-order dependencies ignored in LOLA.
2. **Simplified Implementation**:
   - MAGICBOX integrates seamlessly with deep learning frameworks.
3. **Improved Efficiency**:
   - Reduces redundant calculations with structured dependency tracking.
4. **Generalizability**:
   - Adapts to any multi-agent scenario without additional modifications.

--- 
---

# MFOS
### **LOLA (Learning with Opponent-Learning Awareness)**

- **Core Motivation**: LOLA introduces **Theory of Mind** to maximize collective rewards by understanding opponent behavior.
- **Assumptions**: LOLA assumes the opponent is a **Naive Learner (NL)**, focusing solely on its own rewards and treating the opponent as part of the environment.
- **Update Mechanism**:
  - **NL Update Rule**:
    $$\phi_{t+1}^i = \phi_t^i + \alpha \nabla_{\phi_t^i} R^i(\phi_t^i, \phi_t^{-i}),$$
    where $\phi_t^i$ represents the parameters of agent $i$'s policy.
  - **LOLA Update Rule**:
    $$\phi_{t+1}^i = \phi_t^i + \alpha \nabla_{\phi_t^i} R^i(\phi_t^i, \phi_t^{-i} + \Delta \phi_t^{-i}),$$
    where:
    $$\Delta \phi_t^{-i} = \alpha \nabla_{\phi_t^{-i}} R^{-i}(\phi_t^i, \phi_t^{-i}).$$

---

### **Limitations of LOLA**

1. **Short-sightedness**: Focuses only on the next step of the opponent's update, neglecting long-term strategic impact.
2. **Asymmetry**: Two LOLA agents assuming each other as NL can lead to suboptimal solutions.
3. **High Computational Complexity**: Relies on opponent-specific update mechanisms, making it difficult to generalize or apply in a model-free context.

---

### **Transition to M-FOS**
To address these limitations, **M-FOS (Model-Free Opponent Shaping)** is proposed:
- Resolves short-sightedness by optimizing over long time horizons.
- Eliminates the dependency on higher-order gradient computations using meta-learning to shape update strategies.

---

### **Algorithm Framework and Workflow**

#### **Core Idea**
Opponent shaping can be formulated as a meta-game.
M-FOS builds a meta-game $M_n$ to learn a meta-strategy $\pi_\theta$. The meta-game is modeled as a POMDP:
- **State $s_t$**: Composed of the strategy parameters of all agents $(\phi_t^i, \phi_t^{-i})$.
- **Action $a_t$**: Sampled using the meta-policy $\pi_\theta(\cdot \mid o_t)$, representing strategy updates.
- **State Transition Function $P(\cdot \mid s_t, a_t)$**: Determines the next state $s_{t+1}$.

The cumulative reward for one meta-game is:
$$\bar{r}_t = \sum_{k=0}^K r_k^i(\phi_t^i, \phi_t^{-i}),$$
and the expected reward $J = \sum_{t=0}^T r_t^i(\phi_t^i, \phi_t^{-i})$ serves as the optimization objective for $\theta$.

---

#### **Workflow**

1. **Optimize Meta-Policy**:
   $$\max_\theta \mathbb{E}_{p(\phi_t^i, \phi_t^{-i})} \left[\sum_{t=0}^T R^i(\phi_t^i, \phi_t^{-i})\right].$$

2. **Update Strategies**:
   - For agent $i$:
     $$\phi_{t+1}^i \sim \pi_\theta(\cdot \mid \phi_t^i, \phi_t^{-i}).$$
   - For opponent:
     $$\phi_{t+1}^{-i} = f(\phi_t^i, \phi_t^{-i}),$$
     where $f(\cdot)$ represents the opponent's update rule.

---

#### **Advantages**

1. **Avoids Self-Play Conflicts**:
   - M-FOS does not rely on assumptions about the opponent’s update rule, reducing conflicts during self-play.

2. **Balances in Non-Zero-Sum Games**:
   - Offers a gradual transition from NL to M-FOS, ensuring equilibrium responses in non-zero-sum settings, approaching optimal solutions over time.


1. **Infinite Recursion**:
   - MAML-like opponent shaping methods require each agent to differentiate through the learning step of the other agent. This recursive dependency often results in infinite regress, preventing effective convergence.

2. **Initialization and Stability Issues**:
   - Independent learning in **general-sum games** is highly sensitive to initial parameter settings, leading to instability in the learning process.

3. **Difficulty in Nash Equilibrium Selection**:
   - General-sum games often have multiple Nash equilibria, requiring stricter methods to select the most suitable equilibrium.


#### **Inspiration from the Tracing Procedure**
1. **Theoretical Background**:
   - The **tracing procedure** (Harsanyi et al., 1988) selects equilibria based on a shared Bayesian prior over rational behaviors in a general-sum game.
   - Agents iteratively update their policies against each other until convergence, starting from this prior.

2. **Limitations**:
   - While effective in theory, the tracing procedure is impractical in **high-dimensional function approximation** due to computational complexity.

---

#### **M-FOS Self-Play Implementation**
1. **Adaptation of the Tracing Procedure**:
   - M-FOS uses a simplified and tractable version of the tracing procedure, inspired by its principles.

2. **Parameter $\lambda$ for Gradual Transition**:
   - **Definition**: $\lambda$ represents the probability of an M-FOS agent playing against a naive learner (NL) instead of another M-FOS agent.
   - **Training Procedure**:
     - **Initial Phase** ($\lambda = 1$): Agents are trained against naive learners to approximate a **best response** to NL.
     - **Transition Phase**: Gradually annealing $\lambda$ to 0 allows M-FOS agents to transition into **self-play** over the course of training.
   - **Purpose**: This slow annealing ensures that M-FOS agents remain near-optimal for the given distribution of opponents throughout training.



# meta
### **Meta-Value Learning (MeVa)

#### **1. Introduction**
Meta-Value Learning (MeVa) addresses challenges in multi-agent learning, particularly in general-sum games like Iterated Prisoner’s Dilemma (IPD). It proposes a consistent and far-sighted framework for learning with awareness of other agents' learning processes. Unlike prior methods (e.g., LOLA, M-FOS), MeVa:
- Avoids reliance on higher-order derivatives.
- Implements a value-based learning approach to capture long-term and higher-order interactions.
- Offers consistent results across various game settings.

#### **2. Key Contributions**
1. **Meta-Value Function**: Introduces a value function over meta-states (joint policies) to evaluate long-term interactions in optimization dynamics.
2. **Q-Learning Inspired Updates**: Applies reinforcement learning techniques to approximate gradients of the meta-value function.
3. **Practical Techniques**: Utilizes methods like bootstrapping, target networks, and exploration strategies for stable learning.
4. **Scalability**: Avoids explicitly modeling continuous action spaces, making it applicable to high-dimensional games.

#### **5. Experiments**
1. **Logistic Game**:
   - Demonstrates MeVa’s ability to expand basins of attraction, reliably converging to Pareto-efficient solutions.

2. **Matrix Games**:
   - Compared against Naive, LOLA, and M-FOS in repeated matrix games:
     - **IPD**: Achieves ZD-extortion, outperforming baselines in long-term cooperation and dynamical exploitation.
     - **Iterated Matching Pennies**: Dynamically exploits naive learners and LOLA.
     - **Chicken Game**: Maintains cooperation and avoids self-play disasters, unlike LOLA.

#### **6. Limitations**
- Scalability to neural network policies is challenging due to high-dimensional meta-value approximations.
- Requires visibility into opponent parameters, limiting application to real-world settings.


### **Meta-Value Learning (MeVa) Algorithm Overview**

The MeVa algorithm introduces a novel method for learning meta-policies through the concept of a **meta-value function**. Below is a breakdown of its key components and ideas:

---

#### **1. Meta-Value Function**
- The meta-value function is proposed as a surrogate for the true objective in meta-learning, which aims to capture the **downstream effects of meta-actions** across multiple steps.
- The update rule for the optimization process is defined as:
  $$x_i^{(t+1)} = x_i^{(t)} + \alpha \nabla_{x_i} V_i(x^{(t)}),$$
  where:
  - $x_i^{(t)}$ represents the state of player $i$ at time $t$,
  - $V_i(x)$ is the meta-value function that evaluates the expected return from $x$,
  - $\nabla_{x_i} V_i(x)$ indicates the gradient of the meta-value with respect to the state.

- The meta-value function naturally integrates a **future perspective** by looking ahead multiple steps through a discounted reward framework:
  $$V(x) = f(x) + \gamma V(x'),$$
  where:
  - $x' = x + \alpha \nabla_x V(x)$,
  - $f(x)$ is the immediate reward,
  - $\gamma$ is the discount factor for future rewards.

- Unlike standard value functions, this approach is implicit, meaning gradients of $V(x)$ cannot be directly accessed and require approximation.

---

#### **2. Learning Meta-Values**
To approximate the meta-value function, MeVa employs the following approach:
1. **Model Approximation**:
   - A parameterized model $\hat{V}_i(x_i; \theta_i)$ is learned to approximate $V_i(x_i)$, where $\theta_i$ is the set of parameters for the meta-value function.

2. **Gradient Use**:
   - Gradients of $\hat{V}_i(x_i; \theta_i)$, denoted as $\nabla_{x_i} \hat{V}_i(x_i; \theta_i)$, are used to guide the meta-policy updates.

3. **Learning Objective**:
   - Instead of directly emitting meta-policies (as in M-FOS) or entire policies, MeVa models the **scalar meta-value function** and learns it iteratively by minimizing the **Temporal Difference (TD) error**:
     $$\delta_i^{(t)} = f(x^{(t)}) + \gamma \hat{V}_i(x_i^{(t+1)}; \theta_i) - \hat{V}_i(x_i^{(t)}; \theta_i).$$
   - The loss function for training the meta-value function is:
     $$\sum_t (\delta_i^{(t)})^2,$$
     where $\delta_i^{(t)}$ represents the TD error at time step $t$.

4. **Inner and Outer Loops**:
   - **Inner Loop**: Generates trajectories using the meta-policy based on gradients of the current meta-value function.
   - **Outer Loop**: Updates the parameters $\theta_i$ of the meta-value function by minimizing the TD error.

---

#### **3. Algorithm Summary**
- The algorithm can be viewed as a **generalization of Value-Gradient Learning** (Fairbank & Alonso, 2012) but avoids directly enforcing a Bellman equation.
- The training process aligns with existing meta-learning frameworks like Meta-MAPG and M-FOS but shifts the focus to a local meta-policy rather than higher-order derivatives or global optimization.

---

#### **Advantages of MeVa**
1. **Scalability**:
   - By modeling scalar values instead of entire policies, MeVa reduces computational complexity.
2. **Long-Horizon Optimization**:
   - Incorporates future steps effectively through the meta-value function and discounted rewards.
3. **Flexibility**:
   - MeVa does not require explicit Bellman constraints, allowing for broader applicability in reinforcement learning.
