---
创建时间: 2025-一月-9日  星期四, 9:44:23 上午
---
2025-1-13 周一组会分享：


lola相关算法
mfos
opponent **shaping** ： shaps opponent's actions in the future.



[Learning to Incentivize Others](https://zhuanlan.zhihu.com/p/150688960)

1. “Learning with Opponent-Learning Awareness (LOLA)”2018
   >as a method for controlling the learning dynamics of opponents in a game.
   >A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes $V^1(\theta^1, \theta^2 + \Delta\theta^2)$ where $\Delta\theta^2$ is a naive learning step in the direction that maximizes the opponent’s value function $V^2(\theta^1, \theta^2)$. 
   
2. Stable Opponent Shaping (SOS) (2018)
Variations of LOLA have been introduced to have formal stability guarantees
Letcher, A., Foerster, J., Balduzzi, D., Rockta ̈schel, T., and Whiteson, S. (2021). Stable opponent shaping in differentiable games.

3. Consistent Opponent-Learning Awareness (COLA) (2022)

Willi, T., Letcher, A., Treutlein, J., and Foerster, J. (2022). Cola: Consistent learning with opponent- learning awareness.

4. More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). 

Aghajohari, M., Cooijmans, T., Duque, J. A., Akatsuka, S., and Courville, A. (2024a). Best response shaping.

5. LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.
Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. (2024b). Loqa: Learning with opponent q-learning awareness.


4. Model-Free Opponent Shaping (MFOS) (2022)
   
   Lu, C., Willi, T., de Witt, C. S., and Foerster, J. (2022). Model-free opponent shaping.

5. Meta-value learning: a general framework for learning with learning awareness.
Cooijmans, T., Aghajohari, M., and Courville, A. (2023). Meta-value learning: a general framework for learning with learning awareness.
More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. 




7. Scaling Opponent Shaping to High Dimensional Games (Shaper) (2024)
   
Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information

  8.  A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021.
   
1. Advantage Alignment Algorithms (2024a)
LOQA: “This new approach to opponent shaping offers significant computational advantages over previous methods and lays the foundation for our work.” 


9. COALA-PG (2023)

---
---
### Converted to English (Condensed for PPT)

The paper models current Multi-Agent Reinforcement Learning (MARL) algorithms as **Naive Learners (NL)**, assuming they can compute **exact gradients** and **Hessians** for cumulative discounted rewards with respect to policy parameters. 
For agent $a$, its policy $\pi^a$ is parameterized by $\theta^a$, 
and $V^a(\theta^1, \theta^2)$ represents the expected cumulative reward based on parameters $(\theta^1, \theta^2)$. 
NL updates parameters iteratively:

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i), \quad \theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

In practice, agents lack access to rewards for all parameter combinations. Instead, NL assumes availability of gradients and updates parameters using:

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

where $\delta$ is the step size. This is referred to as the **naive learning rule**.

---

LOLA improves upon NL by introducing a **one-step look-ahead** mechanism, optimizing expected rewards by considering the effect of a Naive Learning update ($\Delta \theta^2$).
Instead of directly optimizing $V^1(\theta^1, \theta^2)$, LOLA optimizes $V^1(\theta^1, \theta^2 + \Delta \theta^2)$, approximated via a Taylor expansion:

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2).$$

Here, $\Delta \theta^2$ is derived from the Naive Learning rule as:

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

where $\eta$ is the step size. Combining these, the LOLA update rule becomes:

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

where $f^{lola}_1$ includes additional corrections that account for the opponent’s learning behavior.


The LOLA learning rule introduces improvements over the Naive Learning Rule by incorporating an additional second-order correction term. The update rule is expressed as:

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates, respectively. This second-order term accounts for the opponent's learning process and its impact on the agent's own strategy updates. These corrections require precise calculations of gradients and Hessians, referred to as NL-Ex and LOLA-Ex.

---

In reinforcement learning, exact cumulative rewards are typically unavailable. Therefore, these gradients and Hessians are estimated using policy gradient methods. For each episode trajectory $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$, the cumulative reward at time $t$ for agent $a$ is:

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

where $\gamma$ is the discount factor.

The expected episodic rewards for policies $\pi^1$ and $\pi^2$ are defined as $ER_0^1(\tau)$ and $ER_0^2(\tau)$, with the gradient calculated as:

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)].$$

By sampling trajectories, these gradients can be estimated to implement the LOLA learning rule.



### Converted to English (Condensed for PPT)

#### Naive Learner (Policy Gradient-based NL-PG Rule)  
The Naive Learner (NL-PG) uses policy gradient to update parameters based on the cumulative reward gradient:

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

where $\delta$ is the step size.

---

#### LOLA-PG Improvements  
LOLA-PG improves NL-PG by introducing first- and second-order correction terms. The second-order term is derived as:

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

The LOLA-PG update rule is given by:

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates.

---

#### Opponent Modeling  
In LOLA, it is often assumed that agent $a$ knows its opponent's policy parameters. To address cases where this assumption is unrealistic, LOLA proposes **Opponent Modeling** to estimate the opponent’s parameters:

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$

By fitting the opponent's strategy using historical data, LOLA can still be applied effectively without explicit access to the opponent’s parameters.
