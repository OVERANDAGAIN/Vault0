---
创建时间: 2025-一月-9日  星期四, 9:44:23 上午
---
2025-1-13 周一组会分享：


lola相关算法
mfos
opponent **shaping** ： shaps opponent's actions in the future.



[Learning to Incentivize Others](https://zhuanlan.zhihu.com/p/150688960)

1. “Learning with Opponent-Learning Awareness (LOLA)”2018
   >as a method for controlling the learning dynamics of opponents in a game.
   >A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes $V^1(\theta^1, \theta^2 + \Delta\theta^2)$ where $\Delta\theta^2$ is a naive learning step in the direction that maximizes the opponent’s value function $V^2(\theta^1, \theta^2)$. 
   
2. Stable Opponent Shaping (SOS) (2018)
Variations of LOLA have been introduced to have formal stability guarantees
Letcher, A., Foerster, J., Balduzzi, D., Rockta ̈schel, T., and Whiteson, S. (2021). Stable opponent shaping in differentiable games.

3. Consistent Opponent-Learning Awareness (COLA) (2022)

Willi, T., Letcher, A., Treutlein, J., and Foerster, J. (2022). Cola: Consistent learning with opponent- learning awareness.

4. More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). 

Aghajohari, M., Cooijmans, T., Duque, J. A., Akatsuka, S., and Courville, A. (2024a). Best response shaping.

5. LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.
Aghajohari, M., Duque, J. A., Cooijmans, T., and Courville, A. (2024b). Loqa: Learning with opponent q-learning awareness.


4. Model-Free Opponent Shaping (MFOS) (2022)
   
   Lu, C., Willi, T., de Witt, C. S., and Foerster, J. (2022). Model-free opponent shaping.

5. Meta-value learning: a general framework for learning with learning awareness.
Cooijmans, T., Aghajohari, M., and Courville, A. (2023). Meta-value learning: a general framework for learning with learning awareness.
More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. 




7. Scaling Opponent Shaping to High Dimensional Games (Shaper) (2024)
   
Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information

  8.  A. Xie, D. Losey, R. Tolsma, C. Finn, and D. Sadigh. Learning latent representations to influence multi-agent interaction. In Conference on Robot Learning, 2021.
   
1. Advantage Alignment Algorithms (2024a)
LOQA: “This new approach to opponent shaping offers significant computational advantages over previous methods and lays the foundation for our work.” 


9. COALA-PG (2023)

---
---

# LOLA
### Converted to English (Condensed for PPT)

The paper models current MARL algorithms as **Naive Learners (NL)**
assuming they can compute **exact gradients** and **Hessians**
$V^a(\theta^1, \theta^2)$ represents the expected cumulative reward based on parameters $(\theta^1, \theta^2)$. 
NL updates parameters iteratively:

$$\theta^1_{i+1} = \arg\max_{\theta^1} V^1(\theta^1, \theta^2_i), \quad \theta^2_{i+1} = \arg\max_{\theta^2} V^2(\theta^1_i, \theta^2).$$

In practice, agents lack access to rewards for all parameter combinations. Instead, NL assumes availability of gradients and updates parameters using:

$$\theta^1_{i+1} = \theta^1_i + f^{nl}_1(\theta^1_i, \theta^2_i) \cdot \delta,$$

where $\delta$ is the step size. This is referred to as the **naive learning rule**.

---

LOLA improves upon NL by introducing a **one-step look-ahead** mechanism
Instead of directly optimizing $V^1(\theta^1, \theta^2)$, LOLA optimizes $V^1(\theta^1, \theta^2 + \Delta \theta^2)$, approximated via a Taylor expansion:

$$V^1(\theta^1, \theta^2 + \Delta \theta^2) \approx V^1(\theta^1, \theta^2) + (\Delta \theta^2)^T \nabla_{\theta^2} V^1(\theta^1, \theta^2).$$

Here, $\Delta \theta^2$ is derived from the Naive Learning rule as:

$$\Delta \theta^2 = \eta \nabla_{\theta^2} V^2(\theta^1, \theta^2),$$

where $\eta$ is the step size. Combining these, the LOLA update rule becomes:

$$\theta^1_{i+1} = \theta^1_i + f^{lola}_1(\theta^1_i, \theta^2_i),$$

$$f^{lola}_1(\theta^1, \theta^2) = \nabla_{\theta^1} V^1(\theta^1, \theta^2) \cdot \delta + (\nabla_{\theta^2} V^1(\theta^1, \theta^2))^T \nabla_{\theta^1} \nabla_{\theta^2} V^2(\theta^1, \theta^2) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates, respectively. 
These corrections require precise calculations of gradients and Hessians, referred to as NL-Ex and LOLA-Ex.

---

In reinforcement learning, exact cumulative rewards are typically unavailable. 
Therefore, these gradients and Hessians are estimated using policy gradient methods. 
For each episode trajectory $\tau = (s_0, u_0, r_0, \ldots, s_T, r_T)$, the cumulative reward at time $t$ for agent $a$ is:

$$R^a_t(\tau) = \sum_{l=t}^T \gamma^{l-t} r^a_l,$$

The expected episodic rewards for policies $\pi^1$ and $\pi^2$ are defined as $ER_0^1(\tau)$ and $ER_0^2(\tau)$, with the gradient calculated as:

$$\nabla_{\theta^1} ER^1_0(\tau) = \mathbb{E}[R^1_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t)].$$

By sampling trajectories, these gradients can be estimated to implement the LOLA learning rule.


#### Naive Learner (Policy Gradient-based NL-PG Rule)  
The Naive Learner (NL-PG) uses policy gradient to update parameters based on the cumulative reward gradient:

$$f^{nl,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta,$$

where $\delta$ is the step size.

---

#### LOLA-PG Improvements  
LOLA-PG improves NL-PG by introducing first- and second-order correction terms. The second-order term is derived as:

$$\nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) = \mathbb{E} \left[ R^2_0(\tau) \nabla_{\theta^1} \log \pi^1(u_t^1 | s_t) \cdot \nabla_{\theta^2} \log \pi^2(u_t^2 | s_t) \right].$$

The LOLA-PG update rule is given by:

$$f^{lola,pg}_1 = \nabla_{\theta^1} ER^1_0(\tau) \cdot \delta + \left( \nabla_{\theta^2} ER^2_0(\tau) \right)^T \nabla_{\theta^1} \nabla_{\theta^2} ER^2_0(\tau) \cdot \delta \eta,$$

where $\delta$ and $\eta$ are step sizes for first-order and second-order updates.

---

#### Opponent Modeling  
In LOLA, it is often assumed that agent $a$ knows its opponent's policy parameters. 
To address cases where this assumption is unrealistic, LOLA proposes **Opponent Modeling** to estimate the opponent’s parameters:

$$\theta^2 = \arg\max_{\theta^2} \sum_t \log \pi^2(u_t^2 | s_t).$$



---

# LOLA-DICE
### **LOLA-DiCE Overview**

---

#### **1. Background and Challenges**
- **Multi-Agent Interaction**: In multi-agent reinforcement learning (RL), each agent's policy update depends not only on its own rewards but also on the dynamic updates of opponents' policies.
- **LOLA’s Limitation**:
  - Uses Taylor expansion for opponent learning, but simplifies and ignores higher-order terms (e.g., $(\Delta \theta_1)^T \nabla_{\theta_2} V^1$), leading to incomplete gradient updates.
  - High-order gradient derivation is complex and error-prone in stochastic environments.

---

#### **2. DiCE’s Contribution**
- **MAGICBOX Operator**:
  - Automatically captures all dependencies in stochastic computation graphs (SCG).
  - Generates accurate higher-order gradients, solving LOLA's gradient truncation issues.
- **Unified Objective**:
  $$\mathcal{L}_\mathcal{Q} = \sum_{c \in \mathcal{C}} \text{MAGICBOX}(\mathcal{W}_c) \cdot c$$
  - Ensures compatibility with auto-differentiation frameworks, eliminating manual derivation.

---

#### **3. LOLA-DiCE Implementation**

##### **Step 1: Opponent Lookahead Updates**
- Simulate the opponent's learning process through inner-loop updates:
  - Sample trajectories $\tau_k \sim (\pi_{\theta_1}, \pi_{\theta_2'})$.
  - Update opponent's policy:
    $$\theta_2' \gets \theta_2' + \alpha_2 \nabla_{\theta_2'} \mathcal{L}_2^{\text{DiCE}}(\theta_1, \theta_2').$$

##### **Step 2: Agent Policy Update**
- After opponent updates, optimize the agent’s policy:
  - Sample trajectories $\tau \sim (\pi_{\theta_1}, \pi_{\theta_2'})$.
  - Update the agent’s policy:
    $$\theta_1 \gets \theta_1 + \alpha_1 \nabla_{\theta_1} \mathcal{L}_1^{\text{DiCE}}(\theta_1, \theta_2').$$

---

#### **4. Advantages of LOLA-DiCE**
1. **Complete Gradients**:
   - Accurately captures higher-order dependencies ignored in LOLA.
2. **Simplified Implementation**:
   - MAGICBOX integrates seamlessly with deep learning frameworks.
3. **Improved Efficiency**:
   - Reduces redundant calculations with structured dependency tracking.
4. **Generalizability**:
   - Adapts to any multi-agent scenario without additional modifications.

--- 
---

# MFOS
