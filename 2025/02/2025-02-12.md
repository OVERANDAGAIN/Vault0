---
创建时间: 2025-二月-12日  星期三, 3:23:46 下午
created: 2025-02-12T15:48
updated: ...
---




### **问题 1：为什么 `obs[:,:,i]` 可以表示每个玩家的位置？**

#### **解析 `obs = obs_flatten[7:].reshape((self.world_height, self.world_width, self.player_num + 3))`**
- `obs_flatten[7:]` 是去掉了前 7 维的观测数据，剩余的数据用于描述环境的二维网格状态。
- `reshape((self.world_height, self.world_width, self.player_num + 3))`：
  - `self.world_height` 和 `self.world_width` 表示网格世界的高度和宽度，说明 `obs` 代表一个 2D 格式的环境状态。
  - `self.player_num + 3` 是通道维度：
    - 前 `self.player_num` 个通道分别代表每个玩家的位置。
    - 倒数第 3 个通道 `obs[:, :, -3]` 可能用于额外环境信息。
    - 倒数第 2 个通道 `obs[:, :, -2]` 代表鹿（stag）的位置。
    - 倒数第 1 个通道 `obs[:, :, -1]` 代表兔子（hare）的位置。

#### **为什么 `obs[:,:,i]` 代表玩家 i 的位置？**
```python
for i in range(self.player_num):
    cur_pos = np.nonzero(obs[:,:,i])
    if len(cur_pos[0]) == 0:
        new_state.extend([-1,-1])
    else:
        new_state.extend([cur_pos[0][0],cur_pos[1][0]])
```
- `obs[:,:,i]` 提取的是 **第 `i` 个玩家的通道**，这个通道的值是一个二维矩阵（`self.world_height x self.world_width`）。
- `np.nonzero(obs[:,:,i])` 找到该矩阵中 **非零元素的索引**，即玩家 `i` 的位置：
  - `cur_pos[0]`：行索引（y 坐标）。
  - `cur_pos[1]`：列索引（x 坐标）。

**示例**
假设 `self.world_height = 5, self.world_width = 5, self.player_num = 3`，`obs` 可能是：
```
obs[:,:,0]: 玩家 1 位置
[[0, 0, 0, 0, 0]
 [0, 1, 0, 0, 0]  (玩家 1 在 (1,1))
 [0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]]

obs[:,:,1]: 玩家 2 位置
[[0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]
 [0, 0, 1, 0, 0]  (玩家 2 在 (2,2))
 [0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]]

obs[:,:,2]: 玩家 3 位置
[[0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]
 [0, 0, 0, 0, 0]
 [0, 0, 0, 1, 0]  (玩家 3 在 (3,3))
 [0, 0, 0, 0, 0]]
```
此时：
```python
cur_pos = np.nonzero(obs[:,:,0])  # 取玩家1的通道
cur_pos[0] = [1]  # y 坐标
cur_pos[1] = [1]  # x 坐标
new_state.extend([1,1])  # 存储玩家1的位置

cur_pos = np.nonzero(obs[:,:,1])  # 取玩家2的通道
cur_pos[0] = [2]  # y 坐标
cur_pos[1] = [2]  # x 坐标
new_state.extend([2,2])  # 存储玩家2的位置

cur_pos = np.nonzero(obs[:,:,2])  # 取玩家3的通道
cur_pos[0] = [3]  # y 坐标
cur_pos[1] = [3]  # x 坐标
new_state.extend([3,3])  # 存储玩家3的位置
```
如果 `cur_pos[0]` 为空（玩家不在地图上），那么存储 `[-1, -1]`，表示玩家消失或未出生。

---

### **问题 2：`self.discounted_stag_times` 表示什么意思？**
#### **定义**
```python
self.discounted_stag_times = np.array([0.5/(1-self.discount_factor)] * self.player_num, dtype=np.float32)
self.discounted_hare_times = np.array([0.5/(1-self.discount_factor)] * self.player_num, dtype=np.float32)
```
- 这里的 `self.discounted_stag_times` 和 `self.discounted_hare_times` 分别存储 **每个玩家参与捕猎鹿（stag）和兔子（hare）的折扣累积次数**。
- 计算公式 `0.5 / (1 - self.discount_factor)` 是一个无限几何级数求和的结果，表示 **未来捕猎的总期望贡献**。
- `self.discount_factor` 是 **折扣因子（通常 0 < γ < 1）**，用于计算长期收益的衰减。

#### **为何初始化？**
这个初始化方法意味着 **默认情况下，玩家捕猎鹿和兔子的概率是对等的（各 50%），并且随着时间步长增加，这个比例会有所变化**。

---

### **调整 `self.discounted_stag_times` 和 `self.discounted_hare_times`**
```python
self.discounted_stag_times = np.array([100, 165, 165, 165], dtype=np.float32)[:self.player_num]
self.discounted_hare_times = np.array([100, 35, 35, 35], dtype=np.float32)[:self.player_num]
```
- `self.discounted_stag_times` = `[100, 165, 165, 165]`
  - 第一个玩家捕猎鹿的时间贡献是 100，其他三个玩家是 165。
- `self.discounted_hare_times` = `[100, 35, 35, 35]`
  - 第一个玩家捕猎兔子的时间贡献是 100，其他三个玩家是 35。

**为什么 `discounted_hare_times` 远小于 `discounted_stag_times`？**
- 可能表示玩家更倾向于捕猎鹿（stag），因为：
  - 鹿的奖励更高。
  - 需要合作才能成功捕获鹿，因此时间贡献较大。
  - 兔子容易被个体捕获，因此捕猎时间贡献较少。

**这个变量的作用**
- `self.stag_prob = self.discounted_stag_times / (self.discounted_hare_times + self.discounted_stag_times)`
  - 计算 **玩家更倾向于捕猎鹿的概率**。
  - 例如：
    ```python
    self.stag_prob = [100 / (100 + 100), 165 / (165 + 35), 165 / (165 + 35), 165 / (165 + 35)]
                 = [0.5, 0.825, 0.825, 0.825]
    ```
    - 玩家 1 选择鹿和兔子的概率相等（0.5）。
    - 其他玩家选择鹿的概率大于 0.8，说明他们更倾向于捕猎鹿。

- `self.hare_prob = 1 - self.stag_prob`
  - 计算 **玩家更倾向于捕猎兔的概率**。

---

### **总结**
1. `obs[:,:,i]` 代表玩家 `i` 在地图上的 **位置通道**，`np.nonzero(obs[:,:,i])` 识别出该玩家的具体位置。
2. `self.discounted_stag_times` 记录玩家 **累积的折扣捕猎鹿的时间贡献**，用于计算玩家的 **狩猎偏好**，并影响玩家后续的策略选择（例如更倾向于猎鹿还是猎兔）。
3. 这些参数用于动态调整玩家的策略，使其在不同环境下学习最优策略。










### **遍历所有对手后得到的 `self.stag_prob[i]` 是什么意思？**

#### **1. `self.stag_prob[i]` 的含义**
- 这是 **玩家 `i` 选择捕猎鹿（stag）的概率**，用于 **估计每个对手更倾向于捕猎鹿还是兔子**。
- 这个概率基于：
  - `self.stag_prob[i]`（上一时刻对 `i` 的估计）
  - `ac_prob_stag` 和 `ac_prob_hare`（玩家 `i` 在上一时刻的实际行为）
  - 通过归一化计算，确保 `self.stag_prob[i] + self.hare_prob[i] = 1`

**公式**
```python
self.stag_prob[i] = (self.stag_prob[i] * ac_prob_stag) / (
    self.stag_prob[i] * ac_prob_stag + self.hare_prob[i] * ac_prob_hare
)
```
- 这是一个 **贝叶斯更新公式**，即：
  - **原始的 `self.stag_prob[i]` 乘以 `ac_prob_stag`（对手实际行为的鹿选择概率）**
  - **对手也可能选择捕猎兔，因此分母是所有可能情况的总和**

---

#### **2. 遍历所有对手后的 `self.stag_prob` 结果**
最终 `self.stag_prob` 变成一个**长度为 `self.player_num` 的数组**，表示**当前玩家对每个玩家选择捕猎鹿的概率的估计**。

示例：
```python
self.stag_prob = [0.5, 0.8, 0.3, 0.9]  # 玩家 1, 2, 3, 4 分别选择捕猎鹿的概率
self.hare_prob = [0.5, 0.2, 0.7, 0.1]  # 玩家 1, 2, 3, 4 分别选择捕猎兔的概率
```
**解释：**
- **玩家 1**（自己）：50% 选择鹿，50% 选择兔。
- **玩家 2**（对手）：80% 选择鹿，20% 选择兔 → **偏好捕猎鹿**。
- **玩家 3**（对手）：30% 选择鹿，70% 选择兔 → **偏好捕猎兔**。
- **玩家 4**（对手）：90% 选择鹿，10% 选择兔 → **几乎总是选择捕猎鹿**。

---

#### **3. `self.stag_prob` 的用途**
这组概率用于：
1. **影响当前玩家的行动选择**
   - 如果大部分玩家都选择猎鹿（stag），那么自己可能更倾向于合作猎鹿，而不是独自去猎兔。
   - 反之，如果其他人都去猎兔，自己可能也更倾向于猎兔，以免鹿猎捕失败。

2. **影响蒙特卡洛树搜索（MCTS）计算**
   - 这些概率用于 MCTS 计算，每个对手的行为建模时，会根据这些概率进行动作采样，来模拟整个回合的未来走向。

3. **影响强化学习中的奖励估计**
   - 如果 `self.stag_prob` 低（比如大部分人猎兔），那么算法可能会调整策略，减少尝试猎鹿的概率，以避免失败。

---

### **总结**
- `self.stag_prob[i]` 是**玩家 `i` 选择捕猎鹿的概率估计**。
- 遍历所有对手后，得到一个 **数组 `self.stag_prob`，每个元素表示对应玩家捕猎鹿的概率**。
- 这个结果影响：
  1. **自己的决策**
  2. **蒙特卡洛树搜索（MCTS）计算**
  3. **强化学习的策略更新**
  
最终，`self.stag_prob` 让当前玩家能够 **根据其他玩家的策略调整自己的行为**，更好地适应游戏环境。









### 1. **树的概率**指的是什么？
在这段代码中，`prob_list` 存储的是每个 MCTS 树节点的概率。具体而言，**树的概率**是基于当前环境状态、其他玩家的策略以及猎物的状态计算得出的，反映了该树节点下某个动作的选择倾向。

对于每个 MCTS 树（在 `self.tree_num` 的范围内），概率会被 **递归地** 更新和调整。它结合了当前玩家、其他玩家以及猎物的状态。概率值本质上用于评估不同节点下选择某个动作的可能性， **较高的概率代表更有可能选择该动作**。

### 2. **两层 `for` 循环的意义**

```python
for node_id in range(self.tree_num):
    for model_id in self.model_id_list:
```

这两层循环分别有以下含义：

- **外层循环（`for node_id in range(self.tree_num)`）**：
  - 遍历每个 MCTS 树节点 (`node_id`)。`self.tree_num` 是树的数量，表示要计算多少个不同的树模型。
  - 对于每个树节点，接下来会计算该节点的 **行为概率**。
  
- **内层循环（`for model_id in self.model_id_list`）**：
  - 遍历每个玩家（`model_id`）的策略。在这里，`model_id_list` 可能包含所有其他玩家的 ID（包括当前玩家自己），用于计算其他玩家对猎物选择的影响。
  - 每个玩家的策略影响计算其在当前树节点的概率（即捕捉猎物的概率）。

### 3. **`if prob_list[node_id] > 1e-5:` 这一步的意义**

```python
if prob_list[node_id] > 1e-5:
    actual_calculate_tree.append(node_id)
```

这一步的作用是 **过滤掉概率非常小的树节点**，即那些概率几乎为零的节点。具体来说：

- `prob_list[node_id]` 是当前树节点的概率，如果该节点的概率非常小（小于 `1e-5`），则认为该节点对应的策略不具有实质意义，可能在决策过程中不会被考虑。
- **`actual_calculate_tree.append(node_id)`** 将 **具有较高概率** 的树节点（`prob_list[node_id] > 1e-5`）添加到 `actual_calculate_tree` 列表中，这个列表会用于后续的 MCTS 计算。

### 4. **`mcts_policy` 指的是什么阶段的策略？为什么 `action` 是 `np.random.choice`？**

- **`mcts_policy` 是基于当前树的 Q 值计算出的策略。**
  - `mcts_policy` 的计算基于每个树节点的 Q 值，经过 **softmax 函数处理**（`np.exp(Q_value_total * self.Q_temp)`）。这使得更高的 Q 值对应更高的概率，最终 `mcts_policy` 代表了在当前树结构下的策略。
  - `mcts_policy` 通过 `np.sum(mcts_policy)` 归一化，以确保概率的总和为 1，从而形成一个有效的概率分布。

- **为什么 `action` 是 `np.random.choice`？**
  - **`action` 由 `mcts_policy` 确定**，但并非直接选择 `np.argmax(mcts_policy)`，而是通过 `np.random.choice(7, p=mcts_policy)` 进行采样。这样做的目的是引入 **随机性**，使得策略不完全是确定性的，从而促进 **探索**，而不是仅仅依赖于已知的最佳动作。
  - 这种随机选择机制帮助平衡 **探索和利用**，避免过早陷入局部最优解。

```python
mcts_policy = np.exp(Q_value_total * self.Q_temp)
mcts_policy = mcts_policy / np.sum(mcts_policy)
action = np.random.choice(7, p=mcts_policy)
```

1. **`mcts_policy = np.exp(Q_value_total * self.Q_temp)`**：
   - 对 Q 值进行温度调整。温度 `self.Q_temp` 控制选择动作的 **确定性与随机性**，较高的温度使得所有动作的选择概率接近，而较低的温度则让策略更加确定性。
   
2. **`mcts_policy = mcts_policy / np.sum(mcts_policy)`**：
   - 对 `mcts_policy` 进行归一化，使得它成为一个有效的概率分布（所有概率之和为 1）。

3. **`action = np.random.choice(7, p=mcts_policy)`**：
   - 根据 **归一化后的策略** `mcts_policy`，**随机选择一个动作**。这里 `7` 表示一共可以选择 7 个不同的动作（例如，选择某个猎物或不选择）。

---

### 总结

4. **树的概率**指的是每个树节点在当前状态下的选择概率，基于其他玩家的策略、猎物的状态等信息进行计算。
   
5. **两层 `for` 循环的作用**：
   - 外层循环遍历每棵树（每个树节点）。
   - 内层循环遍历所有玩家，计算每个玩家对当前节点的概率。

6. **`if prob_list[node_id] > 1e-5:`** 的作用是去除掉概率非常小的节点，减少计算量，只保留有实际意义的树节点。

7. **`mcts_policy`** 是根据每个树节点的 Q 值计算出的策略，代表了不同动作的概率分布；**`action = np.random.choice(7, p=mcts_policy)`** 是基于该策略进行的随机选择，确保引入探索性。





明白了，按照你的要求进行简化，下面是更加精简且结构清晰的思维导图：

---

### **compute_actions_from_input_dict()**  
- **compute_actions_from_input_dict()**: `your_file.py`
  - **初始化数据**  
    - 提取 `obs_batch`, `state_batches`, `episodes`
    - 判断 `time == 0`  
      - 调用 `get_initial_state()`
    - 否则  
      - 更新 `new_state`, `stag_prob`, `hare_prob`
  - **构建 `new_state`**
    - 更新玩家位置和猎物位置
    - 更新 `stag_prob`, `hare_prob`
  - **计算最优动作**
    - 无猎物  
      - 随机选择动作
    - 有猎物  
      - 调用 `self.env.set_state(self.state_dict)`
      - 初始化 MCTS 搜索树
      - 并行 MCTS 计算: `self.mcts.compute_action()`
      - 计算 `Q_value_total`，从中采样动作
  - **存储数据**  
    - `new_state_batch.append(new_state)`
    - `episodes[n].user_data[f"mcts_policies{self.my_id}"].append(mcts_policy)`
    - `episodes[n].user_data[f"recurrent_state{self.my_id}"].append(new_state)`
    - `action_batch.append(action)`
  - **返回**  
    - `action_batch, np.array(new_state_batch).T, {}`

---

### **辅助函数**  
- **get_initial_state()**
- **set_env_states()**
  - 输入: `obs, new_state, time`
  - 输出: `self.state_dict`
- **self.env.set_state()**
  - 输入: `self.state_dict`
- **mcts.compute_action()**
  - 输入: `tree_node_list[i]`
  - 输出: `(mcts_policy, action, tree_node, Q_value)`

---

以下是根据你之前的问题和我的回答，整理的 **`postprocess_trajectory()`** 和 **`learn_on_batch()`** 这两个函数的思维导图：

---

### **postprocess_trajectory()**

- **Start processing the trajectory**
    - **Loop through players** (excluding `self.my_id`)
        - **Get the model ID** for each player (`model_id = self.model_id_list[i]`)
        - **Determine the place ID** based on the model's relative position to `self.my_id`:
            - `place_id = (model_id + 1 - self.my_id) % self.player_num`
        - **Check if the last reward for the player is positive**:
            - `if episode._agent_reward_history[f'player_{model_id+1}'][-1] > 0:`
                - **Add observation, actions, and time indices** to the model buffer
                    - `self.model_buffer[i][0].append(...)` (obs)
                    - `self.model_buffer[i][1].append(...)` (actions)
                    - `self.model_buffer[i][2].append(...)` (time indices)
                - **Update the action type (stag or hare)** based on the player's last action:
                    - **If last action was 'stag' (5)**: Update `self.discounted_stag_times`
                    - **If last action was 'hare' (6)**: Update `self.discounted_hare_times`
                - **Reset model buffer if it exceeds capacity**:
                    - If buffer exceeds `moa_buffer_capacity`, call `self.moa_update(i)`
            - **Add MCTS policies to the sample batch** (`sample_batch["mcts_policies"]`)
            - **Calculate final reward** and adjust using reward normalization (`r2_buffer.add_reward(final_reward)`)
            - **Assign value labels** for each transition in the trajectory (`value_label = final_reward * np.ones_like(sample_batch["t"])`)
        - **End of trajectory processing**

---

### **learn_on_batch()**

- **Start learning from the batch**
    - **Increment learning count** (`self.learn_count`)
    - **Skip learning if `my_id > 1` and `learn_count > 75`**:
        - If conditions met, return early: `return {LEARNER_STATS_KEY: {}}`
    - **Create a tensor dict for the batch**:
        - `train_batch = self._lazy_tensor_dict(postprocessed_batch)`
    - **Compute the loss**:
        - `loss_out, policy_loss, value_loss = self._loss(...)`
    - **Zero out gradients** before backpropagation:
        - `self._optimizers[0].zero_grad()`
    - **Backpropagate the loss**:
        - `loss_out.backward()`
    - **Process gradients** and update the optimizer:
        - `grad_process_info = self.extra_grad_process(self._optimizers[0], loss_out)`
        - `self._optimizers[0].step()`
    - **Track and return gradient information**:
        - `grad_info = self.extra_grad_info(train_batch)`
        - Update `grad_info` with additional stats (`grad_process_info`, total loss, etc.)
    - **End of learning batch processing**

---

### 总结：
1. **`postprocess_trajectory()`** 负责对每个回合的数据进行处理和整理，主要包括更新奖励、动作、MCTS策略等信息，并将它们添加到模型的缓冲区，最后生成带有折扣的奖励标签。
   
2. **`learn_on_batch()`** 则是从处理后的样本批次中进行 **学习**，包括计算损失、反向传播、更新梯度等。它是模型更新的核心部分，在每一批次数据上进行训练。

这两个函数紧密协作：`postprocess_trajectory()` 提供训练数据，`learn_on_batch()` 在这些数据上进行学习。



### **`postprocess_trajectory()` 和 `learn_on_batch()` 中涉及的函数和方法总结：**

---

### **1. `postprocess_trajectory()` 函数中用到的函数和方法：**

#### **外部函数和方法：**

- **`episode._agent_reward_history[f'player_{model_id+1}'][-1]`**  
    - 获取指定玩家的最新奖励历史，`_agent_reward_history` 是一个字典，记录了每个玩家的历史奖励。
    
- **`other_agent_batches[f'player_{model_id+1}'][1]['actions']`**  
    - 从 `other_agent_batches` 中获取特定玩家的动作记录，`other_agent_batches` 是一个字典，包含了其他玩家的观察和动作信息。

- **`episode._agent_to_last_action[f'player_{model_id+1}']`**  
    - 获取该玩家最后一次执行的动作（例如：5 代表 stag，6 代表 hare），`_agent_to_last_action` 是一个字典，用于跟踪每个玩家的最后行动。

- **`episode.user_data[f"s_a_pair{model_id+1}"]`**  
    - 访问 `user_data` 字典中的 `s_a_pair` 数据，这通常包含了状态和动作的配对数据。

- **`np.array(episode.user_data[f"mcts_policies{self.my_id}"])[sample_batch["t"]]`**  
    - 从 `episode.user_data` 中提取当前玩家（`self.my_id`）的 MCTS 策略，并将其添加到 `sample_batch` 中。

- **`self.r2_buffer.add_reward(final_reward)`**  
    - 如果启用了 `r2`，则将最终奖励添加到 `r2_buffer` 中，这样可以对奖励进行归一化。

- **`self.r2_buffer.normalize(final_reward)`**  
    - 归一化奖励，通常用于强化学习中的奖励规范化，避免奖励数值过大或过小对训练造成不稳定影响。

#### **内部函数和方法：**

- **`self.model_buffer[i]`**  
    - 用于存储模型的缓冲区，记录每个模型的观察、动作和时间等信息。
  
- **`self.discounted_stag_times[place_id]`**  
    - 更新当前玩家对应位置的折扣化 stag 时间。根据玩家的动作类型（stag 或 hare），分别更新其对应的时间。

- **`self.discounted_hare_times[place_id]`**  
    - 同上，更新 hare 对应的折扣化时间。

- **`self.moa_update(i)`**  
    - 如果模型的缓冲区容量超过了 `moa_buffer_capacity`，调用此方法更新模型。

- **`sample_batch["value_label"]`**  
    - 为样本批次中的每个时间步分配一个值标签。最终奖励（`final_reward`）会根据折扣因子（`gamma`）递减分配给每个时间步。

#### **数据处理逻辑：**
- 通过对每个玩家进行迭代，根据玩家的奖励和最后的动作类型（stag 或 hare）更新缓冲区。
- 计算并更新每个玩家的折扣化时间和 MCTS 策略。
- 如果启用了 `r2`，则归一化最终奖励，并将其添加到样本批次中。
  
---

### **2. `learn_on_batch()` 函数中用到的函数和方法：**

#### **外部函数和方法：**

- **`self._lazy_tensor_dict(postprocessed_batch)`**  
    - 将 `postprocessed_batch` 转换为张量字典，通常用于后续的训练过程。通过这个函数，数据批次变成张量格式，以便与模型交互。

- **`self._loss(self, self.model, self.dist_class, train_batch)`**  
    - 计算损失函数，包括策略损失（`policy_loss`）和值损失（`value_loss`）。该方法将模型、分布类和训练批次作为输入，返回损失相关的数值。

- **`self._optimizers[0].zero_grad()`**  
    - 清空梯度，通常在每次反向传播前调用，避免梯度累积。

- **`loss_out.backward()`**  
    - 执行反向传播，计算模型的梯度。

- **`self.extra_grad_process(self._optimizers[0], loss_out)`**  
    - 在反向传播后进行梯度处理，可能包括一些额外的优化步骤。

- **`self._optimizers[0].step()`**  
    - 执行优化步骤，更新模型的权重。

- **`self.extra_grad_info(train_batch)`**  
    - 计算和获取额外的梯度信息，用于分析和调试。

- **`grad_info.update()`**  
    - 更新并合并梯度信息，包括损失、策略损失和值损失等。

#### **内部函数和方法：**

- **`self.learn_count`**  
    - 记录学习的次数，用于控制训练频率或调节某些行为。

- **`LEARNER_STATS_KEY`**  
    - 用于返回学习过程中的统计信息。

#### **数据处理逻辑：**
- 在 `learn_on_batch()` 中，首先计算损失值，并使用反向传播更新模型的参数。
- 使用优化器（`self._optimizers[0]`）对模型进行训练，并跟踪梯度信息。
- 更新统计信息，并返回训练过程中的统计数据。

---

### **总结：**

这两个函数涉及多个操作，其中 `postprocess_trajectory()` 主要负责处理来自环境的轨迹数据，更新缓冲区并准备好样本批次。而 `learn_on_batch()` 则是利用这些数据进行模型的训练，通过计算损失、反向传播和优化步骤来更新模型参数。

它们的主要区别在于：  
- **`postprocess_trajectory()`** 更注重对环境数据的后处理与缓冲区的管理。
- **`learn_on_batch()`** 则专注于实际的模型训练过程，执行损失计算和优化步骤。