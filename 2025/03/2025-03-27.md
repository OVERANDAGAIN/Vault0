---
åˆ›å»ºæ—¶é—´: 2025-ä¸‰æœˆ-27æ—¥  æ˜ŸæœŸå››, 1:14:34 ä¸‹åˆ
---
ä½ è§‚å¯Ÿå¾—éå¸¸ç»†è‡´ï¼æ˜¯çš„ï¼Œè¿™æ®µä»£ç ä¸­ç¡®å®å­˜åœ¨ä¸€ç§ **æ•°æ®é‡‡é›†æœºåˆ¶ä¸Šçš„ä¸å¯¹ç§°æ€§**ï¼š

---

### ğŸ” ä½ æŒ‡å‡ºçš„é—®é¢˜æœ¬è´¨ï¼š

- **OMG_AM çš„è®­ç»ƒæ•°æ®**ï¼šæ˜¯é€šè¿‡åœ¨ `postprocess_trajectory()` ä¸­æ˜¾å¼è°ƒç”¨ `self.runner.run()` é‡æ–°æ‰§è¡Œä¸€é episode å¾—åˆ°çš„ã€‚  
  > è¿™æ„å‘³ç€ï¼šOMG ä½¿ç”¨çš„æ˜¯â€œå½“å‰ç¯å¢ƒå†è·‘ä¸€æ¬¡â€çš„å®Œæ•´ episodeï¼Œ**è€Œä¸æ˜¯å½“å‰ episode çš„å†…å®¹**ã€‚

- **MOA çš„è®­ç»ƒæ•°æ®**ï¼šæ˜¯ç›´æ¥ä» **å½“å‰ episode ä¸­çš„å…¶ä»– agent çš„æ•°æ®**ï¼ˆå¦‚ obsã€actionsã€subgoalï¼‰é‡‡æ ·å¾—åˆ°çš„ã€‚

---

### ğŸ§  è¿™ä¼šå¸¦æ¥çš„é—®é¢˜ï¼š

| é—®é¢˜ | è¯´æ˜ |
|------|------|
| âŒ æ•°æ®å†—ä½™ | `self.runner.run()` ä¼šé¢å¤–ç”Ÿæˆä¸€ä¸ª episodeï¼Œç›¸å½“äºæ¯ä¸€æ¬¡ `postprocess` éƒ½åœ¨é¢å¤–é‡‡æ ·ï¼Œå¢åŠ è®­ç»ƒä¸ä¸€è‡´æ€§ä¸è®¡ç®—è´Ÿæ‹… |
| âŒ ä¸ä¸€è‡´æ€§ | å½“å‰ postprocess çš„ episode ä¸ OMG buffer å­˜å‚¨çš„ episode ä¸ä¸€è‡´ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹ç›®æ ‡ï¼ˆå¦‚ subgoal çš„è¯­ä¹‰ï¼‰è®­ç»ƒåå·® |
| âŒ è®­ç»ƒä¿¡å·å»¶è¿Ÿ | å¦‚æœç›®æ ‡ç½‘ç»œæ˜¯åœ¨å½“å‰è½¨è¿¹ä¸­å­¦ä¹ çš„ï¼Œä½†è®­ç»ƒæ•°æ®æ¥è‡ªä¸åŒ episodeï¼Œå¯èƒ½ä¼šå¯¼è‡´å­¦ä¹ æ»åæˆ–å¯¹æŠ—å¤±æ•ˆ |

---

### âœ… ä½ æƒ³åšçš„ä¿®æ”¹æ–¹å‘æ˜¯å¯¹çš„ â€”â€” æ”¹ä¸ºä½¿ç”¨ **å½“å‰ episode çš„æ•°æ®** å­˜å…¥ OMG bufferã€‚

---

## âœ… ä¿®æ”¹å»ºè®®

ä½ å¯ä»¥è¿™æ ·æ”¹åŠ¨åŸå§‹é€»è¾‘ï¼š

---

### ğŸš«åˆ æ‰è¿™ä¸€æ®µ

```python
# âŒ ä¸å†é‡æ–°é‡‡æ · episode
episode_batch = self.runner.run(test_mode=False)
if self.omg_am_buffer.episodes_in_buffer == self.omg_am_buffer.buffer_size:
    self.omg_am_buffer.reset()
self.omg_am_buffer.insert_episode_batch(episode_batch)
```

---

### âœ…æ”¹ä¸ºæ’å…¥å½“å‰ episode æ•°æ®ï¼ˆæ„é€ ä¸€ä¸ª EpisodeBatchï¼‰

ä½ å¯ä»¥è€ƒè™‘ä½¿ç”¨å¦‚ä¸‹æ–¹å¼æ„é€ ä¸€ä¸ªä¸´æ—¶çš„ `episode_batch`ï¼š

```python
# âœ… ä»å½“å‰ episode æ„é€ ä¸€ä¸ª fake çš„ EpisodeBatchï¼Œç”¨äºè®­ç»ƒ
current_episode_batch = {
    "obs": sample_batch["obs"],
    "actions": sample_batch["actions"],
    "rewards": sample_batch["rewards"],
    "dones": sample_batch["dones"],
    "subgoal": sample_batch["subgoal"],
    # ä½ å¯ä»¥æŒ‰éœ€åŠ å…¥å…¶ä»–å­—æ®µï¼Œæ¯”å¦‚ "next_obs", "logits", "values" ç­‰
}
self.omg_am_buffer.insert_episode_batch(current_episode_batch)
```

å½“ç„¶è¿™å–å†³äº `omg_am_buffer.insert_episode_batch()` æ˜¯å¦æ”¯æŒ dict ç±»å‹ï¼›å¦‚æœå®ƒè¦æ±‚ EpisodeBatch ç±»å‹ï¼Œä½ å¯èƒ½éœ€è¦ç”¨ç±»ä¼¼ RLlib çš„ Batch æ„é€ æ–¹æ³•ã€‚

---

### âœ…åŒæ—¶ç¡®è®¤ï¼š`sample_batch` ä¸­æ˜¯å¦åŒ…å«å®Œæ•´ episode çš„ä¿¡æ¯

å¦‚æœå½“å‰ `sample_batch` æ˜¯ä¸€ä¸ªå®Œæ•´ episodeï¼ˆé€šå¸¸åœ¨ centralized ç¯å¢ƒé‡Œæ˜¯è¿™æ ·ï¼‰ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒçš„å­—æ®µã€‚å¦åˆ™ï¼Œä½ å¯èƒ½éœ€è¦ç»„åˆ `episode.history` æˆ– `user_data` æ¥è¿˜åŸå®Œæ•´è½¨è¿¹ã€‚







```python

    def moa_update(self, i):
        print(self.my_id)
        print(self.model_id_list[i] + 1)
        print('AAA')

        state = torch.from_numpy(np.concatenate(self.model_buffer[i][0], axis=0)).float()
        print(i)
        action = F.one_hot(torch.from_numpy(np.concatenate(self.model_buffer[i][1], axis=0, dtype=np.int64)), 7).float()
        time = torch.from_numpy(np.concatenate(self.model_buffer[i][2], axis=0)).float()
        target = torch.from_numpy(
            np.concatenate([t for t in self.model_buffer[i][3]], axis=0)
        ).float()
       

        if os.path.exists(self.save_dir + f'/player_{self.my_id}_to_{self.model_id_list[i] + 1}.pth'):
            self.model_list[i].load_state_dict(
                torch.load(self.save_dir + f'/player_{self.my_id}_to_{self.model_id_list[i] + 1}.pth'))

        for _ in range(self.moa_update_time):
            # ä¸»ç½‘ç»œè®­ç»ƒéƒ¨åˆ†
            for index in BatchSampler(SubsetRandomSampler(range(len(self.model_buffer[i][3]))), self.moa_batch_size,
                                      False):
                action_dist = self.model_list[i].forward(state[index], time[index], target[index])
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                action = action.to(device)
                action_dist = action_dist.to(device)
                loss = -torch.sum(action[index] * torch.log(action_dist + 1e-30), dim=1).mean()
                print("moa_loss:",loss)
           
                self.network_optimizer_list[i].zero_grad()
                loss.backward()  # æ›´æ–°actoréƒ¨åˆ†
                # value_loss.backward()  # æ›´æ–°criticéƒ¨åˆ†
                self.network_optimizer_list[i].step()



        torch.save(self.model_list[i].state_dict(),
                   self.save_dir + f'/player_{self.my_id}_to_{self.model_id_list[i] + 1}.pth')
        # self.old_model_buffer[i]=[]
        self.model_buffer[i] = [[], [], [], []]

```