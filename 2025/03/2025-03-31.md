---
创建时间: 2025-三月-31日  星期一, 2:10:25 下午
---

# 一堆控制变量，变量： 

1.  `omg_buffer` 相关
	1. 定期清除 `buffer`  内的历史轨迹，更新
	2. 写为和 `moa_buffer` 类似的处理 $\Longrightarrow$ ==合并==
2. 重建损失 `recon_loss`  的优化： 
	1. 目标维度变化 $64$ $\Longrightarrow$ $16$ $\Longrightarrow$ $4$ $\Longrightarrow$ $2$ ，使得训练 `loss` 没那么 低 且 不明显
3. `MCTS` 树的数量变化 $\Longrightarrow$ 训练更鲁棒  $1$ $\Longrightarrow$ $5$
	-  可用 `for` 循环处理多棵树，不一定用 `pool` (因为后者只能使用CPU)


# goal_dim 由的变化： 




# tree_num  改为 5 
```markdown
`goal` 维度 ： 64 - > 16

`tree_num` : 1 - > 5 

`vae` : offline rule based agents (3.24)

`reward` : 4.5 +- 0.5

```

![[episode_vs_policy_rewards.png]]


# 

- **OMG_AM 的训练数据**：是通过在 `postprocess_trajectory()` 中显式调用 `self.runner.run()` 重新执行一遍 episode 得到的。  
  > 这意味着：OMG 使用的是“当前环境再跑一次”的完整 episode，**而不是当前 episode 的内容**。

- **MOA 的训练数据**：是直接从 **当前 episode 中的其他 agent 的数据**（如 obs、actions、subgoal）采样得到的。

---

### 🧠 这会带来的问题：

| 问题 | 说明 |
|------|------|
| ❌ 数据冗余 | `self.runner.run()` 会额外生成一个 episode，相当于每一次 `postprocess` 都在额外采样，增加训练不一致性与计算负担 |
| ❌ 不一致性 | 当前 postprocess 的 episode 与 OMG buffer 存储的 episode 不一致，可能导致模型目标（如 subgoal 的语义）训练偏差 |
| ❌ 训练信号延迟 | 如果目标网络是在当前轨迹中学习的，但训练数据来自不同 episode，可能会导致学习滞后或对抗失效 |

---

###  改为使用 **当前 episode 的数据** 存入 OMG buffer。










## ✅ 先回顾原始 HOP 中的两个模块含义

| 模块 | 含义 | 更新频率 | 核心作用 |
|------|------|------------|-----------|
| **Intra-OM** | 当前 episode 内，基于轨迹实时更新 belief | 每一步 | 快速适应对手目标变化 |
| **Inter-OM** | 跨 episode，总结过去 episode 中 co-player 的目标偏好（prior） | 每个 episode 起点 | 提供合理的先验 |

---

## ✅ 现在你提的两个模块是：

| 模块 | 你使用的机制 | 想类比谁 |
|------|---------------|-----------|
| RNN encoder（cond_x） | 编码当前 episode 的历史轨迹 $h_{0:t-1}$ → cond_x | 类似 intra-OM |
| Subgoal selector（hindsight） | 从过去 episode 的未来片段选 subgoal → $\bar{g}$ | 类似 inter-OM |

---

## ✅ 类比上是合理的！但有些本质差异，值得特别注意

---

### 🎯 RNN ≈ Intra-OM（大体合理）

**相似之处**：
- 都在 episode 内更新；
- 都试图捕捉目标在当前 episode 中是如何变化的；
- cond_x 类似于 belief 的 context embedding。

**不同点**：
- 原 intra-OM 是一个 **显式的 belief** over discrete goals；
- 而 RNN 是一个 **隐式的表示**，没有输出概率分布；
- Intra-OM 是可以贝叶斯更新的，而 RNN 是前馈 / 顺序更新，无法控制内部权重代表什么目标。

✅ 所以我们说：**RNN 是 intra-OM 的表征化近似**，而非功能对等体。

---

### 🎯 Subgoal Selector ≈ Inter-OM（差别更大）

**相似之处**：
- 都使用过去 episode 的信息（subgoal selector 向后看未来状态）；
- 都提供了一种 **episode 级别目标表示**；
- 都是用于帮助初始化目标的表示或 belief。

**关键不同**：

| 原 Inter-OM | Subgoal Selector |
|-------------|------------------|
| 总结多个 past episodes 的目标统计（如抓了几次鹿） | 只从当前 episode 向后选择 |
| 明确建模目标的概率分布 $p(g)$ | 输出一个目标 embedding，没有分布含义 |
| 为 belief 提供可贝叶斯的先验 | 提供的是训练 signal，而不是推理先验 |

也就是说：

> ✅ subgoal selector 更像是一个“teacher signal” —— 用于训练目标的 ground truth，不直接用于模型推理。
>
> ❌ 而原来的 inter-OM 是 **policy 推理用的先验 belief**，是真正参与在线行为建模的。







