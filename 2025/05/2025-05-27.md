å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **5.1 éƒ¨åˆ†ï¼ˆHow: Goal Inferenceï¼‰** çš„è‹±æ–‡ PPT å†…å®¹ä¸è®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 5: **How: Goal Inference Module** *(\~2 min)*

**Slide Title**:
**How: Inferring Opponent Goals from Interaction Trajectories**

**Slide Content**:

ğŸ“Œ **Model Design**
We use a **Conditional Variational Autoencoder (CVAE)** to learn a **latent goal distribution**

* Encoder: $q_\phi(g \mid \tau, s)$
* Prior: $p_\psi(g \mid s)$
* Decoder: $p_\gamma(a \mid g, s)$

---

ğŸ” **Inputs**

* Trajectory segment: $\tau = \{ (s_t, a_t) \}_{t=1}^T$
* Current state: $s_t$

ğŸ“¤ **Outputs**

* Latent goal embedding $g \in \mathbb{R}^d$
* Used for: downstream **planning** and **goal-conditioned policy learning**

---

ğŸ“ˆ **Advantages**

* Learns **diverse and abstract** goals
* Supports **few-shot generalization** to unseen behaviors
* Goal inference is **differentiable and efficient**

---

**Optional Visual**:

* A schematic diagram of the CVAE: showing encoder â†’ latent $g$ â†’ decoder â†’ predicted action

---

**Script (what to say)**:

> The core of our approach lies in the **Goal Inference Module**, which is implemented using a Conditional Variational Autoencoder.

> This model learns to infer a latent **goal representation** $g$ conditioned on a short trajectory segment $\tau$ and the current state $s$. The encoder approximates the posterior $q_\phi(g \mid \tau, s)$, while a separate prior network $p_\psi(g \mid s)$ provides regularization. The decoder then learns to reconstruct the action distribution conditioned on the goal and state.

> The inferred goal is not a symbolic label, but a **continuous vector** in latent space, which captures abstract behavioral intent. It serves as an internal representation that guides planning and policy modules.

> This setup enables the agent to reason about **unseen opponent behaviors** by generalizing in the goal space, which is especially useful in **few-shot adaptation** settings.

---

å¦‚æœä½ å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æ’°å†™ **Slide 5.2ï¼šFull Framework Diagram & Modules Explanation** çš„å†…å®¹ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ


Here is the English version of the module description based on the image you provided:

---

### Goal Inference Module

* **Input**: Historical interaction **trajectories** of the opponent
* **Output**: **Latent goal** representation
* **Framework**: A conditional variational autoencoder (CVAE) is adopted to map trajectories to latent goal representations.

This module models the mapping from interaction history to opponent intentions via three key probability distributions:

* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

The objective function: maximize the log-likelihood while minimizing the KL divergence between the posterior and prior:

$$
\langle \hat{\gamma}, \hat{\phi} \rangle = \arg \max_{\gamma, \phi} \, \mathbb{E}_{q_\phi(\hat{g}_t \mid \tau_t, s_t)} \left[ \log p_\gamma(s_t \mid \hat{g}_t, \tau_t) \right] - \text{KL} \left( q_\phi(\hat{g}_t \mid \tau_t, s_t) \,\|\, p_\psi(\hat{g}_t \mid s_t^g) \right)
$$

This optimization ensures that the inferred goals are both semantically aligned with observed behaviors and capable of predicting future trends. By replacing a manually defined goal set with a data-driven latent representation, the module enables more flexible and adaptive decision-making in multi-agent environments.

---

Let me know if you'd like to integrate this into a larger section of your thesis or presentation.
