---
创建时间: 2025-五月-27日  星期二, 5:20:32 下午
---

### Goal Inference Module

* **Input**: Historical interaction **trajectories** of the opponent
* **Output**: **Latent goal** representation
* **Framework**: A conditional variational autoencoder (CVAE) is adopted to map trajectories to latent goal representations.

This module models the mapping from interaction history to opponent intentions via three key probability distributions:

* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

The objective function: maximize the log-likelihood while minimizing the KL divergence between the posterior and prior:

$$
\langle \hat{\gamma}, \hat{\phi} \rangle = \arg \max_{\gamma, \phi} \, \mathbb{E}_{q_\phi(\hat{g}_t \mid \tau_t, s_t)} \left[ \log p_\gamma(s_t \mid \hat{g}_t, \tau_t) \right] - \text{KL} \left( q_\phi(\hat{g}_t \mid \tau_t, s_t) \,\|\, p_\psi(\hat{g}_t \mid s_t^g) \right)
$$


好的，以下是第 **5.2 部分：框架图与模块说明（Framework Overview & Modules Explanation）** 的英文 PPT 内容与讲稿建议：

---

### ✅ Slide 6: **Full Framework Overview**

**Slide Title**:
**HOP+: Hierarchical Opponent Modeling with Goal Reasoning**

---

**Slide Content**:

🧠 **Main Components** (modular view):

1. **Goal Inference Module (CVAE)**
   🔸 Infers latent goal $g \sim q_\phi(g \mid \tau, s)$ from short opponent trajectory
   🔸 Prior $p_\psi(g \mid s)$ provides training regularization

2. **Planning Module (MCTS)**
   🔸 Simulates future interactions using inferred opponent goals
   🔸 Computes action values $Q(s, a_i, g_{-i})$
   🔸 Agent selects action via Boltzmann over value estimates

3. **Policy Module (Goal-conditioned)**
   🔸 Learns low-level actions conditioned on agent’s state and goal
   🔸 Optimized via interaction trajectories

---

📊 **Workflow Pipeline**:

```
Opponent Trajectory → Goal Inference → MCTS Planning → Action Selection
                                              ↓
                                        Goal-conditioned Policy Update
```

---

**Optional Visual**:

* 放置项目中使用的框架结构图，如 Fig. 3 或者手绘模块箭头图
* 每个模块旁边加上标注：input/output 简介

---

**Script (what to说)**:

> This slide presents the **full framework** of HOP+, which integrates goal inference, planning, and learning in a hierarchical structure.

> First, the **Goal Inference Module** uses a CVAE to infer latent goal embeddings based on observed opponent behavior. These goals reflect the **intended sub-objectives** that opponents are likely pursuing.

> Then, these inferred goals are used in the **MCTS-based Planning Module**, where we simulate possible opponent behaviors and evaluate different responses. Instead of relying on predefined rewards or rule-based heuristics, our planner reasons in the latent goal space.

> Finally, we use a **goal-conditioned policy** to execute the selected action, and also update this policy based on rollout results.

> The hierarchical separation of goal inference, planning, and execution allows our method to maintain **interpretability**, **sample efficiency**, and **generalization ability** across different opponents and environments.

---

如需继续，我可以为你撰写 **Slide 5.3：实验结果展示（Self-play & Few-shot adaptation）**。是否继续？



好的，以下是第 **5.3 部分：效果展示（Experimental Results: Self-play & Few-shot Adaptation）** 的英文 PPT 内容与讲稿建议：

---

### ✅ Slide 7: **Experimental Results (Self-Play & Adaptation)**

**Slide Title**:
**Performance in Mixed-Motive Environments**

---

**Slide Content**:

🧪 **Evaluation Settings**:

* **Self-Play Setting**:
	* All agents trained from scratch with the same architecture; 
	* measure learning stability and cooperation emergence.

* **Few-Shot Adaptation**:
	* Evaluate agent's ability to quickly adapt to **unseen opponents** within limited episodes.

📈 **Metrics**:

* Average episodic reward


---

**📊 Experimental Comparison**:

| Method      | Self-Play Reward | Few-Shot Reward vs. Random | vs. NH | vs. NS |
| ----------- | ---------------- | -------------------------- | ------ | ------ |
| HOP+ (Ours) | **3.4**          | **3.2**                    | 2.9    | 2.95   |
| Direct-OM   | 3.1              | 2.7                        | 2.9    | 2.95   |
| A3C         | 2.9              | 1.2                        | 1.5    | 1.6    |
| LOLA        | 3.1              | 2.4                        | 2.6    | 2.7    |

---

🧩 **Key Observations**:

* HOP+ achieves **higher reward** and **faster convergence** in both settings.
* It demonstrates **strong generalization** to new policies without retraining.
* Especially effective in handling opponents with **diverse latent goals**.

---

**Script (what to say)**:

> We evaluate HOP+ under two key scenarios: **Self-Play** and **Few-Shot Adaptation**.

> In the self-play setting, all agents are initialized identically and co-learn from scratch. The goal is to verify whether the agents can **coordinate autonomously** without prior knowledge. HOP+ converges faster and achieves higher final rewards, indicating its strong learning stability.

> The more challenging setting is **Few-Shot Adaptation**: Here, our focal agent must adapt to unseen opponents with unknown behavior within just 50 episodes. HOP+ significantly outperforms baselines like Direct-OM and LOLA, showing **better few-shot generalization**.

> These results confirm the effectiveness of integrating goal inference into the opponent modeling and planning process.

---

**Optional Figures**:

* 柱状图展示 reward 对比
* Learning curve 图（reward vs. episode）

如果你有实验曲线或截图，可以补上；否则用表格展示就足够清晰。

---

是否继续进入 **Slide 5.4 展望（Future Work）** 部分？


以下是 **Slide 5.4：Future Work（未来展望）** 的英文 PPT 内容及讲稿段落，结构紧凑清晰，适用于最终总结：

---

### ✅ Slide 8: **Future Work**

**Slide Title**:
**Future Directions**

---

**Slide Content**:

🔍 **1. Partial Observability**

* Current MCTS planning assumes full observability of other agents’ states.
* explore  **POMDP-based planning** to address this limitation.

⚙️ **2. Computational Efficiency**

* MCTS is effective but computationally expensive in multi-agent settings.
* plan to investigate **lightweight approximations** or **learned planners** for better scalability.


---

**Script (what to say)**:

> While HOP+ demonstrates strong adaptability and generalization, several challenges remain for future work.

> First, our current method relies on full state observability for planning. In real-world environments, this is rarely the case. We aim to extend HOP+ to **partially observable scenarios**, using **belief inference** and **uncertainty-aware planning** to make robust decisions with incomplete information.

> Second, **computational cost** remains a concern. Although MCTS is powerful, it's not always scalable in real-time or large-agent settings. In future work, we will explore alternatives such as **differentiable planners** or **learned value-guided rollouts** to reduce complexity.

> Finally, we plan to evaluate HOP+ in **human-in-the-loop environments**, which pose new challenges such as **strategy non-stationarity**, unpredictability, and the need for interpretability. These efforts would push our method closer to practical deployment in real-world multi-agent systems.

---

**Optional Visuals**:

* 一个“future roadmap”小图：三个方框分别标注 Partial Observability, Efficiency, Human Settings
* 一张 POMDP belief 更新流程图（可选）

---

接下来你可以进入答辩PPT的 **最后一页 Slide 9：Thank You & Q\&A** 部分，是否继续？
