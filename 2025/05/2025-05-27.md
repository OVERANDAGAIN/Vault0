---
åˆ›å»ºæ—¶é—´: 2025-äº”æœˆ-27æ—¥  æ˜ŸæœŸäºŒ, 5:20:32 ä¸‹åˆ
---

### Goal Inference Module

* **Input**: Historical interaction **trajectories** of the opponent
* **Output**: **Latent goal** representation
* **Framework**: A conditional variational autoencoder (CVAE) is adopted to map trajectories to latent goal representations.

This module models the mapping from interaction history to opponent intentions via three key probability distributions:

* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

The objective function: maximize the log-likelihood while minimizing the KL divergence between the posterior and prior:

$$
\langle \hat{\gamma}, \hat{\phi} \rangle = \arg \max_{\gamma, \phi} \, \mathbb{E}_{q_\phi(\hat{g}_t \mid \tau_t, s_t)} \left[ \log p_\gamma(s_t \mid \hat{g}_t, \tau_t) \right] - \text{KL} \left( q_\phi(\hat{g}_t \mid \tau_t, s_t) \,\|\, p_\psi(\hat{g}_t \mid s_t^g) \right)
$$


å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **5.2 éƒ¨åˆ†ï¼šæ¡†æ¶å›¾ä¸æ¨¡å—è¯´æ˜ï¼ˆFramework Overview & Modules Explanationï¼‰** çš„è‹±æ–‡ PPT å†…å®¹ä¸è®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 6: **Full Framework Overview**

**Slide Title**:
**HOP+: Hierarchical Opponent Modeling with Goal Reasoning**

---

**Slide Content**:

ğŸ§  **Main Components** (modular view):

1. **Goal Inference Module (CVAE)**
   ğŸ”¸ Infers latent goal $g \sim q_\phi(g \mid \tau, s)$ from short opponent trajectory
   ğŸ”¸ Prior $p_\psi(g \mid s)$ provides training regularization

2. **Planning Module (MCTS)**
   ğŸ”¸ Simulates future interactions using inferred opponent goals
   ğŸ”¸ Computes action values $Q(s, a_i, g_{-i})$
   ğŸ”¸ Agent selects action via Boltzmann over value estimates

3. **Policy Module (Goal-conditioned)**
   ğŸ”¸ Learns low-level actions conditioned on agentâ€™s state and goal
   ğŸ”¸ Optimized via interaction trajectories

---

ğŸ“Š **Workflow Pipeline**:

```
Opponent Trajectory â†’ Goal Inference â†’ MCTS Planning â†’ Action Selection
                                              â†“
                                        Goal-conditioned Policy Update
```

---

**Optional Visual**:

* æ”¾ç½®é¡¹ç›®ä¸­ä½¿ç”¨çš„æ¡†æ¶ç»“æ„å›¾ï¼Œå¦‚ Fig. 3 æˆ–è€…æ‰‹ç»˜æ¨¡å—ç®­å¤´å›¾
* æ¯ä¸ªæ¨¡å—æ—è¾¹åŠ ä¸Šæ ‡æ³¨ï¼šinput/output ç®€ä»‹

---

**Script (what toè¯´)**:

> This slide presents the **full framework** of HOP+, which integrates goal inference, planning, and learning in a hierarchical structure.

> First, the **Goal Inference Module** uses a CVAE to infer latent goal embeddings based on observed opponent behavior. These goals reflect the **intended sub-objectives** that opponents are likely pursuing.

> Then, these inferred goals are used in the **MCTS-based Planning Module**, where we simulate possible opponent behaviors and evaluate different responses. Instead of relying on predefined rewards or rule-based heuristics, our planner reasons in the latent goal space.

> Finally, we use a **goal-conditioned policy** to execute the selected action, and also update this policy based on rollout results.

> The hierarchical separation of goal inference, planning, and execution allows our method to maintain **interpretability**, **sample efficiency**, and **generalization ability** across different opponents and environments.

---

å¦‚éœ€ç»§ç»­ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æ’°å†™ **Slide 5.3ï¼šå®éªŒç»“æœå±•ç¤ºï¼ˆSelf-play & Few-shot adaptationï¼‰**ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ
