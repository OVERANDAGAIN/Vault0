---
åˆ›å»ºæ—¶é—´: 2025-äº”æœˆ-27æ—¥  æ˜ŸæœŸäºŒ, 5:20:32 ä¸‹åˆ
---

### Goal Inference Module

* **Input**: Historical interaction **trajectories** of the opponent
* **Output**: **Latent goal** representation
* **Framework**: A conditional variational autoencoder (CVAE) is adopted to map trajectories to latent goal representations.

This module models the mapping from interaction history to opponent intentions via three key probability distributions:

* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

The objective function: maximize the log-likelihood while minimizing the KL divergence between the posterior and prior:

$$
\langle \hat{\gamma}, \hat{\phi} \rangle = \arg \max_{\gamma, \phi} \, \mathbb{E}_{q_\phi(\hat{g}_t \mid \tau_t, s_t)} \left[ \log p_\gamma(s_t \mid \hat{g}_t, \tau_t) \right] - \text{KL} \left( q_\phi(\hat{g}_t \mid \tau_t, s_t) \,\|\, p_\psi(\hat{g}_t \mid s_t^g) \right)
$$


å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **5.2 éƒ¨åˆ†ï¼šæ¡†æ¶å›¾ä¸æ¨¡å—è¯´æ˜ï¼ˆFramework Overview & Modules Explanationï¼‰** çš„è‹±æ–‡ PPT å†…å®¹ä¸è®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 6: **Full Framework Overview**

**Slide Title**:
**HOP+: Hierarchical Opponent Modeling with Goal Reasoning**

---

**Slide Content**:

ğŸ§  **Main Components** (modular view):

1. **Goal Inference Module (CVAE)**
   ğŸ”¸ Infers latent goal $g \sim q_\phi(g \mid \tau, s)$ from short opponent trajectory
   ğŸ”¸ Prior $p_\psi(g \mid s)$ provides training regularization

2. **Planning Module (MCTS)**
   ğŸ”¸ Simulates future interactions using inferred opponent goals
   ğŸ”¸ Computes action values $Q(s, a_i, g_{-i})$
   ğŸ”¸ Agent selects action via Boltzmann over value estimates

3. **Policy Module (Goal-conditioned)**
   ğŸ”¸ Learns low-level actions conditioned on agentâ€™s state and goal
   ğŸ”¸ Optimized via interaction trajectories

---

ğŸ“Š **Workflow Pipeline**:

```
Opponent Trajectory â†’ Goal Inference â†’ MCTS Planning â†’ Action Selection
                                              â†“
                                        Goal-conditioned Policy Update
```

---

**Optional Visual**:

* æ”¾ç½®é¡¹ç›®ä¸­ä½¿ç”¨çš„æ¡†æ¶ç»“æ„å›¾ï¼Œå¦‚ Fig. 3 æˆ–è€…æ‰‹ç»˜æ¨¡å—ç®­å¤´å›¾
* æ¯ä¸ªæ¨¡å—æ—è¾¹åŠ ä¸Šæ ‡æ³¨ï¼šinput/output ç®€ä»‹

---

**Script (what toè¯´)**:

> This slide presents the **full framework** of HOP+, which integrates goal inference, planning, and learning in a hierarchical structure.

> First, the **Goal Inference Module** uses a CVAE to infer latent goal embeddings based on observed opponent behavior. These goals reflect the **intended sub-objectives** that opponents are likely pursuing.

> Then, these inferred goals are used in the **MCTS-based Planning Module**, where we simulate possible opponent behaviors and evaluate different responses. Instead of relying on predefined rewards or rule-based heuristics, our planner reasons in the latent goal space.

> Finally, we use a **goal-conditioned policy** to execute the selected action, and also update this policy based on rollout results.

> The hierarchical separation of goal inference, planning, and execution allows our method to maintain **interpretability**, **sample efficiency**, and **generalization ability** across different opponents and environments.

---

å¦‚éœ€ç»§ç»­ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æ’°å†™ **Slide 5.3ï¼šå®éªŒç»“æœå±•ç¤ºï¼ˆSelf-play & Few-shot adaptationï¼‰**ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ



å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **5.3 éƒ¨åˆ†ï¼šæ•ˆæœå±•ç¤ºï¼ˆExperimental Results: Self-play & Few-shot Adaptationï¼‰** çš„è‹±æ–‡ PPT å†…å®¹ä¸è®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 7: **Experimental Results (Self-Play & Adaptation)**

**Slide Title**:
**Performance in Mixed-Motive Environments**

---

**Slide Content**:

ğŸ§ª **Evaluation Settings**:

* **Self-Play Setting**:
	* All agents trained from scratch with the same architecture; 
	* measure learning stability and cooperation emergence.

* **Few-Shot Adaptation**:
	* Evaluate agent's ability to quickly adapt to **unseen opponents** within limited episodes.

ğŸ“ˆ **Metrics**:

* Average episodic reward


---

**ğŸ“Š Experimental Comparison**:

| Method      | Self-Play Reward | Few-Shot Reward vs. Random | vs. NH | vs. NS |
| ----------- | ---------------- | -------------------------- | ------ | ------ |
| HOP+ (Ours) | **3.4**          | **3.2**                    | 2.9    | 2.95   |
| Direct-OM   | 3.1              | 2.7                        | 2.9    | 2.95   |
| A3C         | 2.9              | 1.2                        | 1.5    | 1.6    |
| LOLA        | 3.1              | 2.4                        | 2.6    | 2.7    |

---

ğŸ§© **Key Observations**:

* HOP+ achieves **higher reward** and **faster convergence** in both settings.
* It demonstrates **strong generalization** to new policies without retraining.
* Especially effective in handling opponents with **diverse latent goals**.

---

**Script (what to say)**:

> We evaluate HOP+ under two key scenarios: **Self-Play** and **Few-Shot Adaptation**.

> In the self-play setting, all agents are initialized identically and co-learn from scratch. The goal is to verify whether the agents can **coordinate autonomously** without prior knowledge. HOP+ converges faster and achieves higher final rewards, indicating its strong learning stability.

> The more challenging setting is **Few-Shot Adaptation**: Here, our focal agent must adapt to unseen opponents with unknown behavior within just 50 episodes. HOP+ significantly outperforms baselines like Direct-OM and LOLA, showing **better few-shot generalization**.

> These results confirm the effectiveness of integrating goal inference into the opponent modeling and planning process.

---

**Optional Figures**:

* æŸ±çŠ¶å›¾å±•ç¤º reward å¯¹æ¯”
* Learning curve å›¾ï¼ˆreward vs. episodeï¼‰

å¦‚æœä½ æœ‰å®éªŒæ›²çº¿æˆ–æˆªå›¾ï¼Œå¯ä»¥è¡¥ä¸Šï¼›å¦åˆ™ç”¨è¡¨æ ¼å±•ç¤ºå°±è¶³å¤Ÿæ¸…æ™°ã€‚

---

æ˜¯å¦ç»§ç»­è¿›å…¥ **Slide 5.4 å±•æœ›ï¼ˆFuture Workï¼‰** éƒ¨åˆ†ï¼Ÿ


ä»¥ä¸‹æ˜¯ **Slide 5.4ï¼šFuture Workï¼ˆæœªæ¥å±•æœ›ï¼‰** çš„è‹±æ–‡ PPT å†…å®¹åŠè®²ç¨¿æ®µè½ï¼Œç»“æ„ç´§å‡‘æ¸…æ™°ï¼Œé€‚ç”¨äºæœ€ç»ˆæ€»ç»“ï¼š

---

### âœ… Slide 8: **Future Work**

**Slide Title**:
**Future Directions**

---

**Slide Content**:

ğŸ” **1. Partial Observability**

* Current MCTS planning assumes full observability of other agentsâ€™ states.
* explore  **POMDP-based planning** to address this limitation.

âš™ï¸ **2. Computational Efficiency**

* MCTS is effective but computationally expensive in multi-agent settings.
* plan to investigate **lightweight approximations**  for better scalability.


---

**Script (what to say)**:

> While HOP+ demonstrates strong adaptability and generalization, several challenges remain for future work.

> First, our current method relies on full state observability for planning. In real-world environments, this is rarely the case. We aim to extend HOP+ to **partially observable scenarios**, using **belief inference** and **uncertainty-aware planning** to make robust decisions with incomplete information.

> Second, **computational cost** remains a concern. Although MCTS is powerful, it's not always scalable in real-time or large-agent settings. In future work, we will explore alternatives such as **differentiable planners** or **learned value-guided rollouts** to reduce complexity.

> Finally, we plan to evaluate HOP+ in **human-in-the-loop environments**, which pose new challenges such as **strategy non-stationarity**, unpredictability, and the need for interpretability. These efforts would push our method closer to practical deployment in real-world multi-agent systems.

---

**Optional Visuals**:

* ä¸€ä¸ªâ€œfuture roadmapâ€å°å›¾ï¼šä¸‰ä¸ªæ–¹æ¡†åˆ†åˆ«æ ‡æ³¨ Partial Observability, Efficiency, Human Settings
* ä¸€å¼  POMDP belief æ›´æ–°æµç¨‹å›¾ï¼ˆå¯é€‰ï¼‰

---

æ¥ä¸‹æ¥ä½ å¯ä»¥è¿›å…¥ç­”è¾©PPTçš„ **æœ€åä¸€é¡µ Slide 9ï¼šThank You & Q\&A** éƒ¨åˆ†ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ

ä»¥ä¸‹æ˜¯ç¬¬ 1 å’Œç¬¬ 2 éƒ¨åˆ†çš„è‹±æ–‡ç‰ˆ PPT å†…å®¹ï¼ˆåŒ…æ‹¬å¹»ç¯ç‰‡å†…å®¹å’Œè®²è§£è‰ç¨¿ï¼‰ï¼Œéµå¾ª"Whatâ€“Whyâ€“How"æ¡†æ¶ï¼Œå¹¶æ§åˆ¶åœ¨é¢„æœŸæ—¶é—´èŒƒå›´å†…ã€‚

---

### âœ… Slide 1: **Title Slide** *(\~0.5 min)*

**Slide Title**:
**HOP+**
*A Goal-Inference-Based Opponent Modeling Framework for Mixed-Motive Multi-Agent Settings*

**Content**:

* **Presenter**: \[Your Name]
* **Affiliation**: \[University / Institute Name]
* **Date**: \[Presentation Date]

**Script (what to say)**:

> Good afternoon. Today I will present our recent work titled **HOP+**, which focuses on enhancing opponent modeling and planning in multi-agent mixed-motive environments. This work builds upon the HOP framework proposed at ICML 2024.

---

### âœ… Slide 2: **Background â€“ From HOP to HOP+** *(\~1.5 min)*

**Slide Title**:
**From HOP to HOP+: Motivation and Foundation**

**Slide Content**:

ğŸ§  **What is HOP?**

* HOP: *Hierarchical Opponent Modeling and Planning (ICML 2024)
* Designed for **mixed-motive multi-agent games**

ğŸ”§ **Core Components of HOP ( Inference + Decision Making )**:

1. **Opponent Modeling Module**
   * Infers latent goals of other agents
2. **Planning Module (MCTS-based)**
   * Computes best-response strategies given inferred goals

ğŸ”¥ **Why HOP Matters**:

* Effective performance in **self-play** and cooperative tasks
* Strong **few-shot adaptation** to unseen opponents

**Script (what to say)**:

> Our work extends the HOP framework, which models agent behavior in mixed-motive settings by separating goal inference and planning. HOP achieves this through a hierarchical architecture where opponent modeling guides the planning module. However, the original HOP uses a limited goal space and static inference mechanisms. Our work aims to push this further.

---

éœ€è¦å›¾ç¤ºå»ºè®®ï¼Ÿ

ä½ å¯ä»¥åœ¨ç¬¬ 2 é¡µåŠ ä¸€å¼ ç®€å›¾ï¼š
ğŸŸ¢ â€œAgent Interaction â†’ Opponent Modeling â†’ Goal â†’ MCTS Planning â†’ Actionâ€ è¿™æ ·ä¸€ä¸ªæµç¨‹ç®­å¤´å›¾ï¼Œæ¸…æ™°ä¼ è¾¾ HOP çš„å±‚æ¬¡ç»“æ„ã€‚

---

æ˜¯å¦ç»§ç»­ä¸ºä½ å†™ç¬¬ 3 éƒ¨åˆ†çš„è‹±æ–‡ç‰ˆ PPTï¼Ÿæˆ–è€…éœ€è¦æˆ‘ç”Ÿæˆä¸€ä»½å®Œæ•´å¯å¯¼å‡ºçš„ PowerPoint æ–‡ä»¶ï¼Ÿ


å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **3 éƒ¨åˆ†**ï¼ˆProblem Setting & Environmentï¼‰çš„è‹±æ–‡ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 3: **Problem Setting & Environment** *(\~2 min)*

**Slide Title**:
**Problem Setting: Mixed-Motive Multi-Agent Interaction**

**Slide Content**:

ğŸ¯ **Objective**

* Learn **adaptive agent strategies** in **dynamic, mixed-motive games**
* Enable agents to **model unseen opponents** and **respond effectively**

ğŸ§© **Key Challenges**:

* ğŸ¤ + âš”ï¸ Mixed motives: agents need to **cooperate** while **competing**
* â›“ï¸ Dynamic strategies â†’ **non-stationary environments**
* â“ How to infer opponentsâ€™ intent and respond in **few interactions**

ğŸŒ **Test Environment** â€“ *Stag-Hunt Game*

* Grid-based world (6Ã—6, 8Ã—8)
* Agents decide to **hunt individually** (hare) or **coordinate** (stag)
* Rewards depend on **joint actions** and **opponent behavior**

**Diagram** (recommended):

* A **grid world** image showing:

  * Two agents
  * Hares and stags
  * Decision outcomes: solo vs. cooperative




å¾ˆå¥½ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **4 éƒ¨åˆ†ï¼ˆMotivation: Why Goal Inference?ï¼‰** çš„ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼ˆè‹±æ–‡ï¼‰ï¼š

---

### âœ… Slide 4: **Motivation: Why Goal Inference?** *(\~2 min)*

**Slide Title**:
**Motivation: Limitations of Manual Goal Design**

**Slide Content**:

ğŸ“Œ **Problems with Prior Approaches**

*  **Static goal sets**:
	* Predefined discrete goals are **inflexible**
	* fail to capture **diverse strategies**
* **Manual annotation required**:
	* Goal semantics often need to be designed by hand
	* Handcrafted goals donâ€™t scale well to **unseen opponents**

---
ğŸ’¡ **Our Insight**
>  **infer goals automatically** from interaction trajectories.


ğŸ¯ **Benefits of Goal Inference**:

* âœ… **No manual goal engineering**
* âœ… **Continuous latent goal space**
* âœ… **Adaptation to diverse behaviors**
* âœ… **Improved interpretability & planning**

**Optional visual (if space allows)**:

* Diagram comparing:

  * "Manual goal set" (discrete, rigid)
  * "Learned goal representation" (continuous, flexible)

---

**Script (what to say)**:

> In previous opponent modeling frameworks, agents typically rely on a **static set of predefined goals**. These goals are manually constructed, often with limited expressiveness, and fail to adapt when the opponent behaves in unexpected ways.

> Such methods **do not generalize well** to heterogeneous opponents, and they require **significant human effort** to design appropriate goal spaces for each environment.

> To address this, our idea is to **infer latent goals directly** from trajectory data using deep generative modeling. By doing this, we can capture a **continuous and adaptive** goal space that evolves with interaction and requires **no manual labeling**. This provides a more powerful foundation for downstream planning and policy learning.

---

ä¸‹ä¸€éƒ¨åˆ†å°†æ˜¯ **5.1 - How: Goal Inference** çš„å®ç°ç»†èŠ‚ä»‹ç»ã€‚å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æ’°å†™è¯¥é¡µå†…å®¹ã€‚



æ˜ç™½äº†ï¼ä¸‹é¢æ˜¯**ç®€æ´ã€æ¸…æ™°ã€ç­”è¾©é€‚é…ç‰ˆçš„ Slide 3 è‹±æ–‡å†…å®¹**ï¼Œä¿æŒä¸“ä¸šè¡¨è¾¾ã€å›¾æ–‡ç»“åˆã€é‡ç‚¹çªå‡ºï¼ˆé€‚åˆ6åˆ†é’Ÿè®²ç¨¿åœºæ™¯ï¼‰ï¼š

---

### **Slide 3: Problem & Environment**

---

**ğŸ¯ Goal**
Enable agents to adapt in **mixed-motive** multi-agent games
â†’ Must **infer opponent intent** and **respond effectively**

**âš ï¸ Challenges**

* Mixed incentives: **cooperation** vs. **competition**
* Opponents behave **dynamically**, non-stationary
* Limited interactions for **adaptation**

**ğŸŒ Environment: Stag-Hunt Game** â€”â€” **MSH**

* Grid-based map (6Ã—6 or 8Ã—8)
* Choose:
  * ğŸ‡ Hunt hare (solo, low reward)
  * ğŸ¦Œ Hunt stag (team up, high reward)
* Reward depends on **joint actions**

---

**ğŸ“Š Visual (on right)**

* Grid world with:

  * Agents, hare, stag
  * Outcome arrows: solo hunt vs. team hunt

---

éœ€è¦æˆ‘å¸®ä½ åšè¿™ä¸ªå›¾å—ï¼Ÿæˆ–è€…æˆ‘ä»¬ç»§ç»­åˆ° Slide 4: **Motivation**ï¼Ÿ



å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç®€æ´ã€ä¸“ä¸šé£æ ¼çš„è‹±æ–‡ç‰ˆ Slide 3 å†…å®¹ï¼Œ**å»æ‰æ‰€æœ‰ icon è¡¨æƒ…ç¬¦å·ï¼Œå‡å°‘æ–‡å­—é‡ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹**ï¼Œé€‚åˆç­”è¾© PPTï¼š

---

### **Slide 3: Problem Setting and Environment**

---

**Objective**
Learn adaptive strategies in mixed-motive multi-agent environments.
Agents must model diverse opponents and respond effectively under uncertainty.

---

**Key Challenges**

* Mixed incentives: agents must balance cooperation and competition
* Dynamic interactions lead to non-stationary opponent behaviors
* Limited interactions require fast adaptation to unseen strategies

---

**Test Environment: Stag-Hunt Game**

* Grid-based world (6Ã—6 or 8Ã—8)
* Agents choose to hunt hare (individually) or stag (cooperatively)
* Rewards depend on joint actions and mutual coordination

---

**(Optional Visual)**
A simple grid-world diagram showing two agents, one hare, one stag, and outcome labels:

* Solo action â†’ lower reward
* Coordinated action â†’ higher reward

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆè¿™ä¸ªå›¾ã€‚æ˜¯å¦ç»§ç»­åˆ° Slide 4: Motivationï¼Ÿ
