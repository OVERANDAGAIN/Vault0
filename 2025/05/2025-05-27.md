好的，以下是第 **5.1 部分（How: Goal Inference）** 的英文 PPT 内容与讲稿建议：

---

### ✅ Slide 5: **How: Goal Inference Module** *(\~2 min)*

**Slide Title**:
**How: Inferring Opponent Goals from Interaction Trajectories**

**Slide Content**:

📌 **Model Design**
We use a **Conditional Variational Autoencoder (CVAE)** to learn a **latent goal distribution**

* Encoder: $q_\phi(g \mid \tau, s)$
* Prior: $p_\psi(g \mid s)$
* Decoder: $p_\gamma(a \mid g, s)$

---

🔍 **Inputs**

* Trajectory segment: $\tau = \{ (s_t, a_t) \}_{t=1}^T$
* Current state: $s_t$

📤 **Outputs**

* Latent goal embedding $g \in \mathbb{R}^d$
* Used for: downstream **planning** and **goal-conditioned policy learning**

---

📈 **Advantages**

* Learns **diverse and abstract** goals
* Supports **few-shot generalization** to unseen behaviors
* Goal inference is **differentiable and efficient**

---

**Optional Visual**:

* A schematic diagram of the CVAE: showing encoder → latent $g$ → decoder → predicted action

---

**Script (what to say)**:

> The core of our approach lies in the **Goal Inference Module**, which is implemented using a Conditional Variational Autoencoder.

> This model learns to infer a latent **goal representation** $g$ conditioned on a short trajectory segment $\tau$ and the current state $s$. The encoder approximates the posterior $q_\phi(g \mid \tau, s)$, while a separate prior network $p_\psi(g \mid s)$ provides regularization. The decoder then learns to reconstruct the action distribution conditioned on the goal and state.

> The inferred goal is not a symbolic label, but a **continuous vector** in latent space, which captures abstract behavioral intent. It serves as an internal representation that guides planning and policy modules.

> This setup enables the agent to reason about **unseen opponent behaviors** by generalizing in the goal space, which is especially useful in **few-shot adaptation** settings.

---

如果你已经准备好，我可以继续为你撰写 **Slide 5.2：Full Framework Diagram & Modules Explanation** 的内容。是否继续？
