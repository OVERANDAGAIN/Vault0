以下是第四章第一节 **“4.1 Policy Reconstruction（策略重建）”** 的完整中文翻译：

---

## 4.1 策略重建（Policy Reconstruction）

策略重建方法通过重构智能体的决策过程，生成能够对其动作进行显式预测的模型。大多数方法从某种初始或理想化模型出发，并根据被建模智能体的行为观察来“拟合”模型内部结构。这样得到的模型预测可供规划器使用，从而推理出被建模智能体在不同情境下可能采取的响应动作。例如，蒙特卡洛树搜索（Monte-Carlo Tree Search，Browne 等, 2012）可以自然地整合这些模型，对交互轨迹进行采样，并据此找到相对于该模型的最优动作。

策略重建方法的两个核心设计问题是：

1. **应使用交互历史中的哪些元素来做出预测？**
2. **应如何将这些元素映射为预测结果？**

接下来的各小节介绍了这些方法，并逐步从第一个问题过渡到第二个问题的探讨。

---

### 4.1.1 条件动作频率（Conditional Action Frequencies）

策略重建的典型例子是“虚构博弈”（fictitious play，Brown, 1951），其中智能体将彼此建模为一组动作上的概率分布。该分布通过最大似然估计来拟合，即简单地计算动作的平均频率。这种方法在矩阵博弈中具有良好的收敛性（Fudenberg 和 Levine, 1998），并早期被用于多智能体强化学习（Claus 和 Boutilier, 1998）。但单一分布无法捕捉依赖交互历史的复杂行为。要提升此方法的能力，关键在于**基于历史中的某些信息对动作分布进行条件化**。

例如：

* Sen 和 Arora（1997）、Banerjee 和 Sen（2007）提出：建模智能体可以学习对方在不同自身动作条件下的动作频率；
* Davison 和 Hirsh（1998）提出一种用户模型，学习用户在前一个命令的基础上发出下一个命令的条件概率；
* 更复杂的方法还可基于最近 $n$ 次所有智能体的联合动作来学习分布（Powers 和 Shoham, 2005）。

然而，学习条件动作分布的难点在于我们可能并不知道应选取历史中的哪些元素。若选得太少或不合适，则预测效果不佳；若选得太多，则学习效率低下。为此，有方法提出**自动选择条件变量**：

* Jensen 等（2005）提出：对最近 $n$ 个历史元素的所有子集分别学习动作频率。若某子集对应的条件分布熵太高（表示预测不确定性大），则剔除。最终选择最确定的子集进行预测。
* Chakraborty 和 Stone（2014）提出：从最近的 $n$、$n-1$、$n-2$... 个观察中选出能最好预测动作的“最小”条件集；
* 类似方法也可用于基于抽象特征向量的建模（Chakraborty 和 Stone, 2013）。

此类基于条件频率的方法也被用于**不完全信息的扩展形式博弈**（如扑克）：

* 由于玩家拥有私有信息（如手牌）和公共信息（如桌面牌），决策需在“信息集”中进行，即玩家无法区分的状态集合。
* 可以为每个信息集设立独立的 Dirichlet 分布（Southey 等, 2005），并在每次观察到对方动作后更新该分布；
* 也可以基于博弈的纳什均衡解初始化这些分布，然后逐步向实际观察到的动作频率调整（Ganzfried 和 Sandholm, 2011）；
* Billings 等（2004）提出方法：基于完整动作序列来学习频率，并通过一系列越来越粗的抽象级别实现泛化。此外，引入衰减因子，使得新近观察具有更大权重。

---

### 4.1.2 基于案例推理（Case-Based Reasoning）

上述方法的一个局限是缺乏将过往经验**泛化**到新情境的机制。虽然 Billings 等提出的抽象方法可部分实现泛化，但**基于案例推理**提供了更系统的方法。该方法使用相似性函数将当前观察与历史情境进行匹配：

* 系统维护一个“案例集”，记录过去每次情境与被建模智能体的动作；
* 预测新情境下的动作时，系统搜索最相似的案例，并依据相似度与时间新旧程度加权；
* 例如在机器人足球中，一个“案例”可由球场状态表示，而相似性函数衡量球与球员位置的差异；
* Albrecht 和 Ramamoorthy（2013）提出了上述方法的实现；
* Borck 等（2015）和 Hsieh 与 Sun（2008）也提出类似方法；
* 为了高效匹配与存储案例，有研究提出基于树的检索方法（Denzinger 和 Hamdan, 2004），或对案例集进行裁剪（Borck 等, 2015）；
* 另有研究试图自动优化相似性函数（Steffens, 2004a），例如将相似性表示为各属性差异的线性加权，并结合目标依赖网络进行学习（Ahmadi 等, 2003）。

---

### 4.1.3 紧凑模型表示（Compact Model Representations）

频率分布与案例推理虽通用，但空间复杂度高（如 $m^n$）。因此，研究者也探索了更紧凑的建模方式：

* **确定性有限自动机 DFA**：如 Carmel 和 Markovitch（1996c）通过观察到的动作逐步修改 DFA，使其与观察一致，并始终寻求最小模型；
* **决策树**与**神经网络**也被用于建模（Barrett 等, 2013；Silver 等, 2016；Davidson 等, 2000）；
* **缺失动作信息推理**：Ledezma 等（2009）提出用监督学习训练一系列分类器，从连续两个状态推断出对方动作，再预测下一动作；
* **不确定性场景**：Panella 和 Gmytrasiewicz（2017）提出用概率 DFA（PDFA）配合贝叶斯非参数方法建模；Mealing 和 Shapiro（2017）使用 EM 算法推断对方在扩展形式博弈中的信息集状态。

---

### 4.1.4 效用函数重建（Utility Reconstruction）

上述方法大多未考虑对方的**偏好或效用函数**。若能假设对方行为是理性地最大化某个效用函数，则可以从其行为反推此函数：

* Carmel 和 Markovitch（1993, 1996b）在博弈环境中提出方法：假设效用为状态特征的线性加权，采用爬山算法在多个深度下搜索最优参数组合；
* Chajewska 等（2001）提出：已知加权结构，目标是学习“子效用”函数，用轨迹生成线性约束，再用贝叶斯方法选择效用函数；
* Gal 等（2004）研究社会博弈中人类玩家的偏好，将效用建模为社会因素（如公平、公利）的线性组合，用 EM 与梯度上升训练；
* 在自动谈判领域，也有大量相关研究（Baarslag 等, 2016）：

  * 如 Hindriks 和 Tykhonov（2008）提出通过离散化候选权重与评价函数，并用贝叶斯更新学习对手效用；
  * Coehoorn 和 Jennings（2004）则采用核密度估计方法学习权重；
* 图模型中的效用重建方法将在第 4.6 节进一步讨论。

---

如需继续翻译 **4.2 基于类型的推理（Type-Based Reasoning）**，请随时告知。
