![[prevae.png]]


## 训练逻辑

```python
recons_loss, kld_loss, _, z = self.am_model.loss_func(states)
predict_action_dist = self.policy_model(states, times, z)
policy_loss = CrossEntropy(predict_action_dist, actions)
total_loss = policy_loss + α ⋅ kld_loss
```

即：

$$
\mathcal{L}_{\text{total}} = \underbrace{-\log \hat{a}_{a}}_{\text{MOA 分类损失}} + \alpha \cdot \underbrace{D_{\text{KL}}\left( \mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, I) \right)}_{\text{VAE KL 散度}}
$$


```bash
MCTS.compute_action()
└── for _ in num_sims:
    ├── select() ────────────────────── (路径遍历: O(depth))
    │    ├── best_action() ───────────── (遍历所有动作)
    │    ├── get_child() ─────────────── (env.set_state + get_obs + NN推理)
    │         ├── model.get_action() × 多agent
    │         ├── env.step()
    │         └── model_goal.cal_subgoal()
    ├── expand()
    ├── backup()
    └── final

```


下面是这个周的周报内容。帮我完善一下，还是按照之前的 contents和to-do lists 去看。以下是初步的内容 ：
Contents ： 
1. 讨论了HOP+训练过程中的耗时问题。在一个iteration中，主要是 compute_action()阶段耗时长，其中MCTS阶段又是占主要地位，MCTS中的select阶段又占主要时间。另外，与Direct-OM比较，主要体现在 需要moa阶段推测对手的action以及 goal
2. 讨论了MOA与VAE的重用，把预训练阶段的moa加载进去后，在初期效果并不是很明显，在观察以后的moa更新收敛是否会有优势
3. 讨论了关于当前模型的一些改进点：
	1. 当前的moa还是需要全观测的条件，而MCTS也需要访问环境的转移函数以及全观测条件。考虑到MCTS是当前模型主要耗时的因素，因为思考能否替换掉它，使用其他方案
	2. 首先是采用一步推测，使用简单网络去进行推测
	3. 简单讨论了diffusion，transformer的可行性（还未可知）
	4. 当前的cvae等goal inference模块效果接近稳定，可以暂不改变，考虑只针对MCTS去变化，然后又需要考虑去掉MCTS后 如何利用moa模块以体现我们方法的独特性
	5. 当前的goal（z hidden dim)是连续性的向量，考虑是否可以换成离散型的goal，比如每个agent有四种goal，那么最终维护一个4\*4 的即可（尽管可能的组合是4^4种）。在在下游分配时，可能有一些分配方案（哪个agent对应哪些goal）
	6. 刚播softmax?
	7. 使用pg or PPO？
	8. clean RL

TO_DO Lists
1. 采用一种方案替换掉MCTS，看看效果
2. 探索讨论中提到的一些方案
  


mcts抛弃，；使用一步推测；diffusion，transformer；改一个，vae暂不改变；离散goal 4*4？ ，根据下游任务分配？刚博softmax；与hop比较的时间，也比较长。使用pg or ppo？；clean rl