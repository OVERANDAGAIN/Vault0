![[prevae.png]]


## 训练逻辑

```python
recons_loss, kld_loss, _, z = self.am_model.loss_func(states)
predict_action_dist = self.policy_model(states, times, z)
policy_loss = CrossEntropy(predict_action_dist, actions)
total_loss = policy_loss + α ⋅ kld_loss
```

即：

$$
\mathcal{L}_{\text{total}} = \underbrace{-\log \hat{a}_{a}}_{\text{MOA 分类损失}} + \alpha \cdot \underbrace{D_{\text{KL}}\left( \mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, I) \right)}_{\text{VAE KL 散度}}
$$


```bash
MCTS.compute_action()
└── for _ in num_sims:
    ├── select() ────────────────────── (路径遍历: O(depth))
    │    ├── best_action() ───────────── (遍历所有动作)
    │    ├── get_child() ─────────────── (env.set_state + get_obs + NN推理)
    │         ├── model.get_action() × 多agent
    │         ├── env.step()
    │         └── model_goal.cal_subgoal()
    ├── expand()
    ├── backup()
    └── final

```