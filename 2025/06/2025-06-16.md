![[prevae.png]]


## 训练逻辑

```python
recons_loss, kld_loss, _, z = self.am_model.loss_func(states)
predict_action_dist = self.policy_model(states, times, z)
policy_loss = CrossEntropy(predict_action_dist, actions)
total_loss = policy_loss + α ⋅ kld_loss
```

即：

$$
\mathcal{L}_{\text{total}} = \underbrace{-\log \hat{a}_{a}}_{\text{MOA 分类损失}} + \alpha \cdot \underbrace{D_{\text{KL}}\left( \mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, I) \right)}_{\text{VAE KL 散度}}
$$


```bash
MCTS.compute_action()
└── for _ in num_sims:
    ├── select() ────────────────────── (路径遍历: O(depth))
    │    ├── best_action() ───────────── (遍历所有动作)
    │    ├── get_child() ─────────────── (env.set_state + get_obs + NN推理)
    │         ├── model.get_action() × 多agent
    │         ├── env.step()
    │         └── model_goal.cal_subgoal()
    ├── expand()
    ├── backup()
    └── final

```


下面是这个周的周报内容。帮我完善一下，还是按照之前的 contents和to-do lists 去看。以下是初步的内容 ：
Contents ： 
1. 讨论了HOP+训练过程中的耗时问题。在一个iteration中，主要是 compute_action()阶段耗时长，其中MCTS阶段又是占主要地位


mcts抛弃，；使用一步推测；diffusion，transformer；改一个，vae暂不改变；离散goal 4*4？ ，根据下游任务分配？刚博softmax；与hop比较的时间，也比较长。使用pg or ppo？；clean rl