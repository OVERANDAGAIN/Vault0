---
创建时间: 2025-六月-16日  星期一, 3:20:24 下午
---
![[prevae.png]]


## 训练逻辑

```python
recons_loss, kld_loss, _, z = self.am_model.loss_func(states)
predict_action_dist = self.policy_model(states, times, z)
policy_loss = CrossEntropy(predict_action_dist, actions)
total_loss = policy_loss + α ⋅ kld_loss
```

即：

$$
\mathcal{L}_{\text{total}} = \underbrace{-\log \hat{a}_{a}}_{\text{MOA 分类损失}} + \alpha \cdot \underbrace{D_{\text{KL}}\left( \mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, I) \right)}_{\text{VAE KL 散度}}
$$


```bash
MCTS.compute_action()
└── for _ in num_sims:
    ├── select() ────────────────────── (路径遍历: O(depth))
    │    ├── best_action() ───────────── (遍历所有动作)
    │    ├── get_child() ─────────────── (env.set_state + get_obs + NN推理)
    │         ├── model.get_action() × 多agent
    │         ├── env.step()
    │         └── model_goal.cal_subgoal()
    ├── expand()
    ├── backup()
    └── final

```


下面是这个周的周报内容。帮我完善一下，还是按照之前的 contents和to-do lists 去看。以下是初步的内容 ：
Contents ： 
1. 讨论了HOP+训练过程中的耗时问题。在一个iteration中，主要是 compute_action()阶段耗时长，其中MCTS阶段又是占主要地位，MCTS中的select阶段又占主要时间。另外，与Direct-OM比较，主要体现在 需要moa阶段推测对手的action以及 goal
2. 讨论了MOA与VAE的重用，把预训练阶段的moa加载进去后，在初期效果并不是很明显，在观察以后的moa更新收敛是否会有优势
3. 讨论了关于当前模型的一些改进点：
	1. 当前的moa还是需要全观测的条件，而MCTS也需要访问环境的转移函数以及全观测条件。考虑到MCTS是当前模型主要耗时的因素，因为思考能否替换掉它，使用其他方案
	2. 首先是采用一步推测，使用简单网络去进行推测
	3. 简单讨论了diffusion，transformer的可行性（还未可知）
	4. 当前的cvae等goal inference模块效果接近稳定，可以暂不改变，考虑只针对MCTS去变化，然后又需要考虑去掉MCTS后 如何利用moa模块以体现我们方法的独特性
	5. 当前的goal（z hidden dim)是连续性的向量，考虑是否可以换成离散型的goal，比如每个agent有四种goal，那么最终维护一个4\*4 的即可（尽管可能的组合是4^4种）。在在下游分配时，可能有一些分配方案（哪个agent对应哪些goal）
	6. 刚播softmax?
	7. 使用pg or PPO？
	8. clean RL

TO_DO Lists
1. 采用一种方案替换掉MCTS，看看效果
2. 探索讨论中提到的一些方案
  


mcts抛弃，；使用一步推测；diffusion，transformer；改一个，vae暂不改变；离散goal 4*4？ ，根据下游任务分配？刚博softmax；与hop比较的时间，也比较长。使用pg or ppo？；clean rl



---

### **Contents**

1. 本周主要分析了 HOP+ 的训练耗时问题。一个 iteration 里，`compute_action()` 阶段占比最大，MCTS 是主要瓶颈，MCTS 内部的 select 阶段时间最多。相比于 Direct-OM，HOP+ 的时间开销还多在 MOA 模块中需要推测对手的动作和目标。

2. 讨论了 MOA 与 VAE 的重用问题。把预训练得到的 MOA 模型加载进来后，在初期训练中效果变化不明显，后面观察下是否在 MOA 收敛过程中能体现出优势。

3. 模型结构方面做了一些新的思考：

   * 目前 MOA 需要全观测输入，MCTS 也依赖环境转移函数和完全观测，时间成本大。考虑是否可以用更轻量的方法替换掉 MCTS。
   * 探讨了一步推测（one-step prediction）的方式，用一个简单的网络直接输出动作。
   * 简单聊了一下 diffusion、transformer 这类结构是否能用于规划，目前没有深入验证。
   * CVAE 做 goal inference 的部分目前表现稳定，暂时不动，先只考虑把 MCTS 替换掉。但也要保证替换后还能体现出 MOA 的作用。
   * goal 的表达目前是一个连续向量（z），可以考虑换成离散目标，比如每个 agent 有 4 个 goal，总体是 4\*4 的组合（虽然总共是 4^4 种可能，但可以只关注每个 agent 各自的决策）。
   * Gumbel-Softmax 可以在训练时对离散变量做可微采样，可能适合用来替代当前的连续 z 向量表示 goal。
   * 后续动作策略部分可能考虑 PG 或 PPO。
   * cleanRL 框架可以作为简化实现的参考。

---

### **To-Do Lists**

1. 实现一个不用 MCTS 的替代方案，比如先做 one-step baseline，看训练和推理效率。

2. 探索讨论中提到的一些方案

---
