# 当前框架图
![[Pasted image 20250603114416.png]]

---

## 目标推理模块：
* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

![[Pasted image 20250603122228.png]]

---

## 预训练VAE
离线训练：
	搜集多种rule-based agents的轨迹，使用VAE训练 state $\Longrightarrow$ goal
	重建损失为重建动作[^1]  ![[Pasted image 20250603121911.png]]


---

# 实验
## VAE 预测动作准确性
通过比较VAE预测对手动作和真实动作，预测的准确度在 70%左右。
## CVAE模块
损失




# footnote

[^1]: Shi L X, Lim J J, Lee Y. Skill-based model-based reinforcement learning[J]. arXiv preprint arXiv:2207.07560, 2022.