# 当前框架图
![[Pasted image 20250603114416.png]]

---

## 目标推理模块：
* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

![[Pasted image 20250603122228.png]]

---

## 预训练VAE
离线训练：
		搜集多种rule-based agents的轨迹 (obs-action pair)，使用VAE训练 state $\Longrightarrow$ goal
	重建损失为重建动作[^1]  ![[Pasted image 20250603121911.png]]


---

# 实验
## VAE 预测动作准确性
	通过比较VAE预测对手动作和真实动作，预测的准确度达到 70%左右
	其预测的 动作 用于下一步CVAE的目标生成loss

## CVAE模块
1. 损失包含重建损失(recons)和目标生成损失（KL)
	在去除重建损失，保留目标生成损失后，发现对整体loss训练改善效果不太明显。
2. 预测goal时，agent根据自己当前的obs和历史信息（包括上一步action等） 预测 整体的goal

## MCTS planning
需要获取对手的obs，环境的转移等信息。$\Longrightarrow$ 全观测要求


## 6\*6 地图上训练效果
### self-play达到 11.5/12 reward
![[Pasted image 20250603124003.png]]

### 与Direct-OM比较
在分别面对 Random, Nearert-Stag, Nearest-Hare 时，奖励达到Direct-OM水平，并且面对Random策略的对手时，所获奖励更高：
![[Pasted image 20250603124107.png]]


## 8\*8 地图上
 速度较慢（2 weeks )，最终达到 11.x/12 的奖励
![[2025/05/attchments/episode_vs_policy_rewards 2.png]]






# footnote

[^1]: Shi L X, Lim J J, Lee Y. Skill-based model-based reinforcement learning[J]. arXiv preprint arXiv:2207.07560, 2022.