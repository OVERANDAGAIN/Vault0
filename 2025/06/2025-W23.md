---
åˆ›å»ºæ—¶é—´: 2025-å…­æœˆ-3æ—¥  æ˜ŸæœŸäºŒ, 11:39:17 ä¸­åˆ
---
# å½“å‰æ¡†æ¶å›¾
![[Pasted image 20250603114416.png]]

---

## ç›®æ ‡æ¨ç†æ¨¡å—ï¼š
* **Posterior distribution** $q_\phi(\hat{g}_t \mid \tau_t, s_t)$:
  Estimated by the **encoder** network;  infer latent goal $\hat{g}_t$ based on  trajectory $\tau_t$ and state $s_t$.

* **Likelihood distribution** $p_\gamma(s_t \mid \hat{g}_t, \tau_t)$:
  Output by the **decoder** network;  reconstruct the current state $s_t$ from the inferred latent goal $\hat{g}_t$.

* **Prior distribution** $p_\psi(\hat{g}_t \mid s_t^g)$:
  Generated by a **pretrained VAE**; predicts likely goals based on target-specific features $s_t^g$.

![[Pasted image 20250603122228.png]]

---

## é¢„è®­ç»ƒVAE
ç¦»çº¿è®­ç»ƒï¼š
		æœé›†å¤šç§rule-based agentsçš„è½¨è¿¹ (obs-action pair)ï¼Œä½¿ç”¨VAEè®­ç»ƒ state $\Longrightarrow$ goal
	é‡å»ºæŸå¤±ä¸ºé‡å»ºåŠ¨ä½œ[^1]  ![[Pasted image 20250603121911.png]]


---

# å®éªŒ
## VAE é¢„æµ‹åŠ¨ä½œå‡†ç¡®æ€§
	é€šè¿‡æ¯”è¾ƒVAEé¢„æµ‹å¯¹æ‰‹åŠ¨ä½œå’ŒçœŸå®åŠ¨ä½œï¼Œé¢„æµ‹çš„å‡†ç¡®åº¦è¾¾åˆ° 70%å·¦å³
	å…¶é¢„æµ‹çš„ åŠ¨ä½œ ç”¨äºä¸‹ä¸€æ­¥CVAEçš„ç›®æ ‡ç”Ÿæˆloss

## CVAEæ¨¡å—
1. æŸå¤±åŒ…å«é‡å»ºæŸå¤±(recons)å’Œç›®æ ‡ç”ŸæˆæŸå¤±ï¼ˆKL)
	åœ¨å»é™¤é‡å»ºæŸå¤±ï¼Œä¿ç•™ç›®æ ‡ç”ŸæˆæŸå¤±åï¼Œå‘ç°å¯¹æ•´ä½“lossè®­ç»ƒæ”¹å–„æ•ˆæœä¸å¤ªæ˜æ˜¾ã€‚
2. é¢„æµ‹goalæ—¶ï¼Œagentæ ¹æ®è‡ªå·±å½“å‰çš„obså’Œå†å²ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸Šä¸€æ­¥actionç­‰ï¼‰ é¢„æµ‹ æ•´ä½“çš„goal

## MCTS planning
éœ€è¦è·å–å¯¹æ‰‹çš„obsï¼Œç¯å¢ƒçš„è½¬ç§»ç­‰ä¿¡æ¯ã€‚$\Longrightarrow$ å…¨è§‚æµ‹è¦æ±‚


## 6\*6 åœ°å›¾ä¸Šè®­ç»ƒæ•ˆæœ
### self-playè¾¾åˆ° 11.5/12 reward
![[Pasted image 20250603124003.png]]

### ä¸Direct-OMæ¯”è¾ƒ
åœ¨åˆ†åˆ«é¢å¯¹ Random, Nearert-Stag, Nearest-Hare æ—¶ï¼Œå¥–åŠ±è¾¾åˆ°Direct-OMæ°´å¹³ï¼Œå¹¶ä¸”é¢å¯¹Randomç­–ç•¥çš„å¯¹æ‰‹æ—¶ï¼Œæ‰€è·å¥–åŠ±æ›´é«˜ï¼š
![[Pasted image 20250603124107.png]]


## 8\*8 åœ°å›¾ä¸Š
 é€Ÿåº¦è¾ƒæ…¢ï¼ˆ2 weeks )ï¼Œæœ€ç»ˆè¾¾åˆ° 11.x/12 çš„å¥–åŠ±
![[2025/05/attchments/episode_vs_policy_rewards 2.png]]




# to-dos

ğŸ” **1. Partial Observability**

* Current MCTS planning assumes full observability of other agentsâ€™ states.
* explore  **POMDP-based planning** to address this limitation. 


âš™ï¸ **2. Computational Efficiency**

* MCTS is effective but computationally expensive in multi-agent settings.
* plan to investigate **lightweight approximations**  for better scalability.




---


MA^2E[^2] 
I-NTMCPå‡è®¾å¯¹æ‰‹æ™ºèƒ½ä½“é‡‡ç”¨æŸä¸ªç‰¹å®šå±‚çº§çš„åµŒå¥—æ¨ç† [^3] [^4]
BA-POSGMCP å‡è®¾å¯¹æ‰‹çš„ç±»å‹æ¥è‡ªä¸€ä¸ªå›ºå®šä¸”å·²çŸ¥çš„é›†åˆ[^5]
...
# footnote

[^1]: Shi L X, Lim J J, Lee Y. Skill-based model-based reinforcement learning[J]. arXiv preprint arXiv:2207.07560, 2022.
[^2]: Kang S, Lee Y, Kim G, et al. MA $^ 2$ E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder[C]//The Thirteenth International Conference on Learning Representations.
[^3]: Schwartz J. Towards Scalable and Robust Decision Making in Partially Observable, Multi-Agent Environments[C]//Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. 2023: 2996-2998.
[^4]: Schwartz J, Zhou R, Kurniawati H. Online planning for interactive-pomdps using nested monte carlo tree search[C]//2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022: 8770-8777.
[^5]: Schwartz J, Kurniawati H. Bayes-Adaptive Monte-Carlo Planning for Type-Based Reasoning in Large Partially Observable, Multi-Agent Environments[C]//Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. 2023: 2355-2357.