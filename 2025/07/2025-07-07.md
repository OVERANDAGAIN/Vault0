---
åˆ›å»ºæ—¶é—´: 2025-ä¸ƒæœˆ-7æ—¥  æ˜ŸæœŸä¸€, 2:25:05 ä¸‹åˆ
---

## æ¯”è¾ƒçš„xè½´ä¸ºtimestep

`3compare_timestep.py`

```python
import os
import pandas as pd
import matplotlib.pyplot as plt

# åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
save_dir = "figures"
os.makedirs(save_dir, exist_ok=True)

# æ–‡ä»¶è·¯å¾„æ˜ å°„
file_dict = {
    "ToM w/o MCTS": "ToM-wo MCTS.csv",
    "ToM": "ToM-origin.csv",
    # "Direct_OM": "66dom.csv"
}

# åˆå§‹åŒ–å›¾åƒ
plt.figure(figsize=(10, 6))
max_rewards = {}
x_range = []

for label, filepath in file_dict.items():
    if os.path.exists(filepath):
        df = pd.read_csv(filepath)

        # å¿…é¡»åŒ…å« timesteps_total å’Œ episode_reward_mean
        if "episode_reward_mean" in df.columns and "timesteps_total" in df.columns:
            x = df["timesteps_total"]
            y = df["episode_reward_mean"]

            max_rewards[label] = y.max()
            x_range.extend([x.min(), x.max()])

            plt.plot(x, y, label=label)
        else:
            print(f"'timesteps_total' or 'episode_reward_mean' not found in {filepath}")
    else:
        print(f"File not found: {filepath}")

# è®¾ç½®æ ‡ç­¾
plt.xlabel("Timesteps")
plt.ylabel("Episode Reward Mean")
plt.title("Episode Reward vs. Timesteps")
plt.legend()
plt.grid(True)

# å›¾ä¾‹ä½ç½®
plt.legend(loc='lower right')

# è®¾ç½® reward æ˜¾ç¤ºä¸Šé™
plt.ylim(top=13)

# è®¾ç½® timesteps æœ€å¤§æ˜¾ç¤ºèŒƒå›´ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
plt.xlim(right=4e6)  # ä¾‹å¦‚è®¾ä¸ºæœ€å¤š 100 ä¸‡æ­¥

# æ·»åŠ é¡¶éƒ¨æ³¨é‡Š
xmin, xmax = min(x_range), max(x_range)
annotation_y = 13 * 0.95
x_positions = [xmin + (xmax - xmin) * frac for frac in [0.25, 0.5, 0.75]]

for (label, reward), xpos in zip(max_rewards.items(), x_positions):
    plt.text(
        xpos, annotation_y,
        f"{label} Max: {reward:.2f}",
        ha='center',
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.85)
    )

# ä¿å­˜å›¾åƒ
compare_path = os.path.join(save_dir, "reward_vs_timesteps.png")
plt.savefig(compare_path, dpi=300)
plt.show()

# æ§åˆ¶å°è¾“å‡º
print(f"\nâœ… Plot saved at: {compare_path}")
print("\nğŸ” Maximum Episode Reward per Algorithm:")
for label, reward in max_rewards.items():
    print(f" - {label}: {reward:.4f}")

```

å¹¶è¡Œè¾“å…¥ 10 obs , å¹¶è¡Œè¾“å‡º 10 actions
	`env_per_num_worker` `for` å¾ªç¯å¹¶è¡Œ

seq_lensçš„è®¾ç½® 20 ï¼Ÿ æˆªå–çš„è½¨è¿¹é•¿åº¦




## Contents
1. åœ¨ $4\times4$ çš„å°å‹ç¯å¢ƒä¸­æµ‹è¯•å‘ç°ï¼Œä½¿ç”¨LSTMæ¨¡å‹æ›¿ä»£åŸHOP+ä¸­çš„MCTSåï¼Œæ•´ä½“è®­ç»ƒé€Ÿåº¦æœ‰æ‰€ä¸‹é™ã€‚åœ¨few-shot adaptationä»»åŠ¡ä¸­ï¼Œç›¸æ¯”åŸæ¨¡å‹ï¼Œw/o MCTSåœ¨é¢å¯¹NHå¯¹æ‰‹æ—¶ï¼Œrewardæ˜æ˜¾ä¸‹é™ï¼›è€Œåœ¨é¢å¯¹NSå’ŒRandomå¯¹æ‰‹æ—¶ï¼Œè¡¨ç°ä¸åŸæ¨¡å‹åŸºæœ¬æŒå¹³ã€‚
2. é’ˆå¯¹å½“å‰è®­ç»ƒæ•ˆç‡é—®é¢˜ï¼Œå»ºè®®å°è¯•å¹¶è¡Œå¤„ç†å¤šä¸ªè§‚æµ‹ï¼ˆobsï¼‰ï¼Œä»¥åŒæ—¶è¾“å‡ºå¤šä¸ªåŠ¨ä½œï¼ˆactionï¼‰ã€‚å¯è€ƒè™‘é€‚å½“å¢å¤§ env_per_num_worker å‚æ•°ï¼Œå¹¶åœ¨ compute_action_from_input_dict å‡½æ•°ä¸­å»é™¤forå¾ªç¯é€»è¾‘ï¼Œå¼•å…¥å¹¶è¡ŒåŠ é€Ÿã€‚
3. å…³äºlstmæ¨¡å‹çš„seq_lensçš„å€¼çš„é€‰æ‹©ï¼šç›®å‰ä»£ç ä¸­æ²¡æœ‰ç”¨åˆ°è¿™ä¸ªå˜é‡ï¼Œåç»­å¯ä»¥æ¢ç´¢ã€‚è‹¥è®¾ç½®ä¸º20ï¼Œæ„å‘³ç€æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåŸºäºæœ€è¿‘çš„20æ­¥æ—¶é—´åºåˆ—ä¿¡æ¯è¿›è¡Œå†³ç­–ã€‚
4. å…³äºåˆ©ç”¨moa modelï¼š æ¢è®¨ä¸€ç§ç»“åˆMOAæ¨¡å‹çš„model-basedè§„åˆ’æ–¹æ³•ï¼šè®­ç»ƒä¸€ä¸ªç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼ˆsimulatorï¼‰ï¼Œç”¨äºåœ¨æ¯ä¸€æ­¥æ ¹æ®MOAè¾“å‡ºçš„é¢„æµ‹åŠ¨ä½œæ¨æ¼”ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œä»è€Œæ„å»ºâ€œä¸€æ­¥MCTSâ€æµç¨‹ã€‚å¯¹äºå½“å‰çŠ¶æ€ä¸‹è‡ªèº«çš„å¤šä¸ªå¯é€‰åŠ¨ä½œï¼Œåˆ©ç”¨æ¨¡æ‹Ÿå™¨é¢„æµ‹åç»­çŠ¶æ€ï¼Œå¹¶è®¡ç®—å¯¹åº”çš„Q-valueï¼Œä»ä¸­é€‰æ‹©å…·æœ‰æœ€å¤§æœŸæœ›æ”¶ç›Šçš„åŠ¨ä½œä½œä¸ºæ‰§è¡Œç­–ç•¥ã€‚


## ToDos
1. å½“å‰æ¨¡å‹åœ¨self-playè®­ç»ƒä¸­èƒ½å¤Ÿè¾¾æˆNSåˆä½œç­–ç•¥ï¼Œä½†åœ¨adaptationæµ‹è¯•ä¸­é¢å¯¹NHå¯¹æ‰‹æ—¶æ€§èƒ½ä¸‹é™ã€‚å¯è¿›ä¸€æ­¥æµ‹è¯• Goal inference æ¨¡å—æˆ– MOA æ¨¡å‹åœ¨æ­¤æƒ…å¢ƒä¸‹çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚
2. ç»§ç»­è¿ç§»clean-rlçš„ä»£ç å®ç°
3. å¯å°è¯•å®ç°å­¦ä¹ ä¸€ä¸ªç¯å¢ƒçš„simulator
4. æ¢ç´¢å¤„ç†å¹¶è¡Œé€»è¾‘ï¼ŒåŠ é€Ÿè®­ç»ƒ