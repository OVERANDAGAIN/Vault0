---
åˆ›å»ºæ—¶é—´: 2025-ä¸ƒæœˆ-7æ—¥  æ˜ŸæœŸä¸€, 2:25:05 ä¸‹åˆ
---

## æ¯”è¾ƒçš„xè½´ä¸ºtimestep

`3compare_timestep.py`

```python
import os
import pandas as pd
import matplotlib.pyplot as plt

# åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
save_dir = "figures"
os.makedirs(save_dir, exist_ok=True)

# æ–‡ä»¶è·¯å¾„æ˜ å°„
file_dict = {
    "ToM w/o MCTS": "ToM-wo MCTS.csv",
    "ToM": "ToM-origin.csv",
    # "Direct_OM": "66dom.csv"
}

# åˆå§‹åŒ–å›¾åƒ
plt.figure(figsize=(10, 6))
max_rewards = {}
x_range = []

for label, filepath in file_dict.items():
    if os.path.exists(filepath):
        df = pd.read_csv(filepath)

        # å¿…é¡»åŒ…å« timesteps_total å’Œ episode_reward_mean
        if "episode_reward_mean" in df.columns and "timesteps_total" in df.columns:
            x = df["timesteps_total"]
            y = df["episode_reward_mean"]

            max_rewards[label] = y.max()
            x_range.extend([x.min(), x.max()])

            plt.plot(x, y, label=label)
        else:
            print(f"'timesteps_total' or 'episode_reward_mean' not found in {filepath}")
    else:
        print(f"File not found: {filepath}")

# è®¾ç½®æ ‡ç­¾
plt.xlabel("Timesteps")
plt.ylabel("Episode Reward Mean")
plt.title("Episode Reward vs. Timesteps")
plt.legend()
plt.grid(True)

# å›¾ä¾‹ä½ç½®
plt.legend(loc='lower right')

# è®¾ç½® reward æ˜¾ç¤ºä¸Šé™
plt.ylim(top=13)

# è®¾ç½® timesteps æœ€å¤§æ˜¾ç¤ºèŒƒå›´ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
plt.xlim(right=4e6)  # ä¾‹å¦‚è®¾ä¸ºæœ€å¤š 100 ä¸‡æ­¥

# æ·»åŠ é¡¶éƒ¨æ³¨é‡Š
xmin, xmax = min(x_range), max(x_range)
annotation_y = 13 * 0.95
x_positions = [xmin + (xmax - xmin) * frac for frac in [0.25, 0.5, 0.75]]

for (label, reward), xpos in zip(max_rewards.items(), x_positions):
    plt.text(
        xpos, annotation_y,
        f"{label} Max: {reward:.2f}",
        ha='center',
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.85)
    )

# ä¿å­˜å›¾åƒ
compare_path = os.path.join(save_dir, "reward_vs_timesteps.png")
plt.savefig(compare_path, dpi=300)
plt.show()

# æ§åˆ¶å°è¾“å‡º
print(f"\nâœ… Plot saved at: {compare_path}")
print("\nğŸ” Maximum Episode Reward per Algorithm:")
for label, reward in max_rewards.items():
    print(f" - {label}: {reward:.4f}")

```

å¹¶è¡Œè¾“å…¥ 10 obs , å¹¶è¡Œè¾“å‡º 10 actions
	`env_per_num_worker` `for` å¾ªç¯å¹¶è¡Œ

seq_lensçš„è®¾ç½® 20 ï¼Ÿ æˆªå–çš„è½¨è¿¹é•¿åº¦




## Contents
1. å°†MCTSæ›¿æ¢ä¸ºlstmæ¨¡å‹åï¼Œåœ¨4\*4å°ç¯å¢ƒä¸‹æµ‹è¯•ï¼Œç›¸è¾ƒäºåŸHOP+ï¼Œè®­ç»ƒé€Ÿåº¦å˜æ…¢äº†ï¼›åœ¨few-shot adaptationä¸Šï¼šw/o MCTS åœ¨é¢å¯¹NHæ—¶ï¼Œrewardç›¸æ¯”åŸæ¨¡å‹å˜ä½äº†ï¼Œé¢å¯¹NSå’ŒRandomå¯¹æ‰‹æ—¶ï¼Œè¡¨ç°å·®ä¸å¤š
2. å…³äºæ¨¡å‹çš„å¹¶è¡Œè®­ç»ƒé—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘å¹¶è¡Œè¾“å…¥å¤šä¸ªobs,ä»è€Œè¾“å‡ºå¤šä¸ªaction;æ­¤å¤–ï¼Œä»£ç ä¸­çš„ `env_per_num_worker` å˜é‡è®¾ç½®å¤§ä¸€ç‚¹ï¼Œå¹¶ä¸”åœ¨ `compute_action_from_input_dict` ä¸­æ›¿æ¢forå¾ªç¯ï¼Œå¤„ç†å¹¶è¡Œé€»è¾‘
3. å…³äºlstmæ¨¡å‹çš„seq_lensçš„å€¼çš„é€‰æ‹©ï¼šç›®å‰ä»£ç ä¸­æ²¡æœ‰ç”¨åˆ°è¿™ä¸ªï¼Œåç»­å¯ä»¥æ¢ç´¢ï¼Œä¸€èˆ¬è®¾ç½®ä¸º20ï¼Œè¡¨ç¤ºï¼Ÿï¼Ÿï¼Ÿï¼ˆè¿™éƒ¨åˆ†éœ€è¦Chatgptè¾“å‡ºè¡¥å……ï¼‰
4. å…³äºåˆ©ç”¨moa modelï¼š æ„å»ºä¸€ä¸ªmodel-basedçš„æ¨¡å‹ï¼Œå­¦ä¹ ä¸€ä¸ªç¯å¢ƒçš„simulatorï¼Œè¿›è¡Œâ€ä¸€æ­¥MCTSâ€œçš„æ“ä½œï¼Œåœ¨å¾—åˆ°moaçš„è¾“å‡ºè¾“å…¥åˆ°modelä¸­ä½œä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€çš„é¢„æµ‹ï¼Œå¯¹äºè‡ªå·±çš„å¤šä¸ªå¯èƒ½çš„actionï¼Œé‡‡å–Q-valueæœ€å¤§çš„è¡ŒåŠ¨ã€‚


## ToDos
1. è€ƒè™‘åˆ°self-playä¸‹ç®—æ³•è¾¾æˆäº†åˆä½œç­–ç•¥ï¼ˆNSï¼‰ï¼Œä½†åœ¨adaptationæƒ…å†µä¸‹é¢å¯¹NHå¯¹æ‰‹æ—¶ï¼Œéœ€è¦æµ‹è¯•ä¸€ä¸‹Goal inference æˆ–è€… moaçš„å‡†ç¡®åº¦ã€‚
2. ç»§ç»­è¿ç§»clean-rlçš„ä»£ç å®ç°
3. å°è¯•å®ç°å­¦ä¹ ä¸€ä¸ªç¯å¢ƒçš„simulator