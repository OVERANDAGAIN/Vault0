---
创建时间: 2025-七月-7日  星期一, 2:25:05 下午
---

## 比较的x轴为timestep

`3compare_timestep.py`

```python
import os
import pandas as pd
import matplotlib.pyplot as plt

# 创建保存目录（如果不存在）
save_dir = "figures"
os.makedirs(save_dir, exist_ok=True)

# 文件路径映射
file_dict = {
    "ToM w/o MCTS": "ToM-wo MCTS.csv",
    "ToM": "ToM-origin.csv",
    # "Direct_OM": "66dom.csv"
}

# 初始化图像
plt.figure(figsize=(10, 6))
max_rewards = {}
x_range = []

for label, filepath in file_dict.items():
    if os.path.exists(filepath):
        df = pd.read_csv(filepath)

        # 必须包含 timesteps_total 和 episode_reward_mean
        if "episode_reward_mean" in df.columns and "timesteps_total" in df.columns:
            x = df["timesteps_total"]
            y = df["episode_reward_mean"]

            max_rewards[label] = y.max()
            x_range.extend([x.min(), x.max()])

            plt.plot(x, y, label=label)
        else:
            print(f"'timesteps_total' or 'episode_reward_mean' not found in {filepath}")
    else:
        print(f"File not found: {filepath}")

# 设置标签
plt.xlabel("Timesteps")
plt.ylabel("Episode Reward Mean")
plt.title("Episode Reward vs. Timesteps")
plt.legend()
plt.grid(True)

# 图例位置
plt.legend(loc='lower right')

# 设置 reward 显示上限
plt.ylim(top=13)

# 设置 timesteps 最大显示范围（可根据需要调整）
plt.xlim(right=4e6)  # 例如设为最多 100 万步

# 添加顶部注释
xmin, xmax = min(x_range), max(x_range)
annotation_y = 13 * 0.95
x_positions = [xmin + (xmax - xmin) * frac for frac in [0.25, 0.5, 0.75]]

for (label, reward), xpos in zip(max_rewards.items(), x_positions):
    plt.text(
        xpos, annotation_y,
        f"{label} Max: {reward:.2f}",
        ha='center',
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.85)
    )

# 保存图像
compare_path = os.path.join(save_dir, "reward_vs_timesteps.png")
plt.savefig(compare_path, dpi=300)
plt.show()

# 控制台输出
print(f"\n✅ Plot saved at: {compare_path}")
print("\n🔍 Maximum Episode Reward per Algorithm:")
for label, reward in max_rewards.items():
    print(f" - {label}: {reward:.4f}")

```

并行输入 10 obs , 并行输出 10 actions
	`env_per_num_worker` `for` 循环并行

seq_lens的设置 20 ？ 截取的轨迹长度




## Contents
1. 将MCTS替换为lstm模型后，在4\*4小环境下测试，相较于原HOP+，训练速度变慢了；在few-shot adaptation上：w/o MCTS 在面对NH时，reward相比原模型变低了，面对NS和Random对手时，表现差不多
2. 关于模型的并行训练问题，可以考虑并行输入多个obs,从而输出多个action;此外，代码中的 `env_per_num_worker` 变量设置大一点，并且在 `compute_action_from_input_dict` 中替换for循环，处理并行逻辑
3. 关于lstm模型的seq_lens的值的选择：目前代码中没有用到这个，后续可以探索，一般设置为20，表示？？？（这部分需要Chatgpt输出补充）
4. 关于利用moa model： 构建一个model-based的模型，学习一个环境的simulator，进行”一步MCTS“的操作，在得到moa的输出输入到model中作为下一个状态的预测，对于自己的多个可能的action，采取Q-value最大的行动。


## ToDos
1. 考虑到self-play下算法达成了合作策略（NS），但在adaptation情况下面对NH对手时，需要测试一下Goal inference 或者 moa的准确度。
2. 继续迁移clean-rl的代码实现
3. 尝试实现学习一个环境的simulator