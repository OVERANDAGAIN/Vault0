---
创建时间: 2025-七月-7日  星期一, 2:25:05 下午
---

## 比较的x轴为timestep

`3compare_timestep.py`

```python
import os
import pandas as pd
import matplotlib.pyplot as plt

# 创建保存目录（如果不存在）
save_dir = "figures"
os.makedirs(save_dir, exist_ok=True)

# 文件路径映射
file_dict = {
    "ToM w/o MCTS": "ToM-wo MCTS.csv",
    "ToM": "ToM-origin.csv",
    # "Direct_OM": "66dom.csv"
}

# 初始化图像
plt.figure(figsize=(10, 6))
max_rewards = {}
x_range = []

for label, filepath in file_dict.items():
    if os.path.exists(filepath):
        df = pd.read_csv(filepath)

        # 必须包含 timesteps_total 和 episode_reward_mean
        if "episode_reward_mean" in df.columns and "timesteps_total" in df.columns:
            x = df["timesteps_total"]
            y = df["episode_reward_mean"]

            max_rewards[label] = y.max()
            x_range.extend([x.min(), x.max()])

            plt.plot(x, y, label=label)
        else:
            print(f"'timesteps_total' or 'episode_reward_mean' not found in {filepath}")
    else:
        print(f"File not found: {filepath}")

# 设置标签
plt.xlabel("Timesteps")
plt.ylabel("Episode Reward Mean")
plt.title("Episode Reward vs. Timesteps")
plt.legend()
plt.grid(True)

# 图例位置
plt.legend(loc='lower right')

# 设置 reward 显示上限
plt.ylim(top=13)

# 设置 timesteps 最大显示范围（可根据需要调整）
plt.xlim(right=4e6)  # 例如设为最多 100 万步

# 添加顶部注释
xmin, xmax = min(x_range), max(x_range)
annotation_y = 13 * 0.95
x_positions = [xmin + (xmax - xmin) * frac for frac in [0.25, 0.5, 0.75]]

for (label, reward), xpos in zip(max_rewards.items(), x_positions):
    plt.text(
        xpos, annotation_y,
        f"{label} Max: {reward:.2f}",
        ha='center',
        fontsize=10,
        bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.85)
    )

# 保存图像
compare_path = os.path.join(save_dir, "reward_vs_timesteps.png")
plt.savefig(compare_path, dpi=300)
plt.show()

# 控制台输出
print(f"\n✅ Plot saved at: {compare_path}")
print("\n🔍 Maximum Episode Reward per Algorithm:")
for label, reward in max_rewards.items():
    print(f" - {label}: {reward:.4f}")

```

并行输入 10 obs , 并行输出 10 actions
	`env_per_num_worker` `for` 循环并行

seq_lens的设置 20 ？ 截取的轨迹长度




## Contents
1. 在 $4\times4$ 的小型环境中测试发现，使用LSTM模型替代原HOP+中的MCTS后，整体训练速度有所下降。在few-shot adaptation任务中，相比原模型，w/o MCTS在面对NH对手时，reward明显下降；而在面对NS和Random对手时，表现与原模型基本持平。
2. 针对当前训练效率问题，建议尝试并行处理多个观测（obs），以同时输出多个动作（action）。可考虑适当增大 env_per_num_worker 参数，并在 compute_action_from_input_dict 函数中去除for循环逻辑，引入并行加速。
3. 关于lstm模型的seq_lens的值的选择：目前代码中没有用到这个变量，后续可以探索。若设置为20，意味着模型在训练时会基于最近的20步时间序列信息进行决策。
4. 关于利用moa model： 探讨一种结合MOA模型的model-based规划方法：训练一个环境模拟器（simulator），用于在每一步根据MOA输出的预测动作推演下一个状态，从而构建“一步MCTS”流程。对于当前状态下自身的多个可选动作，利用模拟器预测后续状态，并计算对应的Q-value，从中选择具有最大期望收益的动作作为执行策略。


## ToDos
1. 当前模型在self-play训练中能够达成NS合作策略，但在adaptation测试中面对NH对手时性能下降。可进一步测试 Goal inference 模块或 MOA 模型在此情境下的识别准确性。
2. 继续迁移clean-rl的代码实现
3. 可尝试实现学习一个环境的simulator
4. 探索处理并行逻辑，加速训练