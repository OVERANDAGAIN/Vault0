---
创建时间: 2025-七月-14日  星期一, 10:19:55 上午
---


```python

import time
import torch
from msh_code.component.transforms import OneHot

if __name__=='__main__':

    config['omg'] = {}
    config['omg']['groups'] = {
        "agents": config['env_config']['player_num'],
        "n_actions": 7
    }
    config['omg']['scheme'] = {
        "state": {"vshape": (config['env_config']["world_height"],config['env_config']["world_width"],config['env_config']["player_num"]+3)},
        "obs": {"vshape": (config['env_config']["world_height"],config['env_config']["world_width"],config['env_config']["player_num"]+3), "group": "agents"},
        "actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
        "avail_actions": {"vshape": (7,), "group": "agents", "dtype": torch.int},
        "reward": {"vshape": (1,)},
        "time":{"vshape":(1,)},
        "terminated": {"vshape": (1,), "dtype": torch.uint8},
        "agent_idx": {"vshape": (1,), "dtype": torch.int, "episode_const": True},
    }
    config['omg']['args'] = {
        "input_channels": config['env_config']['player_num']+3,
        "world_height": config['env_config']['world_height'],
        "world_width": config['env_config']['world_width'],
        "subgoal_dim" : 16,
        "action_selector": 'epsilon_greedy',
        "epsilon_start": 1.0,
        "epsilon_finish": 0.05,
        "epsilon_anneal_time": 50000,
        "n_agents": config['env_config']['player_num'],
        "runner": "episode",
        "buffer_size": 200,
        "batch_size": 128,
        "policy_model_config":config_dict,
        # update the target network every {} episodes
        'load_dir': './vae_models/exp20250616_052744_4p/dim16',
        # 'load_dir':None,
        # 'load_dir': './vae_models/exp20250416_150730_4p/dim16',
        'load_cvae_dir': './params/exp20250616_070544_goal_4p',
        # 'load_cvae_dir':None,
        "target_update_interval": '200',
        "double_q": True,
        "am_model": "omg_am",
        "policy_model":"mcts",
        'save_dir':f'./params/exp{time.strftime("%Y%m%d_%H%M%S")}_goal_{config["env_config"]["player_num"]}p',
        "batch_size_run": 1,
        "test_nepisode": 20,
        "test_interval": 2000,
        "test_greedy": True,
        "log_interval": 2000,
        "runner_log_interval": 2000,
        "learner_log_interval": 2000,
        "t_max": 10000,
        "use_cuda": True,
        "buffer_cpu_only": True,
        "final_time": 35,
        "omg_pre_action":False,
        "omg_vae_alpha": 0.001,
        "omg_cvae_alpha": 0.001,
        "omg_horizon": 5,
        "evaluate_path": "",
        "obs_is_state": False,
        "final_eval": False,
        "obs_agent_id": False , # Include the agent's one_hot id in the observation
        "obs_last_action": False , # Include the agent's last action (one_hot) in the observation
        "eta": 0,
        "eta_delta": 0,
        "preprocess": {
        "actions": ("actions_onehot", [OneHot(out_dim=7)])
    },
        "agent_shift": 0,
        "obs_space":observation_space,
        "action_space":action_space,
        "action_space.n": 7,
        "model_config":config['model'],
    }
    config_dict['subgoal_dim'] = config['omg']['args']['subgoal_dim']

    # vae config
    config['vae_lr']=5e-4
    config['learner'] = "am_learner"
    config['n_actions'] = 7
    # config['optim_alpha'] = 0.9
    config['optim_eps'] = 1e-05
    config['learner_log_interval'] = 10000
    config['omg_vae_alpha'] = 0.001
    config['grad_norm_clip'] = 10
    config['am_train'] = False

##  ----------------------------- vae offline train -----------------------------------------
    config['collect_states']=False #是否采用 rule based agents 保存状态
    config['collect_states_dir'] = f'./collect_states/exp{time.strftime("%Y%m%d_%H%M%S")}_{config["env_config"]["player_num"]}p'

    config['vae_offline_train'] = False
    config['vae_sample_path'] = './collect_states/exp20250616_051613_4p/collect_samples.npy'
    config['vae_save_dir'] = f'./vae_models/exp{time.strftime("%Y%m%d_%H%M%S")}_{config["env_config"]["player_num"]}p'




    # strategy_combos = {
    #     "NSNS": ("nearest_stag", "nearest_stag"),
    #     "NSNH": ("nearest_stag", "nearest_hare"),
    #     "NHNH": ("nearest_hare", "nearest_hare"),
    # }

    strategy_combos = {
        "0NS": ("nearest_hare", "nearest_hare", "nearest_hare", "nearest_hare"),
        "1NS": ("nearest_stag", "nearest_hare", "nearest_hare", "nearest_hare"),
        "2NS": ("nearest_stag", "nearest_stag", "nearest_hare", "nearest_hare"),
        "3NS": ("nearest_stag", "nearest_stag", "nearest_stag", "nearest_hare"),
        "4NS": ("nearest_stag", "nearest_stag", "nearest_stag", "nearest_stag"),
    }

    def make_fixed_mapping(*strategies):
        # player_1 对应 strategies[0]，player_2 对应 strategies[1]，以此类推
        def mapping_fn(agent_id, *args, **kwargs):
            idx = int(agent_id.split("_")[-1]) - 1  # 变成 0-based 索引
            return strategies[idx] if idx < len(strategies) else strategies[-1]

        return mapping_fn


    import os


    if config['collect_states']==False: ###正常训练，不使用 rule based agents 采集状态
        pass
    else:### 使用 NH NS 等搜集状态

        max_train_iters = 100
        train_count = 0
        while train_count < max_train_iters:
            if train_count % 50 == 0 and train_count != 0:
                checkpoint = tom_trainer.save()
                print(f"[Checkpoint] Round {train_count} saved at: {checkpoint}")

            # 遍历三种策略组合，交替采集状态
            # for combo_name, (policy_a, policy_b) in strategy_combos.items():
            #     if train_count >= max_train_iters:
            #         break  # 避免超出最大轮数
            #
            #     print(f"\n[INFO] 切换策略组合：{combo_name} ({policy_a} vs {policy_b})")
            #
            #     # 更新 policy_mapping_fn
            #     new_mapping_fn = make_fixed_mapping(policy_a, policy_b)

            # 遍历五种策略组合，交替采集状态
            for combo_name, strategy_tuple in strategy_combos.items():
                if train_count >= max_train_iters:
                    break

                print(f"\n[INFO] 切换策略组合：{combo_name} -> {strategy_tuple}")

                new_mapping_fn = make_fixed_mapping(*strategy_tuple)

                tom_trainer.config["multiagent"]["policy_mapping_fn"] = new_mapping_fn

                #  worker 更新 mapping_fn
                tom_trainer.workers.foreach_worker(
                    lambda w: w.set_policy_mapping_fn(new_mapping_fn)
                )

                # 再调用 train()
                result = tom_trainer.train()
                print(pretty_print(result))
                train_count += 1

    # 以下代码在 搜集状态为false，预训练为true，且输入离线数据路径后运行
    # 使用收集的状态离线训练 vae
    if config.get('vae_offline_train', False):
        from msh_code.component.vae_offline_trainer import OfflineVAETrainer
        off_vae_trainer = OfflineVAETrainer(config)
        off_vae_trainer.train()


# msh_code/vae_offline_trainer.py

from torch.utils.data import BatchSampler, SubsetRandomSampler
from msh_code.component import REGISTRY as le_REGISTRY #即下面的 AMlearner

class OfflineVAETrainer:
    def __init__(self, config):
        self.config = config
        self.sample_path = config['vae_sample_path']
        self.subgoal_dim = config['omg']['args']['subgoal_dim']
        base_save_dir = config['vae_save_dir']
        self.save_dir = os.path.join(base_save_dir, f"dim{self.subgoal_dim}")

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        self.batch_size = 128
        self.epochs = 10
        self.moa_update_time = 10
        self.max_train_size = 500000

        # vae_train and loss_func
        self.am_train = config['am_train']
        self.learner = le_REGISTRY[config['learner']](config).to(self.device)
        self.policy_losses = []
        self.kld_losses = []
        self.total_losses = []
        self.kl_factores = []
        self.global_step = 0

        os.makedirs(self.save_dir, exist_ok=True)

    def should_plot_loss(self):
        return self.global_step % 5000 == 0

    def plot_loss(self):
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))  # 改为3个子图

        # 图1：重建损失 + 总损失
        axes[0].plot(self.policy_losses, label='Policy Loss', color='yellow')
        axes[0].plot(self.total_losses, label='Total Loss', color='green')
        axes[0].set_xlabel("Training Steps")
        axes[0].set_ylabel("Loss")
        axes[0].set_title("VAE Training Loss")
        axes[0].legend()

        # 图2：KL Loss
        axes[1].plot(self.kld_losses, label='KL Divergence Loss', color='red')
        axes[1].set_xlabel("Training Steps")
        axes[1].set_ylabel("Loss")
        axes[1].set_title("KL Divergence Loss")
        axes[1].legend()

        # 图3：KL Factor
        axes[2].plot(self.kl_factores, label='KL Factor', color='purple')
        axes[2].set_xlabel("Training Steps")
        axes[2].set_ylabel("Value")
        axes[2].set_title("KL Factor")
        axes[2].legend()

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, "vae_loss_curve.png"))


    def train(self):
        samples = np.load(self.sample_path, allow_pickle=True)
        print(f"[VAE] Loaded {len(samples)} samples.")

        states, actions, times = zip(*samples)

        states = np.stack(states)
        actions = np.array(actions)
        times = np.array(times)

        if len(states) > self.max_train_size:
            idx = np.random.choice(len(states), self.max_train_size, replace=False)
            states = states[idx]
            actions = actions[idx]
            times = times[idx]

        states_tensor = torch.tensor(states, dtype=torch.float32).to(self.device)
        actions_tensor = torch.tensor(actions, dtype=torch.float32).to(self.device)
        times_tensor = torch.tensor(times, dtype=torch.float32).to(self.device)
        print(f"[VAE] Begin offline training...")
        for epoch in range(self.epochs):
            print(f"\n[Epoch {epoch + 1}/{self.epochs}]")
            for _ in range(self.moa_update_time):
                sampler = BatchSampler(SubsetRandomSampler(range(len(states_tensor))), self.batch_size, drop_last=False)
                for index in sampler:
                    self.global_step += 1
                    self.config['omg_vae_alpha'] = min(0.001, 0.0001 * (1 - np.exp(-self.global_step / 20000)))

                    batch_state = states_tensor[index]
                    batch_action = actions_tensor[index]
                    batch_time = times_tensor[index]
                    policy_loss, kld_loss, total_loss = self.learner.train(batch_state, batch_action, batch_time)

                    self.policy_losses.append(policy_loss.item())
                    self.kld_losses.append(kld_loss.item())
                    self.total_losses.append(total_loss.item())
                    self.kl_factores.append(self.config['omg_vae_alpha'])
                    if self.should_plot_loss():
                        self.plot_loss()

            self.learner.save_models(self.save_dir)
            print(f"[VAE] 模型保存至: {self.save_dir}")




from msh_code.component import REGISTRY as am_REGISTRY #即 下面的 OMG_AM
from msh_code.model.moa_model import MOAModel



class AMLearner:
    '''Just for am-model training, use am-model's loss_func to get the loss'''

    def __init__(self, args):
        self.args = args

        self.am_model = am_REGISTRY["omg_am"](args)

        self.policy_model = MOAModel(args['model']['custom_model_config'])
        self.params = list(self.am_model.parameters())


        self.optimiser = th.optim.Adam(self.params, lr=self.args['vae_lr'], betas=(0.9, 0.999),
                                       eps=self.args['optim_eps'])

        self.log_stats_t = -self.args['learner_log_interval'] - 1


    def train(self, states, actions, times):
        recons_loss, kld_loss, loss, z = self.am_model.loss_func(states)
        predict_action_dist = self.policy_model(states, times, z)


        # 计算 policy_loss: MSE between predicted action and true action

        policy_loss = nn.CrossEntropyLoss()(predict_action_dist, actions.long())
        total_loss = policy_loss + self.args['omg_vae_alpha'] * kld_loss
        self.optimiser.zero_grad()
        total_loss.backward()
        grad_norm = th.nn.utils.clip_grad_norm_(self.params, self.args['grad_norm_clip'])
        self.optimiser.step()
        return policy_loss, kld_loss, total_loss

    def to(self, device):
        self.am_model.to(device)
        return self

    def cuda(self):
        # self.mac.cuda()
        self.am_model.cuda()

    def save_models(self, path):
        self.am_model.save_models(path)
        moa_model_path = os.path.join(path, f'player__to_.pth')
        torch.save(self.policy_model.state_dict(), moa_model_path)

    def load_models(self, path):
        pass


import torch
import torch.nn as nn

FLOAT_MIN = -1e38
FLOAT_MAX = 1e38


class MOAModel(nn.Module):
    def __init__(self, model_config):
        nn.Module.__init__(self)
        self.lstm_state_size = model_config['lstm_state_size']
        self.input_channels = model_config['input_channels']
        self.world_height = model_config['world_height']
        self.world_width = model_config['world_width']
        self.subgoal_dim = model_config['subgoal_dim']

        if torch.cuda.is_available():
            self.device = torch.device("cuda:0")
        else:
            self.device = torch.device("cpu")

        self.shared_layers = nn.Sequential(
            nn.Conv2d(self.input_channels, 16, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.Flatten()
        ).to(self.device)
        self.flatten_layers = nn.Sequential(
            nn.Linear(32 * self.world_height * self.world_width + 1 + self.subgoal_dim, 512),
            nn.ReLU()
        ).to(self.device)
        self.actor_layers = nn.Linear(512, 7).to(self.device)
        self.critic_layers = nn.Linear(512, 1).to(self.device)

        self._value_out = None

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)

    def forward(self, obs_flatten, time, prey_type):
        start_index = 7 + self.input_channels - 3

        x = obs_flatten[..., start_index:].reshape(obs_flatten.shape[0], self.world_height, self.world_width,
                                                   self.input_channels)
        action_mask = obs_flatten[..., :7]
        x = x.to(self.device)
        action_mask = action_mask.to(self.device)
        time = time.to(self.device).float().unsqueeze(1)
        prey_type = prey_type.to(self.device).float()

        x = x.permute(0, 3, 1, 2)
        x = self.shared_layers(x)
        x = torch.cat((x, time, prey_type), dim=1)
        x = self.flatten_layers(x)
        # actor outputs
        logits = self.actor_layers(x)
        inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)
        priors = nn.Softmax(dim=-1)(logits + inf_mask)

        self._value_out = self.critic_layers(x)

        return priors

    def value_function(self):
        return self._value_out

    def get_action(self, obs, time, prey_type):
        with torch.no_grad():
            x = torch.from_numpy(obs['observation']).to(self.device).float().unsqueeze(0)
            action_mask = torch.from_numpy(obs['action_mask']).to(self.device).float().unsqueeze(0)
            time = torch.tensor([[time]]).to(self.device).float()
            prey_type = prey_type.unsqueeze(0)

            x = x.permute(0, 3, 1, 2)
            x = self.shared_layers(x)
            x = torch.cat((x, time, prey_type), dim=1)
            x = self.flatten_layers(x)
            # actor outputs
            logits = self.actor_layers(x)
            inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)
            priors = nn.Softmax(dim=-1)(logits + inf_mask)

        return priors.squeeze().cpu().numpy()

    def get_action_prob(self, obs, time, action):
        with torch.no_grad():
            x = torch.from_numpy(obs['observation']).to(self.device).float().unsqueeze(0)
            action_mask = torch.from_numpy(obs['action_mask']).to(self.device).float().unsqueeze(0)
            time = torch.tensor([[time]]).to(self.device).float()

            inf_mask = torch.clamp(torch.log(action_mask), FLOAT_MIN, FLOAT_MAX)
            x = x.permute(0, 3, 1, 2)
            x = self.shared_layers(x)
            x_stag = torch.cat((x, time, torch.tensor([[0.0]])), dim=1)
            x_hare = torch.cat((x, time, torch.tensor([[1.0]])), dim=1)
            x_stag = self.flatten_layers(x_stag)
            x_hare = self.flatten_layers(x_hare)
            prob_stag = nn.Softmax(dim=-1)(self.actor_layers(x_stag) + inf_mask).squeeze()
            prob_hare = nn.Softmax(dim=-1)(self.actor_layers(x_hare) + inf_mask).squeeze()

        return prob_stag[action].item(), prob_hare[action].item()


import torch as th
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from abc import abstractmethod
import os
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt


class BaseVAE(nn.Module):

    def __init__(self) -> None:
        super(BaseVAE, self).__init__()

    def _encode(self, input):
        raise NotImplementedError

    def _decode(self, input):
        raise NotImplementedError

    def sample(self, batch_size, current_device, **kwargs):
        raise NotImplementedError

    def generate(self, x, **kwargs):
        raise NotImplementedError

    @abstractmethod
    def forward(self, *inputs):
        pass

    @abstractmethod
    def loss_function(self, *inputs, **kwargs):
        pass


class VanillaVAE(BaseVAE):
    def __init__(self, args, input_dim, latent_dim, hidden_dims, **kwargs):
        super(VanillaVAE, self).__init__()

        self.args = args
        self.latent_dim = latent_dim
        self.input_channels = args['input_channels']
        self.world_height = args['world_height']
        self.world_width = args['world_width']

        self.conv_layers = nn.Sequential(
            nn.Conv2d(self.input_channels, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU()
        )

        # After convolution, we need to flatten the result
        conv_output_size = 32 * self.world_height * self.world_width

        # Fully connected layers after convolution
        self.fc_mu = nn.Linear(conv_output_size, latent_dim)
        self.fc_var = nn.Linear(conv_output_size, latent_dim)

        # Decoder: Reconstruct the input from the latent vector
        self.decoder_input = nn.Linear(latent_dim, conv_output_size)

        self.decoder = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (32, self.world_height, self.world_width)),  # 变回4x4 feature map
            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(16, self.input_channels, kernel_size=3, stride=1, padding=1),
            nn.Sigmoid()  # 归一化到 [0,1]
        )

    def forward(self, inputs):
        mu, log_var = self._encode(inputs)
        z = self._reparameterize(mu, log_var)
        return z

    def loss_func(self, inputs, mask):
        mu, log_var = self._encode(inputs)
        z = self._reparameterize(mu, log_var)
        recons = self._decode(z)
        recon_loss = F.mse_loss(inputs * mask, recons * mask)
        # recon_loss = F.binary_cross_entropy(recons * mask, inputs * mask, reduction='sum')

        kld_loss = th.mean(-0.5 * mask * th.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1, keepdim=True))
        return recon_loss, kld_loss

    def generate(self, inputs):
        mu, log_var = self._encode(inputs)
        z = self._reparameterize(mu, log_var)
        recons = self._decode(z)
        return [inputs, recons, mu, log_var, z]

    def _encode(self, inputs):
        """
        Encodes the input by passing through the encoder network
        and returns the latent codes.
        :param input: (Tensor) Input tensor to encoder [N x C x H x W]
        :return: (Tensor) List of latent codes
        """

        batch_size = inputs.size(0)  # 获取批次大小
        inputs = inputs.view(batch_size, self.world_height, self.world_width, self.input_channels).permute(0, 3, 1,
                                                                                                           2)  # 重新reshape为[batch_size, 5, 4, 4]
        result = self.conv_layers(inputs)
        result = result.reshape(result.size(0), -1)  # reshape other than view

        # Split the result into mu and var components
        # of the latent Gaussian distribution
        mu = self.fc_mu(result)
        log_var = self.fc_var(result)

        return [mu, log_var]

    def _decode(self, z):
        result = self.decoder_input(z)
        result = self.decoder(result)
        # result = self.decoder_output(result)
        # 交换维度 (bs, 5, 4, 4) → (bs, 4, 4, 5)
        result = result.permute(0, 2, 3, 1)

        # 展平 (bs, 4, 4, 5) → (bs, 80)
        result = result.reshape(result.size(0), -1)

        return result
        # return th.sigmoid(result)

    def _reparameterize(self, mu, logvar):
        std = th.exp(0.5 * logvar)
        eps = th.randn_like(std)
        return eps * std + mu


class OMG_AM(nn.Module):
    def __init__(self, scheme, groups, args, id) -> None:
        super(OMG_AM, self).__init__()

        self.id = id
        self.args = args
        self.n_actions = groups['n_actions']
        self.n_agents = groups['agents']
        self.input_channels = self.n_agents + 3
        self.world_height = args['world_height']
        self.world_width = args['world_width']
        fc_input, cond_input, vae_input = self._get_input_shape(scheme, groups)
        self._hidden_dim = 128
        self._subgoal_dim = self.args['subgoal_dim']
        self.output_type = "embedding"
        self.hidden_state = None
        # Set up network layers
        self.shared_layers = nn.Sequential(
            nn.Conv2d(self.input_channels, 16, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(32, 32, 3, 1, 1),
            nn.Flatten()
        ).to("cuda")
        self.flatten_layers = nn.Sequential(
            nn.Linear(32 * self.world_height * self.world_width, self._hidden_dim // 4),
            nn.ReLU()
        ).to("cuda")

        self.vae = VanillaVAE(args=args, input_dim=vae_input, latent_dim=self._subgoal_dim, hidden_dims=[128, 128]).to(
            "cuda")

        self.writer = SummaryWriter(log_dir="D:/tensorboard_logs/omg_experiment")

        self.k_logging_count = 0  # 控制记录步数
        self.loss_logging_count = 0
        self.save_subgoal_pair = True
        self.subgoal_pairs_buffer = []
        self.pair_save_count = 0

    def to(self, device):
        # Ensure the model is on the correct device
        super(OMG_AM, self).to(device)
        self.device = device

    def loss_func(self, batch):
        '''
            used by am_learner
            @input: batch
            @output: vae_loss
        '''
        inputs, mask = self._build_vae_inputs(batch)
        recons_loss, kld_loss = self.vae.loss_func(inputs, mask)
        return recons_loss + self.args['omg_vae_alpha'] * kld_loss

    def _build_vae_inputs(self, batch):
        if self.args['obs_is_state']:
            obs = batch["state"][:, :-1]
        else:
            obs = batch["obs"][:, :-1]
        terminated = batch["terminated"][:, :-1].float()
        mask = batch["filled"][:, :-1].float()
        mask[:, 1:] = mask[:, 1:] * (1 - terminated[:, :-1])
        return obs.reshape(-1, obs.shape[-1]), mask.unsqueeze(2).repeat(1, 1, self.n_agents, 1).reshape(-1, 1)

    def parameters(self):
        return self.vae.parameters()

    def save_models(self, path):
        th.save(self.vae.state_dict(), "{}/vae.th".format(path))

    def load_models(self, path):
        self.vae.load_state_dict(th.load("{}/vae.th".format(path), map_location=lambda storage, loc: storage))




    def _get_input_shape(self, scheme, groups):
        if self.args['obs_is_state']:
            obs_shape = np.prod(scheme["state"]["vshape"])
        else:
            obs_shape = np.prod(scheme["obs"]["vshape"])

        fc_input = obs_shape
        if self.args['omg_pre_action']:
            fc_input += scheme["actions_onehot"]["vshape"][0]
            raise NotImplementedError
        cond_input = obs_shape + (self.n_agents + 1) * self.n_actions
        vae_input = obs_shape

        return fc_input, cond_input, vae_input

    @staticmethod
    def get_shapes(scheme, groups, args):
        if args['obs_is_state']:
            input_shape = scheme["state"]["vshape"]
        else:
            input_shape = scheme["obs"]["vshape"]
        output_shape = 64
        return input_shape, output_shape


```