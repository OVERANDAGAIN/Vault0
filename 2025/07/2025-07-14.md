---
创建时间: 2025-七月-14日  星期一, 3:42:25 下午
---

## 多智能体训练的构建方式：
1. **当前采用的方式**：自定义 `StagHuntEnv` 环境（继承自 `MultiAgentEnv`）经过修改后，封装为 `MultiAgentVecEnv`，再手动管理每个 agent 在多个环境中的生存状态，从而支持并行 rollout 和异步退出。

2. **（PettingZoo + Supersuit）**：PettingZoo 标准化接口，配合 Supersuit 将环境向量化实现并行。问题：
	1. 原环境中存在较强的 RLlib 依赖（如 `MultiAgentEnv`、`reset()`/`step()` 返回结构不兼容）；
	2. PettingZoo支持；
	3. 自定义字段接口对齐。

3. **CleanRL 官方示例方式**：使用 `gym.vector.SyncVectorEnv`/`AsyncVectorEnv` 等提供的并行机制，支持单智能体。
![[Pasted image 20250714154226.png]]



### 2. 关于 CleanRL 的结构与适配问题

CleanRL 的核心思想是**极简结构** + **清晰透明的数据流动**，将完整的一个算法训练流程写入一个 `.py` 文件中。但：

1. 在需要加入 **预训练模块（如 VAE）** 和 **rule-based policy 跑轨迹收集** 时，单文件结构变得臃肿，不利于模块复用；
2. 一些通用逻辑（如策略切换、模型加载、VAE嵌入）原先在 RLlib 框架中已有封装，迁移至 CleanRL 后需要**重新封装模块接口与训练调度逻辑**；
3. 文件划分策略需适度调整，例如将 `StagHuntEnv`、`rollout`、`vae_trainer` 拆分为独立模块。


### 3. CleanRL 与 RLlib 的接口差异

CleanRL 不依赖任何中间框架，所有 rollout、优势计算、PPO 更新等过程都需手动实现。虽然逻辑透明，但相较于 RLlib 提供的内置封装：

1. **rollout 部分**：需要自己管理 agent 的生死状态、数据缓存结构、reward 收集与异步退出处理逻辑。
2. **策略更新**：CleanRL 默认使用 shared policy，我们则为每个 agent 绑定独立策略与优化器；
3. **对手建模**与**目标推理模块**的嵌入需通过适当的接口设计嵌入训练主循环，目前也已初步完成联调。
4. 策略切换逻辑的可插拔设计：HOP+ 在训练过程中有“切换 policy 模型”的需求（如 rule-based → ToM），而 CleanRL 默认是单策略/单阶段。



---

1. 讨论cleanrl实现迁移
2. seq_lens
3. num_worker=2 时 出现问题
	1. 内存超了，内存分配： linux迁移
	2. time 尺度上慢了； 每轮训练的         
4. 关于action的准确度 ： 0.6~0.9
