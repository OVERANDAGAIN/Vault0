
## 多智能体训练的构建方式：
![[Pasted image 20250714154226.png]]
* **当前采用的方式**：自定义 `StagHuntEnv` 环境（继承自 `MultiAgentEnv`）经过适当修改后，封装为 `MultiAgentVecEnv`，再手动管理每个 agent 在多个环境中的生存状态，从而支持并行 rollout 和异步退出。

* **（PettingZoo + Supersuit）**：PettingZoo 提供标准化的多智能体并行接口，配合 Supersuit 可将环境向量化并实现高效并行。问题：

  * 原环境中存在较强的 RLlib 依赖（如 `MultiAgentEnv`、`reset()`/`step()` 返回结构不兼容）；
  * PettingZoo支持；
  * 自定义字段接口对齐。

* **CleanRL 官方示例方式**：使用 `gym.vector.SyncVectorEnv`/`AsyncVectorEnv` 等提供的并行机制，仅支持单智能体，但框架清晰、代码简洁，适用于教学与快速原型，但不支持异步 agent 中途退出和多策略组合的复杂 MARL 设置。