---
创建时间: 2025-七月-14日  星期一, 3:42:25 下午
---

## 多智能体训练的构建方式：
1. **当前采用的方式**：自定义 `StagHuntEnv` 环境（继承自 `MultiAgentEnv`）经过修改后，封装为 `MultiAgentVecEnv`，再手动管理每个 agent 在多个环境中的生存状态，从而支持并行 rollout 和异步退出。

2. **（PettingZoo + Supersuit）**：PettingZoo 标准化接口，配合 Supersuit 将环境向量化实现并行。问题：
	1. 原环境中存在较强的 RLlib 依赖（如 `MultiAgentEnv`、`reset()`/`step()` 返回结构不兼容）；
	2. PettingZoo支持；
	3. 自定义字段接口对齐。

3. **CleanRL 官方示例方式**：使用 `gym.vector.SyncVectorEnv`/`AsyncVectorEnv` 等提供的并行机制，支持单智能体。
![[Pasted image 20250714154226.png]]



### 2. 关于 CleanRL 的结构与适配问题

CleanRL 的核心思想是**极简结构** + **清晰透明的数据流动**，将完整的一个算法训练流程写入一个 `.py` 文件中。但：

1. 在需要加入 **预训练模块（如 VAE）** 和 **rule-based policy 跑轨迹收集** 时，单文件结构变得臃肿，不利于模块复用；
2. 一些通用逻辑（如策略切换、模型加载、VAE嵌入）原先在 RLlib 框架中已有封装，迁移至 CleanRL 后需要**重新封装模块接口与训练调度逻辑**；
3. 文件划分策略需适度调整，例如将 `StagHuntEnv`、`rollout`、`vae_trainer` 拆分为独立模块。


### 3. CleanRL 与 RLlib 的接口差异

CleanRL 不依赖任何中间框架，所有 rollout、优势计算、PPO 更新等过程都需手动实现。虽然逻辑透明，但相较于 RLlib 提供的内置封装：

1. **rollout 部分**：需要自己管理 agent 的生死状态、数据缓存结构、reward 收集与异步退出处理逻辑。
2. **策略更新**：CleanRL 默认使用 shared policy，我们则为每个 agent 绑定独立策略与优化器；
3. **对手建模**与**目标推理模块**的嵌入需通过适当的接口设计嵌入训练主循环，目前也已初步完成联调。
4. 策略切换逻辑的可插拔设计：HOP+ 在训练过程中有“切换 policy 模型”的需求（如 rule-based → ToM），而 CleanRL 默认是单策略/单阶段。



---

1. 讨论cleanrl实现迁移
2. seq_lens
3. num_worker=2 时 出现问题
	1. 内存超了，内存分配： linux迁移
	2. time 尺度上慢了； 每轮训练的         
4. 关于action的准确度 ： 0.6~0.9
5. 关于goal的准确度，如何度量？低维空间？
6. 面对NH，快速地推断出目标
7. for 循环并行优化，batch里面优化，
8. num_worker =2出问题？
9. num_env_per_worker

omg 
goal 的表示 ： state / trajecties的巨大空间

time state reward traj

区分的尺度，traj的使用reward!=0

episode之间信息的更新  inter-    intra-

meta-rl : - ood ;  - iod 

just metrics game , no seqence game ?  coin game 属于哪个？

hop 在 zero-sum ; cooper game ;SMAC 等的简单环境简单测一测。在不同环境都能应用即可。 三种博弈环境
原来的HOP，手动定义goal 。  or no SMAC ，melting plot 环境上play ，测试一下

MPE 大球吃小球
   
   


# 周报：
## Contents
1. 讨论了在 CleanRL 框架下适配自定义多智能体环境的方法，支持异质策略组合与异步退出的并行 rollout 收集机制。当前已完成初步联调，并修复了多个环境交互与数据结构不一致的问题。
2. 将 lstm 模型的 seq_lens 设置为20后进行训练，观察到模型收敛速度和性能有明显提升，初步表明更长的时间感知窗口有助于策略提取历史行为模式。
3. 在 adaptation 阶段测试了模型对 NH 类型对手的动作预测准确率，结果显示准确率在 0.6 ~ 0.9 区间波动，表明当前 MOA 模型具有一定的行为建模能力。
4. 为提升训练效率，尝试将 num_envs_per_worker 设置为 2，测试多环境并行。但在 Windows 系统下遇到内存溢出（OOM）问题，初步判断为进程间内存共享机制导致，后续将迁移至 Linux 平台进行验证。
5. 接第3点：尽管对手动作预测表现良好，但无法直接证明当前的 goal inference 是否正向影响了策略表现，尤其在 NH 对手下奖励仍偏低。后续将尝试 goal 的可解释性度量机制（如映射至低维空间或行为簇中心）。
6. 并行优化与 for 循环重构：针对 rollout 中的循环部分进行并行化优化，以缓解采样效率瓶颈。
7. 关于论文创新性的讨论：
	1. Goal 的表示形式：OMG存在一些优缺点。其用 state 的 embedding 表示为 goal； 而另一方面， goal 表示为 trajectory 似乎更直观，但是又面临着巨大的计算开销问题。
	2. 接上一点：goal 的表示可以是 （s,t,r）的多种组合，封老师提出一张可能的方案：当 agent 距离对手目标较远时，goal 推理精度要求可放宽；接近目标时再强化推断精度，有望提升效率与推理稳定性。
	3. 现有 OMG 模型未显式建模跨 episode 的策略先验或历史轨迹信息，可考虑引入 belief filter 或 Bayesian smoothing。
	4. 可调研现有 Meta-RL 方法中对 ID/OOD 的处理机制，探索在 HOP+ 中的融合方式。
	5. 可以测试一下 HOP 在三种博弈环境下的表现，比如 SMAC，Melting Pot, MPE 等简单环境，如果在不同环境设置下表现都可以，也是个可以highlight 的点


## Todos
1. 继续移植至 cleanrl 框架，联调子模块代码
2. 思考如何设计衡量 goal 的预测准确度
3. 继续处理并行训练，for 循环优化等
4. 调研 meta-rl 相关论文，找思路
5. 测试HOP在多种博弈环境下的表现，进行泛化测试




concat过程可能会对 action 的作用产生影响 or ? obs + goal   