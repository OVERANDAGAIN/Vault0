---
创建时间: 2025-七月-14日  星期一, 3:42:25 下午
---

## 多智能体训练的构建方式：
1. **当前采用的方式**：自定义 `StagHuntEnv` 环境（继承自 `MultiAgentEnv`）经过修改后，封装为 `MultiAgentVecEnv`，再手动管理每个 agent 在多个环境中的生存状态，从而支持并行 rollout 和异步退出。

2. **（PettingZoo + Supersuit）**：PettingZoo 标准化接口，配合 Supersuit 将环境向量化实现并行。问题：
	1. 原环境中存在较强的 RLlib 依赖（如 `MultiAgentEnv`、`reset()`/`step()` 返回结构不兼容）；
	2. PettingZoo支持；
	3. 自定义字段接口对齐。

3. **CleanRL 官方示例方式**：使用 `gym.vector.SyncVectorEnv`/`AsyncVectorEnv` 等提供的并行机制，支持单智能体。
![[Pasted image 20250714154226.png]]



### 2. 关于 CleanRL 的结构与适配问题

CleanRL 的核心思想是**极简结构** + **清晰透明的数据流动**，将完整的一个算法训练流程写入一个 `.py` 文件中。但：

1. 在需要加入 **预训练模块（如 VAE）** 和 **rule-based policy 跑轨迹收集** 时，单文件结构变得臃肿，不利于模块复用；
2. 一些通用逻辑（如策略切换、模型加载、VAE嵌入）原先在 RLlib 框架中已有封装，迁移至 CleanRL 后需要**重新封装模块接口与训练调度逻辑**；
3. 文件划分策略需适度调整，例如将 `StagHuntEnv`、`rollout`、`vae_trainer` 拆分为独立模块。


### 3. CleanRL 与 RLlib 的接口差异

CleanRL 不依赖任何中间框架，所有 rollout、优势计算、PPO 更新等过程都需手动实现。虽然逻辑透明，但相较于 RLlib 提供的内置封装：

1. **rollout 部分**：需要自己管理 agent 的生死状态、数据缓存结构、reward 收集与异步退出处理逻辑。
2. **策略更新**：CleanRL 默认使用 shared policy，我们则为每个 agent 绑定独立策略与优化器；
3. **对手建模**与**目标推理模块**的嵌入需通过适当的接口设计嵌入训练主循环，目前也已初步完成联调。
4. 策略切换逻辑的可插拔设计：HOP+ 在训练过程中有“切换 policy 模型”的需求（如 rule-based → ToM），而 CleanRL 默认是单策略/单阶段。



---

1. 讨论cleanrl实现迁移
2. seq_lens
3. num_worker=2 时 出现问题
	1. 内存超了，内存分配： linux迁移
	2. time 尺度上慢了； 每轮训练的         
4. 关于action的准确度 ： 0.6~0.9
5. 关于goal的准确度，如何度量？低维空间？
6. 面对NH，快速地推断出目标
7. for 循环并行优化，batch里面优化，
8. num_worker =2出问题？
9. num_env_per_worker

omg 
goal 的表示 ： state / trajecties的巨大空间

time state reward traj

区分的尺度，traj的使用reward!=0

episode之间信息的更新  inter-    intra-

meta-rl : - ood ;  - iod 

just metrics game , no seqence game ?  coin game 属于哪个？

hop 在 zero-sum ; cooper game ;SMAC 等的简单环境简单测一测。在不同环境都能应用即可。 三种博弈环境
原来的HOP，手动定义goal 。  or no SMAC ，melting plot 环境上play ，测试一下

MPE 大球吃小球
   
   


# 周报：
## Contents
1. 讨论了 cleanrl 框架下一些环境的适配，以支持异质多智能体训练(具体就是刚才讨论的内容)
2. 将 lstm 模型的 seq_lens 设置为20后进行训练，发现效果变好了
3. 测试了一下算法在 adaptation 阶段面对 NH 对手时预测其动作的准确率，结果显示准确率在 0.6 ~ 0.9 之间变化，表明动作预测比较准确
4. 为了实现并行训练，尝试设置 num_envs_per_worker 的数量为2，但是训练过程中出现了内存溢出的问题，猜测可能是windows下内存分配的问题，考虑后续在linux系统下跑。
5. 接第3点：虽然 action 的预测准确率较高，但是不能保证 goal 对其是否是正面作用，需要考虑是否有可行的方法去预测 goal 的准确度（如映射到低维空间等）；并且面对 NH 时奖励偏低，需要考虑是否可以快速推断出它的目标
6. for循环并行优化
7. 关于论文创新性的讨论：
	1. Goal 的表示：OMG存在一些优缺点。其用 state 的 embedding 表示为 goal； 而另一方面， goal 表示为 trajectory 似乎更直观，但是又面临着巨大的计算开销问题。
	2. 接上一点：goal 的表示可以是 （s,t,r）的多种组合，封老师提出一张可能的方案：当距离目标过远时，goal 的推断可能不那么重要；当目标较近时，需要关注 goal 的影响，这种处理思想可以降低推理开销
	3. OMG没有利用到 inter-episode 的信息
	4. 可以关注一下 meta-rl 处理 ID 和 OOD 的一些思想；sequential game
	5. 


## Todos