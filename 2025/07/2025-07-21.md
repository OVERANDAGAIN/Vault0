---
创建时间: 2025-七月-21日  星期一, 3:20:39 下午
---

## 1. 当前整合进展

1. 整合 CleanRL + 多策略  +  rollout 的训练流程；
2.  rollout 支持多智能体的中途终止与 action mask；
3. 整体训练流程可跑通，训练与评估结构。
	1. rule-based policy 等结果符合

---

## 2. ReplayBuffer 模块是否需要整体迁移？

1. 当前迁移进展卡在原系统中的 OMG 模块 buffer 管理逻辑；
2. 原代码中 OMG 模块使用 `ReplayBuffer` + `EpisodeBatch`；
3. 支持了 subgoal 推理、轨迹对齐、多 agent 数据结构、高级采样机制等功能。

---

### OMG Buffer 的结构特征

1. 支持 agent-wise segment（每个 agent 的完整 episode）；
2. 使用 `filled`、`terminated` 字段处理异步退出、动态截断；
3. 可存储 `subgoal`、`avail_actions`、`time` 等辅助字段；
4. 可通过 `can_sample()` 判断是否满足采样训练条件；
5. `scheme/group` 支持多 agent 分组处理与动态字段插入；
6. ReplayBuffer 可被多次训练、支持跨 batch 随机采样。


---

### 当前 CleanRL Buffer 的局限

1. 当前使用简化结构如 `transition`, `agent_traj`；
2. rollout 固定长度，不支持动态裁剪（如 OMG 中的 `max_t_filled()`）；

> 当前我们 CleanRL 移植版本中使用的是较为简化的 `transition` 和 `agent_traj` 数据结构，能够满足 rollout 与基本训练，但在处理多智能体异步终止、subgoal 推理（如时间对齐、填充、跨 agent 组织）时存在一定局限。

---


> 1. **完全迁移** OMG buffer 系统；
> 2. **在现有结构上 patch 异步终止逻辑和subgoal存储**。





## Contents
![[c3c7eec4056d30f030741b932a51e1c5_720.jpg]]
1. moa 的利用，
2. Dreamer
3. 讨论与QOM, OMG的区别等，
	1. 首先是 goal 的选择，其次是goal的准确度：是否可以把多样化的对手归类为几种类型（或者goal种类） ；能否区分不同的策略，准确刻画对手
	2. 与OMG相比，OMG是预测的state，我们是预测的action，更注重对手的行为模式？
	3. 与QOM相比，希望可以替换掉MCTS这个simulator的部分
4. 关于goal何时预测的问题：面对稀疏的奖励，面对稠密的奖励（可过滤一定大小程度的奖励来利用）


好的，以下是白板上识别出的所有公式，使用标准数学公式格式（LaTeX 形式）输出：

---

### 🧩 右上部分：目标建模与对手动作预测

1.

$$
\text{goal} \rightarrow g_{j} \rightarrow a_j'
$$

2.

$$
\pi(a_i \mid o_i^t, g_{j},a_{j}')
$$


---

---

### 🧩 左侧

8.

$$
P(o_i^{t+1} \mid o_i^t, a_j^t, a_i^t)
$$

9.

$$
a_i^{t*} = \arg\max_{a_{i}^{t}} 求和符号 P(o_i^{t+1} \mid o_i^t, a_j^t, a_i^t)V(o_{i}^{t+1})
$$



11.另外， 仿照Dreamer的思路，用embedding表示o，防止稀疏观测导致的loss难以计算等问题

$$
f(o_{i}^{t+1}) = e_{i}^{t+1}
$$
更新后的9式（如下，你更新一下）


![[Pasted image 20250721211540.png]]