---
创建时间: 2025-七月-21日  星期一, 3:20:39 下午
---




1. **已初步整合 CleanRL + 多策略 PPO + 异步 rollout 的训练流程**；
2. **目前正在对齐原 HOP+ 中 OMG 模块的结构逻辑，主要是  buffer 管理部分**；
3. **围绕 ReplayBuffer 与 EpisodeBatch 的数据组织方式进行深度分析，以评估是否整体复用 OMG 设计或重构适配版本**。





HOP+ 中的 OMG 模块使用 `ReplayBuffer` 和 `EpisodeBatch` 支持：

* agent-wise segment 的组织（每个 agent 的完整轨迹）；
* max\_seq\_length、subgoal、filled mask、terminated 等时间序列对齐；
* 异步 episode 截断；
* 定期 OMG\_AM模型训练触发（ `can_sample` 检查）；
* 支持 `scheme/group` 结构，便于后续拓展/分组处理。



> 当前我们 CleanRL 移植版本中使用的是较为简化的 `transition` 和 `agent_traj` 数据结构，能够满足 rollout 与基本训练，但在处理多智能体异步终止、subgoal 推理（如时间对齐、填充、跨 agent 组织）时存在一定局限。



> 1. **完全迁移** OMG buffer 系统（成本高，稳定性强）；
> 2. **在现有结构上 patch 异步终止逻辑和subgoal存储**（最轻，但未来可扩展性差）。




* OMG buffer 中 `filled`, `terminated`, `max_t_filled()`，实际用于动态 episode 剪裁；
* CleanRL 当前 rollout 固定长度？
