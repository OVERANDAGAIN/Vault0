---
创建时间: 2025-七月-28日  星期一, 3:10:22 下午
---



# 状态转移建模

## 公式：

$$
P(o_i^{t+1} \mid o_i^t, a_{-i}^t, a_i^t)
$$

> 给定当前观测 $o_i^t$、自身动作 $a_i^t$ 与对手动作 $a_{-i}^t$，
> 预测可能到达的下一状态 $o_i^{t+1}$ 的分布。
> 该模型承担“简化版模拟器”的作用，用于支撑后续的动作规划。



### 使用 直接计算loss(obs $\Longrightarrow$ next_obs)
```python
class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(5, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 4 * 4, latent_dim),
        )

    def forward(self, x):
        return self.net(x)
```
![[plot_2025-07-28 12-11-05_0.png]]
![[plot_2025-07-28 12-11-05_19.png]]

![[plot_2025-07-28 12-11-05_20.png]]

### 使用 隐变量 计算loss (obs $\Longrightarrow$ next_obs)
```python
    def forward(self, obs_t, self_action, opp_action):
        # 编码当前状态
        z_t = self.encoder(obs_t)

        # 预测下一状态隐变量
        z_t1 = self.dynamics(z_t, self_action, opp_action)

        # 解码下一状态
        obs_t1_pred = self.decoder(z_t1)
        return obs_t1_pred
```

![[plot_2025-07-28 15-31-25_30.png]]
![[plot_2025-07-28 15-31-25_27.png]]

![[plot_2025-07-28 15-31-25_29.png]]


![[plot_2025-07-28 15-31-25_28.png]]


## 直接重建obs (obs $\Longrightarrow$ obs)
![[plot_2025-07-28 12-21-16_51.png]]

![[plot_2025-07-28 12-21-16_50.png]]

规则简单，全观测。


## 代充网站：
9981store
F9HW5S67ZON6SUS4

![[Pasted image 20250728190710.png]]








# Contents
1. 状态转移建模模块，可以较准确地预测下一步的状态
	1. 如果加上reward_dynamics模块去预测reward, 可能可以使隐变量训练变得稳定
	2. 对手策略的变化对训练过程影响显著，应考虑建模或切换机制；
	3. 模型训练与使用阶段的视角只需一致，不强制固定顺序；
	4. 对于loss值较小的情况，建议取对数后绘图以更清晰地观察收敛趋势；
	5. 后续可测试更复杂环境、部分观测、更大空间或更多玩家下的建模能力；
	6. 关于过拟合：在特定环境中出现的过拟合现象未必有害，需结合下游目标判断；
	7. Dreamer的完整工程结构暂不纳入实现计划，但可参考其算法思想；
	8. VQ-VAE相关算法调研
2. Hyper-MARL的融合，目前使用全连接
3. 并行后，reward基本停滞在4，可能与 MOA/OMG 模块中的 `batch_size` 设置不合理有关。
4. 可在非并行场景中测试 Hyper-MARL 的表现，并探索如何更有效地引入 action 信息用于建模。
5. 关于goal的离散或者连续处理： 连续比离散好处理一点，但是可解释性不好
6. 目前的goal的loss计算是使用的其他人的action，所以理论上相当于是其他人们的goal
7. 关于MOA模块假如id的信息，仍有待讨论
8. rollout4000(4\*2\*500) 对应 512，800的训练batch，可能影响不大，设置还算合理
9. 关于reward_dynamics的计算，使用reward比Q函数更好一点，并且 reward计算 用 $r+\gamma V(t)$ 比较合理一点（关于这部分公式可以再讨论一下，大模型帮我再想一下这里的公式应该怎么构建合理一些）

# TO-DOs
1. 加上reward_dynamics模块训练状态转移模块
2. 测试更复杂一点的环境下，状态转移模块能否预测好
3. 调研发的论文
4. 对于并行上出现的问题，可以在非并行环境下测试hyper-marl的思路



以下是你周报内容的润色版本，保持技术准确性的同时增强了表达流畅性和逻辑清晰度：

---

## 📌 Weekly Report

### ✅ Contents

1. **状态转移建模模块**

   * 当前模型可以较准确地预测下一步状态；
   * 如果引入 `reward_dynamics` 模块预测奖励，有望提升隐变量的稳定性；
   * 对手策略的变化对训练过程影响显著，应考虑建模或切换机制；
   * 模型训练与使用阶段的视角只需一致，不强制固定顺序；
   * 对于loss值较小的情况，建议取对数后绘图以更清晰地观察收敛趋势；
   * 后续可测试更复杂环境、部分观测、更大空间或更多玩家下的建模能力；
   * 关于过拟合：在特定环境中出现的过拟合现象未必有害，需结合下游目标判断；
   * Dreamer的完整工程结构暂不纳入实现计划，但可参考其算法思想；
   * 已开始调研 VQ-VAE 等相关离散表征方法。

2. **Hyper-MARL 融合方向**

   * 当前版本中已集成全连接结构，后续可进一步探索结构化融合方式。

3. **并行训练中的 reward 停滞问题**

   * 并行训练后 reward 基本稳定在 4，初步怀疑可能与 MOA/OMG 模块中的 `batch_size` 设置不合理有关。

4. **Hyper-MARL 在非并行环境下的验证**

   * 建议在非并行场景中测试 Hyper-MARL 的表现，并探索如何更有效地引入 action 信息用于建模。

5. **关于 goal 表征的处理方式**

   * 连续表征相对离散更易优化，但可解释性较差；
   * 当前 goal loss 的计算基于其他 agent 的 action，实际建模的是“他人的目标”。

6. **MOA 模块是否引入 agent id 的讨论**

   * 是否使用 id 信息仍在讨论中，需权衡其对泛化能力的影响。

7. **rollout 与 batch 设置分析**

   * 当前 rollout=4000（4 × 2 × 500）对应 batch size 为 512 或 800，初步判断影响不大，设置基本合理。

8. **关于 reward dynamics 建模的建议**

   * 直接建模 reward 会比 Q 函数更稳健；
   * 奖励计算公式建议采用：

     $$
     r + \gamma V(s')
     $$

     可进一步讨论 gamma 的选择及目标函数构造，大模型可辅助推导更合理的公式。

---

### 🧪 TO-DOs

1. 集成 reward dynamics 模块以联合训练状态转移模型；
2. 在更复杂的环境中测试状态转移模型的建模能力；
3. 持续调研相关论文（VQ-VAE、Dreamer 等）；
4. 对并行训练中出现的问题，在非并行环境下测试 Hyper-MARL 的运行效果及鲁棒性。

---
