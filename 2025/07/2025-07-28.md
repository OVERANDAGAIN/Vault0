---
创建时间: 2025-七月-28日  星期一, 3:10:22 下午
---



# 状态转移建模

## 公式：

$$
P(o_i^{t+1} \mid o_i^t, a_{-i}^t, a_i^t)
$$

> 给定当前观测 $o_i^t$、自身动作 $a_i^t$ 与对手动作 $a_{-i}^t$，
> 预测可能到达的下一状态 $o_i^{t+1}$ 的分布。
> 该模型承担“简化版模拟器”的作用，用于支撑后续的动作规划。



### 使用 直接计算loss(obs $\Longrightarrow$ next_obs)
```python
class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(5, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64 * 4 * 4, latent_dim),
        )

    def forward(self, x):
        return self.net(x)
```
![[plot_2025-07-28 12-11-05_0.png]]
![[plot_2025-07-28 12-11-05_19.png]]

![[plot_2025-07-28 12-11-05_20.png]]

### 使用 隐变量 计算loss (obs $\Longrightarrow$ next_obs)
```python
    def forward(self, obs_t, self_action, opp_action):
        # 编码当前状态
        z_t = self.encoder(obs_t)

        # 预测下一状态隐变量
        z_t1 = self.dynamics(z_t, self_action, opp_action)

        # 解码下一状态
        obs_t1_pred = self.decoder(z_t1)
        return obs_t1_pred
```

![[plot_2025-07-28 15-31-25_30.png]]
![[plot_2025-07-28 15-31-25_27.png]]

![[plot_2025-07-28 15-31-25_29.png]]


![[plot_2025-07-28 15-31-25_28.png]]


## 直接重建obs (obs $\Longrightarrow$ obs)
![[plot_2025-07-28 12-21-16_51.png]]

![[plot_2025-07-28 12-21-16_50.png]]

规则简单，全观测。


## 代充网站：
9981store
F9HW5S67ZON6SUS4

![[Pasted image 20250728190710.png]]








# Contents
1. 状态转移建模模块，可以较准确地预测下一步的状态
	1. 如果加上reward_dynamics模块去预测reward, 可能可以使隐变量训练变得稳定
	2. 对手的policy变换对于训练过程是比较重要的
	3. 训练和使用模型时的视角一致即可，不强制要求视角顺序一定
	4. 对于loss很小的曲线，可以取对数后绘图，更好观察变化
	5. 可以测试一下部分观测，更大环境，更多玩家的建模情况
	6. 关于过拟合问题：对于单一环境，过拟合可能不是坏事
	7. dreamer的工程思想暂不考虑，但可以了解一下算法知识
	8. VQ-VAE相关算法调研
2. Hyper-MARL的融合，目前使用全连接
3. 并行后，reward基本停滞在4，可能是moa,omg等的batch_size设置不恰当
4. 可以测试一下Hyper-MARL在非并行上的运行效果,同时可以思考如何使用action的信息
5. 关于goal的离散或者连续处理： 连续比离散好处理一点，但是可解释性不好
6. 目前的goal的loss计算是使用的其他人的action，所以理论上相当于是其他人们的goal
7. 关于MOA模块假如id的信息，仍有待讨论
8. rollout4000(4\*2\*500) 对应 512，800的训练batch，可能影响不大，设置还算合理
9. 关于reward_dynamics的计算，使用reward比Q函数更好一点，并且 reward计算 用 $r+\gamma V(t)$ 比较合理一点（关于这部分公式可以再讨论一下，大模型帮我再想一下这里的公式应该怎么构建合理一些）

# TO-DOs
1. 加上reward_dynamics模块训练状态转移模块
2. 测试更复杂一点的环境下，状态转移模块能否预测好
3. 调研发的论文
4. 对于并行上出现的问题，可以在非并行环境下