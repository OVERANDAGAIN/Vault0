

对于这一页PPT，给出演讲文稿：
Slide 1. 标题页

    论文题目、作者信息

    开场引导：用几句话概括背景问题，引出主题
















我们关注的任务是**目标条件强化学习（goal-conditioned RL）**。



我们从第一个部分开始——学习“目标达成技能”（goal reaching skills）。我将从问题定义开始介绍，也就是所谓的“目标条件强化学习”（goal-conditioned reinforcement learning）问题。

假设我们有一个机器人，我们希望它把绿色的物体捡起来，并放进蓝色的箱子里。传统做法是，我们可以写出那个之前看到的庞大且复杂的奖励函数。

但这一次，我们不这么做，而是让用户直接提供一张图像，来表示他们希望机器人完成的结果，也就是“期望结果”。
具体来说，我们希望学习一个策略，它以当前图像和目标图像为输入，并采取一系列动作，最终使系统到达目标图像所对应的状态。

例如，在这个例子中，我们希望机器人往下移动、抓起绿色物体，然后移动到蓝色箱子上方，把它放进去。

这种“目标条件强化学习”其实是一个非常老的问题，早期的一些AI研究，包括NSH（可能指代某位研究者或团队）的工作，就曾探讨过如何学习目标导向的行为。

而我们研究的是一个特定场景：我们拥有一个数据集，数据来自一些视频，每帧带有动作标签。要注意的是，这些视频中的动作**并不一定是最优的**，动作也不一定是“好”的。但我们要从这些数据中学习，尽可能快速地达成目标的技能。
我们使用一个大型视频数据集来训练这个策略，数据中标注了每一帧的动作。但与标准强化学习中的轨迹不同，这些数据**并没有奖励标注**。

我们定义的奖励函数是：**如果在下一个时间步到达目标状态，则获得奖励**。这种定义完全基于物理过程，不需要任何奖励工程。

我们优化的目标是这些奖励的总和，Q函数就是在当前状态和动作条件下的这个目标函数。

我们方法的核心思想是：**使用对比学习，构建整个强化学习算法**。这个算法不需要额外的表示学习、奖励学习或模型学习。





这个问题为何有用？在实际中，它让用户更容易指定任务；而在数学层面，它让我们可以将方法应用到更一般的环境中，包括**连续状态空间、连续动作空间、随机动态系统**，同时还能保留某些理论性质，因为这个目标是一个良定义的（well-defined）优化目标。

