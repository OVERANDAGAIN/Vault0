---
创建时间: 2025-七月-28日  星期一, 7:56:30 晚上
---
![[Pasted image 20250728195635.png]]![[Pasted image 20250728195645.png]]![[Pasted image 20250728195653.png]]![[Pasted image 20250728195711.png]]


I'm excited to hear our work entitled contrast of learning as goal conditioned reinforcement learning. Deep
reinforcement learning algorithms have a key really great results in the last few years. They've also become a fair bit
more complex. Today feel like an engine, many living perks, one perfectpicture rewards. Maybe you have another
part that learns a model of the environment. Maybe another part learns compact representations of high
dimensional data. In this work, we pose a much simpler approach where we will have a single objective that
simultaneously results in predicting the future rewards, learning a model of the environment and learning good
representations. We focus on goal condition, reinforcement learning. We want to learn a policy that takes this input,
some current image and goal image and takes actions to try to get to this goal invention. We train this policy using a
big data set of the videos labeled with actions, unlike trajectories and the standard reinforcement learning setting.
These are not labeled with rewards. We'll define the reward function to be whether you reach the goal state at the
next time step. But this reward function doesn't require reward engineering, is to find purely in terms of the physics.
We maximize the sum of these rewards, and the Q function is the same objective condition on some current state in
action.


The key idea of our approach is to use contrastive learning, to build an entire reinforcement learning algorithm one
that doesn't require any sort of extra representation learning, reward learning, or model learning ofers explain how
we apply the contrast of learning.


And then the next slide willshow how this is an entire reinforcement learning algorithm. Our method is gonna look
pretty similar to prior methods for representation learning, on video, for audio, on text. But the key difference is that
our method willallow us to solve the reinforcement learning problem. We're going to representations, one of states
and actions and one of future states. The data will be sampled from trajectory. We'll sample the positive examples
using a state in an action, in a future state from the same trajectory. Precisely, we'll use, I think you see example, from
a geometric distribution that allows us to draw a connection with Q values. It will sample random states from
different trajectories and push those representations further apart from each other. Now, intuitively, these
representations as to reason about what states are we likely to visit in the future. And importantly, they will allow us
to figure out which actions are likely to guess to the desired goal state. Here's how the complete algorithm works.
We serve the data set, maybe from a different experiment, maybe from a random policy. We apply contrast to
learning, to learn their presentations, then we learn the policy to maximize the likelihood of the desired goal state.
Optimally, we could go back and collect more data. They feel that our method can work well, even when we don't
collect more data Nothing to hide a mental output.It can also be viewed as giving us a pretty cure the future returns
of AQ function, and also gives us low dimensional representations of states. Training the actor has a nice geometric
interpretation. Let's think about the representation of the goal state and the representation of the current state,
which might be different for different actions. If we simply look at the similarity between these representations, train
the actor coresponds to accusing whatever state action representation brings us closest to the goal representation.
We can think about this as doing planning and representation space.


Now normally planning is hard. For example, you can greedily try to solve this task by moving the arm to the goal,
then you wouldn't pick up the block. But what we've done by learning these representations is make the planning
problem easier. Greedy planning and the representation space is optimal. We evaluate our many tasks, including this
been picking task. The agent has to pick up the block and move it towards the blue. Then it's really hard because we
don't have a word shaping. We don't have demonstrations, and it's done entirely from images. Our first baseline is
goal condition behavioral climbing, which is a pretty simple baseline that works really well according to prior work.
The second one is TD three plus her, which is a temporal difference method combined with hindsight really blame.
Only our method is able to solve this task. We evaluate the base lines and a number of other manipulation tasks and
observe that our method achieves better results and all. But the easiest tasks. Please stop by the poster to learn more
such his experiments and figure directions. Thanks !