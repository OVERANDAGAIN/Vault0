---
创建时间: 2025-七月-28日  星期一, 7:56:30 晚上
---
![[Pasted image 20250728195635.png]]![[Pasted image 20250728195645.png]]![[Pasted image 20250728195653.png]]![[Pasted image 20250728195711.png]]


I'm excited to hear our work entitled contrast of learning as goal conditioned reinforcement learning. Deep
reinforcement learning algorithms have a key really great results in the last few years. They've also become a fair bit
more complex. Today feel like an engine, many living perks, one perfectpicture rewards. Maybe you have another
part that learns a model of the environment. Maybe another part learns compact representations of high
dimensional data. In this work, we pose a much simpler approach where we will have a single objective that
simultaneously results in predicting the future rewards, learning a model of the environment and learning good
representations. We focus on goal condition, reinforcement learning. We want to learn a policy that takes this input,
some current image and goal image and takes actions to try to get to this goal invention. We train this policy using a
big data set of the videos labeled with actions, unlike trajectories and the standard reinforcement learning setting.
These are not labeled with rewards. We'll define the reward function to be whether you reach the goal state at the
next time step. But this reward function doesn't require reward engineering, is to find purely in terms of the physics.
We maximize the sum of these rewards, and the Q function is the same objective condition on some current state in
action.


The key idea of our approach is to use contrastive learning, to build an entire reinforcement learning algorithm one
that doesn't require any sort of extra representation learning, reward learning, or model learning ofers explain how
we apply the contrast of learning.


And then the next slide willshow how this is an entire reinforcement learning algorithm. Our method is gonna look
pretty similar to prior methods for representation learning, on video, for audio, on text. But the key difference is that
our method willallow us to solve the reinforcement learning problem. We're going to representations, one of states
and actions and one of future states. The data will be sampled from trajectory. We'll sample the positive examples
using a state in an action, in a future state from the same trajectory. Precisely, we'll use, I think you see example, from
a geometric distribution that allows us to draw a connection with Q values. It will sample random states from
different trajectories and push those representations further apart from each other. Now, intuitively, these
representations as to reason about what states are we likely to visit in the future. And importantly, they will allow us
to figure out which actions are likely to guess to the desired goal state. Here's how the complete algorithm works.
We serve the data set, maybe from a different experiment, maybe from a random policy. We apply contrast to
learning, to learn their presentations, then we learn the policy to maximize the likelihood of the desired goal state.
Optimally, we could go back and collect more data. They feel that our method can work well, even when we don't
collect more data Nothing to hide a mental output.It can also be viewed as giving us a pretty cure the future returns
of AQ function, and also gives us low dimensional representations of states. Training the actor has a nice geometric
interpretation. Let's think about the representation of the goal state and the representation of the current state,
which might be different for different actions. If we simply look at the similarity between these representations, train
the actor coresponds to accusing whatever state action representation brings us closest to the goal representation.
We can think about this as doing planning and representation space.


Now normally planning is hard. For example, you can greedily try to solve this task by moving the arm to the goal,
then you wouldn't pick up the block. But what we've done by learning these representations is make the planning
problem easier. Greedy planning and the representation space is optimal. We evaluate our many tasks, including this
been picking task. The agent has to pick up the block and move it towards the blue. Then it's really hard because we
don't have a word shaping. We don't have demonstrations, and it's done entirely from images. Our first baseline is
goal condition behavioral climbing, which is a pretty simple baseline that works really well according to prior work.
The second one is TD three plus her, which is a temporal difference method combined with hindsight really blame.
Only our method is able to solve this task. We evaluate the base lines and a number of other manipulation tasks and
observe that our method achieves better results and all. But the easiest tasks. Please stop by the poster to learn more
such his experiments and figure directions. Thanks !




好的，以下是你提供的英文介绍的**整理版内容**，包括**英文润色**和**中文翻译**，方便在演讲或答辩中使用。

---

## 📢 英文整理稿（清晰、连贯版本）

I'm excited to present our work titled **"Contrastive Learning as Goal-Conditioned Reinforcement Learning."**

In recent years, deep reinforcement learning (RL) has achieved impressive results, but modern RL algorithms have also become increasingly complex. They often involve multiple components: one part optimizes rewards, another learns a model of the environment, and yet another learns compact representations of high-dimensional data.

In contrast, our work proposes a much simpler approach: a **single objective** that **simultaneously** learns:

* to predict future rewards,
* a model of the environment,
* and meaningful representations of states and goals.

We focus on **goal-conditioned reinforcement learning**, where the agent receives both a current observation and a goal observation and must choose actions to reach the goal. Instead of labeled rewards, we use **video data annotated with actions**. The reward is defined as simply whether the agent reaches the goal in the next timestep—requiring no reward engineering.

The key idea of our approach is to use **contrastive learning** to construct the **entire reinforcement learning algorithm**—without any need for separate components like representation learning, model learning, or reward learning.

Our method looks similar to prior contrastive approaches in vision, audio, or text. But our novelty is that we solve the full RL problem using this framework. We learn two kinds of representations: one for (state, action) pairs, and another for future states.

Positive pairs are sampled from the **same trajectory** (e.g., current state, action, and a future state sampled via a geometric distribution), while negative pairs come from different trajectories. These representations allow us to reason about what future states are likely and what actions can lead us to the goal.

The overall pipeline is:

1. Use any offline dataset—e.g., random policy rollouts.
2. Apply contrastive learning to learn representations.
3. Train the actor to maximize similarity to the goal embedding.

Interestingly, this training process has a nice **geometric interpretation**. The actor chooses actions whose (state, action) embeddings are closest to the goal embeddings—this can be seen as **planning in the representation space**.

Normally, planning is difficult. For example, in a block-picking task, naively reaching the goal without picking up the block fails. But our learned representations simplify the problem so that greedy planning in the representation space becomes **optimal**.

We evaluate on many tasks—including challenging ones like **bin picking from images with no reward shaping or demonstrations**—and our method is the **only one that succeeds**. We also compare with strong baselines like:

* Goal-conditioned behavioral cloning
* TD3+HER (a strong off-policy RL method with hindsight replay)

Our method outperforms all baselines on all but the simplest tasks.

Please stop by our poster to learn more about our experiments and future directions. Thank you!

---

## 📝 中文翻译（正式表达版）

我很高兴在这里介绍我们的工作，题为《将对比学习视为目标条件强化学习》。

近年来，深度强化学习取得了令人瞩目的成果，但与此同时，现有的算法也变得越来越复杂：它们通常包含多个子模块——比如一个模块用于优化奖励，另一个用于学习环境模型，第三个用于从高维数据中提取紧凑的表示。

而我们提出了一种**更简洁**的方法：我们设计了一个**统一的目标函数**，它能够同时完成以下三项任务：

* 预测未来的奖励；
* 学习环境的状态转移；
* 学习良好的状态与目标的表示。

我们专注于**目标条件强化学习（Goal-conditioned RL）**，即给定当前图像与目标图像，训练策略选择动作以达成目标状态。与传统RL不同，我们使用的是**标注了动作的视频数据**，而不是人工设定的奖励。我们定义的奖励非常简单——仅依据下一步是否到达目标状态，完全不需要奖励工程，直接基于物理定义。

我们提出的核心思想是：**用对比学习构建完整的强化学习算法**，不再依赖额外的表示学习、模型学习或奖励学习模块。

我们的方法在形式上与图像、音频、文本领域的对比学习相似，但我们首次将其用于解决完整的RL问题。我们学习两类表示：一种是（状态, 动作）的表示，另一种是未来状态的表示。

我们从轨迹中采样正样本（三元组来自同一轨迹，未来状态用几何分布采样，关联Q值）；负样本则来自其他轨迹。最终，这些表示能够推理未来可能访问的状态，并指引动作选择以实现目标。

算法整体流程如下：

1. 使用任意离线数据集（如随机策略收集的轨迹）；
2. 应用对比学习来训练状态/目标的表示；
3. 训练策略网络，使其选择能最大化与目标表示相似度的动作。

这种训练方式有一个很直观的**几何解释**：我们通过比较状态-动作表示与目标表示的相似度，引导策略选择将它们距离最小化的动作——这等价于在**表示空间中进行规划**。

通常，规划是很困难的，比如在夹块任务中，单纯向目标移动不能完成任务（必须先夹起方块）。但通过我们学习到的表示，这种问题被有效简化——**贪婪地在表示空间规划就可以获得最优解**。

我们在多个任务上进行了评估，尤其是在没有奖励 shaping、没有专家演示、仅基于图像输入的 bin picking 任务中，只有我们的方法能成功解决问题。

我们也与多个强基线进行比较，包括：

* 目标条件行为克隆（goal-conditioned behavioral cloning）；
* TD3+HER（一种结合了后验经验回放的强化学习算法）。

除了最简单的任务外，我们的方法在所有评估任务中都取得了更好的表现。

欢迎来我们的 poster 展位了解更多实验细节和未来方向，谢谢！

---

如需配合 PPT、海报或答辩稿进一步精炼，我可以继续优化演讲节奏和风格。是否需要我为你输出 2 分钟或 5 分钟口语演讲版？
