---
åˆ›å»ºæ—¶é—´: 2025-ä¸ƒæœˆ-28æ—¥  æ˜ŸæœŸä¸€, 7:56:30 æ™šä¸Š
---
![[Pasted image 20250728195635.png]]![[Pasted image 20250728195645.png]]![[Pasted image 20250728195653.png]]![[Pasted image 20250728195711.png]]


I'm excited to hear our work entitled contrast of learning as goal conditioned reinforcement learning. Deep
reinforcement learning algorithms have a key really great results in the last few years. They've also become a fair bit
more complex. Today feel like an engine, many living perks, one perfectpicture rewards. Maybe you have another
part that learns a model of the environment. Maybe another part learns compact representations of high
dimensional data. In this work, we pose a much simpler approach where we will have a single objective that
simultaneously results in predicting the future rewards, learning a model of the environment and learning good
representations. We focus on goal condition, reinforcement learning. We want to learn a policy that takes this input,
some current image and goal image and takes actions to try to get to this goal invention. We train this policy using a
big data set of the videos labeled with actions, unlike trajectories and the standard reinforcement learning setting.
These are not labeled with rewards. We'll define the reward function to be whether you reach the goal state at the
next time step. But this reward function doesn't require reward engineering, is to find purely in terms of the physics.
We maximize the sum of these rewards, and the Q function is the same objective condition on some current state in
action.


The key idea of our approach is to use contrastive learning, to build an entire reinforcement learning algorithm one
that doesn't require any sort of extra representation learning, reward learning, or model learning ofers explain how
we apply the contrast of learning.


And then the next slide willshow how this is an entire reinforcement learning algorithm. Our method is gonna look
pretty similar to prior methods for representation learning, on video, for audio, on text. But the key difference is that
our method willallow us to solve the reinforcement learning problem. We're going to representations, one of states
and actions and one of future states. The data will be sampled from trajectory. We'll sample the positive examples
using a state in an action, in a future state from the same trajectory. Precisely, we'll use, I think you see example, from
a geometric distribution that allows us to draw a connection with Q values. It will sample random states from
different trajectories and push those representations further apart from each other. Now, intuitively, these
representations as to reason about what states are we likely to visit in the future. And importantly, they will allow us
to figure out which actions are likely to guess to the desired goal state. Here's how the complete algorithm works.
We serve the data set, maybe from a different experiment, maybe from a random policy. We apply contrast to
learning, to learn their presentations, then we learn the policy to maximize the likelihood of the desired goal state.
Optimally, we could go back and collect more data. They feel that our method can work well, even when we don't
collect more data Nothing to hide a mental output.It can also be viewed as giving us a pretty cure the future returns
of AQ function, and also gives us low dimensional representations of states. Training the actor has a nice geometric
interpretation. Let's think about the representation of the goal state and the representation of the current state,
which might be different for different actions. If we simply look at the similarity between these representations, train
the actor coresponds to accusing whatever state action representation brings us closest to the goal representation.
We can think about this as doing planning and representation space.


Now normally planning is hard. For example, you can greedily try to solve this task by moving the arm to the goal,
then you wouldn't pick up the block. But what we've done by learning these representations is make the planning
problem easier. Greedy planning and the representation space is optimal. We evaluate our many tasks, including this
been picking task. The agent has to pick up the block and move it towards the blue. Then it's really hard because we
don't have a word shaping. We don't have demonstrations, and it's done entirely from images. Our first baseline is
goal condition behavioral climbing, which is a pretty simple baseline that works really well according to prior work.
The second one is TD three plus her, which is a temporal difference method combined with hindsight really blame.
Only our method is able to solve this task. We evaluate the base lines and a number of other manipulation tasks and
observe that our method achieves better results and all. But the easiest tasks. Please stop by the poster to learn more
such his experiments and figure directions. Thanks !




å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä½ æä¾›çš„è‹±æ–‡ä»‹ç»çš„**æ•´ç†ç‰ˆå†…å®¹**ï¼ŒåŒ…æ‹¬**è‹±æ–‡æ¶¦è‰²**å’Œ**ä¸­æ–‡ç¿»è¯‘**ï¼Œæ–¹ä¾¿åœ¨æ¼”è®²æˆ–ç­”è¾©ä¸­ä½¿ç”¨ã€‚

---

## ğŸ“¢ è‹±æ–‡æ•´ç†ç¨¿ï¼ˆæ¸…æ™°ã€è¿è´¯ç‰ˆæœ¬ï¼‰

I'm excited to present our work titled **"Contrastive Learning as Goal-Conditioned Reinforcement Learning."**

In recent years, deep reinforcement learning (RL) has achieved impressive results, but modern RL algorithms have also become increasingly complex. They often involve multiple components: one part optimizes rewards, another learns a model of the environment, and yet another learns compact representations of high-dimensional data.

In contrast, our work proposes a much simpler approach: a **single objective** that **simultaneously** learns:

* to predict future rewards,
* a model of the environment,
* and meaningful representations of states and goals.

We focus on **goal-conditioned reinforcement learning**, where the agent receives both a current observation and a goal observation and must choose actions to reach the goal. Instead of labeled rewards, we use **video data annotated with actions**. The reward is defined as simply whether the agent reaches the goal in the next timestepâ€”requiring no reward engineering.

The key idea of our approach is to use **contrastive learning** to construct the **entire reinforcement learning algorithm**â€”without any need for separate components like representation learning, model learning, or reward learning.

Our method looks similar to prior contrastive approaches in vision, audio, or text. But our novelty is that we solve the full RL problem using this framework. We learn two kinds of representations: one for (state, action) pairs, and another for future states.

Positive pairs are sampled from the **same trajectory** (e.g., current state, action, and a future state sampled via a geometric distribution), while negative pairs come from different trajectories. These representations allow us to reason about what future states are likely and what actions can lead us to the goal.

The overall pipeline is:

1. Use any offline datasetâ€”e.g., random policy rollouts.
2. Apply contrastive learning to learn representations.
3. Train the actor to maximize similarity to the goal embedding.

Interestingly, this training process has a nice **geometric interpretation**. The actor chooses actions whose (state, action) embeddings are closest to the goal embeddingsâ€”this can be seen as **planning in the representation space**.

Normally, planning is difficult. For example, in a block-picking task, naively reaching the goal without picking up the block fails. But our learned representations simplify the problem so that greedy planning in the representation space becomes **optimal**.

We evaluate on many tasksâ€”including challenging ones like **bin picking from images with no reward shaping or demonstrations**â€”and our method is the **only one that succeeds**. We also compare with strong baselines like:

* Goal-conditioned behavioral cloning
* TD3+HER (a strong off-policy RL method with hindsight replay)

Our method outperforms all baselines on all but the simplest tasks.

Please stop by our poster to learn more about our experiments and future directions. Thank you!

---

## ğŸ“ ä¸­æ–‡ç¿»è¯‘ï¼ˆæ­£å¼è¡¨è¾¾ç‰ˆï¼‰

æˆ‘å¾ˆé«˜å…´åœ¨è¿™é‡Œä»‹ç»æˆ‘ä»¬çš„å·¥ä½œï¼Œé¢˜ä¸ºã€Šå°†å¯¹æ¯”å­¦ä¹ è§†ä¸ºç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ã€‹ã€‚

è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœï¼Œä½†ä¸æ­¤åŒæ—¶ï¼Œç°æœ‰çš„ç®—æ³•ä¹Ÿå˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼šå®ƒä»¬é€šå¸¸åŒ…å«å¤šä¸ªå­æ¨¡å—â€”â€”æ¯”å¦‚ä¸€ä¸ªæ¨¡å—ç”¨äºä¼˜åŒ–å¥–åŠ±ï¼Œå¦ä¸€ä¸ªç”¨äºå­¦ä¹ ç¯å¢ƒæ¨¡å‹ï¼Œç¬¬ä¸‰ä¸ªç”¨äºä»é«˜ç»´æ•°æ®ä¸­æå–ç´§å‡‘çš„è¡¨ç¤ºã€‚

è€Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§**æ›´ç®€æ´**çš„æ–¹æ³•ï¼šæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª**ç»Ÿä¸€çš„ç›®æ ‡å‡½æ•°**ï¼Œå®ƒèƒ½å¤ŸåŒæ—¶å®Œæˆä»¥ä¸‹ä¸‰é¡¹ä»»åŠ¡ï¼š

* é¢„æµ‹æœªæ¥çš„å¥–åŠ±ï¼›
* å­¦ä¹ ç¯å¢ƒçš„çŠ¶æ€è½¬ç§»ï¼›
* å­¦ä¹ è‰¯å¥½çš„çŠ¶æ€ä¸ç›®æ ‡çš„è¡¨ç¤ºã€‚

æˆ‘ä»¬ä¸“æ³¨äº**ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ ï¼ˆGoal-conditioned RLï¼‰**ï¼Œå³ç»™å®šå½“å‰å›¾åƒä¸ç›®æ ‡å›¾åƒï¼Œè®­ç»ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œä»¥è¾¾æˆç›®æ ‡çŠ¶æ€ã€‚ä¸ä¼ ç»ŸRLä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯**æ ‡æ³¨äº†åŠ¨ä½œçš„è§†é¢‘æ•°æ®**ï¼Œè€Œä¸æ˜¯äººå·¥è®¾å®šçš„å¥–åŠ±ã€‚æˆ‘ä»¬å®šä¹‰çš„å¥–åŠ±éå¸¸ç®€å•â€”â€”ä»…ä¾æ®ä¸‹ä¸€æ­¥æ˜¯å¦åˆ°è¾¾ç›®æ ‡çŠ¶æ€ï¼Œå®Œå…¨ä¸éœ€è¦å¥–åŠ±å·¥ç¨‹ï¼Œç›´æ¥åŸºäºç‰©ç†å®šä¹‰ã€‚

æˆ‘ä»¬æå‡ºçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ç”¨å¯¹æ¯”å­¦ä¹ æ„å»ºå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼Œä¸å†ä¾èµ–é¢å¤–çš„è¡¨ç¤ºå­¦ä¹ ã€æ¨¡å‹å­¦ä¹ æˆ–å¥–åŠ±å­¦ä¹ æ¨¡å—ã€‚

æˆ‘ä»¬çš„æ–¹æ³•åœ¨å½¢å¼ä¸Šä¸å›¾åƒã€éŸ³é¢‘ã€æ–‡æœ¬é¢†åŸŸçš„å¯¹æ¯”å­¦ä¹ ç›¸ä¼¼ï¼Œä½†æˆ‘ä»¬é¦–æ¬¡å°†å…¶ç”¨äºè§£å†³å®Œæ•´çš„RLé—®é¢˜ã€‚æˆ‘ä»¬å­¦ä¹ ä¸¤ç±»è¡¨ç¤ºï¼šä¸€ç§æ˜¯ï¼ˆçŠ¶æ€, åŠ¨ä½œï¼‰çš„è¡¨ç¤ºï¼Œå¦ä¸€ç§æ˜¯æœªæ¥çŠ¶æ€çš„è¡¨ç¤ºã€‚

æˆ‘ä»¬ä»è½¨è¿¹ä¸­é‡‡æ ·æ­£æ ·æœ¬ï¼ˆä¸‰å…ƒç»„æ¥è‡ªåŒä¸€è½¨è¿¹ï¼Œæœªæ¥çŠ¶æ€ç”¨å‡ ä½•åˆ†å¸ƒé‡‡æ ·ï¼Œå…³è”Qå€¼ï¼‰ï¼›è´Ÿæ ·æœ¬åˆ™æ¥è‡ªå…¶ä»–è½¨è¿¹ã€‚æœ€ç»ˆï¼Œè¿™äº›è¡¨ç¤ºèƒ½å¤Ÿæ¨ç†æœªæ¥å¯èƒ½è®¿é—®çš„çŠ¶æ€ï¼Œå¹¶æŒ‡å¼•åŠ¨ä½œé€‰æ‹©ä»¥å®ç°ç›®æ ‡ã€‚

ç®—æ³•æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ä»»æ„ç¦»çº¿æ•°æ®é›†ï¼ˆå¦‚éšæœºç­–ç•¥æ”¶é›†çš„è½¨è¿¹ï¼‰ï¼›
2. åº”ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒçŠ¶æ€/ç›®æ ‡çš„è¡¨ç¤ºï¼›
3. è®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œä½¿å…¶é€‰æ‹©èƒ½æœ€å¤§åŒ–ä¸ç›®æ ‡è¡¨ç¤ºç›¸ä¼¼åº¦çš„åŠ¨ä½œã€‚

è¿™ç§è®­ç»ƒæ–¹å¼æœ‰ä¸€ä¸ªå¾ˆç›´è§‚çš„**å‡ ä½•è§£é‡Š**ï¼šæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒçŠ¶æ€-åŠ¨ä½œè¡¨ç¤ºä¸ç›®æ ‡è¡¨ç¤ºçš„ç›¸ä¼¼åº¦ï¼Œå¼•å¯¼ç­–ç•¥é€‰æ‹©å°†å®ƒä»¬è·ç¦»æœ€å°åŒ–çš„åŠ¨ä½œâ€”â€”è¿™ç­‰ä»·äºåœ¨**è¡¨ç¤ºç©ºé—´ä¸­è¿›è¡Œè§„åˆ’**ã€‚

é€šå¸¸ï¼Œè§„åˆ’æ˜¯å¾ˆå›°éš¾çš„ï¼Œæ¯”å¦‚åœ¨å¤¹å—ä»»åŠ¡ä¸­ï¼Œå•çº¯å‘ç›®æ ‡ç§»åŠ¨ä¸èƒ½å®Œæˆä»»åŠ¡ï¼ˆå¿…é¡»å…ˆå¤¹èµ·æ–¹å—ï¼‰ã€‚ä½†é€šè¿‡æˆ‘ä»¬å­¦ä¹ åˆ°çš„è¡¨ç¤ºï¼Œè¿™ç§é—®é¢˜è¢«æœ‰æ•ˆç®€åŒ–â€”â€”**è´ªå©ªåœ°åœ¨è¡¨ç¤ºç©ºé—´è§„åˆ’å°±å¯ä»¥è·å¾—æœ€ä¼˜è§£**ã€‚

æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰å¥–åŠ± shapingã€æ²¡æœ‰ä¸“å®¶æ¼”ç¤ºã€ä»…åŸºäºå›¾åƒè¾“å…¥çš„ bin picking ä»»åŠ¡ä¸­ï¼Œåªæœ‰æˆ‘ä»¬çš„æ–¹æ³•èƒ½æˆåŠŸè§£å†³é—®é¢˜ã€‚

æˆ‘ä»¬ä¹Ÿä¸å¤šä¸ªå¼ºåŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬ï¼š

* ç›®æ ‡æ¡ä»¶è¡Œä¸ºå…‹éš†ï¼ˆgoal-conditioned behavioral cloningï¼‰ï¼›
* TD3+HERï¼ˆä¸€ç§ç»“åˆäº†åéªŒç»éªŒå›æ”¾çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰ã€‚

é™¤äº†æœ€ç®€å•çš„ä»»åŠ¡å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ›´å¥½çš„è¡¨ç°ã€‚

æ¬¢è¿æ¥æˆ‘ä»¬çš„ poster å±•ä½äº†è§£æ›´å¤šå®éªŒç»†èŠ‚å’Œæœªæ¥æ–¹å‘ï¼Œè°¢è°¢ï¼

---

å¦‚éœ€é…åˆ PPTã€æµ·æŠ¥æˆ–ç­”è¾©ç¨¿è¿›ä¸€æ­¥ç²¾ç‚¼ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¼˜åŒ–æ¼”è®²èŠ‚å¥å’Œé£æ ¼ã€‚æ˜¯å¦éœ€è¦æˆ‘ä¸ºä½ è¾“å‡º 2 åˆ†é’Ÿæˆ– 5 åˆ†é’Ÿå£è¯­æ¼”è®²ç‰ˆï¼Ÿ
