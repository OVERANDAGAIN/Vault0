
# Learning Latent Representations to Influence Multi-Agent Interaction

自我智能体只需观察对方的低层行为（如动作），但可以通过高层策略表示来理解对方意图。 我们将这种表示称为潜在策略（latent strategy），并认为潜在策略是可以随时间变化的。


好的，下面是论文方法部分（第4节）**Learning and Influencing Latent Intent (LILI)** 的总结，包括核心思想、主要模块、关键公式和伪代码逻辑：

---

## LILI 框架（Learning and Influencing Latent Intent）

### 总体目标

在与非平稳智能体的重复交互中，自我智能体不仅要学会响应对方的当前策略，还要**主动影响对方未来策略的变化方向**，以最大化长期累计奖励。

---

### 1. 潜在策略建模（Latent Strategy Modeling）

* 假设其他智能体的策略由一个**不可观测的高层潜在变量** $z_i$ 表征（在第 $i$ 次交互中保持恒定）。
* 该策略在交互之间演化：
  $$
  z_{i+1} \sim f(z_{i+1} \mid z_i, \tau^i)
  $$

  其中 $\tau^i = \{(s_t, a_t, r_t)\}_{t=1}^H$ 是第 $i$ 次交互的轨迹。

### 2. 编码器 + 解码器（Encoder + Decoder）

* **编码器 $E_\phi$**：从上一次交互轨迹中编码出对方的潜在策略

  $$
  z_i = E_\phi(\tau^{i-1})
  $$
* **解码器 $D_\psi$**：以 $z_i$ 为条件，预测下一步状态和奖励

  $$
  D_\psi(s_t, a_t, z_i) \rightarrow s_{t+1}, r_t
  $$

###  表示学习目标（Representation Learning Objective）

最大化潜在策略对下一交互的解释能力：

$$
\max_{\phi, \psi} \sum_{i=2}^{N} \sum_{t=1}^{H} \log p_{\phi,\psi}(s_t^{i}, r_t^{i} \mid s_t^{i}, a_t^{i}, \tau^{i-1})
$$

---

### 3. 策略网络（Latent-Conditioned Policy）

* 策略是条件策略：

  $$
  \pi_\theta(a_t \mid s_t, z_i)
  $$
* 每一轮根据上一交互轨迹预测 $z_i$，然后执行 $\pi(a_t \mid s_t, z_i)$

---

### 4. 长期奖励优化（Influence through Optimization）

强化学习目标是：**不仅当前轮奖励高，还要通过影响对方策略，使未来回合更有利**。

$$
\max_\theta \sum_{i=1}^{\infty} \gamma^i \mathbb{E}_{\rho^\pi_i} \left[ \sum_{t=1}^{H} R(s_t, z_i) \right]
$$

* $\rho^\pi_i$：在当前潜在策略 $z_i$ 下，由策略 $\pi$ 诱导出的轨迹分布；
* 目标：通过选择行为影响 $z_{i+1}$，提高长期回报。

---





* 编码器 $E_\phi$、解码器 $D_\psi$：两层全连接网络，隐藏层大小 128。
* 策略和 Q 网络：两层全连接网络，隐藏层大小 256。
* 潜在变量维度 $z$：设为 8。
* 强化学习算法基础：Soft Actor-Critic (SAC)。
