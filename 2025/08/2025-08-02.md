
# Learning Latent Representations to Influence Multi-Agent Interaction

自我智能体只需观察对方的低层行为（如动作），但可以通过高层策略表示来理解对方意图。 我们将这种表示称为潜在策略（latent strategy），并认为潜在策略是可以随时间变化的。

---

## LILI 框架（Learning and Influencing Latent Intent）

### 总体目标

在与非平稳智能体的重复交互中，自我智能体不仅要学会响应对方的当前策略，还要**主动影响对方未来策略的变化方向**，以最大化长期累计奖励。

---

### 从局部观察中学习潜在策略

* 假设其他智能体的策略由一个**不可观测的高层潜在变量** $z_i$ 表征（在第 $i$ 次交互中保持恒定）。
* 该策略在交互之间演化：
$$
  z_i = E_\phi(\tau^{i-1})
  $$

  其中 $\tau^i = \{(s_t, a_t, r_t)\}_{t=1}^H$ 是第 $i$ 次交互的轨迹。

#### 1. 编码器 + 解码器（Encoder + Decoder）

* **编码器 $E_\phi$**：从上一次交互轨迹中编码出对方的潜在策略

  $$
E_\phi(\tau^i) \approx f(z_{i+1} \mid z_i, \tau^i),
$$
* **解码器 $D_\psi$**：以 $z_i$ 为条件，预测下一步状态和奖励

$$
\max_{\phi, \psi} \sum_{i=2}^{N} \sum_{t=1}^{H} \log p_{\phi,\psi}(s_t^{i}, r_t^{i} \mid s_t^{i}, a_t^{i}, \tau^{i-1}),
$$

1. $E_\phi$ 根据前一轮轨迹 $\tau^{i-1}$ 生成表示 $z_i$，
2. $D_\psi$ 使用当前状态、动作和表示 $z_i$ 来预测下一个状态和奖励。

---

#### 2. 策略网络（Latent-Conditioned Policy）

* 策略是条件策略：

  $$
  \pi_\theta(a_t \mid s_t, z_i)
  $$
* 每一轮根据上一交互轨迹预测 $z_i$，然后执行 $\pi(a_t \mid s_t, z_i)$

---

#### 3. 长期奖励优化（Influence through Optimization）

强化学习目标是：**不仅当前轮奖励高，还要通过影响对方策略，使未来回合更有利**。

$$
\max_\theta \sum_{i=1}^{\infty} \gamma^i \mathbb{E}_{\rho^\pi_i} \left[ \sum_{t=1}^{H} R(s_t, z_i) \right]
$$

* $\rho^\pi_i$：在当前潜在策略 $z_i$ 下，由策略 $\pi$ 诱导出的轨迹分布；
* 目标：通过选择行为影响 $z_{i+1}$，提高长期回报。

---

* 编码器 $E_\phi$、解码器 $D_\psi$：两层全连接网络，隐藏层大小 128。
* 策略和 Q 网络：两层全连接网络，隐藏层大小 256。
* 潜在变量维度 $z$：设为 8。
* 强化学习算法基础：Soft Actor-Critic (SAC)。



| 环境名称                | 类型      | 是否能影响对方策略演化 | 主要挑战/特点                             |
| ------------------- | ------- | ----------- | ----------------------------------- |
| **Point Mass**      | 仿真      | 影响（圆内/外）    | 目标隐形；策略变化受自我行为控制；测试“诱导策略”能力         |
| **Lunar Lander**    | 仿真      | 不可影响，只随时间变化 | 着陆目标交替变动；奖励间接传达意图                   |
| **Driving (2D)**    | 仿真      | 影响          | 对方车辆在下一轮切入上轮 ego 选择的车道              |
| **Driving (CARLA)** | 仿真      | 影响          | CARLA高保真环境；多因素影响；训练更不稳定             |
| **Air Hockey**      | 真实机器人实验 | 影响          | 对手每轮更新目标区域；ego 无法观测目标；测试策略诱导与人类泛化能力 |
