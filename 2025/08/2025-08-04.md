---
创建时间: 2025-八月-4日  星期一, 12:19:26 中午
---
以下是第 3 部分 **“多智能体双层世界模型（Multi-Agent Bi-Level World Model）”** 的翻译：

---

## 3 多智能体双层世界模型（Multi-Agent Bi-Level World Model）

我们提出了一种新颖的基于模型的多智能体强化学习算法，称为 **MABL（Multi-Agent Bi-Level world model）**，其核心是使用**潜变量世界模型架构**。关键思想在于：一些关键的**全局信息**（如所有智能体的观测和训练期间可用的其他信息）**在执行阶段并不需要被直接使用，只需在训练阶段指导潜在状态的学习即可**。这样，我们就能够实现**集中训练 + 去中心化执行（CTDE）**。

具体而言，MABL 提出了一个**双层潜变量世界模型（bi-level latent-variable world model）**：

* 顶层（上层）学习一个**全局潜在状态（global latent state）**，用于编码环境的全局信息；
* 底层（下层）在条件化于全局潜在状态的基础上，学习**智能体潜在状态（agent latent state）**，以捕捉个体的局部信息。

智能体在执行过程中只需根据其当前观测，推断出自己的潜在状态即可。这种设计既**提高了表示能力**（因为训练时用了全局信息），又**保留了执行时的去中心化性**。

本节将介绍：

1. MABL 的模型架构；
2. 如何利用该模型编码全局信息；
3. 如何训练该模型以及在潜在轨迹上进行 MARL 策略学习。

以下内容均以某个智能体 $i$ 为例进行说明。

---

### 3.1 双层世界模型（Bi-Level World Model）

为有效捕捉多智能体环境中的动态，我们设计了一种新颖的双层架构，能够从高维输入中提取出用于预测的潜在表示。

输入是长度为 $T$ 的多智能体轨迹，形式为：

$$
\{o^i_t, s_t, a_{t-1}, r^i_t\}_{t=1}^T
$$

这些轨迹从与环境交互中获得，存储在称为\*\*模型缓冲区（model buffer）\*\*的数据结构中。该模型由一系列神经网络组成，分为两个功能模块：

1. **转移动态模块（Transition Dynamics Components）**：学习状态转移规律；
2. **辅助模块（Auxiliary Components）**：辅助策略训练，如奖励、终止、动作可用性等预测。

这些模块都使用神经网络参数 $\psi$ 表示，并共同训练。每个智能体拥有一个 bi-level 模型，但所有智能体共享参数 $\psi$，以实现可扩展性。

---

#### 3.1.1 转移动态模块（Transition Dynamics Components）

如图 2 所示，主要包括：

* **循环模型（Recurrent Models）**
* **表示模型（Representation Model）**
* **转移模型（Transition Model）**
* **观测模型（Observation Model）**

**循环模型（RNN）用于保留历史信息**：

* **全局循环：**

  $$
  h^g_{i,t} = f^g_\psi(h^g_{i,t-1}, z^g_{i,t-1}, a_{t-1})
  $$
* **智能体循环：**

  $$
  h^a_{i,t} = f^a_\psi(h^a_{i,t-1}, z^a_{i,t-1}, a_{t-1})
  $$

**表示模型（后验分布）：**

* **全局：**

  $$
  z^g_{i,t} \sim q_\psi(z^g_{i,t} | s_t, z^a_{i,t}, h^g_{i,t})
  $$
* **智能体：**

  $$
  z^a_{i,t} \sim q_\psi(z^a_{i,t} | o^i_t, h^a_{i,t})
  $$

**转移模型（先验分布）：**

* **全局：**

  $$
  z^g_{i,t} \sim p_\psi(z^g_{i,t} | h^g_{i,t})
  $$
* **智能体：**

  $$
  z^a_{i,t} \sim p_\psi(z^a_{i,t} | h^a_{i,t}, z^g_{i,t})
  $$

**观测模型：**

* $$
  o^i_t \sim p_\psi(o^i_t | h^a_{i,t}, z^a_{i,t})
  $$

**结构优势：**

这种结构使得智能体的潜在状态 $z^a_{i,t}$ 能够在训练时获得从全局潜在状态 $z^g_{i,t}$ 传下来的信息，而在执行时，**只需根据自身观测推断 $z^a_{i,t}$，无需用到 $z^g_{i,t}$**，从而保证了去中心化执行。

---

#### 3.1.2 辅助模块（Auxiliary Components）

这些组件负责生成用于训练的奖励信号和环境信息，包括：

* **奖励预测器（Reward Predictor）**：

  $$
  r^i_t \sim p_\psi(r^i_t | z^a_{i,t}, h^a_{i,t})
  $$
* **终止预测器（Termination Predictor）**：

  $$
  \gamma^i_t \sim p_\psi(\gamma^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$
* **可用动作预测器（Available Action Predictor）**：

  $$
  A^i_t \sim p_\psi(A^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$
* **动作解码器（Action Decoder）**（辅助）：鼓励潜在状态中包含与动作相关的信息：

  $$
  a^i_t \sim p_\psi(a^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

---

接下来是 3.2 节：**模型训练（Training the Model）**，将会讲解损失函数设计、ELBO 推导和辅助目标。如果你想继续，请回复“继续”。



以下是第 3 部分的后续内容：**3.2 模型训练（Training the Model）** 的翻译：

---

### 3.2 模型训练（Training the Model）

介绍完 MABL 的双层结构后，我们接下来说明用于训练该模型的损失函数。

我们通过联合优化模型的所有组件，最小化总损失函数 $\mathcal{L}(\psi)$。该损失由多个部分组成：

$$
\mathcal{L}(\psi) = \mathcal{L}_{\text{ELBO}} + \mathcal{L}_{r_t} + \mathcal{L}_{\gamma_t} + \mathcal{L}_{A_t} + \mathcal{L}_{a_t}
$$

其中：

* $\mathcal{L}_{\text{ELBO}}$：用于训练转移动态模块；
* $\mathcal{L}_{r_t}$：奖励预测损失；
* $\mathcal{L}_{\gamma_t}$：终止条件预测损失；
* $\mathcal{L}_{A_t}$：可用动作预测损失；
* $\mathcal{L}_{a_t}$：动作解码损失。

---

#### ELBO 损失（Evidence Lower Bound）

该项是使用\*\*变分推理（amortized variational inference）\*\*进行训练的核心目标：

$$
\mathcal{L}_{\text{ELBO}} = - \sum_{t=1}^{T} \left[
\log p_\psi(o^i_t | z^a_{i,t}, h^a_{i,t}) 
- \text{KL}\left( q_\psi(z^a_{i,t} | o^i_t, h^a_{i,t}) \parallel p_\psi(z^a_{i,t} | h^a_{i,t}, z^g_{i,t}) \right)
- \text{KL}\left( q_\psi(z^g_{i,t} | s_t, z^a_{i,t}, h^g_{i,t}) \parallel p_\psi(z^g_{i,t} | h^g_{i,t}) \right)
\right]
$$

* 第一个项最大化了观测的重构概率；
* 后两个 KL 散度项使得先验分布和后验分布之间接近；
* 我们对先验和后验进行“全局 + 智能体”级别的因子化，因此有两个 KL 项；
* 为了提升训练效果，使用了 **KL 平衡（KL balancing）** 技术【参考文献 \[13]】。

---

#### 辅助损失项（Auxiliary Losses）

这些项用于训练辅助模块：

* **奖励预测损失：**

  $$
  \mathcal{L}_{r_t} = - \sum_{t=1}^{T} \log p_\psi(r^i_t | z^a_{i,t}, h^a_{i,t})
  $$

* **终止预测损失：**

  $$
  \mathcal{L}_{\gamma_t} = - \sum_{t=1}^{T} \log p_\psi(\gamma^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

* **可用动作预测损失：**

  $$
  \mathcal{L}_{A_t} = - \sum_{t=1}^{T} \log p_\psi(A^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

* **动作解码损失（可选辅助项）：**

  $$
  \mathcal{L}_{a_t} = - \sum_{t=1}^{T} \log p_\psi(a^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

这些损失项共同训练整个 MABL 模型，使其能够学习高质量的潜在空间，并生成高质量的潜在轨迹，用于后续的策略训练。

---

如果你想继续翻译第 3.3 节 **“多智能体行为学习（Learning Multi-Agent Behavior）”**，请告诉我。
以下是第 3 部分的最后一节：**3.3 多智能体行为学习（Learning Multi-Agent Behavior）** 的翻译：

---

### 3.3 多智能体行为学习（Learning Multi-Agent Behavior）

我们方法的一个重要优势是：**模型训练与策略学习是解耦的**。这意味着我们可以在使用 MABL 生成的潜在轨迹上，使用任意现成的 MARL 算法进行策略训练，无需对原算法做特殊修改。

我们以通用的**actor-critic 多智能体强化学习算法**为例，说明如何使用 MABL 所生成的潜在轨迹来训练策略。

---

#### 策略网络（Actor）

每个智能体都配备一个策略网络（actor），记作 $\pi^i_\theta$，由参数 $\theta$ 表示。该策略用于最大化期望收益。在时间步 $t$，智能体接收输入：

* 执行阶段使用 $z^a_{i,t}$（从观测计算得到）；
* 训练阶段使用从模型生成的潜在状态；

策略选择动作的形式为：

$$
a^i_t \sim \pi_\theta(a^i_t \mid z^a_{i,t}, h^a_{i,t})
$$

其中 $h^a_{i,t}$ 是智能体的 RNN 状态，用于整合历史信息。由于 $z^a_{i,t}$ 可仅由当前观测推得，因此整个策略是**可去中心化执行的**。

---

#### 价值网络（Critic）

每个智能体也配有一个值函数 $V^i_\phi$，由参数 $\phi$ 表示，用于估计状态值：

$$
V^i_t = V^i_\phi(z^i_t, h^a_{i,t}, h^g_{i,t})
$$

其中：

* $z^i_t$：由全局潜在状态和智能体潜在状态拼接而成；
* $h^g_{i,t}$、$h^a_{i,t}$：对应的 RNN 嵌入。

因为 critic 是**集中式的**，可以使用所有信息。这与 CTDE 框架一致。我们在实现中引入了 **自注意力机制（self-attention）**【参考文献 \[39]】，以增强全局表示能力。

在训练过程中，actor 和 critic 的网络参数**在所有智能体之间共享**，这在大规模多智能体任务中有助于提升训练效率【参考文献 \[46]】。

---

#### 价值函数方法的替代选项

如果使用的是基于 Q 值的算法（如 Q-MIX \[31]），也可以用以下方式替代上述结构：

* 每个智能体使用 Q 值函数：

  $$
  Q^i(z^a_{i,t}, h^a_{i,t})
  $$
* 所有智能体的 Q 值通过 mixing network 聚合为全局 Q 值：

  $$
  Q_{\text{tot}}(Q^1, \dots, Q^n, z^g_{1,t}, \dots, z^g_{n,t})
  $$

---

#### 训练流程（如算法 1 所示）

我们在**潜在空间中完成策略学习**，即所有用于训练的轨迹都是由模型生成的潜在状态轨迹。

MABL 的训练过程包含三个阶段：

1. **真实环境交互**：

   * 所有智能体与环境交互，收集真实轨迹数据；
   * 数据存入模型缓冲区 $\mathcal{D}$。

2. **训练世界模型（MABL 模型）**：

   * 从缓冲区采样数据序列 $\mathcal{B}_M$，使用损失 $\mathcal{L}(\psi)$ 训练 bi-level 模型；
   * 训练完成后，**冻结模型权重**，为策略训练做准备。

3. **策略训练（基于潜在轨迹）**：

   * 从缓冲区采样 $\mathcal{B}_R$，提取起始状态；
   * 使用 MABL 模型递推，生成长度为 $H$ 的潜在轨迹；
   * 辅助模块输出奖励、终止标志、可用动作等；
   * 得到潜在轨迹 $\mathcal{D}_L$，其结构为：

     $$
     \{z^a_{i,1:H}, z^g_{i,1:H}, h^a_{i,1:H}, h^g_{i,1:H}, r^i_{1:H}, \gamma^i_{1:H}, A^i_{1:H} \}_{i=1}^n
     $$
   * 使用此潜在轨迹进行策略训练。

我们使用 **MAPPO**【参考文献 \[46]】作为默认策略学习算法，它在多个任务上表现出色。你也可以替换为其他任意符合 CTDE 架构的策略算法。

---

如需继续翻译**第 4 部分“实验评估（Experimental Evaluation）”**，请告诉我。





非常好，这里我们不只是逐段翻译文字，而是需要**系统性地梳理和解释第3章的完整方法思想**，使你能够真正理解 MABL 方法的设计动机、架构组成、信息流动、训练方式，以及与其他模型的关键差异。

---

## ✅ 第3章：MABL 方法核心思想深度解读（含全翻译+结构重构）

---

### 🧠 方法目标简述

> 在多智能体强化学习中，如何利用训练时的全局信息来提升潜在空间建模，同时仍然实现“去中心化执行”？MABL 的答案是：
>
> ✨ 训练时使用双层潜变量架构，**上层编码全局信息，下层用于执行策略**，二者条件化连接，从而实现 CTDE 框架下的模型增强。

---

## 🧱 3.1 Bi-Level World Model 架构解析

### 🔍 核心设计理念

MABL 提出了一种“**双层潜变量模型（Bi-Level Latent Variable World Model）**”，包括：

* **全局潜在状态 `z_g`**：捕捉整个环境的全局信息，如所有智能体的观测、全局状态；
* **智能体潜在状态 `z_a`**：每个智能体专属的表示，仅基于本地观测推导，用于执行决策。

设计目标是：

* 训练时通过 `z_g` → `z_a` 的条件依赖，引导 `z_a` 更好地融合上下文信息；
* 执行时只使用 `z_a`，不依赖 `z_g`，保证每个 agent 独立行动。

### 📊 信息流动流程（时间步 $t$）

| 模块               | 作用            | 依赖信息                                    | 输出                |
| ---------------- | ------------- | --------------------------------------- | ----------------- |
| RNN（全局） $h^g_t$  | 记忆环境历史        | $h^g_{t-1}, z^g_{t-1}, a_{t-1}$         | $h^g_t$           |
| RNN（智能体） $h^a_t$ | 记忆 agent 自身历史 | $h^a_{t-1}, z^a_{t-1}, a_{t-1}$         | $h^a_t$           |
| 表示模型（后验）         | 编码真实观察信息      | 全局：$s_t, z^a_t, h^g_t$；智能体：$o_t, h^a_t$ | $z^g_t, z^a_t$    |
| 转移模型（先验）         | 模拟下一状态        | 全局：$h^g_t$；智能体：$h^a_t, z^g_t$           | 预测 $z^g_t, z^a_t$ |
| 解码器              | 重构观测          | $z^a_t, h^a_t$                          | $\hat{o}_t$       |

**重点：** 在训练时使用后验（真实状态+观测），在采样时使用先验（模型预测）。

---

### 🎯 双层结构的优点

1. **信息融合方向是双向的**：

   * 自顶向下：$z^g_t \to z^a_t$，强化上下文建模；
   * 自底向上：$z^a_t \to z^g_t$，使全局状态能适应当前 agent 状态。

2. **训练与执行解耦**：

   * 训练用全部观测训练 `z_g`；
   * 执行时无需 `z_g`，只用 `z_a` 推出策略，满足 CTDE。

---

## 🎯 3.2 模型训练目标总结

训练目标的核心是最大化数据的证据下界（ELBO）：

$$
\mathcal{L}_{\text{ELBO}} = 
- \sum_t \log p(o^i_t | z^a_{i,t}, h^a_{i,t})
+ \text{KL}(q(z^a_{i,t} | o^i_t, h^a_{i,t}) \| p(z^a_{i,t} | h^a_{i,t}, z^g_{i,t}))
+ \text{KL}(q(z^g_{i,t} | s_t, z^a_{i,t}, h^g_{i,t}) \| p(z^g_{i,t} | h^g_{i,t}))
$$

这个目标训练的是世界模型的转移动态部分。

此外，还有几个**辅助模块的损失函数**：

* 奖励预测 $r_t \sim p(r_t | z^a_t, h^a_t)$
* 终止预测 $\gamma_t \sim p(\cdot | z^a_t, z^g_t, h^a_t, h^g_t)$
* 动作可用性预测 $A_t \sim p(\cdot | z^a_t, z^g_t, h^a_t, h^g_t)$
* 动作解码器（用于鼓励表示关联）$a_t \sim p(\cdot | z^a_t, z^g_t, h^a_t, h^g_t)$

这些模块共同训练模型，使其不仅能生成合成轨迹，还能输出所需的训练标签。

---

## 🧪 3.3 策略学习方式（完全在潜在空间中完成）

使用模型生成的潜在状态轨迹，训练任意 MARL 算法（例如 MAPPO），而非直接用原始环境数据。

### 🌀 采样流程：

1. 从 replay buffer 中取出真实轨迹；
2. 使用后验 $q$ 编码起始潜在状态 $z^g_1, z^a_1$，并初始化 RNN 状态；
3. 使用 transition 模型迭代 rollout $H$ 步；
4. 每一步：

   * 使用当前策略从 $z^a_t$ 采样动作；
   * 由 transition 模型预测 $z^a_{t+1}, z^g_{t+1}$；
   * 辅助模块输出 reward、done、available actions 等；
5. 得到完整的潜在轨迹 $D_L$ 后，用于 actor-critic 更新。

### ✅ 与 Dreamer-v2、MAMBA 等的关键区别

| 模型         | 是否使用全局信息训练        | 是否去中心化执行 | 多智能体结构               |
| ---------- | ----------------- | -------- | -------------------- |
| Dreamer-v2 | ❌（只用局部）           | ✅        | 每个 agent 独立训练        |
| MAMBA      | ✅（使用 transformer） | ❌（执行也集中） | 用 transformer 聚合所有观测 |
| **MABL**   | ✅（全局训练）           | ✅（只用本地）  | 双层结构，参数共享            |

---

## ✅ 总结：MABL 方法的精髓

> MABL 利用了训练时的全局信息进行建模，但在执行时只用局部信息，从而做到了样本效率高、执行解耦、支持多 agent 的泛化能力。

---

如需继续第 4 章 **实验评估（Experimental Evaluation）** 的深度翻译与总结，请继续说“继续”。如果你想基于第3章生成图示或伪代码，我也可以补充。
