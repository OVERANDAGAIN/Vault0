以下是第4节 **Learning and Influencing Latent Intent (LILI)** 的翻译，对应方法部分的核心内容：

---

## 4 学习与影响潜在意图（LILI）

本节介绍我们提出的方法框架 LILI，目标是：**在多智能体交互中学习和影响对方的潜在策略（latent strategy）**。

请参考图 2（原文），LILI 框架包含两大模块：

1. 一个**编码器**（encoder），从低层观察中提取并推断其他智能体的潜在策略；
2. 一个**策略网络**（policy），依据上述潜在策略做出自我决策。

在每轮交互中，自我智能体基于前一次交互轨迹预测对方的当前策略，然后依据该策略选择响应动作。下面逐步解释这个方法的各个组成部分。

---

### 4.1 从局部观察中学习潜在策略

第一步是学习一种能够表示对方行为的潜在策略。
在第 $i$ 次交互中，自我智能体经历了一段轨迹 $\tau^i$，包含状态、动作和奖励序列。我们利用这段经验来**预测对方在第 $i+1$ 次交互中的潜在策略 $z_{i+1}$**。

为此，我们引入一个编码器 $E_\phi$，它将经验 $\tau^i$ 映射为策略表示，建模方式如下：

$$
E_\phi(\tau^i) \approx f(z_{i+1} \mid z_i, \tau^i),
$$

即用 $\tau^i$ 近似替代 $z_i$，建模潜在策略动态 $f$。我们发现这种方式在实际环境中效果良好。需要注意的是，**我们从未显式观察过 $z$** —— 那我们如何训练这个编码器呢？

关键在于：对方智能体的策略决定了它如何响应自我智能体的行为，进而影响自我智能体在环境中经历的**转移函数**与**奖励函数**。因此，我们可以使用一个**解码器 $D_\psi$** 来重建自我智能体在第 $i+1$ 次交互中的观察。

给定一系列交互 $\tau_1, \dots, \tau_N$，我们定义以下最大似然表示学习目标：

$$
\max_{\phi, \psi} \sum_{i=2}^{N} \sum_{t=1}^{H} \log p_{\phi,\psi}(s_t^{i}, r_t^{i} \mid s_t^{i}, a_t^{i}, \tau^{i-1}),
$$

其中：

* $E_\phi$ 根据前一轮轨迹 $\tau^{i-1}$ 生成表示 $z_i$，
* $D_\psi$ 使用当前状态、动作和表示 $z_i$ 来预测下一个状态和奖励。

通过上述目标，我们可以训练一个编码器用于**预测对方下一轮策略**。

---

### 4.2 基于潜在策略进行强化学习决策

有了对方潜在策略的预测，自我智能体可以做出更具针对性的响应策略。
举例来说，假设自动驾驶汽车知道人类司机当前的策略是“如果自动车大幅减速，我就绕行超车”。若此时前方正有交通拥堵，那么自动车就应选择“逐步减速”，保持人类车在后方，从而缓慢调节车速。

反之，如果它认为人类会跟随减速，那么可以直接刹车以快速降低速度。
这说明：**不同的潜在策略要求不同的响应策略**。

因此，我们为自我智能体学习一个**策略网络** $\pi_\theta(a \mid s, z)$，条件是当前状态和预测到的对方策略表示 $z = E_\phi(\tau^{i-1})$。

---

### 4.3 通过长期奖励优化实现策略影响

我们的最终目标不只是让自我智能体被动适应对方策略，而是要让其**主动影响对方的策略**，以最大化长期收益。实现这一点的关键是**编码器 $E_\phi$**，它提供了对潜在策略动态的近似建模。

随着 $E_\phi$ 的学习逐步完善，自我智能体可以调整其当前行为轨迹 $\tau^i$，**主动引导对方策略朝着更有利的方向变化**，即导向某个期望的 $z_{i+1}$。

举例来说，在我们的交通场景中：

* 如果人类司机的策略空间是 $z_1 =$ 避让、$z_2 =$ 跟随，
* 而自动车希望在需要时能减速并让人类配合，
* 那么就需要通过一系列交互，**将人类策略诱导到 $z_2$**（信任并跟随）。

为此，我们的目标函数是：

$$
\max_\theta \sum_{i=1}^{\infty} \gamma^i \mathbb{E}_{\rho^\pi_i} \left[ \sum_{t=1}^H R(s_t, z_i) \right],
$$

其中：

* $\gamma$ 是折扣因子；
* $\rho^\pi_i$ 表示第 $i$ 次交互中，由策略 $\pi_\theta$ 和潜在策略 $z_i$ 所决定的轨迹分布。

这个强化学习目标**天然鼓励自我智能体采取能够正向影响对方策略的行为**，从而为后续交互带来更高的奖励。

---

### 4.4 实现细节与训练算法

我们使用软策略梯度方法 **Soft Actor-Critic (SAC)** 作为基础强化学习算法。整体训练流程如下：

#### 算法 1：LILI 方法训练流程

1. 初始化策略网络 $\pi_\theta$、Q网络 $Q_\theta$、编码器 $E_\phi$、解码器 $D_\psi$；
2. 初始化重放缓冲区 $\mathcal{B}$；
3. 每轮交互：

   * 用当前策略 $\pi_\theta(a \mid s, z_i)$ 与环境交互，收集轨迹 $\tau^i$，加入 $\mathcal{B}$；
   * 多次从缓冲区采样交互对 $(\tau, \tau')$，更新各模块参数：

     * Q 网络（critic）更新；
     * 策略网络（actor）更新；
     * 编码器参数更新（受益于表示学习损失和 Q 函数梯度）；
     * 解码器参数更新（用于还原 $\tau'$）；
   * 用编码器预测下一轮策略表示 $z_{i+1} = E_\phi(\tau^i)$。

> 具体网络结构：编码器与解码器使用两层全连接网络（128维），策略与Q网络使用两层256维MLP，潜在表示维度为8。

---

如需继续第5章“实验”，或希望我先总结第4章方法核心思路，我都可以继续。是否继续？
