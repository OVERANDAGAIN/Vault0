---
创建时间: 2025-八月-4日  星期一, 12:19:26 中午
---
以下是第 3 部分 **“多智能体双层世界模型（Multi-Agent Bi-Level World Model）”** 的翻译：

---

## 3 多智能体双层世界模型（Multi-Agent Bi-Level World Model）

我们提出了一种新颖的基于模型的多智能体强化学习算法，称为 **MABL（Multi-Agent Bi-Level world model）**，其核心是使用**潜变量世界模型架构**。关键思想在于：一些关键的**全局信息**（如所有智能体的观测和训练期间可用的其他信息）**在执行阶段并不需要被直接使用，只需在训练阶段指导潜在状态的学习即可**。这样，我们就能够实现**集中训练 + 去中心化执行（CTDE）**。

具体而言，MABL 提出了一个**双层潜变量世界模型（bi-level latent-variable world model）**：

* 顶层（上层）学习一个**全局潜在状态（global latent state）**，用于编码环境的全局信息；
* 底层（下层）在条件化于全局潜在状态的基础上，学习**智能体潜在状态（agent latent state）**，以捕捉个体的局部信息。

智能体在执行过程中只需根据其当前观测，推断出自己的潜在状态即可。这种设计既**提高了表示能力**（因为训练时用了全局信息），又**保留了执行时的去中心化性**。

本节将介绍：

1. MABL 的模型架构；
2. 如何利用该模型编码全局信息；
3. 如何训练该模型以及在潜在轨迹上进行 MARL 策略学习。

以下内容均以某个智能体 $i$ 为例进行说明。

---

### 3.1 双层世界模型（Bi-Level World Model）

为有效捕捉多智能体环境中的动态，我们设计了一种新颖的双层架构，能够从高维输入中提取出用于预测的潜在表示。

输入是长度为 $T$ 的多智能体轨迹，形式为：

$$
\{o^i_t, s_t, a_{t-1}, r^i_t\}_{t=1}^T
$$

这些轨迹从与环境交互中获得，存储在称为\*\*模型缓冲区（model buffer）\*\*的数据结构中。该模型由一系列神经网络组成，分为两个功能模块：

1. **转移动态模块（Transition Dynamics Components）**：学习状态转移规律；
2. **辅助模块（Auxiliary Components）**：辅助策略训练，如奖励、终止、动作可用性等预测。

这些模块都使用神经网络参数 $\psi$ 表示，并共同训练。每个智能体拥有一个 bi-level 模型，但所有智能体共享参数 $\psi$，以实现可扩展性。

---

#### 3.1.1 转移动态模块（Transition Dynamics Components）

如图 2 所示，主要包括：

* **循环模型（Recurrent Models）**
* **表示模型（Representation Model）**
* **转移模型（Transition Model）**
* **观测模型（Observation Model）**

**循环模型（RNN）用于保留历史信息**：

* **全局循环：**

  $$
  h^g_{i,t} = f^g_\psi(h^g_{i,t-1}, z^g_{i,t-1}, a_{t-1})
  $$
* **智能体循环：**

  $$
  h^a_{i,t} = f^a_\psi(h^a_{i,t-1}, z^a_{i,t-1}, a_{t-1})
  $$

**表示模型（后验分布）：**

* **全局：**

  $$
  z^g_{i,t} \sim q_\psi(z^g_{i,t} | s_t, z^a_{i,t}, h^g_{i,t})
  $$
* **智能体：**

  $$
  z^a_{i,t} \sim q_\psi(z^a_{i,t} | o^i_t, h^a_{i,t})
  $$

**转移模型（先验分布）：**

* **全局：**

  $$
  z^g_{i,t} \sim p_\psi(z^g_{i,t} | h^g_{i,t})
  $$
* **智能体：**

  $$
  z^a_{i,t} \sim p_\psi(z^a_{i,t} | h^a_{i,t}, z^g_{i,t})
  $$

**观测模型：**

* $$
  o^i_t \sim p_\psi(o^i_t | h^a_{i,t}, z^a_{i,t})
  $$

**结构优势：**

这种结构使得智能体的潜在状态 $z^a_{i,t}$ 能够在训练时获得从全局潜在状态 $z^g_{i,t}$ 传下来的信息，而在执行时，**只需根据自身观测推断 $z^a_{i,t}$，无需用到 $z^g_{i,t}$**，从而保证了去中心化执行。

---

#### 3.1.2 辅助模块（Auxiliary Components）

这些组件负责生成用于训练的奖励信号和环境信息，包括：

* **奖励预测器（Reward Predictor）**：

  $$
  r^i_t \sim p_\psi(r^i_t | z^a_{i,t}, h^a_{i,t})
  $$
* **终止预测器（Termination Predictor）**：

  $$
  \gamma^i_t \sim p_\psi(\gamma^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$
* **可用动作预测器（Available Action Predictor）**：

  $$
  A^i_t \sim p_\psi(A^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$
* **动作解码器（Action Decoder）**（辅助）：鼓励潜在状态中包含与动作相关的信息：

  $$
  a^i_t \sim p_\psi(a^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

---

接下来是 3.2 节：**模型训练（Training the Model）**，将会讲解损失函数设计、ELBO 推导和辅助目标。如果你想继续，请回复“继续”。



以下是第 3 部分的后续内容：**3.2 模型训练（Training the Model）** 的翻译：

---

### 3.2 模型训练（Training the Model）

介绍完 MABL 的双层结构后，我们接下来说明用于训练该模型的损失函数。

我们通过联合优化模型的所有组件，最小化总损失函数 $\mathcal{L}(\psi)$。该损失由多个部分组成：

$$
\mathcal{L}(\psi) = \mathcal{L}_{\text{ELBO}} + \mathcal{L}_{r_t} + \mathcal{L}_{\gamma_t} + \mathcal{L}_{A_t} + \mathcal{L}_{a_t}
$$

其中：

* $\mathcal{L}_{\text{ELBO}}$：用于训练转移动态模块；
* $\mathcal{L}_{r_t}$：奖励预测损失；
* $\mathcal{L}_{\gamma_t}$：终止条件预测损失；
* $\mathcal{L}_{A_t}$：可用动作预测损失；
* $\mathcal{L}_{a_t}$：动作解码损失。

---

#### ELBO 损失（Evidence Lower Bound）

该项是使用\*\*变分推理（amortized variational inference）\*\*进行训练的核心目标：

$$
\mathcal{L}_{\text{ELBO}} = - \sum_{t=1}^{T} \left[
\log p_\psi(o^i_t | z^a_{i,t}, h^a_{i,t}) 
- \text{KL}\left( q_\psi(z^a_{i,t} | o^i_t, h^a_{i,t}) \parallel p_\psi(z^a_{i,t} | h^a_{i,t}, z^g_{i,t}) \right)
- \text{KL}\left( q_\psi(z^g_{i,t} | s_t, z^a_{i,t}, h^g_{i,t}) \parallel p_\psi(z^g_{i,t} | h^g_{i,t}) \right)
\right]
$$

* 第一个项最大化了观测的重构概率；
* 后两个 KL 散度项使得先验分布和后验分布之间接近；
* 我们对先验和后验进行“全局 + 智能体”级别的因子化，因此有两个 KL 项；
* 为了提升训练效果，使用了 **KL 平衡（KL balancing）** 技术【参考文献 \[13]】。

---

#### 辅助损失项（Auxiliary Losses）

这些项用于训练辅助模块：

* **奖励预测损失：**

  $$
  \mathcal{L}_{r_t} = - \sum_{t=1}^{T} \log p_\psi(r^i_t | z^a_{i,t}, h^a_{i,t})
  $$

* **终止预测损失：**

  $$
  \mathcal{L}_{\gamma_t} = - \sum_{t=1}^{T} \log p_\psi(\gamma^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

* **可用动作预测损失：**

  $$
  \mathcal{L}_{A_t} = - \sum_{t=1}^{T} \log p_\psi(A^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

* **动作解码损失（可选辅助项）：**

  $$
  \mathcal{L}_{a_t} = - \sum_{t=1}^{T} \log p_\psi(a^i_t | z^a_{i,t}, z^g_{i,t}, h^a_{i,t}, h^g_{i,t})
  $$

这些损失项共同训练整个 MABL 模型，使其能够学习高质量的潜在空间，并生成高质量的潜在轨迹，用于后续的策略训练。

---

如果你想继续翻译第 3.3 节 **“多智能体行为学习（Learning Multi-Agent Behavior）”**，请告诉我。
以下是第 3 部分的最后一节：**3.3 多智能体行为学习（Learning Multi-Agent Behavior）** 的翻译：

---

### 3.3 多智能体行为学习（Learning Multi-Agent Behavior）

我们方法的一个重要优势是：**模型训练与策略学习是解耦的**。这意味着我们可以在使用 MABL 生成的潜在轨迹上，使用任意现成的 MARL 算法进行策略训练，无需对原算法做特殊修改。

我们以通用的**actor-critic 多智能体强化学习算法**为例，说明如何使用 MABL 所生成的潜在轨迹来训练策略。

---

#### 策略网络（Actor）

每个智能体都配备一个策略网络（actor），记作 $\pi^i_\theta$，由参数 $\theta$ 表示。该策略用于最大化期望收益。在时间步 $t$，智能体接收输入：

* 执行阶段使用 $z^a_{i,t}$（从观测计算得到）；
* 训练阶段使用从模型生成的潜在状态；

策略选择动作的形式为：

$$
a^i_t \sim \pi_\theta(a^i_t \mid z^a_{i,t}, h^a_{i,t})
$$

其中 $h^a_{i,t}$ 是智能体的 RNN 状态，用于整合历史信息。由于 $z^a_{i,t}$ 可仅由当前观测推得，因此整个策略是**可去中心化执行的**。

---

#### 价值网络（Critic）

每个智能体也配有一个值函数 $V^i_\phi$，由参数 $\phi$ 表示，用于估计状态值：

$$
V^i_t = V^i_\phi(z^i_t, h^a_{i,t}, h^g_{i,t})
$$

其中：

* $z^i_t$：由全局潜在状态和智能体潜在状态拼接而成；
* $h^g_{i,t}$、$h^a_{i,t}$：对应的 RNN 嵌入。

因为 critic 是**集中式的**，可以使用所有信息。这与 CTDE 框架一致。我们在实现中引入了 **自注意力机制（self-attention）**【参考文献 \[39]】，以增强全局表示能力。

在训练过程中，actor 和 critic 的网络参数**在所有智能体之间共享**，这在大规模多智能体任务中有助于提升训练效率【参考文献 \[46]】。

---

#### 价值函数方法的替代选项

如果使用的是基于 Q 值的算法（如 Q-MIX \[31]），也可以用以下方式替代上述结构：

* 每个智能体使用 Q 值函数：

  $$
  Q^i(z^a_{i,t}, h^a_{i,t})
  $$
* 所有智能体的 Q 值通过 mixing network 聚合为全局 Q 值：

  $$
  Q_{\text{tot}}(Q^1, \dots, Q^n, z^g_{1,t}, \dots, z^g_{n,t})
  $$

---

#### 训练流程（如算法 1 所示）

我们在**潜在空间中完成策略学习**，即所有用于训练的轨迹都是由模型生成的潜在状态轨迹。

MABL 的训练过程包含三个阶段：

1. **真实环境交互**：

   * 所有智能体与环境交互，收集真实轨迹数据；
   * 数据存入模型缓冲区 $\mathcal{D}$。

2. **训练世界模型（MABL 模型）**：

   * 从缓冲区采样数据序列 $\mathcal{B}_M$，使用损失 $\mathcal{L}(\psi)$ 训练 bi-level 模型；
   * 训练完成后，**冻结模型权重**，为策略训练做准备。

3. **策略训练（基于潜在轨迹）**：

   * 从缓冲区采样 $\mathcal{B}_R$，提取起始状态；
   * 使用 MABL 模型递推，生成长度为 $H$ 的潜在轨迹；
   * 辅助模块输出奖励、终止标志、可用动作等；
   * 得到潜在轨迹 $\mathcal{D}_L$，其结构为：

     $$
     \{z^a_{i,1:H}, z^g_{i,1:H}, h^a_{i,1:H}, h^g_{i,1:H}, r^i_{1:H}, \gamma^i_{1:H}, A^i_{1:H} \}_{i=1}^n
     $$
   * 使用此潜在轨迹进行策略训练。

我们使用 **MAPPO**【参考文献 \[46]】作为默认策略学习算法，它在多个任务上表现出色。你也可以替换为其他任意符合 CTDE 架构的策略算法。

---

如需继续翻译**第 4 部分“实验评估（Experimental Evaluation）”**，请告诉我。
