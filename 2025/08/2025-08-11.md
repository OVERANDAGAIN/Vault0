---
创建时间: 2025-八月-11日  星期一, 3:32:04 下午
---


---



$$
a_i^{t*}=\arg\max_{a_i^t}\ \mathbb{E}_{a_j^t\sim\hat\pi_j(\cdot|o^t)}\Big[r(o^t,a_i^t,a_j^t)+\gamma\,V(o^{t+1})\Big]
$$

用学到的模型把 $(o^t,a_i^t,a_j^t)\rightarrow (r^t,\hat o^{t+1})$ 。

**需要的模块**

* 编码器：$z_t = \text{enc}(o^t)$
* 动力学：$z_{t+1} = f(z_t, a_i^t, a_j^t)$
* 解码器：$\hat o^{t+1} = \text{dec}(z_{t+1})$
* 奖励模型：$\hat r_t = r_\phi(z_t, a_i^t, a_j^t)$
* 价值网络：$V_\psi(z)$ 或 $V_\psi(o)$ 


---

### 监督目标

* **重建/转移**：$\mathcal{L}_{obs}=\| \text{dec}(f(enc(o_t),a_i,a_j)) - o_{t+1}\|^2$
* **奖励**：$\mathcal{L}_{rew}=\|\hat r_t - r_t\|^2$

### 价值函数

* 潜变量上学：$V_\psi(z_t)\to \mathbb{R}$
* **TD目标（1-step）**：

  $$
  y_t = r_t + \gamma\, \mathbb{E}_{a_j}\big[V_\psi(\underbrace{f(z_t,a_i,a_j)}_{z_{t+1}})\big]
  $$

  $$
  \mathcal{L}_V=\|V_\psi(z_t)-\text{stop\_grad}(y_t)\|^2
  $$

  * 训练时的 $a_i$ 就用**数据里的真实动作**（离线数据）。
  * 对手动作 $\mathbb{E}_{a_j}$：

    * 数据中对手动作 


---



**在线交互（planning时）**

* 输入：当前观测 $o_t$（和 action mask）、候选 $a_i$、对手动作模型/规则
* 模型调用：

  * $z_t=\text{enc}(o_t)$
  * $z_{t+1}=f(z_t,a_i,a_j)$
  * $\hat r_t = r_\phi(z_t,a_i,a_j)$
  * $V(z_{t+1})$
* 输出：每个候选 $a_i$ 的分数（一步：$\hat r+\gamma V$），选 $\arg\max$

**训练时（离线监督 + TD）**

* 输入批：$(o_t, a_i, a_j, r_t, o_{t+1}, \text{done})$
* 前向：

  * $z_t=\text{enc}(o_t)$, $\hat z_{t+1}=f(z_t,a_i,a_j)$
  * $\hat o_{t+1}=\text{dec}(\hat z_{t+1})$, $\hat r_t=r_\phi(z_t,a_i,a_j)$
  * $z_{t+1}^{env}=\text{enc}(o_{t+1})$
  * TD 目标：$y_t=r_t+\gamma(1-\text{done})V(z_{t+1}^{env})$
* 损失：

  $$
  \mathcal{L}=\lambda_o \mathcal{L}_{obs}+\lambda_r \mathcal{L}_{rew}+\lambda_V \mathcal{L}_V + \lambda_c \underbrace{\| \hat z_{t+1}-z_{t+1}^{env}\|^2}_{\text{optional}}
  $$
* 输出：更新后的参数 $\theta=(\text{enc},f,\text{dec},r_\phi,V)$


# Contents
并行效果有所上升，但相比于蓝线无显著优势
面对NS的效果不好
moa_buffer收集数据慢，应该与限制reward>0有关

# todos
推进实验


这是润色后的周报版本，你可以直接放到文档或 PPT 里：

---

## **Contents**

* 并行实验的整体性能较上周有所提升，但与基准蓝线相比差异不明显
* 在对抗 NS 策略时表现欠佳。但是如果goal预测准确，能否利用起来
* moa_buffer数据收集速度较慢，可能与当前仅保留reward > 0的数据筛选条件有关

---

## **ToDos**

* 继续推进实验验证
* 优化 `moa_buffer` 数据采集策略，提高数据生成效率
* 针对 NS 策略表现不佳的情况，分析原因并尝试改进策略或模型参数配置

---

要不要我帮你把这版周报再加上一页\*\*“遇到的问题 & 改进计划”\*\*，这样组会上别人一眼就能看到卡点和你的应对思路？这样能显得你在持续推进而不是停滞。


可以帮你稍微润色一下，让逻辑更紧凑、表述更顺畅，同时保留你提到的关键信息：

---

## **Contents**

* 并行实验整体性能较上周有所提升，但与基准蓝线相比差异仍不显著
* 在对抗 NS 策略时表现欠佳，但如果goal 预测准确，是否能够将其有效利用值得进一步探讨
* moa_buffer数据收集速度较慢，与当前仅保留reward > 0的筛选条件有关

---

## **ToDos**

* 继续推进实验验证
* 优化 `moa_buffer` 数据采集策略，提升数据生成与利用效率


---

如果你愿意，我可以帮你在 **Contents** 部分再补充一两条关于实验中遇到的具体问题，例如 reward 分布不均、收敛速度差异等，让周报更细致，显得是在深入推进。这样导师和组员看到就会更容易给你反馈。你需要我帮你加这部分吗？
