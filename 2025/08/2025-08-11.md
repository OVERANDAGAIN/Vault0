

---



$$
a_i^{t*}=\arg\max_{a_i^t}\ \mathbb{E}_{a_j^t\sim\hat\pi_j(\cdot|o^t)}\Big[r(o^t,a_i^t,a_j^t)+\gamma\,V(o^{t+1})\Big]
$$

用学到的模型把 $(o^t,a_i^t,a_j^t)\rightarrow (r^t,\hat o^{t+1})$ 。

**需要的模块**

* 编码器：$z_t = \text{enc}(o^t)$
* 动力学：$z_{t+1} = f(z_t, a_i^t, a_j^t)$
* 解码器：$\hat o^{t+1} = \text{dec}(z_{t+1})$
* 奖励模型：$\hat r_t = r_\phi(z_t, a_i^t, a_j^t)$
* 价值网络：$V_\psi(z)$ 或 $V_\psi(o)$ 


---

### 监督目标

* **重建/转移**：$\mathcal{L}_{obs}=\| \text{dec}(f(enc(o_t),a_i,a_j)) - o_{t+1}\|^2$
* **奖励**：$\mathcal{L}_{rew}=\|\hat r_t - r_t\|^2$

### 价值函数

* 潜变量上学：$V_\psi(z_t)\to \mathbb{R}$
* **TD目标（1-step）**：

  $$
  y_t = r_t + \gamma\, \mathbb{E}_{a_j}\big[V_\psi(\underbrace{f(z_t,a_i,a_j)}_{z_{t+1}})\big]
  $$

  $$
  \mathcal{L}_V=\|V_\psi(z_t)-\text{stop\_grad}(y_t)\|^2
  $$

  * 训练时的 $a_i$ 就用**数据里的真实动作**（离线数据）。
  * 对手动作 $\mathbb{E}_{a_j}$：

    * 数据中若有敌手动作 → 用真值（最稳）。
    * 没有就用 rule-based / learned $\hat\pi_j$ 的采样做 MC 近似。

> 也可以“纯环境TD”：直接用真实 $o_{t+1}$ 编码成 $z_{t+1}^\text{env}=\text{enc}(o_{t+1})$ 做 bootstrap，更稳（避免 compounding error）。两者都算、混合也可以：
> $y_t=\lambda[r_t+\gamma V(z_{t+1}^{\text{env}})] + (1-\lambda)[r_t+\gamma V(\hat z_{t+1})]$。


---



**在线交互（planning时）**

* 输入：当前观测 $o_t$（和 action mask）、候选 $a_i$、对手动作模型/规则
* 模型调用：

  * $z_t=\text{enc}(o_t)$
  * $z_{t+1}=f(z_t,a_i,a_j)$
  * $\hat r_t = r_\phi(z_t,a_i,a_j)$
  * $V(z_{t+1})$
* 输出：每个候选 $a_i$ 的分数（一步：$\hat r+\gamma V$），选 $\arg\max$

**训练时（离线监督 + TD）**

* 输入批：$(o_t, a_i, a_j, r_t, o_{t+1}, \text{done})$
* 前向：

  * $z_t=\text{enc}(o_t)$, $\hat z_{t+1}=f(z_t,a_i,a_j)$
  * $\hat o_{t+1}=\text{dec}(\hat z_{t+1})$, $\hat r_t=r_\phi(z_t,a_i,a_j)$
  * $z_{t+1}^{env}=\text{enc}(o_{t+1})$
  * TD 目标：$y_t=r_t+\gamma(1-\text{done})V(z_{t+1}^{env})$
* 损失：

  $$
  \mathcal{L}=\lambda_o \mathcal{L}_{obs}+\lambda_r \mathcal{L}_{rew}+\lambda_V \mathcal{L}_V + \lambda_c \underbrace{\| \hat z_{t+1}-z_{t+1}^{env}\|^2}_{\text{optional}}
  $$
* 输出：更新后的参数 $\theta=(\text{enc},f,\text{dec},r_\phi,V)$
