

---

# Model Based + MPC + Planning + RL 相关

最近在看一些 MPC 和 dynamic model 的文章和工作，这里做一个总结记录。
文章主要分为以下几个模块：

1. 模仿学习
2. model based RL + dynamic model + MBPO
3. Update-To-Data (UTD) ratio
4. MPC
5. model based + planning
6. MPC 结合 RL 方案

---

## 1. 模仿学习

关于模仿学习，可以参考 Berkeley CS294-112 深度增强学习课程的介绍。

模仿学习本质上是一个**监督学习**的过程，需要专家数据（带有 label）。
其上限一般就是专家数据的性能，但可以用来热启动模型。

一个经典的方案是 **DAgger 算法**（Dataset Aggregation）：

1. 从人工提供的数据集

   $$
   D = \{ o_1, a_1, o_2, a_2, \ldots, o_N, a_N \}
   $$

   中训练出策略

   $$
   \pi_\theta(a_t \mid o_t)
   $$

2. 运行策略 $\pi_\theta(a_t \mid o_t)$，采样得到一个新的数据集

   $$
   D_\pi = \{ o_1, \ldots, o_M \}
   $$

3. 由人工对数据集 $D_\pi$ 进行标注，得到一系列动作 $a_t$

4. 合并数据集

   $$
   D \leftarrow D \cup D_\pi
   $$

   返回第 1 步

---

使用学习到的策略在环境中交互，采样新的数据，然后需要人工重新给数据加 label。
缺点是人工标注成本很高，但可以不断迭代优化。

---

要不要我帮你接下来把“2. model based RL + dynamic model + MBPO”部分也整理成同样的笔记风格？
好的 ✅
我帮你把这部分也整理成笔记内容，保持结构清晰，并正确输出公式。以下是整理结果：

---

## 2. Model Based RL + Dynamic Model

### 背景

* **model free** 算法的缺点：需要大量数据，如果没有接近真实的仿真器，一般很难训练。
* **model based** 可以节省与仿真器交互的次数，提高采样效率（sample efficiency）。
* 在不限样本数据的情况下，model free 算法的性能一般优于 model based。
* 但在有限样本或昂贵环境下，model based RL 更具优势（例如 Muzero、Alphazero+ 等）。
* 注意：这里的 *model* 表示的是仿真器的内部运行逻辑。

---

### 一个最简单版本的基于模型的增强学习算法（v0.5 版）

执行三步：

1. 运行某种基本策略 $\pi_\theta(a_t \mid s_t)$（如随机策略）来收集样本数据：

   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$

2. 通过最小化

   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$

   来学习环境模型 $f(s,a)$。

3. 根据 $f(s,a)$ 来计划未来行动。

---

### 初学时的疑惑

1. 既然有仿真器了，为什么还需要用仿真器采集数据，再拟合出一个 model？直接用仿真器不就行了吗？
2. 第三步中，如果我已经有仿真器，是不是就不需要 model free RL 算法，直接用 planning 就够了？

---

### 一些实际问题与经验

1. 仿真器与现实有差距。如果在现实中不一定有大量数据，可以通过数据驱动方法拟合一个 **dynamic model**。
2. 训练 **动态模型** 很难，特别是在 model based RL 用于 planning 的时候，需要高精度预测。

   * 如果模型只能拟合粗略动态，回滚（rollout）时间长就会积累误差。
   * 如果可以在线学习 dynamic model（即便不完美），也能取得不错效果。
3. 一些动态或者复杂的环境（如风暴、市场、MCTS 场景），可以利用规划方法直接生成动作。

   * 例如 **CEM / MPPI / iLQR** 等。

---

要不要我继续帮你整理“3. Update-To-Data (UTD) ratio”那一节？
好的 ✅
我帮你把这部分 **Model-Based Policy Optimization (MBPO)** 整理成笔记，保留公式与算法步骤。

---

## 3. Model-Based Policy Optimization (MBPO)

MBPO 是一种经典的 **model-based RL 算法**，文章中包含严格的公式推导，但操作上比较直观。下面总结其两个核心算法。

---

### Algorithm 1: Monotonic Model-Based Policy Optimization

1. Initialize policy $\pi(a \mid s)$、predictive model $p_\theta(s', r \mid s, a)$、empty dataset $\mathcal{D}$。
2. **for** $N$ epochs **do**

   3. Collect data with $\pi$ in real environment:

      $$
      \mathcal{D} = \mathcal{D} \cup \{(s_i, a_i, s'_i, r_i)\}_i
      $$
   4. Train model $p_\theta$ on dataset $\mathcal{D}$ via maximum likelihood:

      $$
      \theta \leftarrow \arg\max_\theta \; \mathbb{E}_{\mathcal{D}} \big[ \log p_\theta(s', r \mid s, a) \big]
      $$
   5. Optimize policy under predictive model:

      $$
      \pi \leftarrow \arg\max_{\pi'} \; \hat{\eta}[\pi'] - C(\epsilon_m, \epsilon_\pi)
      $$

---

### Algorithm 2: Model-Based Policy Optimization with Deep Reinforcement Learning

1. Initialize policy $\pi_\phi$、predictive model $p_\theta$、environment dataset $\mathcal{D}_{\text{env}}$、model dataset $\mathcal{D}_{\text{model}}$。
2. **for** $N$ epochs **do**

   3. Train model $p_\theta$ on $\mathcal{D}_{\text{env}}$ via maximum likelihood。
   4. **for** $E$ steps **do**

      * Take action in environment using $\pi_\phi$，add to $\mathcal{D}_{\text{env}}$。
   5. **for** $M$ model rollouts **do**

      * Sample $s_t$ uniformly from $\mathcal{D}_{\text{env}}$。
      * Perform $k$-step model rollout starting from $s_t$ using policy $\pi_\phi$，add to $\mathcal{D}_{\text{model}}$。
   6. **for** $G$ gradient updates **do**

      * Update policy parameters on model data:

        $$
        \phi \leftarrow \phi - \lambda_\pi \nabla_\phi J_\pi(\phi, \mathcal{D}_{\text{model}})
        $$

---

### 关键思想

* 通过计算 **短长度 $k$ 的 rollouts** 来控制环境模型的使用（Model Usage）。
* 避免长时间 rollouts 导致的误差累积，利用 short rollouts 来保持 **task horizon 的稳定性**。
* 每次真实环境交互后，可以利用学到的 model 执行 $M$ 次 $k$-step rollout，得到大量数据，增强采样效率。

---

### 实际效果

* 一般 SAC 中 $G = 1$，而 MBPO 中可设 $G=20$ 至 $40$，大幅提升 **sample efficiency**。
* 实验表明 MBPO 可以在有限环境交互下显著超越 model-free baseline。

---

要不要我接下来把 **4. MPC** 部分也帮你整理成同样的格式？
好的 ✅
我来帮你把 **MPC (Model Predictive Control)** 部分整理成清晰的笔记，保持公式完整。

---

## 4. MPC (Model Predictive Control)

### 前提条件

使用 MPC 需要满足两个条件：

1. 存在一个 **dynamic model**，或者一个真实可回滚的仿真器。
2. 有明确的奖励函数定义，可以通过 dynamic model 计算得到奖励。

---

### 数学表达式

对于当前的状态 $s_t$，MPC 可以向前预测 $H$ 步，并选择能够使累计奖励最大的动作序列：

$$
A_t^{(H)} = \arg\max_{A_t^{(H)}} \sum_{t'=t}^{t+H-1} r(\hat{s}_{t'}, a_{t'})
$$

其中：

$$
\hat{s}_t = s_t, \quad \hat{s}_{t'+1} = \hat{s}_{t'} + \hat{f}_\theta(\hat{s}_{t'}, a_{t'})
$$

这里的 $\hat{f}_\theta$ 表示 dynamic model，用于预测下一时刻的状态。

---

### 直观理解

* 可以采用 **random shooting**：随机采样若干条动作序列，分别计算累计奖励，选取回报最高的序列。
* 每次执行序列中的第一个动作，然后再重新采样和规划。
* 类比于制定一周的工作计划：虽然可以先想好整个一周的任务，但实际上每次只会执行当天的第一个计划，再根据结果进行调整。

---

### 与 LQR 的关系

如果模型形式足够简单，可以使用 **LQR** 等解析方法直接求解。
但更多时候需要通过采样来近似。

---

### 简单的 MPC + Model-Based RL 流程（version 1.5）

1. 运行基础策略 $\pi_0(a_t \mid s_t)$（如随机策略），收集数据：

   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. 学习动态模型 $f(s,a)$，最小化：

   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$
3. 使用 $f(s,a)$ 进行规划，选择动作。
4. 执行规划出的第一个动作，观察得到新状态 $s'$（MPC）。
5. 将 $(s, a, s')$ 添加到数据集 $D$。

---

👉 这一节的核心思想：**MPC 是在 dynamic model 上做滚动优化，每次只执行预测序列的第一个动作，避免了长期预测带来的误差累积**。

---

要不要我继续帮你整理 **5. model based + planning** 部分？
👌 好的，我来帮你整理 **MPC 结合 RL 方案 (TD-MPC)** 这一节的内容。

---

## 5. MPC 结合 RL 方案

这一部分主要介绍 **TD-MPC (Temporal Difference Learning for Model Predictive Control)**。

* 论文：*Temporal Difference Learning for Model Predictive Control*
  [arxiv.org/pdf/2203.04955.pdf](https://arxiv.org/pdf/2203.04955.pdf)

* 补充阅读与讲解：
  强化学习实验室文章解读

  > 《Temporal Difference Learning for Model Predictive Control》

* 项目源码：
  [TD-MPC Github 页面](https://nicklashansen.github.io/td-mpc/)

---

### 基本思想

传统 MPC 一般只向前预测 $H$ 步：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Bigg]
$$

这种方式的缺陷是：**只能局部优化短期轨迹，无法直接处理无限时域目标**：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \Bigg] \quad \text{(intractable)}
$$

TD-MPC 的核心改进：

* 结合 **TD-learning (时序差分学习)** 与 **MPC 规划**。
* 通过 **有限步 MPC (H-step)** 获得 *locally optimal trajectory*。
* 将其与 **TD backup** 结合，从而在有限预测范围内近似无限时域目标。

---

### 直观理解

* 传统 MPC：滚动预测 $H$ 步，只能局部最优。
* TD-MPC：在 MPC 的基础上加入 TD 学习，使得短期规划和长期价值估计结合，获得更稳定、更高效的控制策略。

---

👉 这一节的关键：**TD-MPC 将 RL 的 TD-learning 和 MPC 的短期规划结合起来，既保留了模型规划的高效性，又兼顾了长期目标的优化**。

---

要不要我把前面 1–5 部分的所有内容合并成一个完整的 **学习笔记稿**，加上标题和小节编号，方便直接放到你的研究笔记本里？
好的 ✅
我来帮你把这一页关于 **TD-MPC 的 Motivation 和公式** 整理成完整笔记。

---

## 6. TD-MPC 的 Motivation 与公式

### 问题背景

如果要考虑真实的 **episode reward**，需要跑完整个 episode，但存在两个问题：

1. episode 太长时，计算代价过高；
2. episode 太长时，模型预测误差会逐步累积，结果更不准。

---

### Motivation

Data-driven Model Predictive Control 的两大挑战：

* **长时序规划成本高**（costly to plan over long horizons）
* **难以获得精准的环境模型**（challenging to obtain an accurate model of the environment）

因此，TD-MPC 的核心思路：

* 仍然采用 **H-step 规划**；
* 对超出部分，使用 **Value Function** 来估计长期收益。

---

### How can TD-learning help MPC?

* 通过 TD-learning 学习一个 **终端价值函数 (terminal value function)**。
* MPC 本质上得到的是 **局部最优 (local optimal solutions)**。
* Value Function 提供了对 **全局最优 (globally optimal solution)** 的近似。

---

### 规划阶段的评估指标

最终在 planning 时，回报的估计公式变为：

$$
\mathbb{E}_\tau \Bigg[ \gamma^H Q_\theta(z_H, a_H) + \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) \Bigg]
$$

* 前半部分（红色 Value）：使用终端价值函数 $Q_\theta$ 估计长远回报。
* 后半部分（红色 Rewards）：计算 MPC 短期 $H$ 步的累积奖励。

这种结合方式，使得规划兼顾短期和长期目标。

---

👉 总结：
TD-MPC 的关键在于 **用 TD-learning 学到的 Value Function 来补全 MPC 规划范围外的长期收益**，避免了长时预测的代价和误差。

---

要不要我现在帮你把 **1–6 部分所有内容** 合并成一份完整的 **调研笔记文稿（带层次结构和公式）**，方便直接放进你的研究笔记本里？
太好了 ✅
这一页是关于 **TD-MPC 的 inference 阶段**，具体采用 **MPPI (Model Predictive Path Integral)** 方法进行轨迹优化。下面我帮你整理：

---

## 7. TD-MPC Inference 阶段

### 核心思想

* 用到的 MPC 算法是 **MPPI**。
* 通过估计的 **top-k 采样轨迹**，进行加权平均来更新分布参数。
* 属于 **Policy-guided trajectory optimization** 的一种实现。

---

### 关键公式

对采样轨迹 $\tau$ 的加权：

$$
\phi_\pi = \frac{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big) \tau_i}{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big)}
$$

* $\hat{S}(\tau_i)$：轨迹的累计代价（负奖励）。
* $\lambda$：温度参数，用于控制采样分布的“锐度”。

最终得到轨迹分布的更新均值：

$$
\mu' = \sum_{i=1}^N w_i \tau_i, \quad 
w_i = \frac{\exp(-\frac{1}{\lambda}\hat{S}(\tau_i))}{\sum_j \exp(-\frac{1}{\lambda}\hat{S}(\tau_j))}
$$

---

### TD-MPC Inference 伪代码（MPPI）

**Algorithm 1: TD-MPC (inference)**

1. **输入**：网络参数 $\theta$，初始分布参数 $\mu^0, \sigma^0$，采样数 $N, N_\pi$，预测步长 $H$。
2. 将状态编码为 latent：$z_t = h_\theta(s_t)$。
3. **for each iteration $j = 1 \dots J$**

   * 采样 $N$ 条轨迹 $\tau$，每条长度为 $H$。
   * 额外采样 $N_\pi$ 条轨迹，来自 policy + dynamic model。
   * 计算所有轨迹的回报：

     $$
     \phi_\pi = \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) + \gamma^H Q_\theta(z_H, a_H)
     $$
   * 根据回报对轨迹进行加权，更新 $\mu, \sigma$。
4. **输出**：根据新的分布参数选择动作 $a \sim \mathcal{N}(\mu, \sigma^2)$。

---

### Policy-guided Trajectory Optimization 步骤

1. 根据当前 $\mu, \sigma$ 采样 $N$ 条轨迹序列。
2. 同时用 policy 和 dynamic model 采样 $N_\pi$ 条轨迹。
3. 使用所有轨迹的 **terminal value** 更新 $\mu, \sigma$。
4. 引入最小值 $\epsilon$ 来增强 **Exploration**。
5. 重复更新，直到收敛到一个较优的 $\mu, \sigma$。

---

👉 总结：

* **TD-MPC inference** 阶段用 **MPPI** 进行轨迹优化。
* 通过 **policy 引导的采样 + value function 终端修正**，逐步逼近一个较优分布。
* 保持了探索（exploration）与利用（exploitation）的平衡。

---

要不要我现在帮你把 **1–7 部分全部合并**，整理成一份 **完整连贯的调研笔记稿（带公式和算法步骤）**？
👌 收到，这一页是 **TD-MPC 的 Training 阶段 (TOLD)**，涉及训练目标、损失函数和更新方法。我来帮你整理：

---

## 8. TD-MPC Training 阶段 (TOLD)

### 核心思想

在训练阶段，TD-MPC 使用 **TOLD (Temporal Difference Learning with Latent Dynamics)** 框架来联合训练以下五个模块：

* **Representation**: $z_t = h_\theta(s_t)$
* **Latent Dynamics**: $z_{t+1} = d_\theta(z_t, a_t)$
* **Reward**: $\hat{r}_t = R_\theta(z_t, a_t)$
* **Value**: $\hat{q}_t = Q_\theta(z_t, a_t)$
* **Policy**: $\hat{a}_t \sim \pi_\theta(z_t)$

---

### 损失函数 (TOLD minimizes the objective)

$$
J(\theta; \Gamma) = \sum_{t=0}^{H} \lambda^t \mathcal{L}(\theta; \Gamma_t)
$$

其中：

$$
\mathcal{L}(\theta; \Gamma_t) = c_1 \lVert R_\theta(z_i, a_i) - r_i \rVert^2 
+ c_2 \lVert Q_\theta(z_i, a_i) - (r_i + \gamma Q_{\bar{\theta}}(z_{i+1}, \pi_\theta(z_{i+1}))) \rVert^2 
+ c_3 \lVert d_\theta(z_i, a_i) - h_\theta(s_{i+1}) \rVert^2
$$

* 第 1 项：Reward consistency
* 第 2 项：Value TD consistency
* 第 3 项：Latent state consistency

最终 policy 的优化目标是：

$$
J_\pi(\theta; \Gamma) = - \sum_{t=0}^{H} \lambda^t Q_\theta(z_i, \pi_\theta(\text{sg}(z_i)))
$$

---

### Algorithm 2: TOLD (Training)

1. 初始化网络参数 $\theta, \theta^-$，学习率等超参。
2. **收集数据**：每次与环境交互时，使用 **TD-MPC 的第一个 action** 执行。
3. 将采样到的 $(s_t, a_t, r_t, s_{t+1})$ 存入 buffer。
4. **更新 TOLD**：

   * 表示学习：$z_t = h_\theta(s_t)$
   * 预测奖励：$\hat{r}_i = R_\theta(z_i, a_i)$
   * 预测价值：$\hat{q}_i = Q_\theta(z_i, a_i)$
   * 状态转移：$z_{i+1} = d_\theta(z_i, a_i)$
   * 策略动作：$\hat{a}_i = \pi_\theta(z_i)$
   * 逐步累积 loss：

     $$
     J \leftarrow J + \lambda^i \mathcal{L}(\theta; \Gamma_i)
     $$
5. **更新网络参数**：

   * $\theta \leftarrow \theta - \eta \nabla_\theta J$
   * 更新 target 网络：$\theta^- \leftarrow (1-\tau)\theta^- + \tau \theta$

---

### 为什么要学习 policy？

* **Planning 角度**：policy 提供 action proposals，加快收敛。
* **Learning 角度**：直接通过 planning 来估计 Q-target 很慢，用 policy 替代更高效。

---

👉 总结：
训练阶段 (TOLD) 的关键是：在 latent 空间联合优化 **reward、value、dynamics、representation、policy** 五个模块，保证模型既能做短期 MPC 规划，又能通过 TD consistency 捕捉长期信息。

---

要不要我帮你把 **第 1–8 节合并成一份完整的笔记（包含所有公式、算法步骤、直观解释）**，输出成一个 Markdown 版，方便你直接粘贴进 Obsidian 或研究笔记？
