

---

# Model Based + MPC + Planning + RL ç›¸å…³

æœ€è¿‘åœ¨çœ‹ä¸€äº› MPC å’Œ dynamic model çš„æ–‡ç« å’Œå·¥ä½œï¼Œè¿™é‡Œåšä¸€ä¸ªæ€»ç»“è®°å½•ã€‚
æ–‡ç« ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ¨¡å—ï¼š

1. æ¨¡ä»¿å­¦ä¹ 
2. model based RL + dynamic model + MBPO
3. Update-To-Data (UTD) ratio
4. MPC
5. model based + planning
6. MPC ç»“åˆ RL æ–¹æ¡ˆ

---

## 1. æ¨¡ä»¿å­¦ä¹ 

å…³äºæ¨¡ä»¿å­¦ä¹ ï¼Œå¯ä»¥å‚è€ƒ Berkeley CS294-112 æ·±åº¦å¢å¼ºå­¦ä¹ è¯¾ç¨‹çš„ä»‹ç»ã€‚

æ¨¡ä»¿å­¦ä¹ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**ç›‘ç£å­¦ä¹ **çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸“å®¶æ•°æ®ï¼ˆå¸¦æœ‰ labelï¼‰ã€‚
å…¶ä¸Šé™ä¸€èˆ¬å°±æ˜¯ä¸“å®¶æ•°æ®çš„æ€§èƒ½ï¼Œä½†å¯ä»¥ç”¨æ¥çƒ­å¯åŠ¨æ¨¡å‹ã€‚

ä¸€ä¸ªç»å…¸çš„æ–¹æ¡ˆæ˜¯ **DAgger ç®—æ³•**ï¼ˆDataset Aggregationï¼‰ï¼š

1. ä»äººå·¥æä¾›çš„æ•°æ®é›†

   $$
   D = \{ o_1, a_1, o_2, a_2, \ldots, o_N, a_N \}
   $$

   ä¸­è®­ç»ƒå‡ºç­–ç•¥

   $$
   \pi_\theta(a_t \mid o_t)
   $$

2. è¿è¡Œç­–ç•¥ $\pi_\theta(a_t \mid o_t)$ï¼Œé‡‡æ ·å¾—åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®é›†

   $$
   D_\pi = \{ o_1, \ldots, o_M \}
   $$

3. ç”±äººå·¥å¯¹æ•°æ®é›† $D_\pi$ è¿›è¡Œæ ‡æ³¨ï¼Œå¾—åˆ°ä¸€ç³»åˆ—åŠ¨ä½œ $a_t$

4. åˆå¹¶æ•°æ®é›†

   $$
   D \leftarrow D \cup D_\pi
   $$

   è¿”å›ç¬¬ 1 æ­¥

---

ä½¿ç”¨å­¦ä¹ åˆ°çš„ç­–ç•¥åœ¨ç¯å¢ƒä¸­äº¤äº’ï¼Œé‡‡æ ·æ–°çš„æ•°æ®ï¼Œç„¶åéœ€è¦äººå·¥é‡æ–°ç»™æ•°æ®åŠ  labelã€‚
ç¼ºç‚¹æ˜¯äººå·¥æ ‡æ³¨æˆæœ¬å¾ˆé«˜ï¼Œä½†å¯ä»¥ä¸æ–­è¿­ä»£ä¼˜åŒ–ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æ¥ä¸‹æ¥æŠŠâ€œ2. model based RL + dynamic model + MBPOâ€éƒ¨åˆ†ä¹Ÿæ•´ç†æˆåŒæ ·çš„ç¬”è®°é£æ ¼ï¼Ÿ
å¥½çš„ âœ…
æˆ‘å¸®ä½ æŠŠè¿™éƒ¨åˆ†ä¹Ÿæ•´ç†æˆç¬”è®°å†…å®¹ï¼Œä¿æŒç»“æ„æ¸…æ™°ï¼Œå¹¶æ­£ç¡®è¾“å‡ºå…¬å¼ã€‚ä»¥ä¸‹æ˜¯æ•´ç†ç»“æœï¼š

---

## 2. Model Based RL + Dynamic Model

### èƒŒæ™¯

* **model free** ç®—æ³•çš„ç¼ºç‚¹ï¼šéœ€è¦å¤§é‡æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰æ¥è¿‘çœŸå®çš„ä»¿çœŸå™¨ï¼Œä¸€èˆ¬å¾ˆéš¾è®­ç»ƒã€‚
* **model based** å¯ä»¥èŠ‚çœä¸ä»¿çœŸå™¨äº¤äº’çš„æ¬¡æ•°ï¼Œæé«˜é‡‡æ ·æ•ˆç‡ï¼ˆsample efficiencyï¼‰ã€‚
* åœ¨ä¸é™æ ·æœ¬æ•°æ®çš„æƒ…å†µä¸‹ï¼Œmodel free ç®—æ³•çš„æ€§èƒ½ä¸€èˆ¬ä¼˜äº model basedã€‚
* ä½†åœ¨æœ‰é™æ ·æœ¬æˆ–æ˜‚è´µç¯å¢ƒä¸‹ï¼Œmodel based RL æ›´å…·ä¼˜åŠ¿ï¼ˆä¾‹å¦‚ Muzeroã€Alphazero+ ç­‰ï¼‰ã€‚
* æ³¨æ„ï¼šè¿™é‡Œçš„ *model* è¡¨ç¤ºçš„æ˜¯ä»¿çœŸå™¨çš„å†…éƒ¨è¿è¡Œé€»è¾‘ã€‚

---

### ä¸€ä¸ªæœ€ç®€å•ç‰ˆæœ¬çš„åŸºäºæ¨¡å‹çš„å¢å¼ºå­¦ä¹ ç®—æ³•ï¼ˆv0.5 ç‰ˆï¼‰

æ‰§è¡Œä¸‰æ­¥ï¼š

1. è¿è¡ŒæŸç§åŸºæœ¬ç­–ç•¥ $\pi_\theta(a_t \mid s_t)$ï¼ˆå¦‚éšæœºç­–ç•¥ï¼‰æ¥æ”¶é›†æ ·æœ¬æ•°æ®ï¼š

   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$

2. é€šè¿‡æœ€å°åŒ–

   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$

   æ¥å­¦ä¹ ç¯å¢ƒæ¨¡å‹ $f(s,a)$ã€‚

3. æ ¹æ® $f(s,a)$ æ¥è®¡åˆ’æœªæ¥è¡ŒåŠ¨ã€‚

---

### åˆå­¦æ—¶çš„ç–‘æƒ‘

1. æ—¢ç„¶æœ‰ä»¿çœŸå™¨äº†ï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦ç”¨ä»¿çœŸå™¨é‡‡é›†æ•°æ®ï¼Œå†æ‹Ÿåˆå‡ºä¸€ä¸ª modelï¼Ÿç›´æ¥ç”¨ä»¿çœŸå™¨ä¸å°±è¡Œäº†å—ï¼Ÿ
2. ç¬¬ä¸‰æ­¥ä¸­ï¼Œå¦‚æœæˆ‘å·²ç»æœ‰ä»¿çœŸå™¨ï¼Œæ˜¯ä¸æ˜¯å°±ä¸éœ€è¦ model free RL ç®—æ³•ï¼Œç›´æ¥ç”¨ planning å°±å¤Ÿäº†ï¼Ÿ

---

### ä¸€äº›å®é™…é—®é¢˜ä¸ç»éªŒ

1. ä»¿çœŸå™¨ä¸ç°å®æœ‰å·®è·ã€‚å¦‚æœåœ¨ç°å®ä¸­ä¸ä¸€å®šæœ‰å¤§é‡æ•°æ®ï¼Œå¯ä»¥é€šè¿‡æ•°æ®é©±åŠ¨æ–¹æ³•æ‹Ÿåˆä¸€ä¸ª **dynamic model**ã€‚
2. è®­ç»ƒ **åŠ¨æ€æ¨¡å‹** å¾ˆéš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ model based RL ç”¨äº planning çš„æ—¶å€™ï¼Œéœ€è¦é«˜ç²¾åº¦é¢„æµ‹ã€‚

   * å¦‚æœæ¨¡å‹åªèƒ½æ‹Ÿåˆç²—ç•¥åŠ¨æ€ï¼Œå›æ»šï¼ˆrolloutï¼‰æ—¶é—´é•¿å°±ä¼šç§¯ç´¯è¯¯å·®ã€‚
   * å¦‚æœå¯ä»¥åœ¨çº¿å­¦ä¹  dynamic modelï¼ˆå³ä¾¿ä¸å®Œç¾ï¼‰ï¼Œä¹Ÿèƒ½å–å¾—ä¸é”™æ•ˆæœã€‚
3. ä¸€äº›åŠ¨æ€æˆ–è€…å¤æ‚çš„ç¯å¢ƒï¼ˆå¦‚é£æš´ã€å¸‚åœºã€MCTS åœºæ™¯ï¼‰ï¼Œå¯ä»¥åˆ©ç”¨è§„åˆ’æ–¹æ³•ç›´æ¥ç”ŸæˆåŠ¨ä½œã€‚

   * ä¾‹å¦‚ **CEM / MPPI / iLQR** ç­‰ã€‚

---

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ æ•´ç†â€œ3. Update-To-Data (UTD) ratioâ€é‚£ä¸€èŠ‚ï¼Ÿ
å¥½çš„ âœ…
æˆ‘å¸®ä½ æŠŠè¿™éƒ¨åˆ† **Model-Based Policy Optimization (MBPO)** æ•´ç†æˆç¬”è®°ï¼Œä¿ç•™å…¬å¼ä¸ç®—æ³•æ­¥éª¤ã€‚

---

## 3. Model-Based Policy Optimization (MBPO)

MBPO æ˜¯ä¸€ç§ç»å…¸çš„ **model-based RL ç®—æ³•**ï¼Œæ–‡ç« ä¸­åŒ…å«ä¸¥æ ¼çš„å…¬å¼æ¨å¯¼ï¼Œä½†æ“ä½œä¸Šæ¯”è¾ƒç›´è§‚ã€‚ä¸‹é¢æ€»ç»“å…¶ä¸¤ä¸ªæ ¸å¿ƒç®—æ³•ã€‚

---

### Algorithm 1: Monotonic Model-Based Policy Optimization

1. Initialize policy $\pi(a \mid s)$ã€predictive model $p_\theta(s', r \mid s, a)$ã€empty dataset $\mathcal{D}$ã€‚
2. **for** $N$ epochs **do**

   3. Collect data with $\pi$ in real environment:

      $$
      \mathcal{D} = \mathcal{D} \cup \{(s_i, a_i, s'_i, r_i)\}_i
      $$
   4. Train model $p_\theta$ on dataset $\mathcal{D}$ via maximum likelihood:

      $$
      \theta \leftarrow \arg\max_\theta \; \mathbb{E}_{\mathcal{D}} \big[ \log p_\theta(s', r \mid s, a) \big]
      $$
   5. Optimize policy under predictive model:

      $$
      \pi \leftarrow \arg\max_{\pi'} \; \hat{\eta}[\pi'] - C(\epsilon_m, \epsilon_\pi)
      $$

---

### Algorithm 2: Model-Based Policy Optimization with Deep Reinforcement Learning

1. Initialize policy $\pi_\phi$ã€predictive model $p_\theta$ã€environment dataset $\mathcal{D}_{\text{env}}$ã€model dataset $\mathcal{D}_{\text{model}}$ã€‚
2. **for** $N$ epochs **do**

   3. Train model $p_\theta$ on $\mathcal{D}_{\text{env}}$ via maximum likelihoodã€‚
   4. **for** $E$ steps **do**

      * Take action in environment using $\pi_\phi$ï¼Œadd to $\mathcal{D}_{\text{env}}$ã€‚
   5. **for** $M$ model rollouts **do**

      * Sample $s_t$ uniformly from $\mathcal{D}_{\text{env}}$ã€‚
      * Perform $k$-step model rollout starting from $s_t$ using policy $\pi_\phi$ï¼Œadd to $\mathcal{D}_{\text{model}}$ã€‚
   6. **for** $G$ gradient updates **do**

      * Update policy parameters on model data:

        $$
        \phi \leftarrow \phi - \lambda_\pi \nabla_\phi J_\pi(\phi, \mathcal{D}_{\text{model}})
        $$

---

### å…³é”®æ€æƒ³

* é€šè¿‡è®¡ç®— **çŸ­é•¿åº¦ $k$ çš„ rollouts** æ¥æ§åˆ¶ç¯å¢ƒæ¨¡å‹çš„ä½¿ç”¨ï¼ˆModel Usageï¼‰ã€‚
* é¿å…é•¿æ—¶é—´ rollouts å¯¼è‡´çš„è¯¯å·®ç´¯ç§¯ï¼Œåˆ©ç”¨ short rollouts æ¥ä¿æŒ **task horizon çš„ç¨³å®šæ€§**ã€‚
* æ¯æ¬¡çœŸå®ç¯å¢ƒäº¤äº’åï¼Œå¯ä»¥åˆ©ç”¨å­¦åˆ°çš„ model æ‰§è¡Œ $M$ æ¬¡ $k$-step rolloutï¼Œå¾—åˆ°å¤§é‡æ•°æ®ï¼Œå¢å¼ºé‡‡æ ·æ•ˆç‡ã€‚

---

### å®é™…æ•ˆæœ

* ä¸€èˆ¬ SAC ä¸­ $G = 1$ï¼Œè€Œ MBPO ä¸­å¯è®¾ $G=20$ è‡³ $40$ï¼Œå¤§å¹…æå‡ **sample efficiency**ã€‚
* å®éªŒè¡¨æ˜ MBPO å¯ä»¥åœ¨æœ‰é™ç¯å¢ƒäº¤äº’ä¸‹æ˜¾è‘—è¶…è¶Š model-free baselineã€‚

---

è¦ä¸è¦æˆ‘æ¥ä¸‹æ¥æŠŠ **4. MPC** éƒ¨åˆ†ä¹Ÿå¸®ä½ æ•´ç†æˆåŒæ ·çš„æ ¼å¼ï¼Ÿ
å¥½çš„ âœ…
æˆ‘æ¥å¸®ä½ æŠŠ **MPC (Model Predictive Control)** éƒ¨åˆ†æ•´ç†æˆæ¸…æ™°çš„ç¬”è®°ï¼Œä¿æŒå…¬å¼å®Œæ•´ã€‚

---

## 4. MPC (Model Predictive Control)

### å‰ææ¡ä»¶

ä½¿ç”¨ MPC éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š

1. å­˜åœ¨ä¸€ä¸ª **dynamic model**ï¼Œæˆ–è€…ä¸€ä¸ªçœŸå®å¯å›æ»šçš„ä»¿çœŸå™¨ã€‚
2. æœ‰æ˜ç¡®çš„å¥–åŠ±å‡½æ•°å®šä¹‰ï¼Œå¯ä»¥é€šè¿‡ dynamic model è®¡ç®—å¾—åˆ°å¥–åŠ±ã€‚

---

### æ•°å­¦è¡¨è¾¾å¼

å¯¹äºå½“å‰çš„çŠ¶æ€ $s_t$ï¼ŒMPC å¯ä»¥å‘å‰é¢„æµ‹ $H$ æ­¥ï¼Œå¹¶é€‰æ‹©èƒ½å¤Ÿä½¿ç´¯è®¡å¥–åŠ±æœ€å¤§çš„åŠ¨ä½œåºåˆ—ï¼š

$$
A_t^{(H)} = \arg\max_{A_t^{(H)}} \sum_{t'=t}^{t+H-1} r(\hat{s}_{t'}, a_{t'})
$$

å…¶ä¸­ï¼š

$$
\hat{s}_t = s_t, \quad \hat{s}_{t'+1} = \hat{s}_{t'} + \hat{f}_\theta(\hat{s}_{t'}, a_{t'})
$$

è¿™é‡Œçš„ $\hat{f}_\theta$ è¡¨ç¤º dynamic modelï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚

---

### ç›´è§‚ç†è§£

* å¯ä»¥é‡‡ç”¨ **random shooting**ï¼šéšæœºé‡‡æ ·è‹¥å¹²æ¡åŠ¨ä½œåºåˆ—ï¼Œåˆ†åˆ«è®¡ç®—ç´¯è®¡å¥–åŠ±ï¼Œé€‰å–å›æŠ¥æœ€é«˜çš„åºåˆ—ã€‚
* æ¯æ¬¡æ‰§è¡Œåºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œç„¶åå†é‡æ–°é‡‡æ ·å’Œè§„åˆ’ã€‚
* ç±»æ¯”äºåˆ¶å®šä¸€å‘¨çš„å·¥ä½œè®¡åˆ’ï¼šè™½ç„¶å¯ä»¥å…ˆæƒ³å¥½æ•´ä¸ªä¸€å‘¨çš„ä»»åŠ¡ï¼Œä½†å®é™…ä¸Šæ¯æ¬¡åªä¼šæ‰§è¡Œå½“å¤©çš„ç¬¬ä¸€ä¸ªè®¡åˆ’ï¼Œå†æ ¹æ®ç»“æœè¿›è¡Œè°ƒæ•´ã€‚

---

### ä¸ LQR çš„å…³ç³»

å¦‚æœæ¨¡å‹å½¢å¼è¶³å¤Ÿç®€å•ï¼Œå¯ä»¥ä½¿ç”¨ **LQR** ç­‰è§£ææ–¹æ³•ç›´æ¥æ±‚è§£ã€‚
ä½†æ›´å¤šæ—¶å€™éœ€è¦é€šè¿‡é‡‡æ ·æ¥è¿‘ä¼¼ã€‚

---

### ç®€å•çš„ MPC + Model-Based RL æµç¨‹ï¼ˆversion 1.5ï¼‰

1. è¿è¡ŒåŸºç¡€ç­–ç•¥ $\pi_0(a_t \mid s_t)$ï¼ˆå¦‚éšæœºç­–ç•¥ï¼‰ï¼Œæ”¶é›†æ•°æ®ï¼š

   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. å­¦ä¹ åŠ¨æ€æ¨¡å‹ $f(s,a)$ï¼Œæœ€å°åŒ–ï¼š

   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$
3. ä½¿ç”¨ $f(s,a)$ è¿›è¡Œè§„åˆ’ï¼Œé€‰æ‹©åŠ¨ä½œã€‚
4. æ‰§è¡Œè§„åˆ’å‡ºçš„ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œè§‚å¯Ÿå¾—åˆ°æ–°çŠ¶æ€ $s'$ï¼ˆMPCï¼‰ã€‚
5. å°† $(s, a, s')$ æ·»åŠ åˆ°æ•°æ®é›† $D$ã€‚

---

ğŸ‘‰ è¿™ä¸€èŠ‚çš„æ ¸å¿ƒæ€æƒ³ï¼š**MPC æ˜¯åœ¨ dynamic model ä¸Šåšæ»šåŠ¨ä¼˜åŒ–ï¼Œæ¯æ¬¡åªæ‰§è¡Œé¢„æµ‹åºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œé¿å…äº†é•¿æœŸé¢„æµ‹å¸¦æ¥çš„è¯¯å·®ç´¯ç§¯**ã€‚

---

è¦ä¸è¦æˆ‘ç»§ç»­å¸®ä½ æ•´ç† **5. model based + planning** éƒ¨åˆ†ï¼Ÿ
ğŸ‘Œ å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ æ•´ç† **MPC ç»“åˆ RL æ–¹æ¡ˆ (TD-MPC)** è¿™ä¸€èŠ‚çš„å†…å®¹ã€‚

---

## 5. MPC ç»“åˆ RL æ–¹æ¡ˆ

è¿™ä¸€éƒ¨åˆ†ä¸»è¦ä»‹ç» **TD-MPC (Temporal Difference Learning for Model Predictive Control)**ã€‚

* è®ºæ–‡ï¼š*Temporal Difference Learning for Model Predictive Control*
  [arxiv.org/pdf/2203.04955.pdf](https://arxiv.org/pdf/2203.04955.pdf)

* è¡¥å……é˜…è¯»ä¸è®²è§£ï¼š
  å¼ºåŒ–å­¦ä¹ å®éªŒå®¤æ–‡ç« è§£è¯»

  > ã€ŠTemporal Difference Learning for Model Predictive Controlã€‹

* é¡¹ç›®æºç ï¼š
  [TD-MPC Github é¡µé¢](https://nicklashansen.github.io/td-mpc/)

---

### åŸºæœ¬æ€æƒ³

ä¼ ç»Ÿ MPC ä¸€èˆ¬åªå‘å‰é¢„æµ‹ $H$ æ­¥ï¼š

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Bigg]
$$

è¿™ç§æ–¹å¼çš„ç¼ºé™·æ˜¯ï¼š**åªèƒ½å±€éƒ¨ä¼˜åŒ–çŸ­æœŸè½¨è¿¹ï¼Œæ— æ³•ç›´æ¥å¤„ç†æ— é™æ—¶åŸŸç›®æ ‡**ï¼š

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \Bigg] \quad \text{(intractable)}
$$

TD-MPC çš„æ ¸å¿ƒæ”¹è¿›ï¼š

* ç»“åˆ **TD-learning (æ—¶åºå·®åˆ†å­¦ä¹ )** ä¸ **MPC è§„åˆ’**ã€‚
* é€šè¿‡ **æœ‰é™æ­¥ MPC (H-step)** è·å¾— *locally optimal trajectory*ã€‚
* å°†å…¶ä¸ **TD backup** ç»“åˆï¼Œä»è€Œåœ¨æœ‰é™é¢„æµ‹èŒƒå›´å†…è¿‘ä¼¼æ— é™æ—¶åŸŸç›®æ ‡ã€‚

---

### ç›´è§‚ç†è§£

* ä¼ ç»Ÿ MPCï¼šæ»šåŠ¨é¢„æµ‹ $H$ æ­¥ï¼Œåªèƒ½å±€éƒ¨æœ€ä¼˜ã€‚
* TD-MPCï¼šåœ¨ MPC çš„åŸºç¡€ä¸ŠåŠ å…¥ TD å­¦ä¹ ï¼Œä½¿å¾—çŸ­æœŸè§„åˆ’å’Œé•¿æœŸä»·å€¼ä¼°è®¡ç»“åˆï¼Œè·å¾—æ›´ç¨³å®šã€æ›´é«˜æ•ˆçš„æ§åˆ¶ç­–ç•¥ã€‚

---

ğŸ‘‰ è¿™ä¸€èŠ‚çš„å…³é”®ï¼š**TD-MPC å°† RL çš„ TD-learning å’Œ MPC çš„çŸ­æœŸè§„åˆ’ç»“åˆèµ·æ¥ï¼Œæ—¢ä¿ç•™äº†æ¨¡å‹è§„åˆ’çš„é«˜æ•ˆæ€§ï¼Œåˆå…¼é¡¾äº†é•¿æœŸç›®æ ‡çš„ä¼˜åŒ–**ã€‚

---

è¦ä¸è¦æˆ‘æŠŠå‰é¢ 1â€“5 éƒ¨åˆ†çš„æ‰€æœ‰å†…å®¹åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„ **å­¦ä¹ ç¬”è®°ç¨¿**ï¼ŒåŠ ä¸Šæ ‡é¢˜å’Œå°èŠ‚ç¼–å·ï¼Œæ–¹ä¾¿ç›´æ¥æ”¾åˆ°ä½ çš„ç ”ç©¶ç¬”è®°æœ¬é‡Œï¼Ÿ
å¥½çš„ âœ…
æˆ‘æ¥å¸®ä½ æŠŠè¿™ä¸€é¡µå…³äº **TD-MPC çš„ Motivation å’Œå…¬å¼** æ•´ç†æˆå®Œæ•´ç¬”è®°ã€‚

---

## 6. TD-MPC çš„ Motivation ä¸å…¬å¼

### é—®é¢˜èƒŒæ™¯

å¦‚æœè¦è€ƒè™‘çœŸå®çš„ **episode reward**ï¼Œéœ€è¦è·‘å®Œæ•´ä¸ª episodeï¼Œä½†å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼š

1. episode å¤ªé•¿æ—¶ï¼Œè®¡ç®—ä»£ä»·è¿‡é«˜ï¼›
2. episode å¤ªé•¿æ—¶ï¼Œæ¨¡å‹é¢„æµ‹è¯¯å·®ä¼šé€æ­¥ç´¯ç§¯ï¼Œç»“æœæ›´ä¸å‡†ã€‚

---

### Motivation

Data-driven Model Predictive Control çš„ä¸¤å¤§æŒ‘æˆ˜ï¼š

* **é•¿æ—¶åºè§„åˆ’æˆæœ¬é«˜**ï¼ˆcostly to plan over long horizonsï¼‰
* **éš¾ä»¥è·å¾—ç²¾å‡†çš„ç¯å¢ƒæ¨¡å‹**ï¼ˆchallenging to obtain an accurate model of the environmentï¼‰

å› æ­¤ï¼ŒTD-MPC çš„æ ¸å¿ƒæ€è·¯ï¼š

* ä»ç„¶é‡‡ç”¨ **H-step è§„åˆ’**ï¼›
* å¯¹è¶…å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨ **Value Function** æ¥ä¼°è®¡é•¿æœŸæ”¶ç›Šã€‚

---

### How can TD-learning help MPC?

* é€šè¿‡ TD-learning å­¦ä¹ ä¸€ä¸ª **ç»ˆç«¯ä»·å€¼å‡½æ•° (terminal value function)**ã€‚
* MPC æœ¬è´¨ä¸Šå¾—åˆ°çš„æ˜¯ **å±€éƒ¨æœ€ä¼˜ (local optimal solutions)**ã€‚
* Value Function æä¾›äº†å¯¹ **å…¨å±€æœ€ä¼˜ (globally optimal solution)** çš„è¿‘ä¼¼ã€‚

---

### è§„åˆ’é˜¶æ®µçš„è¯„ä¼°æŒ‡æ ‡

æœ€ç»ˆåœ¨ planning æ—¶ï¼Œå›æŠ¥çš„ä¼°è®¡å…¬å¼å˜ä¸ºï¼š

$$
\mathbb{E}_\tau \Bigg[ \gamma^H Q_\theta(z_H, a_H) + \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) \Bigg]
$$

* å‰åŠéƒ¨åˆ†ï¼ˆçº¢è‰² Valueï¼‰ï¼šä½¿ç”¨ç»ˆç«¯ä»·å€¼å‡½æ•° $Q_\theta$ ä¼°è®¡é•¿è¿œå›æŠ¥ã€‚
* ååŠéƒ¨åˆ†ï¼ˆçº¢è‰² Rewardsï¼‰ï¼šè®¡ç®— MPC çŸ­æœŸ $H$ æ­¥çš„ç´¯ç§¯å¥–åŠ±ã€‚

è¿™ç§ç»“åˆæ–¹å¼ï¼Œä½¿å¾—è§„åˆ’å…¼é¡¾çŸ­æœŸå’Œé•¿æœŸç›®æ ‡ã€‚

---

ğŸ‘‰ æ€»ç»“ï¼š
TD-MPC çš„å…³é”®åœ¨äº **ç”¨ TD-learning å­¦åˆ°çš„ Value Function æ¥è¡¥å…¨ MPC è§„åˆ’èŒƒå›´å¤–çš„é•¿æœŸæ”¶ç›Š**ï¼Œé¿å…äº†é•¿æ—¶é¢„æµ‹çš„ä»£ä»·å’Œè¯¯å·®ã€‚

---

è¦ä¸è¦æˆ‘ç°åœ¨å¸®ä½ æŠŠ **1â€“6 éƒ¨åˆ†æ‰€æœ‰å†…å®¹** åˆå¹¶æˆä¸€ä»½å®Œæ•´çš„ **è°ƒç ”ç¬”è®°æ–‡ç¨¿ï¼ˆå¸¦å±‚æ¬¡ç»“æ„å’Œå…¬å¼ï¼‰**ï¼Œæ–¹ä¾¿ç›´æ¥æ”¾è¿›ä½ çš„ç ”ç©¶ç¬”è®°æœ¬é‡Œï¼Ÿ
å¤ªå¥½äº† âœ…
è¿™ä¸€é¡µæ˜¯å…³äº **TD-MPC çš„ inference é˜¶æ®µ**ï¼Œå…·ä½“é‡‡ç”¨ **MPPI (Model Predictive Path Integral)** æ–¹æ³•è¿›è¡Œè½¨è¿¹ä¼˜åŒ–ã€‚ä¸‹é¢æˆ‘å¸®ä½ æ•´ç†ï¼š

---

## 7. TD-MPC Inference é˜¶æ®µ

### æ ¸å¿ƒæ€æƒ³

* ç”¨åˆ°çš„ MPC ç®—æ³•æ˜¯ **MPPI**ã€‚
* é€šè¿‡ä¼°è®¡çš„ **top-k é‡‡æ ·è½¨è¿¹**ï¼Œè¿›è¡ŒåŠ æƒå¹³å‡æ¥æ›´æ–°åˆ†å¸ƒå‚æ•°ã€‚
* å±äº **Policy-guided trajectory optimization** çš„ä¸€ç§å®ç°ã€‚

---

### å…³é”®å…¬å¼

å¯¹é‡‡æ ·è½¨è¿¹ $\tau$ çš„åŠ æƒï¼š

$$
\phi_\pi = \frac{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big) \tau_i}{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big)}
$$

* $\hat{S}(\tau_i)$ï¼šè½¨è¿¹çš„ç´¯è®¡ä»£ä»·ï¼ˆè´Ÿå¥–åŠ±ï¼‰ã€‚
* $\lambda$ï¼šæ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶é‡‡æ ·åˆ†å¸ƒçš„â€œé”åº¦â€ã€‚

æœ€ç»ˆå¾—åˆ°è½¨è¿¹åˆ†å¸ƒçš„æ›´æ–°å‡å€¼ï¼š

$$
\mu' = \sum_{i=1}^N w_i \tau_i, \quad 
w_i = \frac{\exp(-\frac{1}{\lambda}\hat{S}(\tau_i))}{\sum_j \exp(-\frac{1}{\lambda}\hat{S}(\tau_j))}
$$

---

### TD-MPC Inference ä¼ªä»£ç ï¼ˆMPPIï¼‰

**Algorithm 1: TD-MPC (inference)**

1. **è¾“å…¥**ï¼šç½‘ç»œå‚æ•° $\theta$ï¼Œåˆå§‹åˆ†å¸ƒå‚æ•° $\mu^0, \sigma^0$ï¼Œé‡‡æ ·æ•° $N, N_\pi$ï¼Œé¢„æµ‹æ­¥é•¿ $H$ã€‚
2. å°†çŠ¶æ€ç¼–ç ä¸º latentï¼š$z_t = h_\theta(s_t)$ã€‚
3. **for each iteration $j = 1 \dots J$**

   * é‡‡æ · $N$ æ¡è½¨è¿¹ $\tau$ï¼Œæ¯æ¡é•¿åº¦ä¸º $H$ã€‚
   * é¢å¤–é‡‡æ · $N_\pi$ æ¡è½¨è¿¹ï¼Œæ¥è‡ª policy + dynamic modelã€‚
   * è®¡ç®—æ‰€æœ‰è½¨è¿¹çš„å›æŠ¥ï¼š

     $$
     \phi_\pi = \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) + \gamma^H Q_\theta(z_H, a_H)
     $$
   * æ ¹æ®å›æŠ¥å¯¹è½¨è¿¹è¿›è¡ŒåŠ æƒï¼Œæ›´æ–° $\mu, \sigma$ã€‚
4. **è¾“å‡º**ï¼šæ ¹æ®æ–°çš„åˆ†å¸ƒå‚æ•°é€‰æ‹©åŠ¨ä½œ $a \sim \mathcal{N}(\mu, \sigma^2)$ã€‚

---

### Policy-guided Trajectory Optimization æ­¥éª¤

1. æ ¹æ®å½“å‰ $\mu, \sigma$ é‡‡æ · $N$ æ¡è½¨è¿¹åºåˆ—ã€‚
2. åŒæ—¶ç”¨ policy å’Œ dynamic model é‡‡æ · $N_\pi$ æ¡è½¨è¿¹ã€‚
3. ä½¿ç”¨æ‰€æœ‰è½¨è¿¹çš„ **terminal value** æ›´æ–° $\mu, \sigma$ã€‚
4. å¼•å…¥æœ€å°å€¼ $\epsilon$ æ¥å¢å¼º **Exploration**ã€‚
5. é‡å¤æ›´æ–°ï¼Œç›´åˆ°æ”¶æ•›åˆ°ä¸€ä¸ªè¾ƒä¼˜çš„ $\mu, \sigma$ã€‚

---

ğŸ‘‰ æ€»ç»“ï¼š

* **TD-MPC inference** é˜¶æ®µç”¨ **MPPI** è¿›è¡Œè½¨è¿¹ä¼˜åŒ–ã€‚
* é€šè¿‡ **policy å¼•å¯¼çš„é‡‡æ · + value function ç»ˆç«¯ä¿®æ­£**ï¼Œé€æ­¥é€¼è¿‘ä¸€ä¸ªè¾ƒä¼˜åˆ†å¸ƒã€‚
* ä¿æŒäº†æ¢ç´¢ï¼ˆexplorationï¼‰ä¸åˆ©ç”¨ï¼ˆexploitationï¼‰çš„å¹³è¡¡ã€‚

---

è¦ä¸è¦æˆ‘ç°åœ¨å¸®ä½ æŠŠ **1â€“7 éƒ¨åˆ†å…¨éƒ¨åˆå¹¶**ï¼Œæ•´ç†æˆä¸€ä»½ **å®Œæ•´è¿è´¯çš„è°ƒç ”ç¬”è®°ç¨¿ï¼ˆå¸¦å…¬å¼å’Œç®—æ³•æ­¥éª¤ï¼‰**ï¼Ÿ
ğŸ‘Œ æ”¶åˆ°ï¼Œè¿™ä¸€é¡µæ˜¯ **TD-MPC çš„ Training é˜¶æ®µ (TOLD)**ï¼Œæ¶‰åŠè®­ç»ƒç›®æ ‡ã€æŸå¤±å‡½æ•°å’Œæ›´æ–°æ–¹æ³•ã€‚æˆ‘æ¥å¸®ä½ æ•´ç†ï¼š

---

## 8. TD-MPC Training é˜¶æ®µ (TOLD)

### æ ¸å¿ƒæ€æƒ³

åœ¨è®­ç»ƒé˜¶æ®µï¼ŒTD-MPC ä½¿ç”¨ **TOLD (Temporal Difference Learning with Latent Dynamics)** æ¡†æ¶æ¥è”åˆè®­ç»ƒä»¥ä¸‹äº”ä¸ªæ¨¡å—ï¼š

* **Representation**: $z_t = h_\theta(s_t)$
* **Latent Dynamics**: $z_{t+1} = d_\theta(z_t, a_t)$
* **Reward**: $\hat{r}_t = R_\theta(z_t, a_t)$
* **Value**: $\hat{q}_t = Q_\theta(z_t, a_t)$
* **Policy**: $\hat{a}_t \sim \pi_\theta(z_t)$

---

### æŸå¤±å‡½æ•° (TOLD minimizes the objective)

$$
J(\theta; \Gamma) = \sum_{t=0}^{H} \lambda^t \mathcal{L}(\theta; \Gamma_t)
$$

å…¶ä¸­ï¼š

$$
\mathcal{L}(\theta; \Gamma_t) = c_1 \lVert R_\theta(z_i, a_i) - r_i \rVert^2 
+ c_2 \lVert Q_\theta(z_i, a_i) - (r_i + \gamma Q_{\bar{\theta}}(z_{i+1}, \pi_\theta(z_{i+1}))) \rVert^2 
+ c_3 \lVert d_\theta(z_i, a_i) - h_\theta(s_{i+1}) \rVert^2
$$

* ç¬¬ 1 é¡¹ï¼šReward consistency
* ç¬¬ 2 é¡¹ï¼šValue TD consistency
* ç¬¬ 3 é¡¹ï¼šLatent state consistency

æœ€ç»ˆ policy çš„ä¼˜åŒ–ç›®æ ‡æ˜¯ï¼š

$$
J_\pi(\theta; \Gamma) = - \sum_{t=0}^{H} \lambda^t Q_\theta(z_i, \pi_\theta(\text{sg}(z_i)))
$$

---

### Algorithm 2: TOLD (Training)

1. åˆå§‹åŒ–ç½‘ç»œå‚æ•° $\theta, \theta^-$ï¼Œå­¦ä¹ ç‡ç­‰è¶…å‚ã€‚
2. **æ”¶é›†æ•°æ®**ï¼šæ¯æ¬¡ä¸ç¯å¢ƒäº¤äº’æ—¶ï¼Œä½¿ç”¨ **TD-MPC çš„ç¬¬ä¸€ä¸ª action** æ‰§è¡Œã€‚
3. å°†é‡‡æ ·åˆ°çš„ $(s_t, a_t, r_t, s_{t+1})$ å­˜å…¥ bufferã€‚
4. **æ›´æ–° TOLD**ï¼š

   * è¡¨ç¤ºå­¦ä¹ ï¼š$z_t = h_\theta(s_t)$
   * é¢„æµ‹å¥–åŠ±ï¼š$\hat{r}_i = R_\theta(z_i, a_i)$
   * é¢„æµ‹ä»·å€¼ï¼š$\hat{q}_i = Q_\theta(z_i, a_i)$
   * çŠ¶æ€è½¬ç§»ï¼š$z_{i+1} = d_\theta(z_i, a_i)$
   * ç­–ç•¥åŠ¨ä½œï¼š$\hat{a}_i = \pi_\theta(z_i)$
   * é€æ­¥ç´¯ç§¯ lossï¼š

     $$
     J \leftarrow J + \lambda^i \mathcal{L}(\theta; \Gamma_i)
     $$
5. **æ›´æ–°ç½‘ç»œå‚æ•°**ï¼š

   * $\theta \leftarrow \theta - \eta \nabla_\theta J$
   * æ›´æ–° target ç½‘ç»œï¼š$\theta^- \leftarrow (1-\tau)\theta^- + \tau \theta$

---

### ä¸ºä»€ä¹ˆè¦å­¦ä¹  policyï¼Ÿ

* **Planning è§’åº¦**ï¼špolicy æä¾› action proposalsï¼ŒåŠ å¿«æ”¶æ•›ã€‚
* **Learning è§’åº¦**ï¼šç›´æ¥é€šè¿‡ planning æ¥ä¼°è®¡ Q-target å¾ˆæ…¢ï¼Œç”¨ policy æ›¿ä»£æ›´é«˜æ•ˆã€‚

---

ğŸ‘‰ æ€»ç»“ï¼š
è®­ç»ƒé˜¶æ®µ (TOLD) çš„å…³é”®æ˜¯ï¼šåœ¨ latent ç©ºé—´è”åˆä¼˜åŒ– **rewardã€valueã€dynamicsã€representationã€policy** äº”ä¸ªæ¨¡å—ï¼Œä¿è¯æ¨¡å‹æ—¢èƒ½åšçŸ­æœŸ MPC è§„åˆ’ï¼Œåˆèƒ½é€šè¿‡ TD consistency æ•æ‰é•¿æœŸä¿¡æ¯ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ **ç¬¬ 1â€“8 èŠ‚åˆå¹¶æˆä¸€ä»½å®Œæ•´çš„ç¬”è®°ï¼ˆåŒ…å«æ‰€æœ‰å…¬å¼ã€ç®—æ³•æ­¥éª¤ã€ç›´è§‚è§£é‡Šï¼‰**ï¼Œè¾“å‡ºæˆä¸€ä¸ª Markdown ç‰ˆï¼Œæ–¹ä¾¿ä½ ç›´æ¥ç²˜è´´è¿› Obsidian æˆ–ç ”ç©¶ç¬”è®°ï¼Ÿ
