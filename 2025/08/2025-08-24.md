

---

# Model Based + MPC + Planning + RL 相关

最近在看一些 MPC 和 dynamic model 的文章和工作，这里做一个总结记录。
文章主要分为以下几个模块：

1. 模仿学习
2. model based RL + dynamic model + MBPO
3. Update-To-Data (UTD) ratio
4. MPC
5. model based + planning
6. MPC 结合 RL 方案

---

## 1. 模仿学习


模仿学习本质上是一个**监督学习**的过程，需要专家数据（带有 label）。
其上限一般就是专家数据的性能，但可以用来热启动模型。

一个经典的方案是 **DAgger 算法**（Dataset Aggregation）：

1. 从人工提供的数据集
   $$
   D = \{ o_1, a_1, o_2, a_2, \ldots, o_N, a_N \}
   $$
   中训练出策略
   $$
   \pi_\theta(a_t \mid o_t)
   $$

2. 运行策略 $\pi_\theta(a_t \mid o_t)$，采样得到一个新的数据集
   $$
   D_\pi = \{ o_1, \ldots, o_M \}
   $$

3. 由人工对数据集 $D_\pi$ 进行标注，得到一系列动作 $a_t$

4. 合并数据集
   $$
   D \leftarrow D \cup D_\pi
   $$
   返回第 1 步



>使用学习到的策略在环境中交互，采样新的数据，然后需要人工重新给数据加 label。
>缺点是人工标注成本很高，但可以不断迭代优化。

---

## 2. Model Based RL + Dynamic Model

### 背景

* **model free** 算法的缺点：需要大量数据，如果没有接近真实的仿真器，一般很难训练。
* **model based** 可以节省与仿真器交互的次数，提高采样效率（sample efficiency）。
* 在不限样本数据的情况下，model free 算法的性能一般优于 model based。
* 但在有限样本或昂贵环境下，model based RL 更具优势（例如 Muzero、Alphazero+ 等）。

---

### 一个简单版本的基于模型的增强学习算法

执行三步：

1. 运行某种基本策略 $\pi_\theta(a_t \mid s_t)$（如随机策略）来收集样本数据：
   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. 通过最小化
   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$

   来学习环境模型 $f(s,a)$。

3. 根据 $f(s,a)$ 来计划未来行动。



---

## 3. Model-Based Policy Optimization (MBPO)

MBPO 是一种经典的 model-based RL 算法

![[Pasted image 20250825151041.png]]

### Model-Based Policy Optimization with Deep Reinforcement Learning

1. Initialize policy $\pi_\phi$、predictive model $p_\theta$、environment dataset $\mathcal{D}_{\text{env}}$、model dataset $\mathcal{D}_{\text{model}}$。
2. **for** $N$ epochs **do**

   3. Train model $p_\theta$ on $\mathcal{D}_{\text{env}}$ via maximum likelihood。
   4. **for** $E$ steps **do**

      * Take action in environment using $\pi_\phi$，add to $\mathcal{D}_{\text{env}}$。
   5. **for** $M$ model rollouts **do**

      * Sample $s_t$ uniformly from $\mathcal{D}_{\text{env}}$。
      * Perform $k$-step model rollout starting from $s_t$ using policy $\pi_\phi$，add to $\mathcal{D}_{\text{model}}$。
   6. **for** $G$ gradient updates **do**

      * Update policy parameters on model data:

        $$
        \phi \leftarrow \phi - \lambda_\pi \nabla_\phi J_\pi(\phi, \mathcal{D}_{\text{model}})
        $$


---

## 4. MPC (Model Predictive Control)

### 前提条件

使用 MPC 需要满足两个条件：

1. 存在一个 **dynamic model**，或者一个真实可回滚的仿真器。
2. 有明确的奖励函数定义，可以通过 dynamic model 计算得到奖励。

---

### 数学表达式

对于当前的状态 $s_t$，MPC 可以向前预测 $H$ 步，并选择能够使累计奖励最大的动作序列：

$$
A_t^{(H)} = \arg\max_{A_t^{(H)}} \sum_{t'=t}^{t+H-1} r(\hat{s}_{t'}, a_{t'})
$$

其中：

$$
\hat{s}_t = s_t, \quad \hat{s}_{t'+1} = \hat{s}_{t'} + \hat{f}_\theta(\hat{s}_{t'}, a_{t'})
$$

这里的 $\hat{f}_\theta$ 表示 dynamic model，用于预测下一时刻的状态。


---

### 简单的 MPC + Model-Based RL 流程

1. 运行基础策略 $\pi_0(a_t \mid s_t)$（如随机策略），收集数据：
   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. 学习动态模型 $f(s,a)$，最小化：
   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$
3. 使用 $f(s,a)$ 进行规划，选择动作。
4. 执行规划出的第一个动作，观察得到新状态 $s'$（MPC）。
5. 将 $(s, a, s')$ 添加到数据集 $D$。

---

## 5. MPC 结合 RL 方案

这一部分主要介绍 **TD-MPC (Temporal Difference Learning for Model Predictive Control)**。

---

### 基本思想

传统 MPC 一般只向前预测 $H$ 步：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Bigg]
$$

这种方式的缺陷是：**只能局部优化短期轨迹，无法直接处理无限时域目标**：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \Bigg] \quad \text{(intractable)}
$$

TD-MPC 的核心改进：

* 结合 **TD-learning (时序差分学习)** 与 **MPC 规划**。
* 通过 **有限步 MPC (H-step)** 获得 *locally optimal trajectory*。
* 将其与 **TD backup** 结合，从而在有限预测范围内近似无限时域目标。

---

### 直观理解

* 传统 MPC：滚动预测 $H$ 步，只能局部最优。
* TD-MPC：在 MPC 的基础上加入 TD 学习，使得短期规划和长期价值估计结合，获得更稳定、更高效的控制策略。

## 6. TD-MPC 的 Motivation 与公式

### 问题背景

如果要考虑真实的 **episode reward**，需要跑完整个 episode，但存在两个问题：

1. episode 太长时，计算代价过高；
2. episode 太长时，模型预测误差会逐步累积，结果更不准。

---

### Motivation

Data-driven Model Predictive Control 的两大挑战：

* **长时序规划成本高**（costly to plan over long horizons）
* **难以获得精准的环境模型**（challenging to obtain an accurate model of the environment）

因此，TD-MPC 的核心思路：

* 仍然采用 **H-step 规划**；
* 对超出部分，使用 **Value Function** 来估计长期收益。

---

### How can TD-learning help MPC?

* 通过 TD-learning 学习一个 **终端价值函数 (terminal value function)**。
* MPC 本质上得到的是 **局部最优 (local optimal solutions)**。
* Value Function 提供了对 **全局最优 (globally optimal solution)** 的近似。

---

### 规划阶段的评估指标

最终在 planning 时，回报的估计公式变为：

$$
\mathbb{E}_\tau \Bigg[ \gamma^H Q_\theta(z_H, a_H) + \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) \Bigg]
$$

* 前半部分（红色 Value）：使用终端价值函数 $Q_\theta$ 估计长远回报。
* 后半部分（红色 Rewards）：计算 MPC 短期 $H$ 步的累积奖励。

这种结合方式，使得规划兼顾短期和长期目标。

---

## 7. TD-MPC Inference 阶段

### 核心思想

* 用到的 MPC 算法是 **MPPI**。
* 通过估计的 **top-k 采样轨迹**，进行加权平均来更新分布参数。
* 属于 **Policy-guided trajectory optimization** 的一种实现。

---

### 关键公式

对采样轨迹 $\tau$ 的加权：

$$
\phi_\pi = \frac{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big) \tau_i}{\sum_{i=1}^N \exp\Big(-\frac{1}{\lambda} \hat{S}(\tau_i)\Big)}
$$

* $\hat{S}(\tau_i)$：轨迹的累计代价（负奖励）。
* $\lambda$：温度参数，用于控制采样分布的“锐度”。

最终得到轨迹分布的更新均值：

$$
\mu' = \sum_{i=1}^N w_i \tau_i, \quad 
w_i = \frac{\exp(-\frac{1}{\lambda}\hat{S}(\tau_i))}{\sum_j \exp(-\frac{1}{\lambda}\hat{S}(\tau_j))}
$$

---

### TD-MPC Inference 伪代码（MPPI）

**Algorithm 1: TD-MPC (inference)**

1. **输入**：网络参数 $\theta$，初始分布参数 $\mu^0, \sigma^0$，采样数 $N, N_\pi$，预测步长 $H$。
2. 将状态编码为 latent：$z_t = h_\theta(s_t)$。
3. **for each iteration $j = 1 \dots J$**

   * 采样 $N$ 条轨迹 $\tau$，每条长度为 $H$。
   * 额外采样 $N_\pi$ 条轨迹，来自 policy + dynamic model。
   * 计算所有轨迹的回报：

     $$
     \phi_\pi = \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) + \gamma^H Q_\theta(z_H, a_H)
     $$
   * 根据回报对轨迹进行加权，更新 $\mu, \sigma$。
4. **输出**：根据新的分布参数选择动作 $a \sim \mathcal{N}(\mu, \sigma^2)$。

---

### Policy-guided Trajectory Optimization 步骤

1. 根据当前 $\mu, \sigma$ 采样 $N$ 条轨迹序列。
2. 同时用 policy 和 dynamic model 采样 $N_\pi$ 条轨迹。
3. 使用所有轨迹的 **terminal value** 更新 $\mu, \sigma$。
4. 引入最小值 $\epsilon$ 来增强 **Exploration**。
5. 重复更新，直到收敛到一个较优的 $\mu, \sigma$。

---


---

## 8. TD-MPC Training 阶段 (TOLD)

### 核心思想

在训练阶段，TD-MPC 使用 **TOLD (Temporal Difference Learning with Latent Dynamics)** 框架来联合训练以下五个模块：

* **Representation**: $z_t = h_\theta(s_t)$
* **Latent Dynamics**: $z_{t+1} = d_\theta(z_t, a_t)$
* **Reward**: $\hat{r}_t = R_\theta(z_t, a_t)$
* **Value**: $\hat{q}_t = Q_\theta(z_t, a_t)$
* **Policy**: $\hat{a}_t \sim \pi_\theta(z_t)$

---

### 损失函数 (TOLD minimizes the objective)

$$
J(\theta; \Gamma) = \sum_{t=0}^{H} \lambda^t \mathcal{L}(\theta; \Gamma_t)
$$

其中：

$$
\mathcal{L}(\theta; \Gamma_t) = c_1 \lVert R_\theta(z_i, a_i) - r_i \rVert^2 
+ c_2 \lVert Q_\theta(z_i, a_i) - (r_i + \gamma Q_{\bar{\theta}}(z_{i+1}, \pi_\theta(z_{i+1}))) \rVert^2 
+ c_3 \lVert d_\theta(z_i, a_i) - h_\theta(s_{i+1}) \rVert^2
$$

* 第 1 项：Reward consistency
* 第 2 项：Value TD consistency
* 第 3 项：Latent state consistency

最终 policy 的优化目标是：

$$
J_\pi(\theta; \Gamma) = - \sum_{t=0}^{H} \lambda^t Q_\theta(z_i, \pi_\theta(\text{sg}(z_i)))
$$

---

### Algorithm 2: TOLD (Training)

1. 初始化网络参数 $\theta, \theta^-$，学习率等超参。
2. **收集数据**：每次与环境交互时，使用 **TD-MPC 的第一个 action** 执行。
3. 将采样到的 $(s_t, a_t, r_t, s_{t+1})$ 存入 buffer。
4. **更新 TOLD**：

   * 表示学习：$z_t = h_\theta(s_t)$
   * 预测奖励：$\hat{r}_i = R_\theta(z_i, a_i)$
   * 预测价值：$\hat{q}_i = Q_\theta(z_i, a_i)$
   * 状态转移：$z_{i+1} = d_\theta(z_i, a_i)$
   * 策略动作：$\hat{a}_i = \pi_\theta(z_i)$
   * 逐步累积 loss：

     $$
     J \leftarrow J + \lambda^i \mathcal{L}(\theta; \Gamma_i)
     $$
5. **更新网络参数**：

   * $\theta \leftarrow \theta - \eta \nabla_\theta J$
   * 更新 target 网络：$\theta^- \leftarrow (1-\tau)\theta^- + \tau \theta$

---

### 为什么要学习 policy？

* **Planning 角度**：policy 提供 action proposals，加快收敛。
* **Learning 角度**：直接通过 planning 来估计 Q-target 很慢，用 policy 替代更高效。

---
