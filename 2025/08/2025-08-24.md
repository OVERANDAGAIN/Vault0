---
创建时间: 2025-八月-25日  星期一, 2:58:16 下午
---


---

# Model Based + MPC + Planning + RL 相关


1. 模仿学习
2. model based RL + dynamic model + MBPO
3. Update-To-Data (UTD) ratio
4. MPC
5. model based + planning
6. MPC 结合 RL 方案

---

## 1. 模仿学习


模仿学习本质上是一个**监督学习**的过程，需要专家数据（带有 label）。
其上限一般就是专家数据的性能，但可以用来热启动模型。

一个经典的方案是 **DAgger 算法**（Dataset Aggregation）：

1. 从人工提供的数据集
   $$
   D = \{ o_1, a_1, o_2, a_2, \ldots, o_N, a_N \}
   $$
   中训练出策略
   $$
   \pi_\theta(a_t \mid o_t)
   $$

2. 运行策略 $\pi_\theta(a_t \mid o_t)$，采样得到一个新的数据集
   $$
   D_\pi = \{ o_1, \ldots, o_M \}
   $$

3. 由人工对数据集 $D_\pi$ 进行标注，得到一系列动作 $a_t$

4. 合并数据集
   $$
   D \leftarrow D \cup D_\pi
   $$
   返回第 1 步



>使用学习到的策略在环境中交互，采样新的数据，然后需要人工重新给数据加 label。
>缺点是人工标注成本很高，但可以不断迭代优化。

---

## 2. Model Based RL + Dynamic Model

### 背景

* **model free** 算法的缺点：需要大量数据，如果没有接近真实的仿真器，一般很难训练。
* **model based** 可以节省与仿真器交互的次数，提高采样效率（sample efficiency）。
* 在不限样本数据的情况下，model free 算法的性能一般优于 model based。
* 但在有限样本或昂贵环境下，model based RL 更具优势（例如 Muzero、Alphazero+ 等）。

---

### 一个简单版本的基于模型的增强学习算法

执行三步：

1. 运行某种基本策略 $\pi_\theta(a_t \mid s_t)$（如随机策略）来收集样本数据：
   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. 通过最小化
   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$

   来学习环境模型 $f(s,a)$。

3. 根据 $f(s,a)$ 来计划未来行动。



---

## 3. Model-Based Policy Optimization (MBPO)

MBPO 是一种经典的 model-based RL 算法

![[Pasted image 20250825151041.png]]

### Model-Based Policy Optimization with Deep Reinforcement Learning

1. Initialize policy $\pi_\phi$、predictive model $p_\theta$、environment dataset $\mathcal{D}_{\text{env}}$、model dataset $\mathcal{D}_{\text{model}}$。
2. **for** $N$ epochs **do**

   3. Train model $p_\theta$ on $\mathcal{D}_{\text{env}}$ via maximum likelihood。
   4. **for** $E$ steps **do**

      * Take action in environment using $\pi_\phi$，add to $\mathcal{D}_{\text{env}}$。
   5. **for** $M$ model rollouts **do**

      * Sample $s_t$ uniformly from $\mathcal{D}_{\text{env}}$。
      * Perform $k$-step model rollout starting from $s_t$ using policy $\pi_\phi$，add to $\mathcal{D}_{\text{model}}$。
   6. **for** $G$ gradient updates **do**

      * Update policy parameters on model data:

        $$
        \phi \leftarrow \phi - \lambda_\pi \nabla_\phi J_\pi(\phi, \mathcal{D}_{\text{model}})
        $$


---

## 4. MPC (Model Predictive Control)

### 前提条件

使用 MPC 需要满足两个条件：

1. 存在一个 **dynamic model**，或者一个真实可回滚的仿真器。
2. 有明确的奖励函数定义，可以通过 dynamic model 计算得到奖励。

---

### 数学表达式

对于当前的状态 $s_t$，MPC 可以向前预测 $H$ 步，并选择能够使累计奖励最大的动作序列：

$$
A_t^{(H)} = \arg\max_{A_t^{(H)}} \sum_{t'=t}^{t+H-1} r(\hat{s}_{t'}, a_{t'})
$$

其中：

$$
\hat{s}_t = s_t, \quad \hat{s}_{t'+1} = \hat{s}_{t'} + \hat{f}_\theta(\hat{s}_{t'}, a_{t'})
$$

这里的 $\hat{f}_\theta$ 表示 dynamic model，用于预测下一时刻的状态。


---

### 简单的 MPC + Model-Based RL 流程

1. 运行基础策略 $\pi_0(a_t \mid s_t)$（如随机策略），收集数据：
   $$
   D = \{ (s_i, a_i, s'_i) \}_i
   $$
2. 学习动态模型 $f(s,a)$，最小化：
   $$
   \sum_i \lVert f(s_i, a_i) - s'_i \rVert^2
   $$
3. 使用 $f(s,a)$ 进行规划，选择动作。
4. 执行规划出的第一个动作，观察得到新状态 $s'$（MPC）。
5. 将 $(s, a, s')$ 添加到数据集 $D$。

---

## 5. MPC 结合 RL 方案

TD-MPC (Temporal Difference Learning for Model Predictive Control)

---

### 基本思想

传统 MPC 一般只向前预测 $H$ 步：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{H} \gamma^t r(s_t, a_t) \Bigg]
$$

这种方式的缺陷是：**只能局部优化短期轨迹，无法直接处理无限时域目标**：

$$
\mathbb{E}_\tau \Bigg[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \Bigg] \quad \text{(intractable)}
$$

TD-MPC 的核心改进：

* 结合 **TD-learning (时序差分学习)** 与 **MPC 规划**。
* 通过 **有限步 MPC (H-step)** 获得 locally optimal trajectory。

---

### How can TD-learning help MPC?

* 通过 TD-learning 学习一个 **终端价值函数 (terminal value function)**。
* MPC 本质上得到的是 **局部最优 (local optimal solutions)**。
* Value Function 提供了对 **全局最优 (globally optimal solution)** 的近似。

---

### 规划阶段的评估指标

最终在 planning 时，回报的估计公式变为：

$$
\mathbb{E}_\tau \Bigg[ \gamma^H Q_\theta(z_H, a_H) + \sum_{t=0}^{H-1} \gamma^t R_\theta(z_t, a_t) \Bigg]
$$

* 前半部分：使用终端价值函数 $Q_\theta$ 估计长远回报。
* 后半部分：计算 MPC 短期 $H$ 步的累积奖励。





---


1. 预训练的reward和dynamics模型直接加载进去看效果
2. value模型看看能不能加上reward的信息
3. 看一下cleanrl移植是否有错误
4. 奖励模型使用的是reward
5. value模型使用的是return
6.