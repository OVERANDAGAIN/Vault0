好的，图片中的文字内容我帮你转成文本如下：

---


### 强化学习基础篇

* 初探强化学习
* 多臂老虎机
* 马尔可夫决策过程
* 动态规划算法
* 时序差分算法
* Dyna-Q算法

### 强化学习进阶篇

* DQN算法
* DQN改进算法
* 策略梯度算法
* Actor-Critic算法
* TRPO算法
* PPO算法
* DDPG算法
* SAC算法

### 强化学习前沿篇

* 模仿学习
* 模型预测控制
* 基于模型的策略优化
* 离线强化学习
* 目标导向的强化学习
* 多智能体强化学习入门
* 多智能体强化学习进阶

**总结与展望**






太好了，这批简介正好能把“父→子＝动机与改进”的脉络补齐。我按**问题设定/范式→代表算法→改进动机**来重分组，并把每个算法挂上**真实论文链接**（arXiv/PMLR/期刊 HTML）。节点名尽量精简，便于导入你的思维导图工具。

---

# 单智能体强化学习（重分类 · 文献树）

## 价值方法 · 函数逼近

### 动机：高维状态、离散动作

### [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013

#### 改进·稳定性

##### [Double DQN](https://arxiv.org/abs/1509.06461) ; van Hasselt et al., 2015

##### [Dueling DQN](https://arxiv.org/abs/1511.06581) ; Wang et al., 2016

#### 改进·数据效率

##### [Prioritized Replay](https://arxiv.org/abs/1511.05952) ; Schaul et al., 2015

## 策略梯度 · 显式策略

### 动机：直接优化目标策略

### [REINFORCE](https://link.springer.com/article/10.1007/BF00992696) ; Williams, 1992

#### 改进·降方差/收敛

##### [Actor–Critic](https://papers.nips.cc/paper_files/paper/2000/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html) ; Konda & Tsitsiklis, 2000

##### [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015

##### [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017

## 连续控制 · 离线策略

### 动机：连续动作 + 采样效率

### [DDPG](https://arxiv.org/abs/1509.02971) ; Lillicrap et al., 2015

#### 改进·稳定性/鲁棒

##### [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018

## 模仿学习 · 无需奖励

### 动机：奖励难设计/稀疏

### [Behavior Cloning](https://dl.acm.org/doi/10.1145/97243.97245) ; Pomerleau, 1989

### [GAIL](https://arxiv.org/abs/1606.03476) ; Ho & Ermon, 2016

## 基于模型 · 训练提效

### 动机：学环境模型，降交互成本

### [MPC](https://www.sciencedirect.com/science/article/pii/S0005109804002766) ; Mayne, 2005 综述

### [PETS](https://arxiv.org/abs/1805.12114) ; Chua et al., 2018

### [MBPO](https://arxiv.org/abs/1906.08253) ; Janner et al., 2019

## 离线强化学习 · 固定数据

### 动机：不可/不宜在线交互

### [BCQ](https://arxiv.org/abs/1812.02900) ; Fujimoto et al., 2019

### [CQL](https://arxiv.org/abs/2006.04779) ; Kumar et al., 2020

### [IQL](https://arxiv.org/abs/2110.06169) ; Kostrikov et al., 2021

## 目标导向 · 条件化策略

### 动机：多目标/复合任务

### [UVFA](https://arxiv.org/abs/1802.09464) ; Schaul et al., 2015

### [HER](https://arxiv.org/abs/1707.01495) ; Andrychowicz et al., 2017

---

# 多智能体强化学习（提纲位）

## 入门 · 非稳态环境

### 动机：多主体交互

## CTDE 范式 · 训练/执行解耦

### 值分解

#### [VDN](https://arxiv.org/abs/1706.05296) ; Sunehag et al., 2017

#### [QMIX](https://arxiv.org/abs/1803.11485) ; Rashid et al., 2018

### Actor–Critic

#### [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017

#### [COMA](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018

---

### 说明 & 用法建议

* 我把每个“父节点”都用**动机**节点串联，随后挂**代表算法**与**改进点**，读者能直接看到“问题→方法→改进”的逻辑。
* 以上 Markdown 结构可直接粘贴到你的思维导图编辑器；含链接的节点已按你示例的 `[# 标题](url)` 形式写好。
* 如果你希望**再细化一层**（例如把“稳定性/数据效率/样本效率”统一用颜色或图标标注），我可以给一版带**标签约定**的配色方案，便于全图一致呈现。


👌 明白了，你的意思是：
目前我们先**单独整理不同来源的内容**（比如《动手学强化学习》的单智能体算法脉络、以及这篇《A Review of Cooperation in Multi-agent Learning》的合作综述），之后再把它们**在一个总的思维导图里融合**。

针对你贴的这篇综述表格，我可以把它转成和前面一致的**思维导图格式**（带「动机说明 → 方法分类 → 代表算法」这种综述式结构），这样以后合并的时候就能无缝拼接。

---

# 多智能体合作学习综述（思维导图版）

## 研究背景

### 动机：合作是多智能体高效决策的核心

* 难点：部分可观测、个体-群体冲突、学习耦合

## 合作类型

### 纯合作（Pure Cooperation）

### 混合动机（Mixed-motivation）

* 典型：社会困境（Social Dilemmas）

## 关键研究挑战

### 信号归因（Credit Assignment）

### 动态适应（Partner Adaptation）

### 策略泛化不足

### 社会困境诱发背叛

### 缺乏可解释性

## 核心机制分类

### CTDE 架构

* [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017
* [QMIX](https://arxiv.org/abs/1803.11485) ; Rashid et al., 2018
* [QTRAN](https://arxiv.org/abs/1905.05408) ; Son et al., 2019

### 通信机制

* [DIAL](https://arxiv.org/abs/1605.06676) ; Foerster et al., 2016
* [CommNet](https://arxiv.org/abs/1605.07736) ; Sukhbaatar et al., 2016
* [IC3Net](https://arxiv.org/abs/1810.03916) ; Singh et al., 2018

### 归因机制

* Counterfactual Baseline（COMA, 2018）
* Credit Assignment Graph

### 伙伴适应

* [LOLA](https://arxiv.org/abs/1709.04326) ; Foerster et al., 2017
* Bayesian ToM

### 利他激励

* Prosocial RL
* Inequity Aversion
* SVO（Social Value Orientation）
* Intrinsic Motivation

### 影响建模

* [LIO](https://arxiv.org/abs/1810.08647) ; Yang et al., 2018
* Social Influence（Jaques et al., 2019）
* LOLA (also here)

### 声誉与规范

* Competitive Altruism
* Norm-based Cooperation (CNM)

### 契约机制

* Contract-based RL
* Reward Exchange

### 社会困境建模

* [Sequential Social Dilemmas (SSD)](https://arxiv.org/abs/1702.03037) ; Leibo et al., 2017

## 评估环境

* 粒子环境：simple-spread, speaker-listener
* Hanabi
* Overcooked-AI
* Harvest / Cleanup (SSD)

## 评估指标

* 社会福利（SW）
* 公平性（Gini）
* 可泛化性（zero-shot, 多任务）
* 合作稳定性（抗背叛、规范遵守）

## 关键结论

* 合作学习需兼顾：学习机制 + 博弈动力学 + 行为适应
* 未来重点：理论支持、泛化性建模、人类交互适应性
* 当前方法多在人工环境，缺乏现实复杂性验证
* 亟需：标准化评估协议 + 真实世界基准任务

---

这样一来，这个综述的脉络就和我们前面整理的**单智能体 RL 思维导图**保持一致的「动机 → 方法 → 代表算法」风格了。

要不要我接下来帮你把这篇综述和《动手学RL》的内容**合并成一个大总图的“单智能体 + 多智能体”结构雏形**，方便你后续在思维导图软件里直接展开？
