好的，图片中的文字内容我帮你转成文本如下：

---


### 强化学习基础篇

* 初探强化学习
* 多臂老虎机
* 马尔可夫决策过程
* 动态规划算法
* 时序差分算法
* Dyna-Q算法

### 强化学习进阶篇

* DQN算法
* DQN改进算法
* 策略梯度算法
* Actor-Critic算法
* TRPO算法
* PPO算法
* DDPG算法
* SAC算法

### 强化学习前沿篇

* 模仿学习
* 模型预测控制
* 基于模型的策略优化
* 离线强化学习
* 目标导向的强化学习
* 多智能体强化学习入门
* 多智能体强化学习进阶

**总结与展望**






太好了，这批简介正好能把“父→子＝动机与改进”的脉络补齐。我按**问题设定/范式→代表算法→改进动机**来重分组，并把每个算法挂上**真实论文链接**（arXiv/PMLR/期刊 HTML）。节点名尽量精简，便于导入你的思维导图工具。

---

# 单智能体强化学习（重分类 · 文献树）

## 价值方法 · 函数逼近

### 动机：高维状态、离散动作

### [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013

#### 改进·稳定性

##### [Double DQN](https://arxiv.org/abs/1509.06461) ; van Hasselt et al., 2015

##### [Dueling DQN](https://arxiv.org/abs/1511.06581) ; Wang et al., 2016

#### 改进·数据效率

##### [Prioritized Replay](https://arxiv.org/abs/1511.05952) ; Schaul et al., 2015

## 策略梯度 · 显式策略

### 动机：直接优化目标策略

### [REINFORCE](https://link.springer.com/article/10.1007/BF00992696) ; Williams, 1992

#### 改进·降方差/收敛

##### [Actor–Critic](https://papers.nips.cc/paper_files/paper/2000/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html) ; Konda & Tsitsiklis, 2000

##### [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015

##### [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017

## 连续控制 · 离线策略

### 动机：连续动作 + 采样效率

### [DDPG](https://arxiv.org/abs/1509.02971) ; Lillicrap et al., 2015

#### 改进·稳定性/鲁棒

##### [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018

## 模仿学习 · 无需奖励

### 动机：奖励难设计/稀疏

### [Behavior Cloning](https://dl.acm.org/doi/10.1145/97243.97245) ; Pomerleau, 1989

### [GAIL](https://arxiv.org/abs/1606.03476) ; Ho & Ermon, 2016

## 基于模型 · 训练提效

### 动机：学环境模型，降交互成本

### [MPC](https://www.sciencedirect.com/science/article/pii/S0005109804002766) ; Mayne, 2005 综述

### [PETS](https://arxiv.org/abs/1805.12114) ; Chua et al., 2018

### [MBPO](https://arxiv.org/abs/1906.08253) ; Janner et al., 2019

## 离线强化学习 · 固定数据

### 动机：不可/不宜在线交互

### [BCQ](https://arxiv.org/abs/1812.02900) ; Fujimoto et al., 2019

### [CQL](https://arxiv.org/abs/2006.04779) ; Kumar et al., 2020

### [IQL](https://arxiv.org/abs/2110.06169) ; Kostrikov et al., 2021

## 目标导向 · 条件化策略

### 动机：多目标/复合任务

### [UVFA](https://arxiv.org/abs/1802.09464) ; Schaul et al., 2015

### [HER](https://arxiv.org/abs/1707.01495) ; Andrychowicz et al., 2017

---

# 多智能体强化学习（提纲位）

## 入门 · 非稳态环境

### 动机：多主体交互

## CTDE 范式 · 训练/执行解耦

### 值分解

#### [VDN](https://arxiv.org/abs/1706.05296) ; Sunehag et al., 2017

#### [QMIX](https://arxiv.org/abs/1803.11485) ; Rashid et al., 2018

### Actor–Critic

#### [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017

#### [COMA](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018

---

### 说明 & 用法建议

* 我把每个“父节点”都用**动机**节点串联，随后挂**代表算法**与**改进点**，读者能直接看到“问题→方法→改进”的逻辑。
* 以上 Markdown 结构可直接粘贴到你的思维导图编辑器；含链接的节点已按你示例的 `[# 标题](url)` 形式写好。
* 如果你希望**再细化一层**（例如把“稳定性/数据效率/样本效率”统一用颜色或图标标注），我可以给一版带**标签约定**的配色方案，便于全图一致呈现。


