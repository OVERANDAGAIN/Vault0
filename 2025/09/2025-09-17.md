

## Learning with Opponent-Learning Awareness
The study of general-sum games has a long history in game theory and evolution. Many papers address the iterated prisoners’ dilemma (IPD) in particular, including the seminal work on the topic by Axelrod [1]. This work popularised tit-for-tat (TFT), a strategy in which an agent cooperates on the first move and then copies the opponent’s most recent move, as an effective and simple strategy. A number of methods in multi-agent RL aim to achieve convergence in self-play and rationality in sequential, general sum games. Seminal work includes the family of WoLF algorithms [3], which uses different learning rates depending on whether an agent is winning or losing, joint-action-learners (JAL), and AWESOME [9]. Unlike LOLA, these algorithms typically have well understood convergence behaviour given an appropriate set of constraints. However, none of these algorithm have the ability to shape the learning behaviour of the opponents in order to obtain higher payouts at convergence. AWESOME aims to learn the equilibria of the one-shot game, a subset of the equilibria of the iterated game. Detailed studies have analysed the dynamics of JALs in general sum settings: This includes work by Uther and Veloso [43] in zerosum settings and by Claus and Boutilier [8] in cooperative settings. Sandholm and Crites [39] study the dynamics of independent Qlearning in the IPD under a range of different exploration schedules and function approximators. Wunder et al. [45] and Zinkevich et al. [47] explicitly study the convergence dynamics and equilibria  of learning in iterated games. Unlike LOLA, these papers do not propose novel learning rules. Littman [26] propose a method that assumes each opponent either to be a friend, i.e., fully cooperative, or foe, i.e., fully adversarial. Instead, LOLA considers general sum games. By comparing a set of models with different history lengths, Chakraborty and Stone [7] propose a method to learn a best response to memory bounded agents with fixed policies. In contrast, LOLA assumes learning agents, which effectively correspond to unbounded memory policies. Brafman and Tennenholtz [4] introduce the solution concept of an efficient learning equilibrium (ELE), in which neither side is encouraged to deviate from the learning rule. The algorithm they propose applies to settings where all Nash equilibria can be computed and enumerated; LOLA does not require either of these assumptions. By contrast, most work in deep multi-agent RL focuses on fully cooperative or zero-sum settings, in which learning progress is easier to evaluate, [13, 14, 34] and emergent communication in particular [11, 12, 22, 32, 40]. As an exception, Leibo et al. [24] analyse the outcomes of independent learning in general sum settings using feedforward neural networks as policies. Lowe et al. [27] propose a centralised actor-critic architecture for efficient training in these general sum environments. However, none of these methods explicitly reasons about the learning behaviour of other agents. Lanctot et al. [21] generalise the ideas of game-theoretic best-responsestyle algorithms, such as NFSP [17]. In contrast to LOLA, these best-response algorithms assume a given set of opponent policies, rather than attempting to shape the learning of the other agents. The problem setting and approach of Lerer and Peysakhovich [25] is closest to ours. They directly generalise tit-for-tat to complex environments using deep RL. The authors explicitly train a fully cooperative and a defecting policy for both agents and then construct a tit-for-tat policy that switches between these two in order to encourage the opponent to cooperate. Similar in spirit to this work, Munoz de Cote and Littman [33] propose a Nash equilibrium algorithm for repeated stochastic games that attempts to find the egalitarian equilibrium by switching between competitive and cooperative strategies. A similar idea underlies M-Qubed, [10], which balances best-response, cautious and optimistic learning biases. Reciprocity and cooperation are not emergent properties of the learning rules in these algorithms but rather directly coded into the algorithm via heuristics, limiting their generality. Our work also relates to opponent modelling, such as fictitious play [5] and action-sequence prediction [29, 36]. Mealing and Shapiro [30] also propose a method that finds a policy based on predicting the future action of a memory bounded opponent. Furthermore, Hernandez-Leal and Kaisers [18] directly model the distribution over opponents. While these methods model the opponent strategy, or distribution thereof, and use look-ahead to find optimal response policies, they do not address the learning dynamics of opponents. For further details we refer the reader to excellent reviews on the subject [6, 19]. By contrast, Zhang and Lesser [46] carry out policy prediction under one-step learning dynamics. However, the opponents’ policy updates are assumed to be given and only used to learn a best response to the anticipated updated parameters. By contrast, a LOLA agent directly shapes the policy updates of all opponents in order to maximise its own reward. Differentiating through the opponent’s learning step, which is unique to LOLA, is crucial for the emergence of tit-for-tat and reciprocity. To the best of our knowledge, LOLA is the first method that aims to shape the learning of other agents in a multi-agent RL setting. With LOLA, each agent differentiates through the opponents’ policy update. Similar ideas were proposed by Metz et al. [31], whose training method for generative adversarial networks differentiates through multiple update steps of the opponent. Their method relies on an end-to-end differentiable loss function, and thus does not work in the general RL setting. However, the overall results are similar: differentiating through the opponent’s learning process stabilises the training outcome in a zero sum setting. Outside of purely computational studies the emergence of cooperation and defection in RL settings has also been studied and compared to human data [20].

## DiCE: The Infinitely Differentiable Monte-Carlo Estimator

 Related Work  Gradient estimation is well studied, although many methods have been named and explored independently in different fields, and the primary focus has been on first order gradients. Fu (2006) provides an overview of methods from the point of view of simulation optimization.  The score function (SF) estimator, also referred to as the likelihood ratio estimator or REINFORCE, has received considerable attention in many fields. In reinforcement learning, policy gradient methods (Williams, 1992) have proven  highly successful, especially when combined with variance reduction techniques (Weaver & Tao, 2001; Grondman et al., 2012). The SF estimator has also been used in the analysis of stochastic systems (Glynn, 1990), as well as for variational inference (Wingate & Weber, 2013; Ranganath et al., 2014). Kingma & Welling (2013) and Rezende et al. (2014) discuss Monte-Carlo gradient estimates in the case where the stochastic parts of a model can be reparameterised.  These approaches are formalised for arbitrary computation graphs by Schulman et al. (2015), but to our knowledge our paper is the first to present a practical and correct approach for generating higher order gradient estimators utilising auto-diff. To easily make use of these estimates for optimising neural network models, automatic differentiation for backpropagation has been widely used (Baydin et al., 2015).  One rapidly growing application area for such higher order gradient estimates is meta-learning for reinforcement learning. Finn et al. (2017) compute a loss after a number of policy gradient learning steps, differentiating through the learning step to find parameters that can be quickly finetuned for different tasks. Li et al. (2017) extend this work to also meta-learn the fine-tuning step direction and magnitude. Al-Shedivat et al. (2017) and Stadie et al. (2018) derive the proper higher order gradient estimators for their work by reapplying the score function trick. Foerster et al. (2018) use a multi-agent version of the same higher order gradient estimators in combination with a Taylor expansion of the expected return. None present a general strategy for constructing higher order gradient estimators for arbitrary stochastic computation graphs.


## Stable Opponent Shaping in Differentiable Games

A growing number of learning methods are actually differentiable games whose players optimise multiple, interdependent objectives in parallel -- from GANs and intrinsic curiosity to multi-agent RL. Opponent shaping is a powerful approach to improve learning dynamics in these games, accounting for player influence on others' updates. Learning with Opponent-Learning Awareness (LOLA) is a recent algorithm that exploits this response and leads to cooperation in settings like the Iterated Prisoner's Dilemma. Although experimentally successful, we show that LOLA agents can exhibit 'arrogant' behaviour directly at odds with convergence. In fact, remarkably few algorithms have theoretical guarantees applying across all (n-player, non-convex) games. In this paper we present Stable Opponent Shaping (SOS), a new method that interpolates between LOLA and a stable variant named LookAhead. We prove that LookAhead converges locally to equilibria and avoids strict saddles in all differentiable games. SOS inherits these essential guarantees, while also shaping the learning of opponents and consistently either matching or outperforming LOLA experimentally.


## Learning to Incentivize Other Learning Agents
2 Related work  Learning to incentivize other learning agents is motivated by the problem of cooperation among independent learning agents in intertemporal social dilemmas (ISDs) [23], in which defection is preferable to individuals in the short term but mutual defection leads to low collective performance in the long term. Algorithms for fully-cooperative MARL [13, 31, 35] may not be applied as ISDs have mixed motives and cannot canonically be reduced to fully cooperative problems. Previous work showed that collective performance can be improved by independent agents with intrinsic rewards [10, 18, 42, 21, 17], which are either hand-crafted or slowly evolved based on other agents’ performance and modulate each agent’s own total reward. In contrast, a reward-giver’s incentive function in our work is learned on the same timescale as policy learning and is given to, and maximized by, other agents. Empirical research shows that augmenting an agent’s action space with a “give-reward” action can improve cooperation during certain training phases in ISDs [27].  Learning to incentivize is a form of opponent shaping, whereby an agent learns to influence the learning update of other agents for its own benefit. While LOLA [12] and SOS [25] exert influence via actions taken by its policy, whose effects manifest through the Markov game state transition, our proposed agent exerts direct influence via an incentive function, which is distinct from its policy and which explicitly affects the recipient agent’s learning update. Hence the need to influence other agents does not restrict a reward-giver’s policy, potentially allowing for more flexible and stable shaping. We describe the mathematical differences between our method and LOLA in Section 3.1, and experimentally compare with LOLA agents augmented with reward-giving actions.  Our work is related to a growing collection of work on modifying or learning a reward function that is in turn maximized by another learning algorithm [5, 34, 44]. Previous work investigate the evolution of the prisoner’s dilemma payoff matrix when altered by a “mutant” player who gives a fixed incentive for opponent cooperation [2]; employ a centralized operator on utilities in 2-player games with side payments [34]; and directly optimize collective performance by centralized rewarding in 2-player matrix games [5]. In contrast, we work with N -player Markov games with self-interested agents who must individually learn to incentivize other agents and cannot optimize collective performance directly. Our technical approach is inspired by online cross validation [36], which is used to optimize hyperparameters in meta-gradient RL [43], and by the optimal reward framework [33], in which a single agent learns an intrinsic reward by ascending the gradient of its own extrinsic objective [44].

## A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning
4. Related Work  The standard approach for addressing non-stationarity in  MARL is to consider information about the other agents and  reason about the effects of their joint actions (HernandezLeal et al., 2017). The literature on opponent modeling,  for instance, infers opponents’ behaviors and conditions an  agent’s policy on the inferred behaviors of others (He et al.,  2016; Raileanu et al., 2018; Grover et al., 2018). Studies  regarding the centralized training with decentralized execution framework (Lowe et al., 2017; Foerster et al., 2018b;  Yang et al., 2018; Wen et al., 2019), which accounts for the  behaviors of others through a centralized critic, can also be  classified into this category. While this body of work alleviates non-stationarity, it is generally assumed that each agent  will have a stationary policy in the future. Because other  agents can have different behaviors in the future as a result  of learning (Foerster et al., 2018a), this incorrect assumption  can cause sample inefficient and improper adaptation. In  contrast, Meta-MAPG models each agent’s learning process,  allowing a meta-learning agent to adapt efficiently.  Our approach is also related to prior work that considers  the learning of other agents in the environment. This includes Zhang & Lesser (2010) who attempted to discover  the best response adaptation to the anticipated future policy  of other agents. Our work is also related, as discussed previously, to LOLA (Foerster et al., 2018a) and more recent  improvements (Foerster et al., 2018c). Another relevant idea  explored by Letcher et al. (2019) is to interpolate between  the frameworks of Zhang & Lesser (2010) and Foerster et al.  (2018a) in a way that guarantees convergence while influencing the opponent’s future policy. However, all of these  approaches only account for the learning processes of other  agents and fail to consider an agent’s own non-stationary  policy dynamics as in the own learning gradient discussed  in the previous section. Additionally, these papers do not  leverage meta-learning. As a result, these approaches may  require many samples to properly adapt to new agents.  Meta-learning (Schmidhuber, 1987; Bengio et al., 1992) has  recently become very popular as a method for improving  sample efficiency in the presence of changing tasks in the  deep RL literature (Wang et al., 2016a; Duan et al., 2016b;  Finn et al., 2017; Mishra et al., 2017; Nichol & Schulman,  2018). See Vilalta & Drissi (2002) and Hospedales et al.  (2020) for in-depth surveys of meta-learning. In particular, our work builds on the popular model-agnostic metalearning framework (Finn et al., 2017), where gradientbased learning is used both for conducting so called innerloop learning and to improve this learning by computing  gradients through the computational graph. When we train  our agents so that the inner loop can accommodate for a  dynamic Markov chain of other agent policies, we are leveraging an approach that has recently become popular for  supervised learning called meta-continual learning (Riemer  et al., 2019; Javed & White, 2019; Spigler, 2019; Beaulieu  et al., 2020; Caccia et al., 2020; Gupta et al., 2020). This  means that our agent trains not just to adapt to a single set of  policies during meta-training, but rather to adapt to a set of  changing policies with Markovian updates. As a result, we  avoid an issue of past work (Al-Shedivat et al., 2018) that  required the use of importance sampling during meta-testing  (see Appendix F.1 for more details).

## Learning Latent Representations to Influence Multi-Agent Interaction
2 Related Work  Opponent Modeling. Several prior works in multi-agent RL (MARL) and human-robot interaction (HRI) handle non-stationary interactions by modeling the other agents. These approaches either model intention [20, 21], assume an assignment of roles [22], exploit opponent learning dynamics [11, 23], or incorporate features of opponent behavior, which can be modeled using a recursive reasoning paradigm [24], handcrafted [9], or learned [5, 25, 26]. Since explicitly modeling and reasoning over the opponent’s intentions or policy can quickly become computationally intractable, we sidestep this recursion by learning a low-dimensional representation of the other agent’s behavior. Unlike prior works with learned models, however, we recognize that this representation can dynamically change and be influenced by the ego agent’s policy.

## Model-Free Opponent Shaping
2. Related Work  Opponent Shaping: Several methods recognise that their current actions influence the future policies of learning opponents and take advantage of this to “shape” an opponent’s policy to desirable values. Most of these works assume white-box access to an opponent’s learning algorithm and reward in order to take higher-order derivatives through an opponent’s update (Foerster et al., 2018a; Letcher et al., 2019a; Kim et al., 2021; Willi et al., 2022). Such updates are also myopic since anticipating many steps is intractable. In self-play, these methods inconsistently assume that their opponent is a naive learner. M-FOS does not assume whitebox access to an opponent’s underlying learning algorithm or reward, does not require higher-order derivatives (which are often high-variance), can shape opponents across a large number of updates, and is consistent in self-play.  Opponent Modeling: Much work in MARL has focused on the idea of opponent modeling in which an agent attempts to model some aspect of the policy of other agents in the environment. This includes explicitly modeling opponent policies (Mealing & Shapiro, 2017), modeling opponent intentions (Raileanu et al., 2018), classifying opponent strategies (Weber & Mateas, 2009; Synnaeve & Bessi`ere, 2011), and modeling an opponent’s nested beliefs (Wen et al., 2019). LILI (Xie et al., 2020) models an opponent’s high-level latent strategy from local observations with a latent dynamics model rather than explicitly modeling the opponent’s policy. Other work (Chakraborty & Stone, 2014) has also considered learning effective policies in the presence of opponents that have memory. However, these methods are not capable of actively shaping their opponents’ learning dynamics, thus they do not address the issue that we address in this paper.  Multi-Agent Meta-Learning: M-FOS is a form of multiagent meta-learning where the meta-policy is parameterized by a neural network. Existing multi-agent meta-learning methods, such as Meta-Policy Gradient (Meta-PG) (AlShedivat et al., 2018), Meta-MAPG (Kim et al., 2021), and Learning to Exploit (L2E) (Wu et al., 2021) instead parameterize the meta-policy using a method similar to that of Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017), in which they learn initial parameters and meta-learn across their own gradient updates. While this type of metalearning can adapt to any task at test time in single-agent settings (Xiong et al., 2021), in multi-agent settings, the calculated gradient may not correspond to a direction of improvement as the updates of other agents change the underlying dynamics. Rather than being restricted to a gradient update within the episode, M-FOS allows for arbitrary metapolicies that can carry out long horizon opponent shaping.

## COLA: Consistent Learning with Opponent-Learning Awareness
2. Related work  General-sum learning algorithms have been investigated from different perspectives in the reinforcement learning, game theory, and GAN literature (Schmidhuber, 1991; Barto & Mahadevan, 2003; Goodfellow et al., 2014; Racani`ere et al., 2017). Next, we will highlight a few of the approaches to the mutual opponent shaping problem.  Opponent modeling maintains an explicit belief of the opponent, allowing to reason over their strategies and compute optimal responses. Opponent modeling can be divided into different subcategories: There are classification methods, classifying the opponents into pre-defined types (Weber & Mateas, 2009; Synnaeve & Bessie`re, 2011), or policy reconstruction methods, where we explicitly predict the actions of the opponent (Mealing & Shapiro, 2017). Most closely related to opponent shaping is recursive reasoning, where methods model nested beliefs of the opponents (He & BoydGraber, 2016; Albrecht & Stone, 2017; Wen et al., 2019).  In comparison, COLA assumes that we have access to the ground-truth model of the opponent, e.g., the opponent’s payoff function, parameters, and gradients, putting COLA into the framework of differentiable games (Balduzzi et al., 2018). Various methods have been proposed, investigating the local convergence properties to different solution concepts (Mescheder et al., 2017; Mazumdar et al., 2019; Letcher et al., 2019b; Sch ̈afer & Anandkumar, 2019; Azizian et al., 2020; Sch ̈afer et al., 2020; Hutter, 2021). Most of the work in differentiable games has not focused on opponent shaping or consistency. Mescheder et al. (2017) and Mazumdar et al. (2019) focus solely on zero-sum games without shaping. To improve upon LOLA, Letcher et al. (2019b) suggested Stable Opponent Shaping (SOS), which applies ad-hoc corrections to the LOLA update, leading to theoretically guaranteed convergence to SFPs. However, despite its desirable convergence properties, SOS still does not solve the conceptual issue of inconsistent assumptions about the opponent. CGD (Sch ̈afer & Anandkumar, 2019) addresses the inconsistency issue for zero-sum games but not for general-sum games. The exact difference between CGD, LOLA and our method is addressed in Section 4.2. Model-Free Opponent Shaping (M-FOS) (Lu et al., 2022)
3. frames opponent shaping as a meta-learning problem requiring only first-order derivatives. M-FOS is consistent in that it does not make any assumption about the learning algorithm of the opponent. However, the work does not investigate consistency specifically.


## Proximal Learning With Opponent-Learning Awareness
5 Related Work  POLA-DiCE (Section 3.4) can alternatively be motivated as replacing the policy gradient update inherent in LOLA-DiCE with a proximal-style update like that of TRPO [Schulman et al., 2015a] or the adaptive KL-penalty version of PPO [Schulman et al., 2017].  There are other extensions to LOLA that address problems orthogonal to policy parameterization sensitivity. In concurrent work, Willi et al. [2022] highlight that LOLA is inconsistent in self-play; the update each player assumes the opponent makes is different from their actual update. They propose COLA, learning update rules ∆θ1 and ∆θ2 that are consistent with each other. However, their notion of consistency is gradient-based and thus not invariant to policy parameterization, whereas POLA is not consistent. SOS [Letcher et al., 2018], which combines the advantages of LOLA and LookAhead [Zhang and Lesser, 2010], is also not invariant to policy parameterization.  Al-Shedivat et al. [2018] formulate a meta-RL framework with a policy gradient theorem, and apply PPO as their optimization algorithm; this is one instance of combining proximal style operators with reinforcement learning style policy gradients for optimization with higher order gradients. However, they focus on adaptation rather than explicitly modeling and shaping opponent behaviour.  MATRL [Wen et al., 2021] sets up a metagame between the original policies and new policies calculated with independent TRPO updates for each agent, solving for a Nash equilibrium in the metagame at each step. MATRL finds a best response without actively shaping its opponents’ learning, so it is similar to LookAhead [Zhang and Lesser, 2010] (which results in unconditional defection) rather than LOLA.  M-FOS [Lu et al., 2022] sets up a metagame where each meta-step is a full environment rollout, and the meta-reward at each step is the cumulative discounted return of the rollout. They use model-free  1See Appendix B.3 for why the LOLA results cannot be directly compared with Foerster et al. [2018a] optimization methods to learn meta-policies that shape the opponent’s behaviour over meta-steps. We do not compare against it as it is concurrent work without published code at the time of writing.  Badjatiya et al. [2021] introduce status-quo loss, in which agents imagine the current situation being repeated for several steps; combined with the usual RL objective, this leads to cooperation in social dilemmas. However, their method learns to defect in the one-step memory IPD in the states DC and CD, and thus does not learn reciprocity-based cooperation.  Zhao et al. [2021] provide a detailed numerical analysis of the function approximation setting for two-player zero-sum Markov Games. Fiez et al. [2021] considers a proximal method for minimax optimization. In contrast to both, we consider the general-sum setting.

## LOQA: Learning with Opponent Q-Learning Awareness
3 RELATED WORK  3.1 OPPONENT SHAPING  Learning with opponent learning awareness (LOLA), (Foerster et al., 2018b) introduces the concept of opponent shaping, i.e. the idea of steering the other agent throughout its learning process. LOLA assumes that the opponent is a naive learner and attempts to shape it by considering one step in its optimization process. Rather than optimizing the value under the current policies at the current iteration i, V 1(θ1  i , θ2  i ), LOLA optimizes V 1(θ1  i , θ2  i +∆θ2  i ) where ∆θ2  i is a learning step of the opponent. ∆θ2  i is as a function that depends on the agent’s parameters and that is thus differentiable with  respect to θ1. Since the derivative of function V 1(θ1  i , θ2  i + ∆θ2  i ) is difficult to compute, the authors consider the surrogate value given by its first order Taylor expansion. POLA, (Zhao et al., 2022) builds an idealized version LOLA that, unlike its predecessor, is invariant to the parameterization of the policy. In a similar fashion to proximal policy optimization (PPO) (Schulman et al., 2017), each agent increases the probability of actions that increase their expected return, while trying to minimize the l2 distance between the updated policy and the old policy. This combined objective of maximizing the return and minimizing the l2 distance in policy space is equivalent to the Proximal Point method, hence the name Proximal-LOLA or POLA. Other modifications to the original LOLA algorithm attempt to resolve its shortcomings. Consistent learning with opponent learning awareness (COLA), (Willi et al., 2022) attempts to resolve the inherent inconsistency of LOLA assuming that the other agent is a naive learner instead of another LOLA agent. Stable opponent shaping (SOS), (Letcher et al., 2021) introduces an interpolation between LOLA and a more stable variant called look ahead, which has strong theoretical convergence guarantees.  3.2 META LEARNING  Other methods have been used to generate agents that have near optimal behavior in social dilemmas. First used by Al-Shedivat et al. (2018) for this setting, meta learning redefines the MARL problem as a meta-game in the space of policy parameters in an attempt to deal with the non-stationary nature of the environment. In this meta-game, the meta-state is the joint policy, the meta-reward is the return on the underlying game, and a meta-action is a change to the inner policy (i.e. the policy in the original game). Model free opponent shaping (M-FOS) (Lu et al., 2022) applies policy gradient methods to this meta-game to find a strong meta-policy. Meta-Value Learning (Cooijmans et al., 2023) applies value learning to model the long-term effects of policy changes, and uses the gradient of the value as an improvement direction.


## Meta-Value Learning: a General Framework for Learning with Learning Awareness
2.1 NAIVE LEARNING  Under naive learning, agents i simultaneously update their policies according to  x(t+1)  i = x(t)  i + α∇xi fi(x(t)) (1)  where α is a learning rate and ∇xi is the gradient with respect to player i’s parameters. We may write this in vector form as  x(t+1) = x(t) + α  ̄∇xf (x(t)),  where  ̄∇x denotes differentiation of each fi with respect to the corresponding policy xi.  Naive learning is the straightforward application of standard gradient descent which is popular when optimizing a single objective. However, the gradient derives from a local first-order model – it reflects
change in the objective due to change in each of the individual parameters, all others held equal. Simultaneous updates depart from this model, and while they are effective in the single-objective case, they fail when different elements of x optimize different objectives.  2.2 LOOKING AHEAD  A number of approaches in the literature aim to address this issue by “looking ahead”, considering not just the current parameter values x(t) but also an extrapolation based on an imagined update. They essentially replace the game f with a surrogate game f ̃ that evaluates f after an imagined naive update with learning rate α:  f ̃(x) = f (x + α  ̄∇xf (x)). (2)  Concretely, Zhang & Lesser (2010) consider a surrogate that only extrapolates opponents:  f ̃i(x) = fi(xi, x−i + α  ̄∇x−i f−i(x)). (3)  Here ∇ ̄ x−i f−i(x) stands for the naive gradients of the opponents −i. In computing the associated  updates  ̄∇xf ̃(x), the authors considered the imagined update  ̄∇xf (x) constant with respect to  x. LOLA (Foerster et al., 2018a;c) introduced the idea of differentiating through  ̄∇xf (x), thus incorporating crucial second-order information that accounts for interactions between the learners.  Unfortunately, this surrogate trades one assumption for another: while it no longer assumes opponents to stand still, it now assumes them to update according to naive learning. Foerster et al. (2018a) also proposed Higher-Order LOLA (HOLA), where HOLA0 assumes opponents are fixed, HOLA1 assumes opponents are naive learners, HOLA2 assumes opponents use LOLA, and so on. There is nevertheless always an inconsistency between the agent’s assumption and the opponent’s true learning process. Moreover, by imagining only the updates of the opponents, these approaches can be said to assume themselves to stand still. In this sense, LOLA and several of its derivatives are inconsistent.  To avoid both sources of inconsistency, we should like to use a consistent surrogate, such as  f ̃⋆(x) = f (x + α  ̄∇xf ̃⋆(x)), (4)  which assumes all players follow the naive gradient on f ̃⋆ itself. When all players follow the naive gradient on f ̃⋆, the extrapolation is consistent with the true learning process. Unfortunately, (4) is an implicit equation; it is unclear how to obtain the associated update ∇ ̄ xf ̃⋆(x).  COLA (Willi et al., 2022) solves such an implicit equation in the case where only the opponents are extrapolated, but the approach is generally applicable. In essence, they approximate the gradient  ̄∇xf ̃⋆(x) by a model gˆ(x; θ). The model is trained to satisfy  gˆ(x; θ) =  ̄∇xf (x + αgˆ(x; θ)) (5)  by minimizing the squared error between both sides. When the equation is tight, gˆ(x; θ) =  ̄∇xf ̃⋆(x), providing access to the gradient of the consistent surrogate (4).  2.3 GOING META  Several approaches consider optimization as a meta-game. In its simplest form, the meta-game is a repeated game that has f as its stage game. Thus the meta-state space is that of joint policies x ∈ RP ×N , and each player’s meta-action x′  i ∈ RN is the policy they wish to play next. Meta-actions  x′  i are chosen according to meta-policies πi, resulting in joint meta-action x′ ∼ π( · | x) according to  joint meta-policy π(x′ | x) = Q  i πi(x′  i | x), and joint meta-rewards f (x′) given by the stage game. The expected meta-return from a given meta-state x(t) is given by the meta-value  Vπ  i (x(t)) = E  x(>t) ∼π  ∞  X  τ =t  γτ−tfi(x(τ)) = fi(x(t)) + γ E  x(t+1) ∼π  Vπ  i (x(t+1)). (6)  In this context, gradient methods like naive learning and LOLA can be seen as deterministic metapolicies, although they become stochastic when policy gradients are involved, and non-Markov when path-dependent information like momentum is used.
Meta-PG (Al-Shedivat et al., 2018) was the first to consider such a meta-game, applying policy gradients to find initializations xi that maximize V π  i , with π assumed to be naive learning on f . Meta-MAPG (Kim et al., 2021) tailor Meta-PG to multi-agent learning, taking the learning process of other agents into account. However, Meta-MAPG (like Meta-PG) assumes all agents use naive learning, and hence is inconsistent like LOLA.  M-FOS (Lu et al., 2022) considers a partially observable meta-game, thus allowing for the scenario in which opponent policies are not directly observable. M-FOS trains parametric meta-policies πi(· · · ; θi) to maximize V π  i using policy gradients. However, the move to arbitrary meta-policies, away from gradient methods, discards the gradual dynamics that are characteristic of learning. As such, M-FOS does not learn to learn with learning awareness so much as learn to act with learning awareness. Indeed, M-FOS uses arbitrarily fast policy changes to derail naive and LOLA learners.

## Scaling Opponent Shaping to High Dimensional Games

6 RELATED WORK  Opponent Shaping methods explicitly account for their opponent’s learning. Just like Shaper, these approaches recognise that the actions of any one agent influence their co-players policy and  seek to use this mechanism to their advantage [12, 14, 20, 27, 44, 47]. However, in contrast to Shaper, these approaches require privileged information to shape their opponents. These models are also myopic since anticipating many steps is intractable due to the difficulty of estimating higher-order gradients. Balaguer et al. [4] and Lu et al. [30] solve the issues above by framing opponent shaping as a meta reinforcement learning problem, which allows them to account for long-term shaping, where there is no need for higherorder gradients.  Algorithms for Social Dilemmas often achieve desirable outcomes in high-dimensional social dilemmas yet assume access to hand-crafted notions of adherence [46], social influence [3, 19], gifting [32] or social conventions [22]. While these approaches can achieve desirable outcomes, they change the agent’s objectives and alter the dynamics of the underlying game. Multi-Agent Meta-Learning methods have also shown success in general-sum games with other learners [1, 21, 45]. Similar to Shaper, they take inspiration from meta-RL - their approach is to learn the optimal initial parameterisation for the meta-agent akin to Model-Agnostic Meta Learning [11]. In contrast, Shaper uses an approach similar to RL2 [9], which trains an RNN-based agent to implement efficient learning for its next task. Finally, Shaper is optimised using ES, which empirically performs better with longtime horizons than policy-gradient methods [29–31].

## Best Response Shaping
3 Related Work  LOLA Foerster et al. (2018) attempts to shape the opponent by taking the gradient of the value with respect to a one-step look ahead of the opponent’s parameters. Instead of considering the expected return under the current policy parameter pair, V 1(θ1  i , θ2  i ), LOLA optimizes V 1(θ1  i , θ2  i + ∆θ2  i)  where ∆θ2  i denotes a naive learning step of the opponent. To make a gradient calculation of the  update ∆θ2  i , LOLA considers the surrogate value given by the first order Taylor approximation of  V 1(θ1  i , θ2  i +∆θ2  i ). Since for most games the exact value cannot be calculated analytically, the authors introduce a policy gradient formulation that relies on environment roll-outs to approximate it. This method is able to find tit-for-tat strategies on the Iterated Prisoner’s Dilemma.  POLA Zhao et al. (2022) introduces an idealized version of LOLA that is invariant to policy parameterization. To do so, each player attempts to increase the probability of actions that lead to higher returns while penalizing the Kullback-Leibler divergence in policy space relative to their policies at the previous time step. Similar to the proximal point method, each step of POLA constitutes an optimization problem that is solved approximately through gradient descent. Like LOLA, POLA uses trajectory roll-outs to estimate the value of each player and applies the reinforce estimator to compute gradients. POLA effectively achieves non exploitable cooperation on the IPD and the Coin Game improving on the shortcomings of its predecessor.  Lu et al. (2022) considers a meta-game where at each meta-step a full game is played and the metareward is the return of that game. The agent is then a meta-policy that learns to influence the opponent’s behaviour over these rollouts. M-FOS changes the game and is not comparable to our method which considers learning a single policy. Baker (2020) changes the structure of the game where each agent is sharing reward with other agents. The agents are aware of this grouping of rewards via a noisy version of the reward sharing matrix. In the test time, the representation matrix is set to no reward sharing and no noise is added to this matrix.  Stackelberg Games Colman & Stirk (1998) revolve around a leader’s initial action selection followed by a follower’s subsequent move. The Bi-Level Actor-Critic(Bi-AC) Zhang et al. (2020) framework introduces an innovative approach for training both leader and follower simultaneously during the training period while maintaining independent executability, making it well-suited for addressing coordination challenges in MARL. In contrast to our setup, where the detective functions as a training harness discarded post-training, the Bi-AC varies by deploying both leader and follower jointly during test time (as the main concern is coordination between the leader and the follower). The interactions between the agent and the detective mirror the foundational Stackelberg setup, casting the agent as the leader and the detective as the follower.  Good Shepherd Balaguer et al. (2022) trains a best response to a learning agent, mirroring the best response to the best response idea. The authors offer two methods for training against this optimal response. First, by creating an expansive computational graph for the agent’s optimization. Second, employing evolutionary strategies. Neither of these methods is scalable. Constructing a full optimization computational graph for every agent’s optimization step is very inefficient. Moreover, evolutionary strategies require training the opponent against new data points each time. Our approach circumvents this problem by using a neural network to amortize the optimization process. PSRO Lanctot et al. (2017) unifies many MARL training frameworks like Independent RL, Iterated Best Response, and Fictitious Self-Play. PSRO-family methods iteratively extend a set of past policies, by adding the best response to a mixture of those past policies. In contrast to BRS, PSRO does not differentiate through the best response

## Advantage Alignment Algorithms
6 RELATED WORK  The Iterated Prisoner’s Dilemma (IPD) was introduced by Rapoport and Chammah (1965). Tit-fortat was discovered as a robust strategy against a population of opponents in IPD by Axelrod (1984), who organized multiple IPD tournaments. It was discovered only recently that IPD contains strategies that extort rational opponents into exploitable cooperation (Press and Dyson, 2012). Sandholm and Crites (1996) were the first to demonstrate that two Q-learning agents playing IPD converge to mutual defection, which is suboptimal. Later, Foerster et al. (2018b) demonstrated that the same is true for policy gradient methods. Bertrand et al. (2023) were able to show that with optimistic initialization and self-play, Q-learning agents find a Pavlov strategy in IPD.  Opponent shaping was first introduced in LOLA Foerster et al. (2018b), as a method for controlling the learning dynamics of opponents in a game. A LOLA agent assumes the opponents are naive learners and differentiates through a one step look-ahead optimization update of the opponent. More formally, LOLA maximizes V 1(θ1, θ2 + ∆θ2) where ∆θ2 is a naive learning step in the direction that maximizes the opponent’s value function V 2(θ1, θ2). Variations of LOLA have been introduced to have formal stability guarantees (Letcher et al., 2021), learn consistent update functions assuming mutual opponent shaping (Willi et al., 2022) and be invariant to policy parameterization (Zhao et al., 2022). More recent work performs opponent shaping by having an agent play against a best response approximation of their policy (Aghajohari et al., 2024a). LOQA (Aghajohari et al., 2024b), on which this work is based, performs opponent shaping by controlling the Q-values of the opponent using REINFORCE (Williams, 1992) estimators.  Another approach to finding socially beneficial equilibria in general sum games relies on modeling the problem as a meta-game, where meta-rewards correspond to the returns on the inner game, metastates correspond to joint policies of the players, and the meta-actions are updates to these policies. Al-Shedivat et al. (2018) introduce a continuous adaptation framework for multi-task learning that uses meta-learning to deal with non-stationary environments. MFOS (Lu et al., 2022) uses modelfree optimization methods like PPO and genetic algorithms to optimize the meta-value of the metagame. More recently Meta-Value Learning (Cooijmans et al., 2023) parameterizes the meta-value as a neural network and applies Q-learning to capture the future effects of changes to the inner policies. Shaper (Khan et al., 2024), scales opponent shaping to high-dimensional general-sum games with temporally extended actions and long time horizons. It does so, by simplifying MFOS and effectively capturing both intra-episode and inter-episode information.  Melting Pot 2.0 (Agapiou et al., 2023) introduces a comprehensive suite of multi-agent reinforcement learning environments that focus on social interactions and coordination challenges, providing a valuable benchmark for evaluating the scalability and effectiveness of reinforcement learning algorithms in complex, cooperative-competitive settings. The Negotiation Game, introduced by DeVault et al. (2015); Lewis et al. (2017) and subsequently refined by Cao et al. (2018), has proven to be a significant benchmark for studying general-sum games. It integrates elements of strategy and social dilemmas, necessitating that agents balance cooperation and competition to optimize their outcomes. Noukhovitch et al. (2021) analyze this complex benchmark, underscoring its importance in the field. Future investigations will turn towards an even more sophisticated simulation proposed by Zhang et al. (2022), which involves negotiations among countries and regions with diverse resource distributions and preferences in addressing climate change.
## Multi-agent cooperation through learning-aware policy gradients
2.2. Co-player learning awareness  We aim to address the above two major challenges of multi-agent learning in this paper. Our work builds upon recent efforts that are based on adding a meta level to the multi-agent POSG, where the higher-order variable represents the learning algorithm used by each agent (Balaguer et al., 2022; Khan et al., 2024; Lu et al., 2022). In this meta-problem, the environment includes the learning dynamics of other agents. At the meta-level, one episode now extends across multiple episodes of actual game play, allowing the ‘ego agent’, i, to observe how its co-players, −i, learn, see Fig. 1. The goal of this meta-agent may be intuitively understood as that of shaping co-player learning to its own advantage.  Provided that co-player learning algorithms remain constant, the above reformulation yields a singleagent problem that is amenable to standard reinforcement learning techniques. This setup is fundamentally asymmetric: while the meta agent (ego agent) is endowed with co-player learning awareness (i.e., observing multiple episodes of game play), the remaining agents remain oblivious to the fact that the environment is non-stationary. We thus refer to them here as naive agents (see Fig. 1B). Despite this asymmetry, prior work has observed that introducing a learning-aware agent in a group of naive learners often leads to better learning outcomes for all agents involved, avoiding mutual defection equilibria (Balaguer et al., 2022; Khan et al., 2024; Lu et al., 2022). Moreover, Foerster et al. (2018a) has shown that certain forms of learning awareness can lead to the emergence of cooperation even in symmetric cases, a surprising finding that is not yet well understood.  These observations motivate our study, leading us to derive novel efficient learning-aware reinforcement learning algorithms, and to investigate their efficacy in driving a group of agents (possibly composed of both meta and naive agents) towards more beneficial equilibria. Below, we proceed by first formalizing asymmetric co-player shaping problems, which we solve with a novel policy gradient algorithm (Section 3). In Section 4, we then return to the question of why and when co-player learning awareness can result in cooperation in multi-agent systems with equally capable agents. Co-player shaping. Following Lu et al. (2022), we first introduce a meta-game with a single meta-agent whose goal is to shape the learning of naive co-players to its advantage. This metagame is defined formally as a single-agent partially observable Markov decision process (POMDP) (S ̃, A ̃, P ̃t, P ̃r, P ̃i, O ̃, γ ̃, M). The meta-state consists of the policy parameters φ−i of all co-players together with the agent’s own parameters φi. The meta-environment dynamics represent the fixed learning rules of the co-players, and the meta-reward distribution represents the expected return Ji (φi, φ−i) collected by agent i during an inner episode, with ‘inner’ referring to the actual game being played. The initialization distribution P ̃i reflects the policy initializations of all players. Finally, we introduce a meta-policy μ(φi  m+1 | φi  m, φ−i  m ; θ) parameterized by θ, that decides the update to the parameter φi  m+1  (the meta-action) to shape the co-player learning towards highly rewarding regions for agent i over a horizon of M meta steps. This leads to the co-player shaping problem



## 对手建模与对手塑形（Opponent Modeling / Shaping）

* 基础范式与可微工具

  * 通过“显式考虑对手的学习更新”来影响其未来策略；为此需要能对期望回报在“对手一次或多次学习步”上求导

    * [LOLA](https://arxiv.org/abs/1709.04326) ; Foerster et al., 2018

      * 改进/扩展：用通用的高阶梯度估计器稳定实现“对手学习可微化”

        * [DiCE](https://arxiv.org/abs/1802.05098) ; Foerster et al., 2018

* 稳定性与一致性（避免“自作聪明”的不收敛、假设不一致等）

  * 在对手塑形的同时，保证局部收敛/不落入鞍点，并消除“我假设对手是天真学习者”的不一致

    * [Stable Opponent Shaping (SOS)](https://arxiv.org/abs/1811.08469) ; Letcher et al., 2019

      * 改进/扩展：两方都做塑形时学到**一致**的更新规则

        * [COLA](https://arxiv.org/abs/2209.07125) ; Willi et al., 2022
      * 改进/扩展：用**近端**约束替代原始一阶近似，使更新对参数化不敏感

        * [POLA](https://openreview.net/forum?id=sq3jtWc2O1n) ; Zhao et al., 2022

* 直接激励（显式给予“奖励/惩罚”信号来塑形对手）

  * 将“激励函数”与策略解耦，奖励由施加者学习并直接进入受体的回报，避免仅靠环境动态间接影响

    * [LIO](https://openreview.net/forum?id=j9kqa82Yqfm) ; Yang et al., 2020

* 元学习/无模型塑形（长视角、多步塑形，无需白盒对手）

  * 把“与会学习的对手交互”提升为**元博弈**：一次元步=一整局游戏，学会跨多局逐步塑形

    * [M-FOS](https://openreview.net/forum?id=naY7Qqg8mO) ; Lu et al., 2022

      * 改进/扩展：在高维、长时序任务中扩展可塑形能力（结合ES/RNN样式的跨局信息整合）

        * [Shaper](https://arxiv.org/abs/2402.01068) ; Khan et al., 2024
      * 改进/扩展：把“会学习的对手”正式化为**元POMDP**并给出高效PG算法

        * [Multi-agent Cooperation through Learning-Aware Policy Gradients](https://arxiv.org/abs/2406.04378) ; Bertrand et al., 2024
      * 改进/扩展：以**元价值函数**显式建模“当前更新的长远效应”，从而给出方向

        * [Meta-Value Learning: A General Framework for Learning with Learning Awareness](https://arxiv.org/abs/2306.02338) ; Cooijmans et al., 2023

* 最优响应视角与可扩展替代

  * 通过**近似/摊销的最优响应**来塑形，避免高阶梯度和白盒假设

    * [Best Response Shaping](https://arxiv.org/abs/2404.06519) ; Aghajohari et al., 2024

* 面向对手的值函数/算法特化

  * 直接**控制对手的Q值或学习信号**来塑形，而非只对环境动作施加影响

    * [LOQA: Learning with Opponent Q-Learning Awareness](https://arxiv.org/abs/2406.02920) ; Aghajohari et al., 2024

* 表征驱动的对手影响

  * 学习低维**对手行为表征**并通过自身策略**可控地影响**该表征的演化，绕开深层递归推理的计算爆炸

    * [Learning Latent Representations to Influence Multi-Agent Interaction](https://proceedings.mlr.press/v164/xie22a.html) ; Xie et al., 2021

* 关系小结（如何从“建模对手”走向“塑形对手”）

  * **LOLA**开启“对手一步更新可微化”的范式 → **DiCE**提供通用高阶梯度工具
  * **SOS/LookAhead**保证可收敛性；**COLA**让双方塑形的更新**互相一致**；**POLA**引入**近端约束**降低参数化敏感性
  * **LIO**把影响从“通过环境间接作用”升级为“直接奖励转移”，与LOLA/SOS互补
  * **M-FOS/Shaper/学习感知PG**把塑形扩展到**多局、长视角、黑盒对手**；**Meta-Value**进一步显式估计**长程影响**
  * **BRS/LOQA**走“最优响应/值函数通道”，在**无需白盒**前提下降低高阶梯度与多步展望的复杂度
  * **表征影响**路线则以**可控潜变量**承载对手行为，兼顾可解释与计算效率
