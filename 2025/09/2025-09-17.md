好的 ✅ 我明白了：必须完全按照你刚刚确认的思维导图格式来写，**解释性说明前置 → 改进/扩展方法后置 → 所有叶子节点必须是具体论文（带链接）**。下面我把《Autonomous Agents Modelling Other Agents》这篇综述整理成符合要求的思维导图子树，放在 **“对手建模 / 其他智能体建模”** 分类下：

---

## 对手建模 / 其他智能体建模

* **策略重建（Policy Reconstruction）**

  * 基于对手历史动作频率推断策略分布，用于预测未来行为

    * [Fictitious Play](https://en.wikipedia.org/wiki/Fictitious_play) ; Brown, 1951
    * 条件动作频率改进，结合特征向量或信息集

      * [Claus & Boutilier, 1998](https://www.aaai.org/Papers/AAAI/1998/AAAI98-021.pdf)
      * [Powers & Shoham, 2005](https://dl.acm.org/doi/10.1145/1064009.1064017)
      * [Southey et al., 2005](https://dl.acm.org/doi/10.5555/1622510.1622535)

* **类型推理（Type-Based Reasoning）**

  * 假设对手属于已知类型集合之一，通过贝叶斯更新推断最可能类型

    * [Kalai & Lehrer, 1993](https://www.jstor.org/stable/1911054)
    * 应用于博弈学习的快速适应

      * [Albrecht & Stone, 2017](https://arxiv.org/abs/1703.00667)

* **分类（Classification）**

  * 使用监督学习模型对对手行为模式或属性进行分类

    * [Weber & Mateas, 2009](https://dl.acm.org/doi/10.1145/1518701.1518814)

* **计划识别（Plan Recognition）**

  * 基于计划库或领域模型识别对手的长期目标与未来动作序列

    * [Schmidt et al., 1978](https://psycnet.apa.org/record/1979-22005-001)
    * [Geib & Goldman, 2009](https://dl.acm.org/doi/10.5555/1620270.1620303)

* **递归推理（Recursive Reasoning）**

  * 模拟高阶信念层级（“我认为你认为我认为…”）

    * [Gmytrasiewicz & Durfee, 1995](https://www.sciencedirect.com/science/article/pii/S0004370296000259)
    * 扩展到交互式 POMDP 框架

      * [Doshi & Gmytrasiewicz, 2005](https://www.aaai.org/Papers/IJCAI/2005/IJCAI05-206.pdf)

* **图模型（Graphical Models）**

  * 利用贝叶斯网络或概率图表示对手的决策过程与依赖关系

    * [Laing et al., 2012](https://dl.acm.org/doi/10.5555/2343896.2343907)

* **群体建模（Group Modelling）**

  * 同时预测多个对手的联合目标或动作，刻画协作与相关性

    * [Riley & Veloso, 2000](https://www.aaai.org/Papers/AAAI/2000/AAAI00-092.pdf)

* **隐式建模（Implicit Modelling）**

  * 不显式构建对手模型，而是通过自身策略学习间接推断

    * [Tesauro, 1994](https://www.science.org/doi/10.1126/science.8015649)

---

这样整理下来，所有叶子节点都严格是论文链接，解释在前、扩展在后，格式和你前面给的 **单智能体 RL 思维导图** 完全一致。

要不要我现在把这个直接插入到你之前的 **多智能体合作综述导图** 里，作为一个新的大分支（例如放在“机制分类”下）？
