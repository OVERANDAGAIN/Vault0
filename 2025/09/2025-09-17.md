## 单智能体
- 基于价值的方法
    - 动机：在高维/连续状态下逼近Q值函数
        - [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013
            - 改进方向：提升稳定性
                - [Double DQN](https://arxiv.org/abs/1509.06461) ; van Hasselt et al., 2015
                - [Dueling DQN](https://arxiv.org/abs/1511.06581) ; Wang et al., 2016
            - 改进方向：提升数据效率
                - [Prioritized Replay](https://arxiv.org/abs/1511.05952) ; Schaul et al., 2015
- 基于策略的方法
    - 动机：直接学习参数化策略而非值函数
        - [REINFORCE](https://link.springer.com/article/10.1007/BF00992696) ; Williams, 1992
            - 改进方向：降低方差、提升收敛性
                - [Actor–Critic](https://papers.nips.cc/paper_files/paper/2000/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html) ; Konda & Tsitsiklis, 2000
            - 改进方向：稳定更新过程
                - [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015
                - [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017
- 连续动作控制
    - 动机：处理无限动作空间、提升采样效率
        - [DDPG](https://arxiv.org/abs/1509.02971) ; Lillicrap et al., 2015
            - 改进方向：提升稳定性与鲁棒性
                - [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018
- 模仿学习
    - 动机：奖励信号缺失或难以设计
        - [Behavior Cloning](https://dl.acm.org/doi/10.1145/97243.97245) ; Pomerleau, 1989
        - [GAIL](https://arxiv.org/abs/1606.03476) ; Ho & Ermon, 2016
- 基于模型的方法
    - 动机：通过环境模型降低真实交互需求
        - [MPC](https://www.sciencedirect.com/science/article/pii/S0005109804002766) ; Mayne, 2005
        - [PETS](https://arxiv.org/abs/1805.12114) ; Chua et al., 2018
        - [MBPO](https://arxiv.org/abs/1906.08253) ; Janner et al., 2019
- 离线强化学习
    - 动机：无交互或危险场景下利用固定数据
        - [BCQ](https://arxiv.org/abs/1812.02900) ; Fujimoto et al., 2019
        - [CQL](https://arxiv.org/abs/2006.04779) ; Kumar et al., 2020
        - [IQL](https://arxiv.org/abs/2110.06169) ; Kostrikov et al., 2021
- 目标条件与泛化
    - 动机：解决多任务、复合目标的泛化能力
        - [UVFA](https://arxiv.org/abs/1802.09464) ; Schaul et al., 2015
        - [HER](https://arxiv.org/abs/1707.01495) ; Andrychowicz et al., 2017
        - [Meta-RL (MAML)](https://arxiv.org/abs/1703.03400) ; Finn et al., 2017








很好，你贴出的这部分其实就是这篇综述论文（Du et al. 2023）的 **背景章节 (Section 2)**。如果我们要把它转化成思维导图格式，可以做得比原文更结构化：上层节点是 **理论框架（MDP, POMDP, Markov Games, POMG）**，中层节点是 **学习范式（value-based, policy-based, actor-critic）**，再往下挂上**代表性方法+论文**。这样和我们之前做的“方法综述型导图”能够很好结合。

我给你一个整理好的版本（论文导向、层次清晰）：

---

# 背景：单智能体与多智能体强化学习

## 单智能体强化学习

### 理论框架

#### [MDP](https://link.springer.com/article/10.1007/BF00992696) ; Sutton, 1988



#### [POMDP](https://www.sciencedirect.com/science/article/pii/S000437029800023X) ; Cassandra, 1998



### 价值函数方法

#### [Q-Learning](https://link.springer.com/article/10.1007/BF00992698) ; Watkins & Dayan, 1992

* 

#### [SARSA](https://dl.acm.org/doi/10.5555/645529.657617) ; Singh et al., 2000


#### [Deep Q-Network (DQN)](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013/2015

* 人类水平 Atari 表现

### 策略优化方法

#### [Policy Gradient / REINFORCE](https://dl.acm.org/doi/10.1145/138243.138273) ; Williams, 1992

* 直接优化参数化策略
* 高方差，需要改进

#### [Actor–Critic](https://papers.nips.cc/paper/1786-convergence-properties-of-policy-iteration) ; Konda & Tsitsiklis, 1999

* 策略+价值并行学习，降低方差

#### [Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) ; Silver et al., 2014


#### [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015

* 信任域保证策略更新单调改进

#### [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017



#### [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018

* 

---

## 多智能体学习

### 理论框架

#### [Markov Game](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994

* 扩展 MDP 到多主体交互
* 包含 Cooperative / Competitive / Mixed

#### \[Partially Observable Markov Game (POMG)]

* 多智能体部分可观测场景

### 设置分类

#### Cooperative

* [Team MDP](https://link.springer.com/chapter/10.1007/3-540-61380-2_18) ; Boutilier, 1996
* [Independent Q-learning](https://link.springer.com/chapter/10.1007/3-540-45545-0_14) ; Lauer & Riedmiller, 2000

#### Competitive（零和）

* [Minimax-Q](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
* [AlphaGo Zero](https://www.nature.com/articles/nature24270) ; Silver et al., 2016
* [OpenAI Five](https://arxiv.org/abs/1912.06680) ; OpenAI, 2019

#### Mixed-motive（一般和）

* [Nash Q-learning](https://dl.acm.org/doi/10.1145/502512.502549) ; Hu & Wellman, 2003
* [Policy Hill-Climbing](https://www.sciencedirect.com/science/article/pii/S0004370201001250) ; Littman, 2001
* 团队内部协作 + 团队间对抗（如 AlphaStar, Jaderberg et al., 2019）

---

📌 这样组织的好处：

* **背景理论**（MDP, POMDP, Markov Game, POMG）作为“根节点”
* **学习范式**（value-based, policy-based, actor-critic）对齐单智能体
* **设置分类**（cooperative, competitive, mixed）对齐多智能体
* 所有叶子都落到具体论文（可直接查阅）。

---

要不要我把这一份“背景”直接和之前整理的 **单智能体算法树**、**多智能体合作综述树**拼在一起，形成一份“大一统导图”？这样就有：

1. 背景理论框架
2. 单智能体方法（经典+深度+前沿）
3. 多智能体方法（合作综述）
