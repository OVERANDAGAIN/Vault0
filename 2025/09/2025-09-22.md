---
创建时间: 2025-九月-22日  星期一, 5:27:46 下午
---
Contents：
1. 针对hop+训练效果不上升的现象，做出了以下分析：
	1. 可能是框架问题
	2. 可能是算法问题
2. 针对1.1猜想，实现了仅PPO的baseline，在同样的设置下进行实验，发现可以达成合作，总奖励10.8左右。但是一个现象是：四个智能体中，第一个智能体表现出明显更好的学习效果，而其余三个智能体训练结果较差且相似，反映出 个体间性能差异与群体内一致性 并存的现象。
3. 针对1.2猜想，进行以下实验：
	1. 测试VAE效果
	2. 测试world_model效果
	3. moa的ce_loss（交叉熵损失）随着训练上升到30，是不正常的
4. 测试3.1预训练VAE效果：其中VAE的训练包括 state——goal 的 KL_loss 和 moa 的loss。经过测试，离线moa对数据的预测准确率达到0.6-0.7，较为高，但是数据中大多数动作是静止动作。所以离线数据的分布可以优化
5. 测试3.2world_model的效果：loss最终降到0.07 对下一步的reward和obs预测大致正确。但是其中obs有部分实体的位置不完全对应，可能会有所影响
6. 测试3.3moa的loss，经过分析，moa的buffer采集数据中包含了一些对手退出时的动作，而这部分对于对手动作的预测是不利的，剔除这部分动作后，moa的ce_loss来到了1.4左右，最终moa的top-1准确率来到了0.25，top-3准确率来到了0.75。预测捕兔动作正确率0.7左右，预测补鹿正确率0.25左右。（但不平稳，会大幅度震荡）
7. PPO的loss方面，entropy这一项逐渐下降（1 --> 0.6/0)说明策略探索逐渐减少。最终策略没达到捕鹿





---

### 1. 问题背景

针对 HOP+ 训练效果未能有效提升 的现象，主要提出了两方面可能原因：

1. 框架问题
2. 算法问题

### 2. 基线实验（对应框架问题）

为验证是否是框架导致问题，实验实现了仅基于 PPO 的 baseline。在相同设置下，PPO 能够成功达成合作，总奖励约 10.8。

1. 现象：在四个智能体中，第一个智能体的学习效果显著优于其余三个，而另外三个智能体的训练结果较差且高度相似，体现出 个体间性能差异与群体内一致性并存 的现象。

### 3. 模块实验（对应算法问题）

为验证算法设计问题，对 HOP+ 的核心模块进行了独立测试：

#### 3.1 VAE 模块

1. 训练目标：包含 state → goal 的 KL\_loss 和 moa 的 loss。
2. 结果：离线 MOA 在数据上的预测准确率可达 0.6–0.7，但由于数据中大部分是“静止动作”，分布存在偏差，仍需优化。

#### 3.2 World Model 模块

1. Loss 最终降至 0.07，能够较好预测下一步 reward 与 obs。
2. 问题：obs 中部分实体的位置预测不完全对应，可能影响整体性能。

#### 3.3 MOA 模块

1. 现象：在初始实验中，MOA 的 ce\_loss 随训练上升至 30，显然不合理。
2. 原因：buffer 采样中包含了对手退出时的动作，削弱了预测效果。
3. 改进：剔除这部分动作后，ce\_loss 降至 1.4 左右。
4. 性能：
	1. Top-1 准确率约 0.25
	2. Top-3 准确率约 0.75
	3. 捕兔动作预测准确率约 0.7
	4. 捕鹿动作预测准确率约 0.25
5. 问题：准确率曲线不平稳，存在较大震荡。

### 4. PPO Loss 分析

1. Entropy 项逐渐下降（从 1 → 0.6/0），表明策略探索逐步减少。
2. 最终策略仍未能学会有效的捕鹿行为。


好的 👍 我帮你把 **To-Do List** 梳理出来，基于你目前周报里的问题定位和分析：

---

## To-Do List（下阶段计划）

1. 个体差异问题

   * 分析为什么第一个 agent 学习效果显著优于其余三个：

     * 检查初始化参数和随机种子对训练曲线的影响
     * 对比不同 agent 的 buffer 分布（动作/奖励/子目标）
     * 测试在对称初始化和共享经验下，是否仍存在差异

1. MOA 模块优化

   * 更严格地剔除对手退出后的无效数据
   * 平衡动作分布（对“静止动作”下采样或重加权）
   * 稳定 MOA 训练过程，减少 loss 的大幅度震荡（例如：调低学习率/加入正则化/使用 Polyak EMA）
   * 设计评估指标：跟踪 Top-k 准确率随训练进程的变化

1. World Model 精度提升

   * 分析 obs 预测中“实体位置错位”的情况
   * 尝试更强的表示学习（例如加入 attention 或 residual block）
   * 引入 multi-step rollout 评估，检测模型长时间预测的稳定性

1. VAE & Subgoal 学习

   * 检查 state→goal 的 KL\_loss 曲线，确保收敛且稳定
   * 对比不同 subgoal 表征方式（连续 vs 离散）对规划结果的影响
   * 建立 VAE 输出与实际捕鹿/捕兔动作之间的相关性分析

1. 整体框架验证

   * 在剔除无效数据、优化 MOA 后，重新跑 HOP+ 自博弈，看是否能突破捕鹿瓶颈
   * 对比 “PPO baseline vs HOP+” 的收敛曲线，量化增益或退化
   * 考虑迁移到 CleanRL + Gymnasium 框架下，减少框架层面的不确定性

---

这样列出后，每一步都有明确的任务点和方向。

要不要我帮你把 To-Do List 分成 **短期（1周内能做的）和中期（需要持续跟进的）** 两类？这样更方便安排优先级。
