---
创建时间: 2025-九月-22日  星期一, 5:27:46 下午
---
Contents：
1. 针对hop+训练效果不上升的现象，做出了以下分析：
	1. 可能是框架问题
	2. 可能是算法问题
2. 针对1.1猜想，实现了仅PPO的baseline，在同样的设置下进行实验，发现可以达成合作，总奖励10.8左右。但是一个现象是：四个智能体中，第一个智能体表现出明显更好的学习效果，而其余三个智能体训练结果较差且相似，反映出 个体间性能差异与群体内一致性 并存的现象。
3. 针对1.2猜想，进行以下实验：
	1. 测试VAE效果
	2. 测试world_model效果
	3. moa的ce_loss（交叉熵损失）随着训练上升到30，是不正常的
4. 测试3.1预训练VAE效果：其中VAE的训练包括 state——goal 的 KL_loss 和 moa 的loss。经过测试，离线moa对数据的预测准确率达到0.6-0.7，较为高，但是数据中大多数动作是静止动作。所以离线数据的分布可以优化
5. 测试3.2world_model的效果：loss最终降到0.07 对下一步的reward和obs预测大致正确。但是其中obs有部分实体的位置不完全对应，可能会有所影响
6. 测试3.3moa的loss，经过分析，moa的buffer采集数据中包含了一些对手退出时的动作，而这部分对于对手动作的预测是不利的，剔除这部分动作后，moa的ce_loss来到了1.4左右，最终moa的top-1准确率来到了0.25，top-3准确率来到了0.75。预测捕兔动作正确率0.7左右，预测补鹿正确率0.25左右。（但不平稳，会大幅度震荡）
7. PPO的loss方面，entropy这一项逐渐下降（1 --> 0.6/0)说明策略探索逐渐减少。最终策略没达到捕鹿





---

### 1. 问题背景

针对 HOP+ 训练效果未能有效提升 的现象，主要提出了两方面可能原因：

1. 框架问题
2. 算法问题

### 2. 基线实验（对应框架问题）

为验证是否是框架导致问题，实验实现了仅基于 PPO 的 baseline。在相同设置下，PPO 能够成功达成合作，总奖励约 10.8。

1. 现象：在四个智能体中，第一个智能体的学习效果显著优于其余三个，而另外三个智能体的训练结果较差且高度相似，体现出 个体间性能差异与群体内一致性并存 的现象。

### 3. 模块实验（对应算法问题）

为验证算法设计问题，对 HOP+ 的核心模块进行了独立测试：

#### 3.1 VAE 模块

1. 训练目标：包含 state → goal 的 KL\_loss 和 moa 的 loss。
2. 结果：离线 MOA 在数据上的预测准确率可达 0.6–0.7，但由于数据中大部分是“静止动作”，分布存在偏差，仍需优化。

#### 3.2 World Model 模块

1. Loss 最终降至 0.07，能够较好预测下一步 reward 与 obs。
2. 问题：obs 中部分实体的位置预测不完全对应，可能影响整体性能。

#### 3.3 MOA 模块

1. 现象：在初始实验中，MOA 的 ce\_loss 随训练上升至 30，显然不合理。
2. 原因：buffer 采样中包含了对手退出时的动作，削弱了预测效果。
3. 改进：剔除这部分动作后，ce\_loss 降至 1.4 左右。
4. 性能：
	1. Top-1 准确率约 0.25
	2. Top-3 准确率约 0.75
	3. 捕兔动作预测准确率约 0.7
	4. 捕鹿动作预测准确率约 0.25
5. 问题：准确率曲线不平稳，存在较大震荡。

### 4. PPO Loss 分析

1. Entropy 项逐渐下降（从 1 → 0.6/0），表明策略探索逐步减少。
2. 最终策略仍未能学会有效的捕鹿行为。
