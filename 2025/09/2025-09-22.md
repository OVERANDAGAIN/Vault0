Contents：
1. 针对hop+训练效果不上升的现象，做出了以下分析：
	1. 可能是框架问题
	2. 可能是算法问题
2. 针对1.1猜想，实现了仅PPO的baseline，在同样的设置下进行实验，发现可以达成合作，总奖励10.8左右。但是一个现象是：四个智能体中，第一个智能体表现出明显更好的学习效果，而其余三个智能体训练结果较差且相似，反映出 个体间性能差异与群体内一致性 并存的现象。
3. 针对1.2猜想，进行以下实验：
	1. 测试VAE效果
	2. 测试world_model效果
	3. moa的ce_loss（交叉熵损失）随着训练上升到30，是不正常的
4. 测试3.1预训练VAE效果：其中VAE的训练包括 state——goal 的 KL_loss 和 moa 的loss。经过测试，离线moa对数据的预测准确率达到0.6-0.7，较为高，但是数据中大多数动作是静止动作。所以离线数据的分布可以优化
5. 测试3.2world_model的效果：loss最终降到0.07 对下一步的reward和obs预测大致正确。但是其中obs有部分实体的位置不完全对应，可能会有所影响
6. 测试3.3moa的loss，经过分析，moa的buffer采集数据中包含了一些对手退出时的动作，而这部分对于对手动作的预测是不利的，剔除这部分动作后，moa的ce_loss来到了1.4左右，最终moa的top-1准确率来到了0.25，top-3准确率来到了0.75。预测捕兔动作正确率0.7左右，预测补鹿正确率0.25左右。（但不平稳，会大幅度震荡）