当然。下面是**完整的 MDP 与 SMDP（options 框架）Bellman 方程**，同时给出 **$V$ 函数**与**$Q$ 函数**的期望形式与最优形式（全部使用 `$` 或 `$$` 包裹的公式格式）。

---

## 🧩 一、普通 MDP

### (1) Bellman 期望方程（给定策略 $\pi$）

状态价值函数：
$$
V^{\pi}(s)=\sum_{a}\pi(a\mid s)\Big[r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)V^{\pi}(s')\Big]
$$

动作价值函数：
$$
Q^{\pi}(s,a)=r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)\sum_{a'}\pi(a'\mid s')Q^{\pi}(s',a')
$$

两者关系：
$$
V^{\pi}(s)=\sum_{a}\pi(a\mid s)Q^{\pi}(s,a)
$$

---

### (2) Bellman 最优方程（Optimality）

状态价值函数：
$$
V^{*}(s)=\max_{a}\Big[r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)V^{*}(s')\Big]
$$

动作价值函数：
$$
Q^{*}(s,a)=r(s,a)+\gamma\sum_{s'}P(s'\mid s,a)\max_{a'}Q^{*}(s',a')
$$

以及：
$$
V^{*}(s)=\max_{a}Q^{*}(s,a)
$$

---

## 🧩 二、SMDP / Options 框架

设选项 $o=\langle \mathcal{I},\pi,\beta\rangle$，执行时长为随机变量 $k$。

定义：
$$
R_{s}^{o}=\mathbb{E}!\left[\sum_{i=0}^{k-1}\gamma^{i}r_{t+i}\mid s,o\right], \quad
p_{ss'}^{o}=\sum_{k\ge1}\gamma^{k}P(s',k\mid s,o)
$$
（若写为显式 $k$ 形式，则 $p_{ss'}^{o}$ 内不包含 $\gamma^{k}$。）

---

### (1) Bellman 期望方程（给定高层策略 $\mu$）

状态价值函数：
$$
V^{\mu}(s)=\sum_{o}\mu(o\mid s)\Big[r_{s}^{o}+\sum_{s'}p_{ss'}^{o}V^{\mu}(s')\Big]
$$

选项价值函数：
$$
Q^{\mu}(s,o)=r_{s}^{o}+\sum_{s'}p_{ss'}^{o}\sum_{o'}\mu(o'\mid s')Q^{\mu}(s',o')
$$

两者关系：
$$
V^{\mu}(s)=\sum_{o}\mu(o\mid s)Q^{\mu}(s,o)
$$

若使用显式 $k$ 形式，则为：
$$
V^{\mu}(s)=\sum_{o}\mu(o\mid s)\Big[R_{s}^{o}+\sum_{s'}\sum_{k\ge1}\gamma^{k}P(s',k\mid s,o)V^{\mu}(s')\Big]
$$
$$
Q^{\mu}(s,o)=R_{s}^{o}+\sum_{s'}\sum_{k\ge1}\gamma^{k}P(s',k\mid s,o)\sum_{o'}\mu(o'\mid s')Q^{\mu}(s',o')
$$

---

### (2) Bellman 最优方程（Optimality）

状态价值函数：
$$
V^{*}(s)=\max_{o}\Big[r_{s}^{o}+\sum_{s'}p_{ss'}^{o}V^{*}(s')\Big]
$$

选项价值函数：
$$
Q^{*}(s,o)=r_{s}^{o}+\sum_{s'}p_{ss'}^{o}\max_{o'}Q^{*}(s',o')
$$

同样：
$$
V^{*}(s)=\max_{o}Q^{*}(s,o)
$$

若写显式 $k$：
$$
V^{*}(s)=\max_{o}\Big[R_{s}^{o}+\sum_{s'}\sum_{k\ge1}\gamma^{k}P(s',k\mid s,o)V^{*}(s')\Big]
$$
$$
Q^{*}(s,o)=R_{s}^{o}+\sum_{s'}\sum_{k\ge1}\gamma^{k}P(s',k\mid s,o)\max_{o'}Q^{*}(s',o')
$$

---

### ✅ 总结对比表

| 项目   | 普通 MDP                                               | SMDP（Options）                                           |
| ---- | ---------------------------------------------------- | ------------------------------------------------------- |
| 决策单位 | 原子动作 $a$                                             | 选项 $o$（多步动作）                                            |
| 时长   | 固定为1步                                                | 随机 $k\ge1$                                              |
| 奖励项  | $r(s,a)$                                             | $r_s^{o}=\mathbb{E}[\sum_{i=0}^{k-1}\gamma^{i}r_{t+i}]$ |
| 折扣项  | $\gamma$                                             | $\gamma^{k}$ 或吸收于 $p_{ss'}^{o}$                         |
| 状态方程 | $V^{\pi}(s)=\sum_{a}\pi(a\mid s)[r+\gamma PV^{\pi}]$ | $V^{\mu}(s)=\sum_{o}\mu(o\mid s)[r^{o}+p^{o}V^{\mu}]$   |
| 最优方程 | $\max_{a}[r+\gamma PV^{*}]$                          | $\max_{o}[r^{o}+p^{o}V^{*}]$                            |
| 关系   | $V=\Pi Q$                                            | $V=\Mu Q$                                               |

---

是否希望我再帮你把这组 MDP vs SMDP 的方程对照制成表格式 LaTeX 源码（适合直接放进论文或笔记）？
