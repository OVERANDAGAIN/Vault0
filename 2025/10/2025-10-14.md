

# 资源分配
Multi-Agent Reinforcement Learning-Based Coordinated Dynamic Task Allocation for Heterogenous UAVs 
>本文研究了存在环境不确定性时异构无人机（UAV）的协同动态任务分配（CDTA）问题。





# 速度，控制
Champion-level drone racing using deep reinforcement learning

Learning High-Speed Flight in the Wild

# 抗干扰
Multi-Agent Reinforcement Learning Based UAV Swarm Communications Against Jamming













---

# 1) 直接计算类（Exact / Numerical Equilibrium Computation）

**① 二人双矩阵博弈：Lemke–Howson（L–H）与几何/单纯形分割**

* **代表**：Lemke–Howson（1964）；Porter 等综述指出 L–H 仍是二人博弈的“默认”实现（Gambit 里也用）。优点：**精确**，理论成熟；缺点：扩展到**多人/高维**复杂度爆炸、很难在实时/在环使用。 ([www1.cmc.edu][1])
* **拓展/同类**：多玩家用**simplicial subdivision**（van der Laan–Talman 系）等离散化路径，也存在维数灾难。 ([robotics.stanford.edu][2])

**② 同伦/路径跟踪（Homotopy/Continuation）**

* **代表**：Govindan–Wilson 路线；Herings 的**同伦方法综述**系统总结了用同伦求纳什与精炼均衡的技术。优点：可处理**多均衡**/选择问题；缺点：**数值开销大**、实现复杂、对大规模/在线不友好。 ([科学直通车][3])

> **给无人机多对多的启示**：精确算法适合**线下小规模**基准或做**离线真值**，但**不适合在环**与多群对抗；可用作“**小规模验证或离线标注器**”。

---

# 2) 启发式/演化优化近似（Heuristic / Evolutionary Approximation）

**PSO / GA / DE 等把“找纳什”转成优化问题**

* **PSO 求纳什**（含 UAV 群对抗案例）：有工作直接用改进 PSO 搜索纳什，用在**多 UAV 攻防/任务分配**场景；优点：**实现简单、可扩展**；缺点：**无全局均衡保证**、对参数/初值敏感。 ([PLOS][4])
* **遗传/排序 GA**：提出**基于遗憾的排序 GA（RNESGA）**，可在一次运行中找到多个纳什；但仍缺少严格收敛保证。 ([direct.mit.edu][5])
* **混合 DE+PSO**：面向**N 人有限博弈**的混合 DE-PSO，并给了**收敛性分析**；但在高维/连续动作下仍面临算时压力。 ([aimspress.com][6])

> **启示**：可作为**在环近似求解器/初始化器**（先给个近似均衡，再交给学习/分布式算法精化），但**不单独作为最终均衡保证**。

---

# 3) 学习型均衡逼近（Learning-based; MARL 经典）

**① 经典强化学习求均衡（马尔可夫/随机博弈）**

* **Minimax-Q**（零和 MG，Littman 1994）：把 Q-learning 扩展到**概率策略**的零和均衡。优点：零和情形有理论框架；缺点：推广到一般和、多玩家更难。 ([courses.cs.duke.edu][7])
* **Nash-Q**（Hu & Wellman 2003）：在**一般和随机博弈**中学习**混合纳什**；但在并发学习/维数大时**收敛慢/不稳**。 ([jmlr.org][8])
* **Friend-or-Foe Q**（Littman 2001）：把对手分“友/敌”两类简化学习；**WoLF-PHC**（Bowling & Veloso 2002）通过**可变学习率**改进收敛到纳什。 ([jmvidal.cse.sc.edu][9])

**优缺点小结**

* **优点**：可通过交互逼近均衡；与分布式实现契合。
* **缺点**：**非平稳**（对手也在学）+ **联合动作空间指数增长** → 收敛/稳定性差，在多对多 UAV 场景直接用很吃力。 ([arXiv][10])

---

# 4) 深度 MARL + 均衡/对手建模（近十年热点）

**① 深度值分解/Actor-Critic（合作/混合环境）**

* **MADDPG**（集中训练分布执行；混合合作-对抗）指出并发学习导致**非平稳**与方差问题，给出 CTDE 框架。 ([arXiv][10])
* **QMIX**（单调值分解，JMLR 版）与 **QTRAN** 等：解决**多人联合 Q 函数**的可分解性与可训练性。优点：可扩展到较多智能体；缺点：偏**合作/半合作**，与纳什严格意义有间隙。 ([jmlr.org][11])

**② 对手建模 / 策略塑造 → 稳定均衡**

* **LOLA**（Learning with Opponent-Learning Awareness）：一阶前瞻干预对手更新，实现**对手塑造**。后续 **CoLA**、**MFOS**、**Meta-Value Learning** 强化了稳定性/泛化与元层建模。适合在复杂对抗中提高**收敛与稳健**。 ([arXiv][12])

> **优缺点**：深度方法在复杂场景**更可扩展**，可融入**对手建模/元博弈**缓解非平稳；但**训练代价高**，严格“达到纳什”的证明多为**特殊条件**或**近似**。

---

# 5) 分布式/连续时间的均衡寻优（Networked / GNE / 具理论保证）

（这条线与“任务-能力-资源耦合”的**广义纳什（GNE）**尤其契合）

* **连续时间/全分布式 GNE**（Automatica 系，Bianchi & Grammatico 等）：**基于共识+原始-对偶梯度**，在**强单调/利普希茨**条件下收敛到**变分均衡**；适合耦合约束（资源/碰撞/编队）。 ([科学直通车][13])
* **规定时间/全分布式 NE**（2024）：在**给定时间**内收敛（处方时间控制），用于非线性 MAS；强调网络/切换拓扑可达。 ([科学直通车][14])
* **综述**：**Ye 等（2023 IEEE Proceedings）**系统综述了**分布式 NE 寻优**（连续/离散、估计-共识、鲁棒/事件触发等）。这是写“领域进展”的权威入口。 ([Astrophysics Data System][15])

> **优缺点**：这条线有**严格收敛/稳定性**与**分布式**优势，天然贴合“在环+多对多+耦合约束”；但要求**单调/凸性/梯度可得**等条件，落地需与**学习/建模**结合。

---

# 6) 小结对照表（可直接放材料）

| 路线          | 代表文献                                   | 进展要点           | 优点              | 局限/风险                                              |
| ----------- | -------------------------------------- | -------------- | --------------- | -------------------------------------------------- |
| **精确/数值**   | Lemke–Howson；同伦综述                      | 小规模精确解、均衡选择    | 理论严谨、精确         | **多人/高维难、算时长**，不适合在环。 ([www1.cmc.edu][1])          |
| **启发式/演化**  | PSO/GA/DE 求纳什（含 UAV 攻防）                | 作为近似/初始化器使用增多  | 实现简、可扩展         | **无全局保证**，对参数敏感。 ([PLOS][4])                       |
| **RL 经典**   | Minimax-Q；Nash-Q；WoLF-PHC；FoF-Q        | 从随机博弈出发逼近纳什    | 分布式、可自适应        | **非平稳 + 维数灾** → 收敛慢/不稳。 ([courses.cs.duke.edu][7]) |
| **深度 MARL** | MADDPG；QMIX/QTRAN；对手塑造（LOLA/CoLA/MFOS） | CTDE、值分解、对手建模  | 可扩展、能处理复杂交互     | 训练重，**严格纳什保证弱**。 ([arXiv][12])                     |
| **分布式/GNE** | Automatica 连续时间 GNE；规定时间 NE；权威综述       | **有收敛保证**、网络原生 | 贴合**耦合约束**与“在环” | 需单调/凸性/梯度信息。 ([科学直通车][13])                         |








---

如果你要，我可以把上面表格和要点**排版成可直接粘贴的“研究现状”小节**（约 600–800 字），并附**参考文献清单**（按照你学校/申报格式）。

[1]: https://www1.cmc.edu/pages/faculty/MONeill/math188/papers/lemkehowson.pdf?utm_source=chatgpt.com "Equilibrium Points of Bimatrix Games - CE Lemke"
[2]: https://robotics.stanford.edu/~eugnud/papers/computing-nash-eq.pdf?utm_source=chatgpt.com "Simple Search Methods for Finding a Nash Equilibrium"
[3]: https://www.sciencedirect.com/science/article/abs/pii/S0165188903001088?utm_source=chatgpt.com "Computing Nash equilibria by iterated polymatrix ..."
[4]: https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0260231&utm_source=chatgpt.com "An improved predator-prey particle swarm optimization ..."
[5]: https://direct.mit.edu/evco/article/30/3/447/109778/Regret-Based-Nash-Equilibrium-Sorting-Genetic?utm_source=chatgpt.com "Regret-Based Nash Equilibrium Sorting Genetic Algorithm ..."
[6]: https://www.aimspress.com/article/10.3934/math.2021081?utm_source=chatgpt.com "Differential evolution particle swarm optimization algorithm ..."
[7]: https://courses.cs.duke.edu/cps296.3/spring07/littman94markov.pdf?utm_source=chatgpt.com "Markov games as a framework for multi-agent ..."
[8]: https://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf?utm_source=chatgpt.com "Nash Q-Learning for General-Sum Stochastic Games"
[9]: https://jmvidal.cse.sc.edu/library/littman01a.pdf?utm_source=chatgpt.com "Friend-or-Foe Q-learning in General-Sum GAmes"
[10]: https://arxiv.org/abs/1706.02275?utm_source=chatgpt.com "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive ..."
[11]: https://jmlr.org/papers/volume21/20-081/20-081.pdf?utm_source=chatgpt.com "Monotonic Value Function Factorisation for Deep Multi- ..."
[12]: https://arxiv.org/pdf/1706.02275?utm_source=chatgpt.com "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive ..."
[13]: https://www.sciencedirect.com/science/article/pii/S0005109821001801?utm_source=chatgpt.com "Continuous-time fully distributed generalized Nash ..."
[14]: https://www.sciencedirect.com/science/article/abs/pii/S0005109824003418?utm_source=chatgpt.com "Prescribed-time fully distributed Nash equilibrium seeking ..."
[15]: https://ui.adsabs.harvard.edu/abs/2023IEEEP.111..140Y/abstract?utm_source=chatgpt.com "Distributed Nash Equilibrium Seeking in Games With ..."
