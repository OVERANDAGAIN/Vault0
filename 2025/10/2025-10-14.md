

# 资源分配
Multi-Agent Reinforcement Learning-Based Coordinated Dynamic Task Allocation for Heterogenous UAVs 
>本文研究了存在环境不确定性时异构无人机（UAV）的协同动态任务分配（CDTA）问题。





# 速度，控制
Champion-level drone racing using deep reinforcement learning

Learning High-Speed Flight in the Wild

# 抗干扰
Multi-Agent Reinforcement Learning Based UAV Swarm Communications Against Jamming







## 一、研究脉络概览

论文将**博弈均衡求解**的研究路径分为两类：

1. **基于传统博弈理论的直接求解方法**（Direct computation methods）；
2. **基于学习的均衡逼近方法**（Learning-based equilibrium methods），特别是在多智能体强化学习（MARL）框架下的应用 。

---

## 二、传统“直接计算”类方法

### 1. **经典解析算法**

代表方法包括：

* **几何法（Graphical/Geometric methods）**
* **Lemke–Howson 算法**
* **同伦路径法（Homotopy method）**

这些方法在小规模静态博弈中具有严格的理论保证，可求出**精确的混合策略纳什均衡**。
**缺点**：在多人、连续动作或高维状态下计算复杂度呈指数增长，难以用于实际系统（如无人机集群对抗） 。

---

### 2. **启发式与智能优化算法**

由于传统算法的复杂度限制，近年来出现了多种基于智能优化的**近似均衡求解方法**：

* **粒子群优化（PSO）**：将均衡求解转化为一个最小化代价函数的问题，通过群体搜索逼近均衡点；
* **遗传算法（GA）**、**蚁群算法（ACO）**：基于群体演化迭代搜索纳什点；
* **差分进化（DE）**：通过种群差异调整迭代搜索路径；
* **协同搜索算法（Collaborative Search）**：结合局部搜索与群体探索机制。

**优点**：可扩展、实现简便、在复杂多智能体系统中具有较好的工程可行性。
**缺点**：缺乏理论收敛性和均衡最优性保证；搜索稳定性依赖参数调节 。

---

## 三、基于学习的均衡求解方法（Learning-based Methods）

### 1. **强化学习框架下的均衡逼近**

* **Minimax-Q 学习（Littman, 1994）**：在零和博弈下学习安全策略；
* **Nash-Q Learning（Hu & Wellman, 2003）**：在广义和博弈中逼近混合策略均衡；
* **Friend-or-Foe Q-learning**：通过不同对手建模假设加速收敛；
* **WoLF-PHC（Win or Learn Fast）算法**：根据胜负状态调整学习率，改善动态均衡收敛性。

**优点**：

* 可通过交互逐步逼近均衡；
* 具备一定的分布式学习潜力；
* 理论上可收敛至纳什均衡（在理想条件下）。

**缺点**：

* 在多智能体并行学习下环境非平稳；
* 收敛速度慢、稳定性依赖对手策略建模；
* 当代理数增加时，**联合动作空间指数增长**，导致求解困难 。

---

### 2. **深度强化学习结合博弈求解**

* **MADDPG（Multi-Agent Deep Deterministic Policy Gradient）**：采用集中式训练、分布式执行策略；
* **QTRAN、VDN、QMIX**：利用价值函数分解提升合作性学习效率；
* **LOLA、CoLA、MFOS、Meta-Value Learning**：引入对手建模与元博弈学习思想，通过预测对手更新方向或策略元值来获得更稳定的均衡。

这些算法将均衡求解问题从显式的解析计算转化为隐式的“学习趋近过程”，尤其适用于复杂任务场景（如**无人集群的资源竞争与任务协作**） 。

---

## 四、代表性进展对比（汇总表）

| 方法类别        | 代表算法                      | 主要思路            | 优点           | 局限性           |
| :---------- | :------------------------ | :-------------- | :----------- | :------------ |
| **解析求解法**   | Lemke–Howson、同伦路径法        | 显式求解混合策略方程      | 理论精确、收敛有保证   | 多人博弈计算复杂、无法实时 |
| **启发式算法**   | PSO、GA、ACO、DE             | 通过群智能迭代搜索近似均衡点  | 实现简单、可扩展     | 缺乏均衡最优保证、稳定性差 |
| **强化学习法**   | Minimax-Q、Nash-Q、WoLF-PHC | 交互式学习近似 Nash 均衡 | 具备自适应与分布式特征  | 非平稳环境下易震荡、不收敛 |
| **深度学习结合法** | MADDPG、QMIX、LOLA、MFOS     | 联合对手建模与策略优化     | 泛化性强、可应用复杂场景 | 计算代价高、依赖大规模训练 |

---

## 五、总体趋势与启示

1. **从静态计算 → 动态学习**
   博弈求解逐步从数学解析法向**学习驱动的均衡逼近**过渡，以适应复杂环境下的实时性需求。

2. **从集中式求解 → 分布式自适应**
   无人机群体等多体系统中，集中式均衡计算难以在环执行，需采用**分布式强化学习**、**对手建模**等结构。

3. **从固定对手假设 → 元博弈与策略塑造**
   近期研究强调通过预测对手学习方向实现**稳定均衡（Meta-Game Equilibrium）**，提升泛化性与鲁棒性。

---

## 六、与“无人机多对多博弈均衡求解技术”的契合方向

| 研究进展                           | 对应改进方向                          |
| :----------------------------- | :------------------------------ |
| Lemke–Howson / Homotopy 无法实时计算 | → 采用分布式近似均衡（Approximate GNE）求解  |
| PSO/GA 可用于启发式均衡搜索              | → 可作为**在环近似求解器**或**初始策略生成器**    |
| Nash-Q / Minimax-Q 学习稳定性不足     | → 引入对手建模（Opponent Modeling）增强收敛 |
| 深度学习均衡求解泛化性强                   | → 用于复杂场景下的**多对多博弈策略生成**         |

---

### ✅ 建议写法（适合申报材料段落）

> 当前关于博弈均衡求解的研究主要分为两大类：一是基于解析算法的**直接求解方法**（如几何法、Lemke–Howson、同伦路径），在小规模静态博弈中具有精确性但难以扩展至多对多任务；二是基于学习的**均衡逼近方法**，包括强化学习、深度强化学习与对手建模框架（如 Nash-Q、MADDPG、LOLA、MFOS 等），能够在复杂、非平稳环境下实现动态自适应学习。近年来，研究者尝试结合群智能算法与深度学习，以实现多智能体环境下的**分布式、在环均衡求解**，为无人集群多对多任务中的实时博弈策略生成提供了新的解决思路。

---

是否希望我帮你把这一部分改写成**正式的综述段落版本**（中英文对照 + 可直接放进材料）？那样我可以整理成约 2–3 段正文格式并标注引用。
