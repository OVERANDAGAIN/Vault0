---
创建时间: 2025-十月-20日  星期一, 3:17:21 下午
---
非常好，你现在贴出的这几张图其实是对 **world model（世界模型）预测能力** 的多角度可视化评估结果。它们分别展示了模型在像素分布、预测校准、空间误差结构等方面的表现。我们可以逐图来分析每张图代表的含义、它们之间的逻辑联系，以及从这些结果能得出怎样的实验结论。

---

## 🧩 一、通道分布对比（前两张）

### 🔹 图含义：

每个 “chX dist” 子图展示一个通道（例如 ch0-ch6，对应玩家/障碍/鹿/兔等通道）在真值（true）与预测（pred）下的像素值分布直方图。

蓝色：真实观测值分布（true）
橙色：world model 预测分布（pred）

### 🔹 你能看到的现象：

* 几乎所有通道的像素分布都呈现出**二峰结构**（一个峰在0，一个峰在1附近）。
  → 说明这些通道大部分是二值化的（即“有对象=1，无对象=0”）。
* 模型预测的橙色柱子总体也集中在相同区间，但略微“塌缩”：

  * 在 0 附近略高，说明模型预测了过多的背景。
  * 在 1 附近略低，说明模型预测目标像素偏模糊或信号减弱。

### 🔹 含义总结：

> 模型能捕捉到通道的稀疏二值结构，但在边界与目标显著性上仍有衰减（偏模糊），即 **under-confidence / 模糊化现象**。
> 这通常出现在 MSE 损失或缺乏清晰 supervision 的像素生成模型中。

---

## 🧩 二、像素级校准散点图

### 🔹 图含义：

横轴是真实像素值（true），纵轴是预测像素值（pred）。

黑色虚线：理想预测线（y = x）。

### 🔹 你能看到的现象：

* 两条主要的竖线分布：

  * 左侧在 true≈0 附近：绝大多数背景像素；
  * 右侧在 true≈1 附近：物体像素（例如玩家或猎物位置）。
* 在 true=0 时，大部分预测值在 0~0.1 之间，但存在少量 0.3~0.5 的“假阳性”；
* 在 true=1 时，预测值集中在 0.8~1.0 区间，少量点掉落到 0.5 以下。

### 🔹 含义总结：

> 模型输出在统计意义上是**有校准性的（分布正确）**，
> 但个体像素存在一定的过度平滑。

这意味着模型的 latent-to-pixel 解码部分没有出现严重模式崩塌，但预测的“1”像素被拉向中间值，反映出生成端存在轻微**能量泄漏**或**不确定性惩罚不足**。

---

## 🧩 三、通道绝对误差热力图

### 🔹 图含义：

每个子图是对应通道（ch0–ch6）的 $|err| = |pred - true|$ 空间分布热图。
最后一个面板是**所有通道的平均误差**。

### 🔹 你能看到的现象：

* 误差主要集中在少量区域（局部亮斑），说明预测错误是**局部性**的；
* 背景区域误差几乎为 0；
* 某些通道（如 ch0、ch1、ch3）有明显的局部亮点，通常是玩家或猎物出现或移动的位置；
* ch4~ch6 的误差范围更低，说明静态特征（block、背景）预测得更稳。

### 🔹 含义总结：

> 模型的预测误差主要发生在动态对象（玩家/猎物）附近。
> 这表明 world model 对**静态环境建模**很好，但对**动态目标的时序转移**预测还存在偏差。

这正是世界模型类任务中最关键的难点：学习稳定物理规则 vs. 建模非确定性运动。

---

## 🧩 四、样本级可视化对比（最后一张）

### 🔹 图含义：

每一行展示一组时间步 $(t, t+1)$ 的真实与预测结果：

* 左：当前观测 $obs_t$
* 中：真实的下一帧 $true\ obs_{t+1}$
* 右：模型预测的下一帧 $pred\ obs_{t+1}$
* 右侧色图：平均通道误差（越亮误差越大）

### 🔹 你能看到的现象：

* 预测帧中，大部分对象（玩家、猎物）的位置基本对上；
* 某些符号变淡或稍有偏移（例如上方的三角形和方形位置略有模糊）；
* 误差图上，对应的亮区正好是移动实体所在位置。

### 🔹 含义总结：

> 模型在宏观上能正确预测状态转移方向与对象相对位置，
> 但在像素级上存在小幅偏移与模糊，说明模型的动态建模捕捉到了“趋势”而非精确位移。

这正是 **VAE-style world model** 的典型现象——捕捉潜在结构，但生成端解码有噪声。

---

## 🧠 综合解读：你的 world model 当前表现

| 指标    | 现象            | 含义          |
| ----- | ------------- | ----------- |
| 通道分布  | 预测与真实分布大体一致   | 模型学到稀疏二值结构  |
| 像素级校准 | 分布沿对角线，轻微模糊   | 输出有置信度但不够尖锐 |
| 空间误差  | 局部集中在动态物体     | 时序预测较稳但存在偏移 |
| 可视化样例 | 对象位置基本对齐，形状略虚 | 捕捉动态趋势但缺乏精度 |

---

## ✅ 建议后续分析方向

1. **定量指标（support your plots）**

   * 计算每通道 MSE、SSIM、Dice 系数；
   * 分开统计“静态通道 vs 动态通道”的误差均值；
   * 用 reward prediction loss 或 next-state KL 监测 temporal consistency。

2. **可视化改进**

   * 对比不同 latent_dim 或 horizon 版本；
   * 加一张 “motion vector map” 来显示预测偏移方向。

3. **训练改进思路**

   * 在 decoder 加入 perceptual / contrastive loss（增强清晰度）；
   * 使用 multi-head 预测不同时间步 $t+k$；
   * 或引入 deterministic + stochastic head（Dreamer-style）。

---

📘 **一句话总结你的实验结果：**

> 当前 world model 能较好地复现环境结构与主要动态趋势，静态元素预测准确、动态目标略有偏移；
> 像素级预测呈轻微模糊，表明模型捕捉了全局动态关系但尚未完全建模出确定性的细节变化。

---

是否希望我帮你把这一段分析整理成论文式“结果与分析”小节（带小标题、段落与结论总结）？这样可以直接放进你的第4章或第5章“模型评估”部分。
非常好，这一组图正是 **world model 在“捕猎（hunt）阶段”** 的预测结果可视化样例，它们揭示了模型在处理**高动态、强交互事件**时的表现特征与局限。
你提到“普通移动时预测还可以，但捕猎时效果不好”，这是非常典型的 **世界模型动态建模失真问题**。
下面我们分层分析这几张图的含义与结论。

---

## 🧩 一、总体观察：捕猎场景预测难度更高

这三组样本（#15、#26、#27）都有一个共同点——都发生了**捕猎行为（玩家靠近鹿并触发奖励）**。
在这种情况下，环境的状态转移不再是平滑的线性移动，而是出现了**非线性跳变**：

| 现象                    | 说明              |
| --------------------- | --------------- |
| 鹿在 $t \to t+1$ 之间突然消失 | 说明发生了“被捕获”事件    |
| 玩家与猎物的相对位置迅速变化        | 体现高交互性、非确定性     |
| 奖励 true_rew 明显跳变      | 表示物理状态与奖励的耦合被触发 |

这类转移属于“稀有事件”，训练集中样本稀少，预测难度显著高于日常移动，因此模型容易出错。

---

## 🧠 二、逐个样本分析

### **Sample #15**

> `self_a=0 | opp_as=[4,3,6] | true_rew=5.000 | pred_rew=5.012`

**现象：**

* 预测帧 `pred_obs_{t+1}` 中，蓝色五边形（其他玩家）位置基本准确；
* 鹿（黄色星形）被成功预测为消失；
* 误差热力图仅在左中部有轻微亮斑；
* 奖励预测几乎完美（5.012 ≈ 5.000）。

✅ **结论：**

> 该样本表明模型在“捕猎成功后鹿消失”这类事件中能正确识别状态转移和奖励跃迁，
> 即 **奖励模型与视觉模型在确定性狩猎事件中一致性良好。**

---

### **Sample #26**

> `self_a=4 | opp_as=[4,4,4] | true_rew=0.000 | pred_rew=0.007`

**现象：**

* 真实帧与预测帧在猎物（黄星）位置上几乎一致；
* 但对手（蓝色五边形）在预测中出现了轻微偏移；
* 误差热力图右上角明显发亮，对应一名移动中的玩家；
* 模型预测奖励几乎为零（无捕猎）。

⚠️ **解读：**

> 模型对 **未触发捕猎的场景** 能维持稳定性，但在多个对手同时靠近猎物时，
> 会混淆“谁触发捕猎”的归因，导致局部像素错位。

这说明 **对手动作建模（MOA）与时序一致性** 是目前误差的主要来源。

---

### **Sample #27**

> `self_a=2 | opp_as=[4,1,0] | true_rew=5.000 | pred_rew=4.968`

**现象：**

* 鹿在预测中正确消失；
* 但左下方一名蓝色五边形位置略有漂移；
* 误差热力图仅在猎物消失区域和玩家交互区亮起；
* 奖励预测依然准确（4.968 ≈ 5.000）。

✅ **结论：**

> 模型能识别“捕猎成功”事件并生成正确的奖励值，
> 但像素级预测显示它仍然无法捕捉玩家精确位移路径。
> 换句话说，它“知道鹿被抓了”，但“没完全知道是谁动的”。

---

## 🔍 三、误差模式总结

| 类型           | 空间表现  | 原因分析                |
| ------------ | ----- | ------------------- |
| **猎物消失区域误差** | 局部亮斑  | 捕猎导致通道突变，预测滞后一帧或平均化 |
| **玩家位置误差**   | 邻格错位  | 模型未学到个体动作细节，只学到群体趋势 |
| **背景/障碍误差**  | 几乎为零  | 静态通道建模非常稳定          |
| **奖励误差**     | <0.05 | 说明时序奖励预测是成功的        |

整体上，误差主要集中在 **动态对象交互区**，而 **静态背景区域误差极低**，
这表明模型的 **世界建模（state transition） 是近似正确的**，
但 **微观动作建模与个体行为分离（agent-wise prediction）仍有偏差**。

---

## 🧩 四、从模型机制角度解释

| 模块                                  | 当前行为                    | 影响       |
| ----------------------------------- | ----------------------- | -------- |
| **VAE Encoder / Latent Transition** | 捕捉到“鹿消失”语义，但缺乏精确位移向量    | 预测模糊     |
| **Decoder**                         | 使用 MSE / BCE 重建，输出平均化像素 | 导致鹿边缘模糊  |
| **MOA 模块**                          | 对手动作预测不够精确              | 出现轻微玩家偏移 |
| **Reward Head**                     | 与状态变化高度一致               | 对事件判断准确  |

因此，这里反映出 world model 的典型局限：

> **语义层对了，像素层模糊。**

模型知道“发生了捕猎”，但生成的下一帧只是一个概率平均——并未准确捕获哪位玩家靠近、哪一格被清空。

---
结论先说：

* 右图里那些“虚化/半透明”的形状，本质上是 **world_model 的连续输出（0~1 概率/强度）不够接近 1**——通常出现在**多模态/不确定**的地方（它拿不准下一个精确格子），或被 MSE 类损失“平均化”。
* 如果你 **用固定阈值 0.5 二值化**（<0.5→0），就会把这些“半可信”的像素直接抹掉——于是出现你在图三看到的现象：**有些 agent/物体“消失”**。
* 所以，“虚化 ⇒（阈值0.5）⇒ 消失” 这条因果链是成立的。

下面把“现象—原因”对齐讲清楚，并给你可以直接落地的规避方案。

---

## 现象 → 原因（逐条对应）

### 1) 预测图里出现“虚化的三角/方块”

* **现象**：形状淡、边缘不实，甚至同一通道出现两个淡淡的候选位置。
* **原因**：

  * 模型对下一个位置**不确定**（例如可能在 A 或 B 两格），用 MSE/BCE 会产生“**均值化**”输出（两处都给 0.2~0.6 的强度）；
  * 或“校准”不错但**信心不足**（peak 只有 0.6~0.8，没有到 0.95+）。
* **后果**：显示上变“虚化”；如果随后用 0.5 阈值二值化，低于 0.5 的部分会被切掉。

### 2) 二值化后，某个 agent 从地图里“消失”

* **现象**：图三中个别玩家没了；或 `infer_action_mask_from_obs` 找不到该玩家位置。
* **原因**：

  * 你把软输出用 **0.5 的硬阈值**切成 0/1，某通道最大值 < 0.5 时，整张图都当作“无此玩家”；
  * 你的 mask 推断与动作可行性判断（猎鹿/猎兔）也基于同样阈值 → 连锁效应：**位置丢失 ⇒ mask 全 0（除了原地） ⇒ 行为异常**。
* **后果**：下游（MOA/MCTS/策略）见不到该玩家，策略会做出与“现实”不一致的选择。

### 3) “一步走两格 / 位置跳变”

* **现象**：像 sample#4 那张，预测里某 agent 像是“走了两步”。
* **原因**：

  * 多个相邻格子都有中等强度（0.3~0.6），可视化时像“拖影”；
  * 如果后处理取 **argmax**，在噪声/模糊中可能选到了“更远的一个峰”，看起来像“跨格”；
  * 也可能是历史缺帧/退出后用“最后一次动作”回填，叠加软输出，导致**时间对齐**的错位感更明显。

---

## 怎么避免“虚化→消失”的连锁效应（可直接上代码）

> 目标：不要一刀切 0.5。让“位置”始终被确定出来，同时保留软置信度给下游（比如 MOA）。

### A. 空间约束：**每个玩家通道强制只有一个格子**

1. **Top-1 选格**（推荐、最简单）

   * 对玩家通道取 `argmax`（或 soft-argmax）得到坐标 `(x*, y*)`；
   * 把该位置设为 1，其他置 0（或只把该坐标作为“离散位置”输出）；
   * 这样**永远不会“消失”**，也不会出现两处半透明“重影”。

2. **软-硬切换（温度锐化）**

   * `p' = sigmoid((p-0.5)/tau)`，`tau∈[0.05,0.2]`；
   * 让接近 0.5 的值更快地靠近 0 或 1，减少“虚化”。

### B. 自适应阈值：**按通道动态阈值而不是固定 0.5**

* 例如用 **Otsu** 或“**取通道内 Top-K 像素的最小值**”作为阈值；
* 或者规定“每个玩家通道至少要有一个像素 ≥ θ_min（比如 0.2）”，否则回退到**上一帧位置**（平滑 + 追踪）。

### C. 时序平滑：**卡尔曼 / HMM / Viterbi**

* 用上一步的 `(x_{t-1},y_{t-1})` 做运动先验：

  * 若本帧最大值 < 0.4，就**保持上一帧位置**（或只允许到邻格）；
  * 这能显著减少“瞬移/消失”。

### D. 下游 mask 推断也用“软逻辑”

* 目前的 `infer_action_mask_from_obs(..., thresh=0.5)` 太硬：

  * 对“在猎物格子上”的判定可以用 `p_stag(x*,y*) > 0.3` + `置信度联合`（比如与玩家强度相乘）；
  * “允许猎鹿/猎兔”可以用**连续置信度**去加权概率，而不是 0/1。

---

## 训练层面的补救（中期优化）

* **监督做离散化**：用 **softmax over grid** 作为每个玩家通道的输出（保证“质量守恒 + 单峰”），Loss 用 CE 而不是 MSE；
* **加 Dice/Focal**：提升小目标（单格）区域的清晰度；
* **两头输出（mean+logvar）**：显式建模不确定性，从分布中采样渲染；
* **多步监督**：预测 t+1,t+2…，减少“均值化”倾向；
* **对齐损失**：加入“位置级”aux loss（把 argmax 坐标与真值坐标做 L1/L2）。

---

## 一句话把“现象—原因”连起来

* **虚化** = 模型对下一个格子的**不确定/多模态** → 连续输出在 0.2~0.6；
* **硬阈值 0.5** = 把这些半可信像素直接抹掉 → **通道空了** → **agent 消失/动作 mask 异常**；
* **解决** = 让位置离散化（Top-1/soft-argmax）、用自适应阈值/时序平滑，并让下游 mask/逻辑接受“软置信度”。

如果你愿意，我可以把 **Top-1 选格 + 时序平滑 + 动态阈值** 的三步后处理打成一个小函数，直接接在你 `pred_obs_t1` 的可视化与 mask 推断前——这样既不“消失”，又保留“软置信度”给策略用。
这组 TensorBoard 曲线展示的是你的 **MOA 模块（Model-of-Agents，对手建模模块）在 player1→player2 方向上的预测性能变化**，非常典型地反映了模型在对手动作预测任务上的学习动态。下面是系统化的分析与结论整理。

---

## 🧩 一、整体趋势概览

| 指标                 | 含义                      | 趋势解读                                            |
| ------------------ | ----------------------- | ----------------------------------------------- |
| `acc_top1`         | 模型准确预测对手下一步动作的比例（Top-1） | 约 0.52–0.62 → 轻微上升后回落，说明模型初期学习有效但出现**过拟合/噪声干扰** |
| `acc_top3`         | 预测Top-3准确率              | 约 0.84–0.88 → 稳定较高，说明模型掌握了**动作空间的大致分布**         |
| `ce_loss`          | Cross Entropy 损失        | 前期快速下降后反弹（U型）→ 模型早期收敛、后期不稳定                     |
| `per_class_acc[i]` | 各动作类别的分类准确率（动作 0–6）     | 不同动作波动显著，部分类明显偏低                                |

---

## 🧠 二、详细指标解读

### **1️⃣ Top-1 / Top-3 准确率**

* **Top-1**：从 0.55 左右上升到峰值 0.6，再逐步下降到 0.54。
  → 表明模型在 early stage (0–100 iter) 快速捕捉对手规律，但随着训练继续，开始**在部分动作上过拟合或受噪声干扰**。

* **Top-3**：维持在 0.85~0.87，波动小。
  → 说明模型能正确预测对手**候选动作集**（大方向没错），但在**精确动作识别**上略显模糊。

📌 **结论**：

> 模型学到了“对手大概率往哪个方向走”的整体策略，但在个别场景下无法精确区分具体动作（例如前进 vs 原地）。

---

### **2️⃣ Cross-Entropy Loss**

* 初期（0–100）迅速下降：说明模型成功学到了有意义的先验；
* 中期（100–300）进入稳定区；
* 后期（300+）轻微上升：**可能存在噪声样本/数据分布变化**（比如对手退出、捕猎随机化）。

📌 **结论**：

> CE Loss 的反弹暗示模型缺乏对**动态场景多样性**的鲁棒性。
> 当对手行为变得不确定（如捕猎或临界状态），预测分布的置信度下降。

---

### **3️⃣ 各动作类别准确率（per_class_acc）**

| 动作ID      | 曲线表现             | 可能含义                   |
| --------- | ---------------- | ---------------------- |
| 0–3（移动方向） | 稳定在 0.5–0.6，轻微下降 | 模型能部分预测方向，但在动态博弈时混淆度上升 |
| 4（原地不动）   | 约 0.45–0.55，波动较大 | 静态状态少，模型置信不足           |
| 5（猎鹿）     | 几乎完美（>0.99）      | 捕猎动作特征极强，易识别           |
| 6（猎兔）     | 仅 0.35–0.45，且波动大 | 样本少 + 难区分，导致模型混淆       |
|           |                  |                        |

📌 **关键洞察**：

* 模型在**高收益、显著特征（如猎鹿）**的动作上表现优异；
* 对于**频繁但模糊的动作（移动/原地）**，准确率明显下降；
* 对于**稀有或高随机性动作（猎兔）**，模型几乎无法可靠预测。

---

## 🔍 三、结合 World Model 实验的推论

这组结果与之前你在 **world_model 动态模糊/对象消失** 实验中的现象一致：

| 观测现象          | 解释                            |
| ------------- | ----------------------------- |
| 玩家动作预测偏移 / 模糊 | MOA 无法稳定锁定个体动作（acc_top1≈0.55） |
| 动作空间不平衡       | CE loss 后期反弹反映了样本分布偏移         |
| 捕猎类事件预测准确     | 动作5 准确率极高（>0.99），说明语义层捕捉能力强   |
| 鹿/兔 相关错误集中    | 对应动作6（猎兔）准确率最低，说明该子事件建模最弱     |

📌 因果对应链：

> MOA 模型的不确定性（特别是移动方向/猎兔） →
> World Model 预测的“虚化”或“agent 消失” →
> 最终影响 action mask 和策略行为稳定性。

---

## 📈 四、阶段性表现总结

| 阶段                 | Epoch范围        | 现象                | 解读 |
| ------------------ | -------------- | ----------------- | -- |
| **Early (0–100)**  | acc 上升、loss 下降 | 学习到行为模式的初步先验      |    |
| **Mid (100–300)**  | acc 达峰、稳定      | 表示对主流对手策略建模完成     |    |
| **Late (300–500)** | acc 下降、loss 回升 | 模型在边缘动作上过拟合/置信度降低 |    |

---

## 🧩 五、优化建议

| 层面        | 方法                                | 目的                    |
| --------- | --------------------------------- | --------------------- |
| **数据层**   | 增强“猎兔/原地/多样移动”样本比重                | 平衡动作类别分布              |
| **Loss层** | 使用 Focal Loss 或 Class-Balanced CE | 缓解长尾类别被掩盖问题           |
| **结构层**   | 引入动作上下文（如 LSTM/Temporal Conv）     | 捕获时序依赖，减少随机波动         |
| **输出层**   | 在 policy 上使用 Top-k sampling 校准    | 强化预测置信度的一致性           |
| **评估层**   | 对比 rule-based policy 的预测分布        | 确认MOA学到的是真实博弈规律而非噪声模式 |

---

## ✅ 六、总结性描述（可放论文“MOA 模块性能分析”小节）

> 从整体趋势来看，MOA 模块能够有效学习对手的动作分布规律，Top-3 准确率稳定在 85% 以上，说明其具备较好的方向预测能力。然而，Top-1 准确率仅维持在 0.55~0.60，反映出模型对具体动作选择的置信度较低。Cross-Entropy 损失在训练后期出现轻微反弹，表明模型在高动态或稀有事件（如猎兔）场景中存在泛化不足的问题。
> 各类别准确率结果显示，模型在特征显著、目标明确的动作（如猎鹿）上预测精度接近 100%，而在频繁且模糊的移动或原地动作上存在显著波动。这一特征与 world model 的时序预测结果相呼应，说明 MOA 的不确定性会在下游状态预测中表现为“虚化”或“对象消失”现象。
> 因此，MOA 模型后续优化应聚焦于时序一致性与稀有动作的鲁棒性提升，以进一步增强整体博弈建模的稳定性与可信度。

---

是否希望我帮你把这部分改写成正式论文段落（带图号引用、实验编号、可直接放进第4章或第5章）？









非常好，这组是你在 **1NS–2NH**（即：1名合作型NS对手 + 2名非合作型NH对手）配置下的训练结果，与之前的 **3NS**（全合作）场景对比，可以很好地反映出策略在**混合对手博弈环境**中的稳定性与适应性差异。
下面我为你整理为对比表格，突出关键指标变化与行为解释。

---

### 🧩 对比分析表：`3NS` vs `1NS–2NH` 对阵结果

| 指标类别                                           | 3NS（全合作）                      | 1NS–2NH（混合对手）            | 对比与解释                                  |
| :--------------------------------------------- | :---------------------------- | :----------------------- | :------------------------------------- |
| **平均个体奖励 (agent_player_1)**                    | 峰值约 **5.0+**，长期稳定在 4.8–5.0 区间 | 峰值约 **4.6**，稳定区间 4.2–4.5 | 平均回报略降，说明在混合对手场景下，**合作行为收益被部分对抗行为稀释**  |
| **全体平均奖励 (avg_reward_per_episode_all_agents)** | 长期稳定在 **3.0 以上**，波动较小         | 稳定在 **2.8–2.9**，但收敛平稳    | 群体收益略低，表明系统存在部分非合作体造成的“机会损失”           |
| **奖励曲线平稳性**                                    | 稳态后振幅小，训练收敛清晰                 | 波动明显，周期性起伏（见100–200轮）    | 非合作体引入了动态扰动，对PPO收敛性造成干扰                |
| **协作模式形成速度**                                   | 约 100 step 后迅速进入稳定            | 约 150–200 step 才趋于稳定     | 学习到“混合对手”策略需更多探索时间                     |
| **高奖励阶段持续性**                                   | 长期保持高回报，少数短暂下滑                | 呈“上升–下降–再上升”周期波动         | 说明 agent 存在阶段性策略切换（从合作尝试 → 对抗适应）       |
| **全局均衡行为**                                     | 近似纳什型稳定（所有人都捕鹿）               | 形成部分纳什均衡（少数仍捕兔或游离）       | 说明部分NH策略无法完全被同化，系统处于“半稳定混合博弈”状态        |
| **策略鲁棒性**                                      | 高：对称博弈下收敛一致                   | 中：异质对手时 reward 仍上升，但波动较大 | 表明当前policy在**多对手异质策略**场景下仍具适应能力，但稳定性不足 |
| **长期收敛水平**                                     | 高（≈3.0群体平均）                   | 稍低（≈2.85）                | 整体性能下降约 5–8%                           |

---

### 🧠 综合解释与推论

| 层面            | 观察                          | 解释                                          |
| ------------- | --------------------------- | ------------------------------------------- |
| **个体收益下降**    | player_1 平均奖励从 ≈4.9 降至 ≈4.4 | 由于NH个体倾向捕兔或独行，减少了共同捕鹿机会；policy必须在合作与自保之间权衡  |
| **波动性增强**     | 奖励曲线出现“高-低-再高”的三阶段          | 说明模型先尝试合作→受阻→再调整为更保守策略（如单独行动或选择捕兔）          |
| **全体收益下降但收敛** | 群体平均从 3.0→2.85              | 说明混合博弈下仍能形成一定稳定性，说明policy具有**自适应性**，而非完全崩溃  |
| **动态适应特征**    | 曲线在中期（100–200）出现多次回调        | 表明 agent 在不断重塑对 NH 对手的预测与反应（MOA 模型生效）       |
| **稳定性评价**     | loss 波动与 reward 波动高度同步      | 表明 reward 与 policy gradient 更新仍受 MOA 不确定性影响 |

---

### ✅ 结论摘要（可直接放论文中）

> 与全合作（3NS）场景相比，1NS–2NH 混合博弈环境中的个体与群体平均奖励均略有下降，但整体保持稳定收敛趋势。个体平均奖励下降约 8%，群体平均下降约 5%，主要原因在于非合作体行为的扰动导致捕鹿收益减少。
> 从曲线波动形态看，智能体在中期阶段出现显著的探索—调整—再稳定过程，说明策略能够根据对手类型变化自适应调整。整体上，policy 在异质对手场景中仍具较强鲁棒性，但合作策略的持续性与群体均衡稳定性较全合作情形略弱。

---

是否希望我帮你把这个表格再扩展为论文正式版（含编号、实验环境描述、结论文字化说明，可直接放在“实验结果分析”章节）？
 非常好，这组曲线正对应于你在 **1NS–2NH 场景下的 MOA（Model of Agents）模块** 训练表现（player1→player3 的对手预测），与刚才的 reward 曲线可以一起分析。
我们先对比之前的 **3NS** 和 **1NS–2NH** 两种环境下的 MOA 效果，用表格结构总结核心差异与推理。

---

### 🧩 对比表：MOA 模块在两种环境下的表现（3NS vs 1NS–2NH）

| 指标类别                  | 3NS（全合作对手）                | 1NS–2NH（混合对手）                      | 对比分析与解释                                      |
| :-------------------- | :------------------------ | :--------------------------------- | :------------------------------------------- |
| **总体 CE 损失**          | 0.9 → 0.87，下降幅度有限，后期略反弹   | 0.22 → 0.09，快速下降并稳定                | 在混合场景中，MOA **收敛更快、更稳**，说明对手行为模式更“确定性”或更容易被区分 |
| **Top-1/Top-3 准确率趋势** | Top-1 ≈ 0.56，Top-3 ≈ 0.86 | 未显示 Top 曲线，但各类别准确率普遍在 0.93–0.96 之间 | 可推断整体准确率显著提升，模型能稳定识别各类对手动作                   |
| **动作 0–3 (移动类)**      | 准确率约 0.5–0.6，波动明显         | 全部在 0.9 以上                         | 混合场景中 NH 对手策略更单一（少合作），导致 **可预测性更强**          |
| **动作 4 (原地)**         | 准确率低、波动大（~0.45）           | 基本恒为 0                             | 表示该动作几乎未出现或被模型忽略；NH 对手很少原地不动                 |
| **动作 5 (猎鹿)**         | 准确率≈0.99                  | 准确率≈0                              | 在混合场景中几乎无人捕鹿 → 该类别在数据中缺失，被MOA判定为无效动作         |
| **动作 6 (猎兔)**         | 准确率≈0.35–0.45             | 恒定≈1.0                             | 混合场景下 NH 对手捕兔频繁，模型快速学到确定性规律                  |
| **模型输出稳定性**           | 各类别曲线波动明显                 | 几乎无波动（平稳收敛）                        | 说明 MOA 已“锁死”在某种稳定对手模式上（例如持续捕兔）               |
| **泛化性与动态性**           | 有探索空间，仍在学习不同合作动作          | 模型早早收敛，缺乏变化                        | 反映 NH 对手行为单一，MOA 只需学习一个固定分布即可                |

---

### 🧠 解释与推论

| 层面                    | 观察               | 含义                              |
| --------------------- | ---------------- | ------------------------------- |
| **CE Loss 显著降低**      | 混合场景下 Loss < 0.1 | 对手行为不再复杂，模型只需识别有限几类动作（主要是捕兔）即可  |
| **类别 6 (猎兔) 准确率 1.0** | 模型几乎“完美”预测       | NH 对手高度确定性，完全放弃合作行为             |
| **类别 5 (猎鹿) 准确率 0**   | 无捕鹿样本            | NS 对手稀少（仅1名），其合作行为在全局分布中被稀释     |
| **类别 4 (原地) 准确率 0**   | 无此动作样本           | 所有对手均保持持续运动或捕猎行为                |
| **高精度但低多样性**          | 各移动类动作准确率均>0.9   | 说明 MOA 学到的不是“预测复杂策略”，而是识别“固定模式” |
| **稳定但无学习弹性**          | 后期所有指标平滑、无波动     | 模型失去探索动力，环境策略空间变得确定（无交互复杂性）     |

---

### 📈 与 Reward 结果的关系（从行为到奖励的因果链）

| 现象           | MOA 侧表现            | 奖励曲线侧反映          | 解释                         |
| ------------ | ------------------ | ---------------- | -------------------------- |
| 对手策略趋于固定（捕兔） | CE loss 极低，acc≈1   | reward 曲线平稳但略低   | 对手容易预测，但捕鹿收益减少             |
| MOA 准确率极高    | 0.9–1.0 稳定         | agent 仍需探索最优反应   | agent 学到“预测谁会捕兔”，但合作策略空间变窄 |
| 无捕鹿行为        | per_class_acc(5)=0 | 群体平均reward ≈ 2.9 | 团体未形成合作收益上限，仅维持中等回报        |
| 对手多样性下降      | 曲线波动消失             | reward 起伏变少      | 博弈复杂度下降 → 策略学习更稳但潜力受限      |

---

### ✅ 综合结论（用于报告或论文小节）

> 在 1NS–2NH 的混合博弈场景中，MOA 模块的预测精度显著提升，交叉熵损失迅速下降至 0.1 以下，各动作类别准确率稳定在 0.9 以上。特别是捕兔动作（action 6）预测准确率接近 100%，说明模型能够完全识别非合作型对手的行为模式。然而，由于环境中捕鹿行为（action 5）样本稀缺，MOA 在该类动作上几乎没有学习信号，导致模型过早收敛于单一策略分布。
> 从宏观上看，MOA 模型虽然在混合场景下更“确定”与“稳定”，但这种高确定性对应的是行为多样性的丧失：智能体只需预测对手的确定性动作即可，不再具备复杂策略适应与博弈推理的能力。这与奖励曲线中表现出的“平稳但不高”的收益相一致，说明策略系统已进入**低复杂度均衡状态**。

---

是否希望我帮你把上面的表格和文字整合成论文“4.3.2 MOA 模块性能对比分析”格式？（包括表号、图号引用、结论语句，适合直接放进章节中）





---


# 《论文写作指导》小作业改动


论文框架图







