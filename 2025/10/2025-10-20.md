[[HOP+迁移至cleanrl]]


# Objectives

* **定位问题根源**：针对 HOP+ 迁移至 CleanRL 框架后无法稳定训练出合作策略的现象，本周的主要目标是通过逐模块排查的方式，明确问题究竟出在 VAE、world model、MOA 模块，还是在整体策略训练逻辑。
* **验证 VAE 表现**：通过对比原始 VAE 与新版 VAE 的 loss 曲线，检验不同 loss 设计（重建项 vs. MOA loss）对模型训练稳定性和表征能力的影响，确认 VAE 是否在信息压缩和动作预测中存在缺陷。
* **评估 world_model 能力**：重点分析 world_model 在状态预测和奖励建模中的精度，观察其是否能为策略提供可靠的前瞻性信息，并判断异常预测（如 agent “跳动”）是否会干扰训练。
* **检查 MOA 模块训练效果**：通过混淆矩阵、动作分布和召回率指标，评估 MOA 在多类动作上的预测性能，分析是否由于离线数据分布偏差或 buffer 逻辑设计，导致模型在自博弈训练中失效。
* **建立 baseline 对照**：结合 PPO baseline 与 HOP+（含 MCTS/不含 MCTS）实验结果，形成性能对比，进一步缩小问题来源，为后续改进提供明确方向。





## Problem1: VAE
- [?] 

### 原来的VAE: loss图: 总体loss很低
4p5b8\*8 
```terminal
D:\BaiduNetdiskDownload\msh-records\4p5b8_8\vae\exp20250401_203634p\dim16
```
![[myplot_4p_16dim.png]]

### moa-VAE：loss图： loss比较大
4p5b8\*8
```
D:\cleanrl\cleanRL\vae_ckpts\vae__1758371809\dim16
```

![[vae_loss_curve.png]]


### moa情况
总体保持稳定，也就是moa预测0.6左右

#### 混淆矩阵：对角线越深越好
![[moa_eval_epoch9_confusion_matrix.png]]

#### 召回率：整体动作准确率

$$
\text{Recall}_i = \frac{\text{预测正确为 i 的样本数}}{\text{真实标签为 i 的总样本数}}
$$

也就是：在所有 **真实为某类** 的样本里，模型有多少被成功识别出来。

![[moa_eval_epoch9_per_class_recall.png]]



## Problem：World_model
- [?] 

![[Pasted image 20250921015931.png]]

在上面这张图的例子中，world_model预测的agent一下子走了两个，并且取得了0.042的奖励。
>不符合物理规律 —————— 联想到：对比学习促进强化学习的那篇文章。

world_model总体预测的还是大体上位置正确，但是做不到非常精准.




- [?] 看一下具体channel 中具体的值是多少，而不是直接四舍五入











### 2_Answers

## Problem： moa 的训练效果不好

### 效果图：
>下面是moa模块的训练情况分析：在最后的收敛中，在7个动作中，除了动作4准确度25%左右，其他的动作预测准确度都降到了0。
>推测出现这一情况的原因是（结合之前训练vae出现的情况），得到的训练数据中，由于这个环境中对手捕猎后就停止了，所以动作会被默认设置为0.当对手补完猎物后，会和猎物一起消失，但是此时agent可能仍然需要预测对手的动作，这当然都是4了。
>但是奇怪的是，在长期的训练过程中，top-1大概是15%（这当中还可能大部分都是静止的动作）而ce_loss也是在10左右不下降。所以说moa这个模块的训练效果就很差。


![[Pasted image 20250921233945.png|1175]]


### 原本moa逻辑

一句话总结：**MOA buffer 是以“对手在局中实际经历过的轨迹”为存储对象，前提是这段轨迹获得过奖励；对手退出后就不会再有数据进入 buffer，因此不会用到退出后的信息。**


由代码可知的：
```python
 @override(Policy)
    def postprocess_trajectory(
        self, sample_batch, other_agent_batches=None, episode=None
    ):
        start_time = tm()

        for i in range(self.player_num-1):
            model_id=self.model_id_list[i]
            place_id=(model_id+1-self.my_id)%self.player_num
            if episode._agent_reward_history[f'player_{model_id+1}'][-1]>0:
                total_time=other_agent_batches[f'player_{model_id+1}'][1]['actions'].shape[0]
                self.model_buffer[i][0].append(other_agent_batches[f'player_{model_id+1}'][1]['obs'].copy())
                self.model_buffer[i][1].append(other_agent_batches[f'player_{model_id+1}'][1]['actions'].copy())
                self.model_buffer[i][2].append(np.array(range(total_time)))

                if episode._agent_to_last_action[f'player_{model_id+1}']==5:
                    #self.old_model_buffer[i].extend(episode.user_data[f"s_a_pair{model_id+1}"])
                    self.model_buffer[i][3].extend([0]*total_time) # 0 for stag

                    self.discounted_stag_times[place_id]=self.discount_factor*self.discounted_stag_times[place_id]+1
                    self.discounted_hare_times[place_id]=self.discount_factor*self.discounted_hare_times[place_id]

                elif episode._agent_to_last_action[f'player_{model_id+1}']==6:
                    '''
                    for pair in episode.user_data[f"s_a_pair{model_id+1}"]:
                        pair[-1]=1
                    self.old_model_buffer[i].extend(episode.user_data[f"s_a_pair{model_id+1}"])
                    '''
                    self.model_buffer[i][3].extend([1]*total_time) # 1 for hare

                    self.discounted_stag_times[place_id]=self.discount_factor*self.discounted_stag_times[place_id]
                    self.discounted_hare_times[place_id]=self.discount_factor*self.discounted_hare_times[place_id]+1

                else:
                    pass
                    #raise ValueError('No way!!!')

                # if len(self.model_buffer[i])>self.moa_buffer_capacity:
                if len(self.model_buffer[i][3])>self.moa_buffer_capacity:
                    if self.my_id == 1:
                        self.moa_update(i)###moa_update
                    else:
                        self.model_buffer[i]=[[],[],[],[]]
        # add mcts policies to sample batch
        sample_batch["mcts_policies"] = np.array(episode.user_data[f"mcts_policies{self.my_id}"])[
            sample_batch["t"]
        ]
        # final episode reward corresponds to the value (if not discounted)
        # for all transitions in episode
        final_reward = sample_batch["rewards"][-1]

        # if r2 is enabled, then add the reward to the buffer and normalize it
        if self.r2_enable:
            self.r2_buffer.add_reward(final_reward)
            final_reward = self.r2_buffer.normalize(final_reward)

        # sample_batch["value_label"] = final_reward * np.ones_like(sample_batch["t"])
        sample_batch["value_label"] = final_reward * (self.gamma**np.array(sample_batch["t"][::-1]))
        train_duration = tm() - start_time  # ⏱️ 计算耗时
        print(f"[Train] progress time  耗时: {train_duration:.4f} 秒")
        return sample_batch
```


```ad-info
## 1. sample\_batch 的长度

* 在 RLlib 中，每个 agent 的 `sample_batch` **长度不一定相同**。
* 具体长度由该 agent 在本次采样中经历的步数决定：

  
## 2. 关于退出后的数据

* RLlib 在 agent done 后不再生成 `obs/actions`，所以 `other_agent_batches` 不包含退出后的轨迹。
* 你的代码又完全依赖 `total_time = actions.shape[0]` 来存储，因此不会人工补齐退出后的部分。
* 结论：**moa\_buffer 只记录对手在场时的轨迹，不会包含任何退出后的数据。**

```




### 思考
>离线训练时的moa效果还不错是吧？但是为什么hopplus真正训练时效果却非常差呢？一开始效果差可以理解，因为直接加载的moa是使用的离线的rule-based 的离线轨迹数据去拟合的，而且基本上是训练成了过拟合的样子，所以说在hopplus的self-play中一开始预测准确率不高也是正常的。但是随着训练的进程，moa似乎没有很好地利用到离线数据去训练（我们刚才认为可能是因为训练数据中动作4的占比太大，但是在离线数据中4的占比也很大，但是训练的效果并不差，所以需要分析一下moa的训练过程是否有问题）




# 实验结果



平均 `reward` 2.6-2.7 
![[Pasted image 20250929132236.png]]


## HOP+-MCTS
![[Pasted image 20250929132441.png]]
![[Pasted image 20250929140820.png]]


## HOP+
![[Pasted image 20250929132813.png]]


moa_buffer改进前：

![[Pasted image 20250921233945.png|925]]

moa_buffer改进后： 
![[Pasted image 20250929132909.png]]







---



## ToDo


* **单智能体 + 3 个 rule-based 对照实验**
  >如果对手也是学习策略，MOA 的预测可能受震荡或训练逻辑影响，难以判断真实效果；而对 rule-based，对应动作模式更稳定（尤其捕鹿与捕兔差异明显），更容易验证 MOA 的区分能力。

* **PPO Baseline 多随机种子实验**
  >之前出现 player1 学得更好而其他较差的现象，可能是种子固定导致的偶然性。需更换 `random.seed / np.random.seed / torch.manual_seed` 并多次运行，确认结果是否稳健。

* **world_model 预测可视化改进**
  >当前可视化用四舍五入可能掩盖了细节误差（如 0.4 被当成 0，导致 agent“消失”），需直接检查各通道连续值，避免误判模型精度。


本阶段的实验主要是针对 **reward 长期维持较低** 的问题进行系统排查。我们按照模块（VAE、world_model、MOA、buffer 机制等）逐步验证其效果，虽然已经发现并修复了部分问题（例如 **moa_buffer 的逻辑缺陷**），但在完整的 **HOP+ 版本** 中，最终的 reward 依然没有显著提升。
同时，**HOP-MCTS 版本**虽然一度表现较好，但出现了 reward 先升后降的现象。
因此，下一步的重点将放在对 MCTS 版本进行深入分析，优先寻找其`训练`或者`数据搜集`原因




# Limitations
# Future Work
# FootNotes