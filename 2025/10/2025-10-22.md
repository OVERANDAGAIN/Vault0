


### ✅ Slide 1: **Title Slide** *(\~0.5 min)*

**Slide Title**:
**HOP+**
*A Goal-Inference-Based Opponent Modeling Framework for Mixed-Motive Multi-Agent Settings*

**Content**:

* **Presenter**: \$$Your Name$$
* **Affiliation**: \$$University / Institute Name$$
* **Date**: \$$Presentation Date$$

**Script (what to say)**:

> Good afternoon. Today I will present our recent work titled **HOP+**, which focuses on enhancing opponent modeling and planning in multi-agent mixed-motive environments. This work builds upon the HOP framework proposed at ICML 2024.

---

### ✅ Slide 2: **Background – From HOP to HOP+** *(\~1.5 min)*

**Slide Title**:
**From HOP to HOP+: Motivation and Foundation**

**Slide Content**:

**What is HOP?**

* HOP: *Hierarchical Opponent Modeling and Planning (ICML 2024)
* Designed for **mixed-motive multi-agent games**

 **Core Components of HOP ( Inference + Decision Making )**:

1. **Opponent Modeling Module**
   * Infers latent goals of other agents
2. **Planning Module (MCTS-based)**
   * Computes best-response strategies given inferred goals

 **Why HOP Matters**:

* Effective performance in **self-play** and cooperative tasks
* Strong **few-shot adaptation** to unseen opponents

**Script (what to say)**:

> Our work extends the HOP framework, which models agent behavior in mixed-motive settings by separating goal inference and planning. HOP achieves this through a hierarchical architecture where opponent modeling guides the planning module. However, the original HOP uses a limited goal space and static inference mechanisms. Our work aims to push this further.

---

需要图示建议？

你可以在第 2 页加一张简图：
🟢 “Agent Interaction → Opponent Modeling → Goal → MCTS Planning → Action” 这样一个流程箭头图，清晰传达 HOP 的层次结构。

---

是否继续为你写第 3 部分的英文版 PPT？或者需要我生成一份完整可导出的 PowerPoint 文件？


好的，以下是第 **3 部分**（Problem Setting & Environment）的英文 PPT 内容和讲稿建议：

---

### ✅ Slide 3: **Problem Setting & Environment** *(\~2 min)*

**Slide Title**:
**Problem Setting: Mixed-Motive Multi-Agent Interaction**

**Slide Content**:

🎯 **Objective**

* Learn **adaptive agent strategies** in **dynamic, mixed-motive games**
* Enable agents to **model unseen opponents** and **respond effectively**

🧩 **Key Challenges**:

* 🤝 + ⚔️ Mixed motives: agents need to **cooperate** while **competing**
* ⛓️ Dynamic strategies → **non-stationary environments**
* ❓ How to infer opponents’ intent and respond in **few interactions**

🌍 **Test Environment** – *Stag-Hunt Game*

* Grid-based world (6×6, 8×8)
* Agents decide to **hunt individually** (hare) or **coordinate** (stag)
* Rewards depend on **joint actions** and **opponent behavior**

**Diagram** (recommended):

* A **grid world** image showing:

  * Two agents
  * Hares and stags
  * Decision outcomes: solo vs. cooperative




很好，以下是第 **4 部分（Motivation: Why Goal Inference?）** 的 PPT 内容和讲稿建议（英文）：

---

### ✅ Slide 4: **Motivation: Why Goal Inference?** *(\~2 min)*

**Slide Title**:
**Motivation: Limitations of Manual Goal Design**

**Slide Content**:

📌 **Problems with Prior Approaches**

*  **Static goal sets**:
	* Predefined discrete goals are **inflexible**
	* fail to capture **diverse strategies**
* **Manual annotation required**:
	* Goal semantics often need to be designed by hand
	* Handcrafted goals don’t scale well to **unseen opponents**

---
💡 **Our Insight**
>  **infer goals automatically** from interaction trajectories.


🎯 **Benefits of Goal Inference**:

* ✅ **No manual goal engineering**
* ✅ **Continuous latent goal space**
* ✅ **Adaptation to diverse behaviors**
* ✅ **Improved interpretability & planning**

**Optional visual (if space allows)**:

* Diagram comparing:

  * "Manual goal set" (discrete, rigid)
  * "Learned goal representation" (continuous, flexible)

---

**Script (what to say)**:

> In previous opponent modeling frameworks, agents typically rely on a **static set of predefined goals**. These goals are manually constructed, often with limited expressiveness, and fail to adapt when the opponent behaves in unexpected ways.

> Such methods **do not generalize well** to heterogeneous opponents, and they require **significant human effort** to design appropriate goal spaces for each environment.

> To address this, our idea is to **infer latent goals directly** from trajectory data using deep generative modeling. By doing this, we can capture a **continuous and adaptive** goal space that evolves with interaction and requires **no manual labeling**. This provides a more powerful foundation for downstream planning and policy learning.

---

下一部分将是 **5.1 - How: Goal Inference** 的实现细节介绍。如果你准备好了，我可以继续为你撰写该页内容。



明白了！下面是**简洁、清晰、答辩适配版的 Slide 3 英文内容**，保持专业表达、图文结合、重点突出（适合6分钟讲稿场景）：

---

### **Slide 3: Problem & Environment**

---

**Goal**
Enable agents to adapt in **mixed-motive** multi-agent games
→ Must **infer opponent intent** and **respond effectively**

**Challenges**

* Mixed incentives: **cooperation** vs. **competition**
* Opponents behave **dynamically**, non-stationary
* Limited interactions for **adaptation**

**Environment: Stag-Hunt Game** —— **MSH**

* Grid-based map (6×6 or 8×8)
* Choose:
  * 🐇 Hunt hare (solo, low reward)
  * 🦌 Hunt stag (team up, high reward)
* Reward depends on **joint actions**

---

**📊 Visual (on right)**

* Grid world with:

  * Agents, hare, stag
  * Outcome arrows: solo hunt vs. team hunt

---

需要我帮你做这个图吗？或者我们继续到 Slide 4: **Motivation**？



好的，以下是简洁、专业风格的英文版 Slide 3 内容，**去掉所有 icon 表情符号，减少文字量，突出核心要点**，适合答辩 PPT：

---

### **Slide 3: Problem Setting and Environment**

---

**Objective**
Learn adaptive strategies in mixed-motive multi-agent environments.
Agents must model diverse opponents and respond effectively under uncertainty.

---

**Key Challenges**

* Mixed incentives: agents must balance cooperation and competition
* Dynamic interactions lead to non-stationary opponent behaviors
* Limited interactions require fast adaptation to unseen strategies

---

**Test Environment: Stag-Hunt Game**

* Grid-based world (6×6 or 8×8)
* Agents choose to hunt hare (individually) or stag (cooperatively)
* Rewards depend on joint actions and mutual coordination

---

**(Optional Visual)**
A simple grid-world diagram showing two agents, one hare, one stag, and outcome labels:

* Solo action → lower reward
* Coordinated action → higher reward

---

如果你需要，我可以帮你生成这个图。是否继续到 Slide 4: Motivation？




非常好的问题，而且问得非常关键。
对于 **HOP+ 项目**（Hierarchical Opponent-Modeling & Planning Plus），“**formulation**” 是整篇论文或汇报中最能体现算法思想与理论深度的部分。
这一部分不能只是写“我们用了一个VAE+MOA的结构”，而要通过**数学建模**把“我们想解决的问题”“我们的结构逻辑”“我们的优化目标”都系统地表达出来。

我来一步步带你拆解：

---

## 🧭 一、Formulation 的总体思路

HOP+ 的 formulation 可以从 **三条主线**来组织：

| 主线                                            | 描述                | 数学建模重点                                                                                    |
| --------------------------------------------- | ----------------- | ----------------------------------------------------------------------------------------- |
| **1. 博弈过程建模（Game Formulation）**               | 描述多智能体交互的总体问题     | 状态、动作、奖励、转移、目标函数                                                                          |
| **2. 对手建模结构（Opponent Modeling Formulation）**  | 描述如何建模对手的隐变量或策略表征 | 隐变量 $z$、编码器$q_\phi(zs,a)$、预测器$\pi_\theta(as,z)$                                           |
| **3. 联合优化目标（Joint Optimization Formulation）** | 描述整个系统的联合学习目标     | $\mathcal{L} = \mathcal{L}*{policy} + \alpha \mathcal{L}*{VAE} + \beta \mathcal{L}_{MOA}$ |

---

## 🧩 二、Formulation 的分层结构建议（HOP+专用）

下面是 HOP+ 非常合适的 formulation 框架（你可以在论文或PPT中直接按这个逻辑组织）：

---

### **2.1 Multi-Agent Environment Formulation**

我们首先把交互过程形式化为一个 **部分可观测的多智能体马尔可夫博弈**（POMG）：

$$
\mathcal{G} = \langle \mathcal{S}, {\mathcal{A}*i}*{i=1}^N, {\mathcal{O}*i}*{i=1}^N, P, {r_i}_{i=1}^N, \gamma \rangle
$$

其中：

* $\mathcal{S}$：全局状态空间
* $\mathcal{A}_i$：智能体 $i$ 的动作空间
* $\mathcal{O}_i$：观测空间（HOP+中为局部网格视野）
* $P(s'|s, a_1,\dots,a_N)$：状态转移函数
* $r_i(s,a_1,\dots,a_N)$：个体奖励函数
* $\gamma$：折扣因子

每个智能体的目标是最大化自己的期望累计回报：
$$
J_i(\pi_i, \pi_{-i}) = \mathbb{E}\left$$\sum_{t=0}^T \gamma^t r_i(s_t, a_t^1, \dots, a_t^N) \right$$
$$





---

### **2.2 Opponent Modeling Formulation**

在 HOP+ 中，核心创新是引入了一个 **隐变量 $z_t$** 表征对手的潜在行为模式。
我们使用一个 **变分自编码器（VAE）** 来捕获这种潜在结构：

* 编码器：$q_\phi(z_t|s_t, a_t)$
* 先验分布：$p(z_t) = \mathcal{N}(0,I)$
* 解码器/策略：$\pi_\theta(a_t|s_t, z_t)$

VAE 优化目标：
$$
\mathcal{L}*{\text{VAE}} =
\mathbb{E}*{q_\phi(z_t|s_t,a_t)}$$-\log p_\theta(a_t|s_t,z_t)$$

* D_{\text{KL}}(q_\phi(z_t|s_t,a_t) \Vert p(z_t))
  $$

这样，$z_t$ 就成为一个可学习的对手特征嵌入，用于增强策略网络的感知能力。

---

### **2.3 Policy Learning Formulation**

在策略层面，HOP+ 仍然是基于强化学习（如 PPO）的优化：
$$
\mathcal{L}*{\text{policy}} =
\mathbb{E}*{t}\left$$-\log \pi_\theta(a_t|s_t, z_t) A_t

* \beta D_{\text{KL}}(\pi_\theta(\cdot|s_t,z_t)\Vert \pi_{\theta_{\text{old}}}(\cdot|s_t,z_t))
  \right$$
  $$

其中 $A_t$ 为优势函数，$\beta$ 为 KL 正则系数。

---

### **2.4 MOA Formulation（Model-of-Agents）**

HOP+ 的另一个核心部分是 **MOA（对手行为预测模块）**。
它的目标是从历史轨迹中预测对手的未来动作：

$$
\mathcal{L}*{\text{MOA}} =
\mathbb{E}*{(s_t, a_t^{-i})\sim D}
\left$$-\log p_\psi(a_t^{-i}|s_t, z_t)\right$$
$$

这一项鼓励模型在潜空间中学到“对手如何决策”的可预测结构。

---

### **2.5 Joint Optimization Objective**

最终，HOP+ 的整体优化目标为：

$$
\min_{\theta, \phi, \psi}
\mathcal{L}*{\text{total}} =
\mathcal{L}*{\text{policy}}

* \alpha \mathcal{L}_{\text{VAE}}
* \beta \mathcal{L}_{\text{MOA}}
  $$

其中：

* $\mathcal{L}_{\text{policy}}$ —— 主策略更新项
* $\mathcal{L}_{\text{VAE}}$ —— 对手行为嵌入学习项
* $\mathcal{L}_{\text{MOA}}$ —— 对手动作预测项
* $\alpha, \beta$ 为权重系数（通常通过实验调节）

---



## 📚 四、总结

所以，HOP+ 的 formulation 可以总结为一句话：

> **我们将多智能体博弈形式化为部分可观测马尔可夫博弈，通过引入潜变量 $z_t$ 来表征对手行为分布，并在统一的优化框架下联合训练策略网络、对手建模网络与潜空间生成器。**





非常好，这个要求非常专业。你的意思是：希望我用**正式英文PPT写作风格**撰写每页的内容，而不是那种“AI风格的符号堆叠”（如→、→、—），整体语言要自然、有逻辑、适合实际展示。
下面我给出**两页英文PPT版本**（浓缩后的人类协作 → AI社会智能 → 混合动机博弈 → 适应性挑战），每页包含：

* 🎯 标题（可直接放PPT标题栏）
* 📋 主体内容（完整英文要点）
* 💡 讲解提示（中文说明，帮你理解语气与节奏）
* 📚 页脚参考文献建议

---

## **Slide 1 — Motivation and Background**

### **Title:**

**From Human Cooperation to Intelligent Collaboration**

### **Main Content (PPT Text):**

* Cooperation has always been a cornerstone of human society.
  From daily activities such as driving and teamwork to global issues like climate governance or pandemic control, effective collaboration determines collective success.

* Yet, achieving cooperation is inherently difficult.
  Mismatched information, conflicting interests, and lack of trust often prevent individuals from reaching mutually beneficial outc omes.

* The progress of human civilization largely depends on our capacity to cooperate under uncertainty and conflicting goals.

* As Artificial Intelligence (AI) systems increasingly participate in complex social and industrial processes, a new challenge emerges:
  how to enable intelligent agents not only to **assist human cooperation** but also to **cooperate among themselves**.

* This demands **social intelligence** — the ability to understand others, predict intentions, and adapt strategies in interactive settings.


---

## **Slide 2 — Problem Statement**

### **Title:**

**Challenges of Adaptation in Mixed-Motive Multi-Agent Games**

### **Main Content (PPT Text):**

* Multi-Agent Reinforcement Learning (MARL) provides a framework for agents to learn interaction strategies through experience.
  However, most real-world scenarios involve **mixed motives**, where agents simultaneously face opportunities for cooperation and competition.

* In such environments, an agent’s optimal policy is highly dependent on the behaviors of others, leading to **non-stationary dynamics** and unstable learning.

* When encountering unfamiliar or evolving opponents, an agent must rapidly infer their underlying intentions to respond effectively.
  Failing to do so can result in lost cooperative opportunities or adversarial escalation.

* This gives rise to the central research question:
  **How can intelligent agents achieve rapid and robust adaptation in mixed-motive environments?**

### **Presenter Notes (中文说明):**

这页是问题定义页，用正式的“问题→挑战→研究问题”结构。讲解时可以分三步：

1. 简述MARL框架；
2. 说明mixed-motive的复杂性（合作与竞争并存）；
3. 最后提出你的核心研究问题，用略慢语速读出最后一句，形成“问题落点”。

### **Suggested Visuals:**

* Diagram or payoff matrix of the *Stag Hunt* game (showing cooperation vs. defection).
* A small curve showing performance drop under opponent shift, labeled as “Need for Adaptation”.

### **References (small footer):**

*(Leibo et al., 2017; Foerster et al., 2018; Yang et al., 2020)*

---

## 📘 Summary of Tone & Style

| Section    | Writing Style                                | Function                |
| ---------- | -------------------------------------------- | ----------------------- |
| Titles     | concise, declarative (no symbols, no arrows) | quickly orient audience |
| Paragraphs | complete, natural English sentences          | readable & academic     |
| Keywords   | **bold** instead of arrows                   | visual emphasis         |
| Visuals    | meaningful but minimal                       | supports narrative      |

---

是否希望我接着帮你写 **Slide 3**（即从“Problem Statement”过渡到“Research Objective / Our Approach”）？
那一页可以正式引出 *HOP+ framework* 的动机与思路，用类似的英文风格来写。


你说得对：**adaptation 在你的叙述中是一个能力/指标，类似 generalization/robustness，而非达成合作的必要条件**。下面给你一套**两页英文PPT脚本**，前后衔接顺畅，第二页把 Few-Shot Adaptation 明确为“能力与评估维度”，并自然过渡到后续方法部分。中文只放在讲解提示里；正文全部英文；不使用箭头/破折号。

---

## Slide 1 — Why Cooperation Matters in Complex Systems

**Title**
Cooperation in Complex Societies: Background and Challenges

**Main Content (PPT text)**

* Cooperation underpins outcomes from everyday coordination to global governance. Collective success often depends on aligning heterogeneous goals across agents and institutions.
* Achieving cooperation is difficult in practice. Information asymmetry, strategic uncertainty, and conflicting incentives frequently lead to suboptimal equilibria even when mutual benefit exists.
* As AI systems increasingly participate in social and industrial processes, intelligent agents must operate in interactive settings where others learn and change over time. This creates additional strategic uncertainty beyond static optimization.

**Suggested visuals**
A small matrix or schematic illustrating a cooperation dilemma (e.g., Stag Hunt/PD) paired with real-world examples (traffic merging, scheduling, climate cooperation).

**Footer references (small)**
Axelrod (1981); Ostrom (1990)

**Presenter notes（中文）**
这一页只立背景：协作重要但困难；AI 介入让交互更具不确定性。不要提前谈RL或方法，把技术细节留到下一页。

---

## Slide 2 — Few-Shot Adaptation as a Core Capability and Metric

**Title**
Few-Shot Adaptation in Mixed-Motive Multi-Agent Settings

**Main Content (PPT text)**

* Many real scenarios are mixed-motive: cooperation opportunities coexist with incentive conflicts. Optimal behavior depends on others’ strategies, which may be unfamiliar or shifting.
* **Few-Shot Adaptation**: the capability to infer opponents’ intentions and adjust one’s policy effectively within limited interactions.
* We treat adaptation as an evaluative dimension alongside generalization and robustness. An agent that adapts quickly can recover performance under opponent shift and preserve opportunities for cooperation when they arise.
* **Problem statement**
  How can an agent learn representations and models that support reliable few-shot adaptation to new or evolving opponents in mixed-motive environments?

**Suggested visuals**
Left: a short learning curve showing rapid recovery after an opponent change. Right: a compact checklist of evaluation axes: Generalization, Robustness, Few-Shot Adaptation.

**Footer references (small)**
Leibo et al. (2017); Foerster et al. (2018); Rabinowitz et al. (2018)

**Presenter notes（中文）**
明确“Few-Shot Adaptation 是能力与指标”，并提出研究问题。最后一句话自然引出后续内容：“接下来介绍我们如何通过表征学习、对手建模与联合目标来支撑这种适应能力。”

---

需要的话，我可以把这两页再接一页**“Research Objective & Approach Preview”**，用一句话目标 + 三个要点（representation, opponent modeling, unified training objective）做成过渡页，紧贴你的 Methodology。


好的，下面是经过重新组织、专门为正式英文PPT撰写的两页内容。
每页都控制在**可展示的文字量**（6–8行主体文本），保持逻辑连贯、语言正式、节奏自然，适合用于开场背景部分。

---

## **Slide 1 — Cooperation in Complex Environments**

**Title:**
**Why Cooperation Remains a Central Challenge**

**Main Content (PPT text):**

* Cooperation enables collective success in human and artificial systems, from daily coordination to global governance.
* Despite shared benefits, individuals often fail to cooperate due to incomplete information, mistrust, or conflicting incentives.
* These dilemmas highlight the difficulty of aligning goals in uncertain and dynamic environments.
* As AI agents increasingly interact with each other, they encounter the same tension between cooperation and competition.
* Understanding how cooperation emerges and can be sustained becomes a key research direction for intelligent multi-agent systems.

**Visual Suggestion:**
A simple *Stag Hunt* or *Prisoner’s Dilemma* matrix beside real-world cooperation examples (e.g., driving, resource allocation).

**Footer Reference:**
Axelrod (1981); Ostrom (1990); Leibo et al. (2017)

---

## **Slide 2 — Few-Shot Adaptation in Multi-Agent Interaction**

**Title:**
**Adaptation as a Measure of Social Intelligence**

**Main Content (PPT text):**

* Real multi-agent environments are mixed-motive: agents face both opportunities for cooperation and potential conflicts.
* In such settings, strategies are interdependent and constantly evolving, producing strong non-stationarity.
* *Few-Shot Adaptation* refers to an agent’s ability to infer others’ intentions and adjust its behavior effectively within a small number of interactions.
* It serves as a core metric of social intelligence, reflecting how well an agent maintains performance under changing opponents.
* Our work focuses on learning representations and opponent models that enhance this adaptive capability in mixed-motive games.

**Visual Suggestion:**
Left – curve showing quick recovery after opponent shift;
Right – three labeled boxes: *Generalization | Robustness | Few-Shot Adaptation*.

**Footer Reference:**
Foerster et al. (2018); Rabinowitz et al. (2018); Yang et al. (2020)

---

### 💡结构逻辑说明（中文）

* **Slide 1**：提出“合作困境”的现实与普遍性，引入AI系统中同样的问题。
* **Slide 2**：从复杂博弈转向“适应能力”这一研究指标，明确Few-Shot Adaptation的定义与研究目标。
  这两页衔接自然，为后续 Methodology 部分（对手建模与联合优化）奠定语义和逻辑基础。

非常好的问题 🎯
你现在的汇报逻辑到这里，其实正处在整个PPT的“**关键转折点**”：
前面两页讲清楚了

> “为什么合作难 + 为什么需要适应 + 为什么对手建模重要”，
> 现在就要自然过渡到
> “我们如何去刻画这种交互问题，并在其中实现学习（也就是强化学习和建模部分）”。

下面我帮你拆解这一页（或两页）**的过渡逻辑 + 具体写法建议（含PPT文案）**。

---

## 🎯 一、逻辑路径（为什么此时引入RL）

你要回答听众心中的下一个问题：

> “既然合作和适应都很重要，我们如何在算法层面去实现它？”

自然的回答是：

> “我们使用多智能体强化学习（MARL）框架来刻画这种博弈式交互，并在其中嵌入对手建模机制，以提升适应能力。”

也就是说，**强化学习是建模交互的语言**，**Formulation 是数学表达的起点**。
因此可以分两步过渡：

1. **先引入 MARL 作为描述智能体交互的建模框架（语言）**
2. **再通过 Formulation 给出数学形式，定义状态、动作、奖励等基础符号**

---

## 🧩 二、推荐结构：Slide 3 + Slide 4

### **Slide 3 — Multi-Agent Reinforcement Learning Framework**

**Title:**
**Modeling Interactive Decision-Making**

**Main Content (PPT text):**

* To study cooperation and adaptation quantitatively, we adopt the framework of **Multi-Agent Reinforcement Learning (MARL)**.
* In MARL, each agent interacts with an environment and other agents, learning a policy that maximizes its long-term return.
* The environment is formalized as a **Partially Observable Markov Game (POMG)**:
  $$\mathcal{G} = \langle \mathcal{S}, {\mathcal{A}_i}, {\mathcal{O}_i}, P, {r_i}, \gamma \rangle$$
* Each agent seeks to maximize:
  $$J_i = \mathbb{E} \Big$$\sum_t \gamma^t r_i(s_t, a_t^1, \dots, a_t^N)\Big$$$$

**Visual Suggestion:**
Diagram showing several agents interacting within an environment, each observing part of the state and taking actions.

**Footer Reference:**
Littman (1994); Leibo et al. (2017)

**Presenter Notes（中文）**
这一页目的是“建模语言化”：让听众看到我们用RL去表达合作与竞争的过程。
公式不必讲细，只强调这是我们刻画交互与学习的基础。

---

### **Slide 4 — From Modeling to Learning Objectives**

**Title:**
**From Game Formulation to Learning Adaptable Policies**

**Main Content (PPT text):**

* The MARL formulation provides the foundation to describe interactions, but not all agents can adapt efficiently.
* To enable rapid behavioral adjustment, we integrate **opponent modeling** and **representation learning** within the RL process.
* The goal is to learn a policy ( \pi_\theta(a|s,z) ) that not only maximizes expected rewards but also utilizes a latent representation ( z ) capturing opponents’ strategies.
* This leads to a joint optimization objective combining:

  * **Policy learning**
  * **Opponent modeling**
  * **Adaptation efficiency**

**Visual Suggestion:**
Show a simple schematic:
“Environment ↔ Agents ↔ Opponent Model (latent z) → Policy Update”

**Footer Reference:**
Foerster et al. (2018); Rabinowitz et al. (2018)

**Presenter Notes（中文）**
这一页是从RL到你的**方法框架（HOP+）**的过渡页。
不需要马上展开细节，只要埋下概念：“我们不仅学policy，还要学别人怎么想”，
为下一章 Methodology / Formulation 打下逻辑地基。

---

## 🧠 三、整体过渡逻辑（连起来看）

| 顺序      | 核心主题                                 | 逻辑目的            |
| ------- | ------------------------------------ | --------------- |
| Slide 1 | Cooperation as a social foundation   | 提出问题：合作的重要性     |
| Slide 2 | Few-Shot Adaptation & modeling       | 提出核心能力与挑战       |
| Slide 3 | Multi-Agent Reinforcement Learning   | 提出建模语言与数学框架     |
| Slide 4 | From Modeling to Learning Objectives | 自然过渡到方法设计（HOP+） |

---

## ✨ 四、总结写法建议

* **不要直接跳进RL算法细节**（如PPO、policy gradient），而是讲“为什么需要用RL来表达交互”。
* **Formulation页**只保留最核心两个公式（环境定义 + 目标函数），不要堆式列。
* **过渡句式**可口语化一点：

  > “To operationalize these ideas, we describe the multi-agent interaction using the reinforcement learning framework.”
  > “This allows us to define what agents observe, decide, and optimize, before introducing our method.”

---

是否希望我接着帮你写 **下一页（Slide 5）**，即从这部分正式进入你的 *HOP+ Framework / Joint Formulation*（带公式 + 目标函数解释）？





### 基于奖励设计（reward shaping）的算法

**合作问题：如何在混合动机博弈中实现合作？**

最直接的方法——通过奖励设计（reward shaping），改变博弈范式，使合作成为个体最优策略。

 亲社会（pro-social）收益
 基于不公平厌恶（Inequity-aversion）的收益
 基于自己行为对别人影响的收益


图中文字识别如下：

---

### 基于奖励设计（reward shaping）的算法

**合作问题：如何在混合动机博弈中实现合作？**

最直接的方法——通过奖励设计（reward shaping），改变博弈范式，使合作成为个体最优策略。

公式：
$$U_i^t = R_i^t + F(s, s')$$
智能体 (i) 的收益 = 从博弈中获得的收益 + 内在收益

---

#### ① 亲社会（pro-social）收益【8】

$$U_i^t = \frac{1}{N}\sum_{i=1}^{N} R_i^t$$

#### ② 基于不公平厌恶（Inequity-aversion）的收益【9】

$$U_i^t = R_i^t - \frac{\alpha_i}{N-1}\sum_{j\neq i}\max(r_j - r_i, 0)
- \frac{\beta_i}{N-1}\sum_{j\neq i}\max(r_i - r_j, 0)$$
对劣势的厌恶 / 对优势的厌恶

---

#### ③ 基于自己行为对别人影响的收益【10】


自己的动作对别人策略的影响

---

如果你希望我帮你把这页内容**改写成英文PPT版本**（比如放在“Existing Approaches: Reward-based Learning”那一页），我可以把上面的文字变成正式的英文讲解稿和排版要点。是否需要？
非常好，这一段是你PPT中“**Opponent Modeling 方法综述 + 研究动机转折**”的关键内容。
你想达到的效果是：
1️⃣ 先总结已有工作（对手建模的代表路径与机制）；
2️⃣ 再指出它们的核心局限（依赖可访问对手参数、扩展性差、假设过强）；
3️⃣ 最后自然引出你的创新点——**“自主推测对手目标（goal inference）”**，作为从行为层建模到目标层建模的升级。

我为你整理成两页英文PPT脚本（内容控制适中、逻辑自然、汇报可读性强），并在讲稿注释中解释每一句的节奏与重点。

---

## **Slide 7 — Opponent Modeling in Multi-Agent Reinforcement Learning**

**Title:**
**Learning from Others: The Role of Opponent Modeling**

**Main Content (PPT text):**

* Opponent modeling plays a crucial role in improving the **robustness** and **stability** of multi-agent reinforcement learning.
* In complex environments, each agent must adapt to diverse and dynamically changing opponent strategies.
  Treating others as part of the environment makes the dynamics highly **non-stationary** and difficult to model.
* To mitigate this, opponent modeling enables agents to recognize and utilize information about others —
  including their **behavior patterns**, **goals**, or **belief states**.
* By explicitly modeling others, agents can better predict behavioral trends and stabilize interactions,
  effectively transforming a multi-agent problem into a more controllable single-agent task.

**Visual Suggestion:**
Two agents interacting, one has an internal “Opponent Model” block predicting the other’s actions.

**Footer Reference:**
Rabinowitz et al. (2018); Foerster et al. (2018); He et al. (2016)

**Presenter Notes（中文讲稿提示）**
这一页是对“对手建模”的基本定义和意义说明。
要强调三个关键词：**robustness, stability, adaptation**。
重点是说明“为什么要做Opponent Modeling”，而不是细节算法。

---

## **Slide 8 — Existing Methods and Their Limitations**

**Title:**
**From Opponent Shaping to Meta-Level Reasoning**

**Main Content (PPT text):**

* Early methods such as **LOLA (Foerster et al., 2018)** introduced *opponent shaping*,
  where an agent anticipates and influences its opponent’s policy updates via one-step look-ahead optimization.
* Extensions include **stable variants** (Letcher et al., 2019), **bi-directional shaping** (Willi et al., 2022),
  and **meta-game formulations** such as MFOS and Meta-Value Learning.
* These approaches effectively model strategy-level feedback but often rely on **access to opponent parameters** or **explicit gradient information**,
  which is unrealistic in large-scale or partially observable settings.
* Other probabilistic models (e.g., I-POMDP) capture nested beliefs but suffer from **computational intractability** in high-dimensional domains.
* **Limitation:**
  Despite progress, most methods focus on *behavioral prediction* rather than *goal inference*.
  They lack the capacity to autonomously infer opponents’ latent objectives — a key to few-shot adaptation in mixed-motive environments.

**Visual Suggestion:**
Timeline/flow chart of methods:
Reward Shaping → LOLA → MFOS → I-POMDP → (gap) Goal Inference (HOP+).
At the end, highlight “Behavior-level modeling → Goal-level inference”.

**Footer Reference:**
Foerster et al. (2018); Letcher et al. (2019); Lu et al. (2021); Aghajohari et al. (2023)

**Presenter Notes（中文讲稿提示）**
这一页是“现有方法与局限性”：

* 前半段讲几类代表性算法（LOLA、meta-level、I-POMDP）；
* 后半段总结共性问题：“依赖对手参数”、“扩展性差”、“缺乏goal-level inference”；
  最后一句话语气要下沉，为下一页自然过渡：

> “Therefore, instead of only predicting what others do, we aim to infer what they want to achieve.”

---

## ✅ 逻辑闭环总结

| 段落      | 内容焦点      | 关键词                                          | 作用       |
| ------- | --------- | -------------------------------------------- | -------- |
| Slide 7 | 什么是对手建模   | robustness, stability, belief modeling       | 铺垫背景     |
| Slide 8 | 现有代表方法与不足 | LOLA, meta-game, I-POMDP, limitations        | 引出研究空白   |
| 下一页     | 我们的创新点    | Goal inference, latent intention, adaptation | 切入HOP+方法 |

---

是否希望我现在帮你继续写下一页 —— **“Slide 9: From Opponent Modeling to Goal Inference (Our Approach)”**？
这一页将正式阐述你的思想升级：从预测行为（action-level）到推断目标（goal-level），并引出 HOP+ 模型的结构草图与公式。

非常好 👍
这一页是你整个 **Existing MARL Paradigms** 部分的**第一层逻辑（Reward-based methods）**，也是最自然的开篇：
用简洁的一页讲清楚

> “在混合动机博弈中，如何通过 reward shaping 改变个体激励结构以促成合作”，
> 同时指出这些方法的局限（人工设定、依赖他人信息、缺乏适应性），
> 为后续转向 **opponent modeling → goal inference** 做铺垫。

---

## 🎯 **Slide — Reward Shaping Methods**

**Title:**
**Reward Shaping: Directly Designing for Cooperation**

---

**Main Content (PPT text):**

* **Cooperation problem:** how can agents achieve cooperation in mixed-motive games?
  A straightforward approach is to modify the payoff structure so that cooperation becomes an individually optimal strategy.

* **Key formulations:**

  1. **Pro-social rewards** – each agent receives the group’s average return:
     [
     U_i^t = \frac{1}{N}\sum_{i=1}^N R_i^t
     ]
  2. **Inequity-aversion rewards** – penalize unfair outcomes among agents:
     [
     U_i^t = R_i^t - \frac{\alpha_i}{N-1}!\sum_{j\neq i}!\max(r_j-r_i,0)
     - \frac{\beta_i}{N-1}!\sum_{j\neq i}!\max(r_i-r_j,0)
     ]
  3. **Social influence rewards** – encourage actions that affect others’ decisions:
     [
     U_i^t = \alpha R_i^t + \beta !\sum_{j\neq i}! D_{KL}!\left[p(a_t^j|a_t^i,s_t)\Vert p(a_t^j|s_t)\right]
     ]

* These methods can promote cooperation, but they **require manual reward shaping**,
  **assume access to co-players’ private states**, and **lack adaptability** in dynamic settings.

---

**Visual Suggestion:**
Top: equation (U_i^t = R_i^t + F(s,s'))
Below: three boxes labeled *Pro-social*, *Inequity-aversion*, *Social influence*, each with a short formula.

**Footer Reference:**
Hughes et al. (2018); Eccles et al. (2019); Jaques et al. (2019)

---

**Presenter Notes（中文）**
这一页讲得要非常紧凑：

* 第一行提出问题（How to achieve cooperation?）
* 中间展示三类奖励塑造机制（群体平均、不公平厌恶、社会影响）；
* 最后两行一句话总结局限性。

这页的语气收尾要为后续铺垫：

> “While reward shaping can enforce cooperation by design, it depends on manual specification and fails to generalize to new partners — motivating opponent modeling approaches.”

---

👉 下一页（你可以命名为 “Opponent Modeling Approaches”）就从这一句过渡出去，讲“让智能体学会通过建模他人，而不是手动改奖励结构，来实现自适应合作”。
是否希望我直接帮你写那一页英文内容？（可与这页无缝衔接）

# 引文