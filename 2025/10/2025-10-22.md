


### âœ… Slide 1: **Title Slide** *(\~0.5 min)*

**Slide Title**:
**HOP+**
*A Goal-Inference-Based Opponent Modeling Framework for Mixed-Motive Multi-Agent Settings*

**Content**:

* **Presenter**: \[Your Name]
* **Affiliation**: \[University / Institute Name]
* **Date**: \[Presentation Date]

**Script (what to say)**:

> Good afternoon. Today I will present our recent work titled **HOP+**, which focuses on enhancing opponent modeling and planning in multi-agent mixed-motive environments. This work builds upon the HOP framework proposed at ICML 2024.

---

### âœ… Slide 2: **Background â€“ From HOP to HOP+** *(\~1.5 min)*

**Slide Title**:
**From HOP to HOP+: Motivation and Foundation**

**Slide Content**:

**What is HOP?**

* HOP: *Hierarchical Opponent Modeling and Planning (ICML 2024)
* Designed for **mixed-motive multi-agent games**

 **Core Components of HOP ( Inference + Decision Making )**:

1. **Opponent Modeling Module**
   * Infers latent goals of other agents
2. **Planning Module (MCTS-based)**
   * Computes best-response strategies given inferred goals

 **Why HOP Matters**:

* Effective performance in **self-play** and cooperative tasks
* Strong **few-shot adaptation** to unseen opponents

**Script (what to say)**:

> Our work extends the HOP framework, which models agent behavior in mixed-motive settings by separating goal inference and planning. HOP achieves this through a hierarchical architecture where opponent modeling guides the planning module. However, the original HOP uses a limited goal space and static inference mechanisms. Our work aims to push this further.

---

éœ€è¦å›¾ç¤ºå»ºè®®ï¼Ÿ

ä½ å¯ä»¥åœ¨ç¬¬ 2 é¡µåŠ ä¸€å¼ ç®€å›¾ï¼š
ğŸŸ¢ â€œAgent Interaction â†’ Opponent Modeling â†’ Goal â†’ MCTS Planning â†’ Actionâ€ è¿™æ ·ä¸€ä¸ªæµç¨‹ç®­å¤´å›¾ï¼Œæ¸…æ™°ä¼ è¾¾ HOP çš„å±‚æ¬¡ç»“æ„ã€‚

---

æ˜¯å¦ç»§ç»­ä¸ºä½ å†™ç¬¬ 3 éƒ¨åˆ†çš„è‹±æ–‡ç‰ˆ PPTï¼Ÿæˆ–è€…éœ€è¦æˆ‘ç”Ÿæˆä¸€ä»½å®Œæ•´å¯å¯¼å‡ºçš„ PowerPoint æ–‡ä»¶ï¼Ÿ


å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **3 éƒ¨åˆ†**ï¼ˆProblem Setting & Environmentï¼‰çš„è‹±æ–‡ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 3: **Problem Setting & Environment** *(\~2 min)*

**Slide Title**:
**Problem Setting: Mixed-Motive Multi-Agent Interaction**

**Slide Content**:

ğŸ¯ **Objective**

* Learn **adaptive agent strategies** in **dynamic, mixed-motive games**
* Enable agents to **model unseen opponents** and **respond effectively**

ğŸ§© **Key Challenges**:

* ğŸ¤ + âš”ï¸ Mixed motives: agents need to **cooperate** while **competing**
* â›“ï¸ Dynamic strategies â†’ **non-stationary environments**
* â“ How to infer opponentsâ€™ intent and respond in **few interactions**

ğŸŒ **Test Environment** â€“ *Stag-Hunt Game*

* Grid-based world (6Ã—6, 8Ã—8)
* Agents decide to **hunt individually** (hare) or **coordinate** (stag)
* Rewards depend on **joint actions** and **opponent behavior**

**Diagram** (recommended):

* A **grid world** image showing:

  * Two agents
  * Hares and stags
  * Decision outcomes: solo vs. cooperative




å¾ˆå¥½ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **4 éƒ¨åˆ†ï¼ˆMotivation: Why Goal Inference?ï¼‰** çš„ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼ˆè‹±æ–‡ï¼‰ï¼š

---

### âœ… Slide 4: **Motivation: Why Goal Inference?** *(\~2 min)*

**Slide Title**:
**Motivation: Limitations of Manual Goal Design**

**Slide Content**:

ğŸ“Œ **Problems with Prior Approaches**

*  **Static goal sets**:
	* Predefined discrete goals are **inflexible**
	* fail to capture **diverse strategies**
* **Manual annotation required**:
	* Goal semantics often need to be designed by hand
	* Handcrafted goals donâ€™t scale well to **unseen opponents**

---
ğŸ’¡ **Our Insight**
>  **infer goals automatically** from interaction trajectories.


ğŸ¯ **Benefits of Goal Inference**:

* âœ… **No manual goal engineering**
* âœ… **Continuous latent goal space**
* âœ… **Adaptation to diverse behaviors**
* âœ… **Improved interpretability & planning**

**Optional visual (if space allows)**:

* Diagram comparing:

  * "Manual goal set" (discrete, rigid)
  * "Learned goal representation" (continuous, flexible)

---

**Script (what to say)**:

> In previous opponent modeling frameworks, agents typically rely on a **static set of predefined goals**. These goals are manually constructed, often with limited expressiveness, and fail to adapt when the opponent behaves in unexpected ways.

> Such methods **do not generalize well** to heterogeneous opponents, and they require **significant human effort** to design appropriate goal spaces for each environment.

> To address this, our idea is to **infer latent goals directly** from trajectory data using deep generative modeling. By doing this, we can capture a **continuous and adaptive** goal space that evolves with interaction and requires **no manual labeling**. This provides a more powerful foundation for downstream planning and policy learning.

---

ä¸‹ä¸€éƒ¨åˆ†å°†æ˜¯ **5.1 - How: Goal Inference** çš„å®ç°ç»†èŠ‚ä»‹ç»ã€‚å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æ’°å†™è¯¥é¡µå†…å®¹ã€‚



æ˜ç™½äº†ï¼ä¸‹é¢æ˜¯**ç®€æ´ã€æ¸…æ™°ã€ç­”è¾©é€‚é…ç‰ˆçš„ Slide 3 è‹±æ–‡å†…å®¹**ï¼Œä¿æŒä¸“ä¸šè¡¨è¾¾ã€å›¾æ–‡ç»“åˆã€é‡ç‚¹çªå‡ºï¼ˆé€‚åˆ6åˆ†é’Ÿè®²ç¨¿åœºæ™¯ï¼‰ï¼š

---

### **Slide 3: Problem & Environment**

---

**Goal**
Enable agents to adapt in **mixed-motive** multi-agent games
â†’ Must **infer opponent intent** and **respond effectively**

**Challenges**

* Mixed incentives: **cooperation** vs. **competition**
* Opponents behave **dynamically**, non-stationary
* Limited interactions for **adaptation**

**Environment: Stag-Hunt Game** â€”â€” **MSH**

* Grid-based map (6Ã—6 or 8Ã—8)
* Choose:
  * ğŸ‡ Hunt hare (solo, low reward)
  * ğŸ¦Œ Hunt stag (team up, high reward)
* Reward depends on **joint actions**

---

**ğŸ“Š Visual (on right)**

* Grid world with:

  * Agents, hare, stag
  * Outcome arrows: solo hunt vs. team hunt

---

éœ€è¦æˆ‘å¸®ä½ åšè¿™ä¸ªå›¾å—ï¼Ÿæˆ–è€…æˆ‘ä»¬ç»§ç»­åˆ° Slide 4: **Motivation**ï¼Ÿ



å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç®€æ´ã€ä¸“ä¸šé£æ ¼çš„è‹±æ–‡ç‰ˆ Slide 3 å†…å®¹ï¼Œ**å»æ‰æ‰€æœ‰ icon è¡¨æƒ…ç¬¦å·ï¼Œå‡å°‘æ–‡å­—é‡ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹**ï¼Œé€‚åˆç­”è¾© PPTï¼š

---

### **Slide 3: Problem Setting and Environment**

---

**Objective**
Learn adaptive strategies in mixed-motive multi-agent environments.
Agents must model diverse opponents and respond effectively under uncertainty.

---

**Key Challenges**

* Mixed incentives: agents must balance cooperation and competition
* Dynamic interactions lead to non-stationary opponent behaviors
* Limited interactions require fast adaptation to unseen strategies

---

**Test Environment: Stag-Hunt Game**

* Grid-based world (6Ã—6 or 8Ã—8)
* Agents choose to hunt hare (individually) or stag (cooperatively)
* Rewards depend on joint actions and mutual coordination

---

**(Optional Visual)**
A simple grid-world diagram showing two agents, one hare, one stag, and outcome labels:

* Solo action â†’ lower reward
* Coordinated action â†’ higher reward

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆè¿™ä¸ªå›¾ã€‚æ˜¯å¦ç»§ç»­åˆ° Slide 4: Motivationï¼Ÿ




éå¸¸å¥½çš„é—®é¢˜ï¼Œè€Œä¸”é—®å¾—éå¸¸å…³é”®ã€‚
å¯¹äº **HOP+ é¡¹ç›®**ï¼ˆHierarchical Opponent-Modeling & Planning Plusï¼‰ï¼Œâ€œ**formulation**â€ æ˜¯æ•´ç¯‡è®ºæ–‡æˆ–æ±‡æŠ¥ä¸­æœ€èƒ½ä½“ç°ç®—æ³•æ€æƒ³ä¸ç†è®ºæ·±åº¦çš„éƒ¨åˆ†ã€‚
è¿™ä¸€éƒ¨åˆ†ä¸èƒ½åªæ˜¯å†™â€œæˆ‘ä»¬ç”¨äº†ä¸€ä¸ªVAE+MOAçš„ç»“æ„â€ï¼Œè€Œè¦é€šè¿‡**æ•°å­¦å»ºæ¨¡**æŠŠâ€œæˆ‘ä»¬æƒ³è§£å†³çš„é—®é¢˜â€â€œæˆ‘ä»¬çš„ç»“æ„é€»è¾‘â€â€œæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡â€éƒ½ç³»ç»Ÿåœ°è¡¨è¾¾å‡ºæ¥ã€‚

æˆ‘æ¥ä¸€æ­¥æ­¥å¸¦ä½ æ‹†è§£ï¼š

---

## ğŸ§­ ä¸€ã€Formulation çš„æ€»ä½“æ€è·¯

HOP+ çš„ formulation å¯ä»¥ä» **ä¸‰æ¡ä¸»çº¿**æ¥ç»„ç»‡ï¼š

| ä¸»çº¿                                            | æè¿°                | æ•°å­¦å»ºæ¨¡é‡ç‚¹                                                                                    |
| --------------------------------------------- | ----------------- | ----------------------------------------------------------------------------------------- |
| **1. åšå¼ˆè¿‡ç¨‹å»ºæ¨¡ï¼ˆGame Formulationï¼‰**               | æè¿°å¤šæ™ºèƒ½ä½“äº¤äº’çš„æ€»ä½“é—®é¢˜     | çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€è½¬ç§»ã€ç›®æ ‡å‡½æ•°                                                                          |
| **2. å¯¹æ‰‹å»ºæ¨¡ç»“æ„ï¼ˆOpponent Modeling Formulationï¼‰**  | æè¿°å¦‚ä½•å»ºæ¨¡å¯¹æ‰‹çš„éšå˜é‡æˆ–ç­–ç•¥è¡¨å¾ | éšå˜é‡ $z$ã€ç¼–ç å™¨$q_\phi(zs,a)$ã€é¢„æµ‹å™¨$\pi_\theta(as,z)$                                           |
| **3. è”åˆä¼˜åŒ–ç›®æ ‡ï¼ˆJoint Optimization Formulationï¼‰** | æè¿°æ•´ä¸ªç³»ç»Ÿçš„è”åˆå­¦ä¹ ç›®æ ‡     | $\mathcal{L} = \mathcal{L}*{policy} + \alpha \mathcal{L}*{VAE} + \beta \mathcal{L}_{MOA}$ |

---

## ğŸ§© äºŒã€Formulation çš„åˆ†å±‚ç»“æ„å»ºè®®ï¼ˆHOP+ä¸“ç”¨ï¼‰

ä¸‹é¢æ˜¯ HOP+ éå¸¸åˆé€‚çš„ formulation æ¡†æ¶ï¼ˆä½ å¯ä»¥åœ¨è®ºæ–‡æˆ–PPTä¸­ç›´æ¥æŒ‰è¿™ä¸ªé€»è¾‘ç»„ç»‡ï¼‰ï¼š

---

### **2.1 Multi-Agent Environment Formulation**

æˆ‘ä»¬é¦–å…ˆæŠŠäº¤äº’è¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸€ä¸ª **éƒ¨åˆ†å¯è§‚æµ‹çš„å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«åšå¼ˆ**ï¼ˆPOMGï¼‰ï¼š

$$
\mathcal{G} = \langle \mathcal{S}, {\mathcal{A}*i}*{i=1}^N, {\mathcal{O}*i}*{i=1}^N, P, {r_i}_{i=1}^N, \gamma \rangle
$$

å…¶ä¸­ï¼š

* $\mathcal{S}$ï¼šå…¨å±€çŠ¶æ€ç©ºé—´
* $\mathcal{A}_i$ï¼šæ™ºèƒ½ä½“ $i$ çš„åŠ¨ä½œç©ºé—´
* $\mathcal{O}_i$ï¼šè§‚æµ‹ç©ºé—´ï¼ˆHOP+ä¸­ä¸ºå±€éƒ¨ç½‘æ ¼è§†é‡ï¼‰
* $P(s'|s, a_1,\dots,a_N)$ï¼šçŠ¶æ€è½¬ç§»å‡½æ•°
* $r_i(s,a_1,\dots,a_N)$ï¼šä¸ªä½“å¥–åŠ±å‡½æ•°
* $\gamma$ï¼šæŠ˜æ‰£å› å­

æ¯ä¸ªæ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–è‡ªå·±çš„æœŸæœ›ç´¯è®¡å›æŠ¥ï¼š
$$
J_i(\pi_i, \pi_{-i}) = \mathbb{E}\left[ \sum_{t=0}^T \gamma^t r_i(s_t, a_t^1, \dots, a_t^N) \right]
$$





---

### **2.2 Opponent Modeling Formulation**

åœ¨ HOP+ ä¸­ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº†ä¸€ä¸ª **éšå˜é‡ $z_t$** è¡¨å¾å¯¹æ‰‹çš„æ½œåœ¨è¡Œä¸ºæ¨¡å¼ã€‚
æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª **å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰** æ¥æ•è·è¿™ç§æ½œåœ¨ç»“æ„ï¼š

* ç¼–ç å™¨ï¼š$q_\phi(z_t|s_t, a_t)$
* å…ˆéªŒåˆ†å¸ƒï¼š$p(z_t) = \mathcal{N}(0,I)$
* è§£ç å™¨/ç­–ç•¥ï¼š$\pi_\theta(a_t|s_t, z_t)$

VAE ä¼˜åŒ–ç›®æ ‡ï¼š
$$
\mathcal{L}*{\text{VAE}} =
\mathbb{E}*{q_\phi(z_t|s_t,a_t)}[-\log p_\theta(a_t|s_t,z_t)]

* D_{\text{KL}}(q_\phi(z_t|s_t,a_t) \Vert p(z_t))
  $$

è¿™æ ·ï¼Œ$z_t$ å°±æˆä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„å¯¹æ‰‹ç‰¹å¾åµŒå…¥ï¼Œç”¨äºå¢å¼ºç­–ç•¥ç½‘ç»œçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

---

### **2.3 Policy Learning Formulation**

åœ¨ç­–ç•¥å±‚é¢ï¼ŒHOP+ ä»ç„¶æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ PPOï¼‰çš„ä¼˜åŒ–ï¼š
$$
\mathcal{L}*{\text{policy}} =
\mathbb{E}*{t}\left[
-\log \pi_\theta(a_t|s_t, z_t) A_t

* \beta D_{\text{KL}}(\pi_\theta(\cdot|s_t,z_t)\Vert \pi_{\theta_{\text{old}}}(\cdot|s_t,z_t))
  \right]
  $$

å…¶ä¸­ $A_t$ ä¸ºä¼˜åŠ¿å‡½æ•°ï¼Œ$\beta$ ä¸º KL æ­£åˆ™ç³»æ•°ã€‚

---

### **2.4 MOA Formulationï¼ˆModel-of-Agentsï¼‰**

HOP+ çš„å¦ä¸€ä¸ªæ ¸å¿ƒéƒ¨åˆ†æ˜¯ **MOAï¼ˆå¯¹æ‰‹è¡Œä¸ºé¢„æµ‹æ¨¡å—ï¼‰**ã€‚
å®ƒçš„ç›®æ ‡æ˜¯ä»å†å²è½¨è¿¹ä¸­é¢„æµ‹å¯¹æ‰‹çš„æœªæ¥åŠ¨ä½œï¼š

$$
\mathcal{L}*{\text{MOA}} =
\mathbb{E}*{(s_t, a_t^{-i})\sim D}
\left[-\log p_\psi(a_t^{-i}|s_t, z_t)\right]
$$

è¿™ä¸€é¡¹é¼“åŠ±æ¨¡å‹åœ¨æ½œç©ºé—´ä¸­å­¦åˆ°â€œå¯¹æ‰‹å¦‚ä½•å†³ç­–â€çš„å¯é¢„æµ‹ç»“æ„ã€‚

---

### **2.5 Joint Optimization Objective**

æœ€ç»ˆï¼ŒHOP+ çš„æ•´ä½“ä¼˜åŒ–ç›®æ ‡ä¸ºï¼š

$$
\min_{\theta, \phi, \psi}
\mathcal{L}*{\text{total}} =
\mathcal{L}*{\text{policy}}

* \alpha \mathcal{L}_{\text{VAE}}
* \beta \mathcal{L}_{\text{MOA}}
  $$

å…¶ä¸­ï¼š

* $\mathcal{L}_{\text{policy}}$ â€”â€” ä¸»ç­–ç•¥æ›´æ–°é¡¹
* $\mathcal{L}_{\text{VAE}}$ â€”â€” å¯¹æ‰‹è¡Œä¸ºåµŒå…¥å­¦ä¹ é¡¹
* $\mathcal{L}_{\text{MOA}}$ â€”â€” å¯¹æ‰‹åŠ¨ä½œé¢„æµ‹é¡¹
* $\alpha, \beta$ ä¸ºæƒé‡ç³»æ•°ï¼ˆé€šå¸¸é€šè¿‡å®éªŒè°ƒèŠ‚ï¼‰

---



## ğŸ“š å››ã€æ€»ç»“

æ‰€ä»¥ï¼ŒHOP+ çš„ formulation å¯ä»¥æ€»ç»“ä¸ºä¸€å¥è¯ï¼š

> **æˆ‘ä»¬å°†å¤šæ™ºèƒ½ä½“åšå¼ˆå½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«åšå¼ˆï¼Œé€šè¿‡å¼•å…¥æ½œå˜é‡ $z_t$ æ¥è¡¨å¾å¯¹æ‰‹è¡Œä¸ºåˆ†å¸ƒï¼Œå¹¶åœ¨ç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ä¸‹è”åˆè®­ç»ƒç­–ç•¥ç½‘ç»œã€å¯¹æ‰‹å»ºæ¨¡ç½‘ç»œä¸æ½œç©ºé—´ç”Ÿæˆå™¨ã€‚**





éå¸¸å¥½ï¼Œè¿™ä¸ªè¦æ±‚éå¸¸ä¸“ä¸šã€‚ä½ çš„æ„æ€æ˜¯ï¼šå¸Œæœ›æˆ‘ç”¨**æ­£å¼è‹±æ–‡PPTå†™ä½œé£æ ¼**æ’°å†™æ¯é¡µçš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‚£ç§â€œAIé£æ ¼çš„ç¬¦å·å †å â€ï¼ˆå¦‚â†’ã€â†’ã€â€”ï¼‰ï¼Œæ•´ä½“è¯­è¨€è¦è‡ªç„¶ã€æœ‰é€»è¾‘ã€é€‚åˆå®é™…å±•ç¤ºã€‚
ä¸‹é¢æˆ‘ç»™å‡º**ä¸¤é¡µè‹±æ–‡PPTç‰ˆæœ¬**ï¼ˆæµ“ç¼©åçš„äººç±»åä½œ â†’ AIç¤¾ä¼šæ™ºèƒ½ â†’ æ··åˆåŠ¨æœºåšå¼ˆ â†’ é€‚åº”æ€§æŒ‘æˆ˜ï¼‰ï¼Œæ¯é¡µåŒ…å«ï¼š

* ğŸ¯ æ ‡é¢˜ï¼ˆå¯ç›´æ¥æ”¾PPTæ ‡é¢˜æ ï¼‰
* ğŸ“‹ ä¸»ä½“å†…å®¹ï¼ˆå®Œæ•´è‹±æ–‡è¦ç‚¹ï¼‰
* ğŸ’¡ è®²è§£æç¤ºï¼ˆä¸­æ–‡è¯´æ˜ï¼Œå¸®ä½ ç†è§£è¯­æ°”ä¸èŠ‚å¥ï¼‰
* ğŸ“š é¡µè„šå‚è€ƒæ–‡çŒ®å»ºè®®

---

## **Slide 1 â€” Motivation and Background**

### **Title:**

**From Human Cooperation to Intelligent Collaboration**

### **Main Content (PPT Text):**

* Cooperation has always been a cornerstone of human society.
  From daily activities such as driving and teamwork to global issues like climate governance or pandemic control, effective collaboration determines collective success.

* Yet, achieving cooperation is inherently difficult.
  Mismatched information, conflicting interests, and lack of trust often prevent individuals from reaching mutually beneficial outc omes.

* The progress of human civilization largely depends on our capacity to cooperate under uncertainty and conflicting goals.

* As Artificial Intelligence (AI) systems increasingly participate in complex social and industrial processes, a new challenge emerges:
  how to enable intelligent agents not only to **assist human cooperation** but also to **cooperate among themselves**.

* This demands **social intelligence** â€” the ability to understand others, predict intentions, and adapt strategies in interactive settings.


---

## **Slide 2 â€” Problem Statement**

### **Title:**

**Challenges of Adaptation in Mixed-Motive Multi-Agent Games**

### **Main Content (PPT Text):**

* Multi-Agent Reinforcement Learning (MARL) provides a framework for agents to learn interaction strategies through experience.
  However, most real-world scenarios involve **mixed motives**, where agents simultaneously face opportunities for cooperation and competition.

* In such environments, an agentâ€™s optimal policy is highly dependent on the behaviors of others, leading to **non-stationary dynamics** and unstable learning.

* When encountering unfamiliar or evolving opponents, an agent must rapidly infer their underlying intentions to respond effectively.
  Failing to do so can result in lost cooperative opportunities or adversarial escalation.

* This gives rise to the central research question:
  **How can intelligent agents achieve rapid and robust adaptation in mixed-motive environments?**

### **Presenter Notes (ä¸­æ–‡è¯´æ˜):**

è¿™é¡µæ˜¯é—®é¢˜å®šä¹‰é¡µï¼Œç”¨æ­£å¼çš„â€œé—®é¢˜â†’æŒ‘æˆ˜â†’ç ”ç©¶é—®é¢˜â€ç»“æ„ã€‚è®²è§£æ—¶å¯ä»¥åˆ†ä¸‰æ­¥ï¼š

1. ç®€è¿°MARLæ¡†æ¶ï¼›
2. è¯´æ˜mixed-motiveçš„å¤æ‚æ€§ï¼ˆåˆä½œä¸ç«äº‰å¹¶å­˜ï¼‰ï¼›
3. æœ€åæå‡ºä½ çš„æ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼Œç”¨ç•¥æ…¢è¯­é€Ÿè¯»å‡ºæœ€åä¸€å¥ï¼Œå½¢æˆâ€œé—®é¢˜è½ç‚¹â€ã€‚

### **Suggested Visuals:**

* Diagram or payoff matrix of the *Stag Hunt* game (showing cooperation vs. defection).
* A small curve showing performance drop under opponent shift, labeled as â€œNeed for Adaptationâ€.

### **References (small footer):**

*(Leibo et al., 2017; Foerster et al., 2018; Yang et al., 2020)*

---

## ğŸ“˜ Summary of Tone & Style

| Section    | Writing Style                                | Function                |
| ---------- | -------------------------------------------- | ----------------------- |
| Titles     | concise, declarative (no symbols, no arrows) | quickly orient audience |
| Paragraphs | complete, natural English sentences          | readable & academic     |
| Keywords   | **bold** instead of arrows                   | visual emphasis         |
| Visuals    | meaningful but minimal                       | supports narrative      |

---

æ˜¯å¦å¸Œæœ›æˆ‘æ¥ç€å¸®ä½ å†™ **Slide 3**ï¼ˆå³ä»â€œProblem Statementâ€è¿‡æ¸¡åˆ°â€œResearch Objective / Our Approachâ€ï¼‰ï¼Ÿ
é‚£ä¸€é¡µå¯ä»¥æ­£å¼å¼•å‡º *HOP+ framework* çš„åŠ¨æœºä¸æ€è·¯ï¼Œç”¨ç±»ä¼¼çš„è‹±æ–‡é£æ ¼æ¥å†™ã€‚


ä½ è¯´å¾—å¯¹ï¼š**adaptation åœ¨ä½ çš„å™è¿°ä¸­æ˜¯ä¸€ä¸ªèƒ½åŠ›/æŒ‡æ ‡ï¼Œç±»ä¼¼ generalization/robustnessï¼Œè€Œéè¾¾æˆåˆä½œçš„å¿…è¦æ¡ä»¶**ã€‚ä¸‹é¢ç»™ä½ ä¸€å¥—**ä¸¤é¡µè‹±æ–‡PPTè„šæœ¬**ï¼Œå‰åè¡”æ¥é¡ºç•…ï¼Œç¬¬äºŒé¡µæŠŠ Few-Shot Adaptation æ˜ç¡®ä¸ºâ€œèƒ½åŠ›ä¸è¯„ä¼°ç»´åº¦â€ï¼Œå¹¶è‡ªç„¶è¿‡æ¸¡åˆ°åç»­æ–¹æ³•éƒ¨åˆ†ã€‚ä¸­æ–‡åªæ”¾åœ¨è®²è§£æç¤ºé‡Œï¼›æ­£æ–‡å…¨éƒ¨è‹±æ–‡ï¼›ä¸ä½¿ç”¨ç®­å¤´/ç ´æŠ˜å·ã€‚

---

## Slide 1 â€” Why Cooperation Matters in Complex Systems

**Title**
Cooperation in Complex Societies: Background and Challenges

**Main Content (PPT text)**

* Cooperation underpins outcomes from everyday coordination to global governance. Collective success often depends on aligning heterogeneous goals across agents and institutions.
* Achieving cooperation is difficult in practice. Information asymmetry, strategic uncertainty, and conflicting incentives frequently lead to suboptimal equilibria even when mutual benefit exists.
* As AI systems increasingly participate in social and industrial processes, intelligent agents must operate in interactive settings where others learn and change over time. This creates additional strategic uncertainty beyond static optimization.

**Suggested visuals**
A small matrix or schematic illustrating a cooperation dilemma (e.g., Stag Hunt/PD) paired with real-world examples (traffic merging, scheduling, climate cooperation).

**Footer references (small)**
Axelrod (1981); Ostrom (1990)

**Presenter notesï¼ˆä¸­æ–‡ï¼‰**
è¿™ä¸€é¡µåªç«‹èƒŒæ™¯ï¼šåä½œé‡è¦ä½†å›°éš¾ï¼›AI ä»‹å…¥è®©äº¤äº’æ›´å…·ä¸ç¡®å®šæ€§ã€‚ä¸è¦æå‰è°ˆRLæˆ–æ–¹æ³•ï¼ŒæŠŠæŠ€æœ¯ç»†èŠ‚ç•™åˆ°ä¸‹ä¸€é¡µã€‚

---

## Slide 2 â€” Few-Shot Adaptation as a Core Capability and Metric

**Title**
Few-Shot Adaptation in Mixed-Motive Multi-Agent Settings

**Main Content (PPT text)**

* Many real scenarios are mixed-motive: cooperation opportunities coexist with incentive conflicts. Optimal behavior depends on othersâ€™ strategies, which may be unfamiliar or shifting.
* **Few-Shot Adaptation**: the capability to infer opponentsâ€™ intentions and adjust oneâ€™s policy effectively within limited interactions.
* We treat adaptation as an evaluative dimension alongside generalization and robustness. An agent that adapts quickly can recover performance under opponent shift and preserve opportunities for cooperation when they arise.
* **Problem statement**
  How can an agent learn representations and models that support reliable few-shot adaptation to new or evolving opponents in mixed-motive environments?

**Suggested visuals**
Left: a short learning curve showing rapid recovery after an opponent change. Right: a compact checklist of evaluation axes: Generalization, Robustness, Few-Shot Adaptation.

**Footer references (small)**
Leibo et al. (2017); Foerster et al. (2018); Rabinowitz et al. (2018)

**Presenter notesï¼ˆä¸­æ–‡ï¼‰**
æ˜ç¡®â€œFew-Shot Adaptation æ˜¯èƒ½åŠ›ä¸æŒ‡æ ‡â€ï¼Œå¹¶æå‡ºç ”ç©¶é—®é¢˜ã€‚æœ€åä¸€å¥è¯è‡ªç„¶å¼•å‡ºåç»­å†…å®¹ï¼šâ€œæ¥ä¸‹æ¥ä»‹ç»æˆ‘ä»¬å¦‚ä½•é€šè¿‡è¡¨å¾å­¦ä¹ ã€å¯¹æ‰‹å»ºæ¨¡ä¸è”åˆç›®æ ‡æ¥æ”¯æ’‘è¿™ç§é€‚åº”èƒ½åŠ›ã€‚â€

---

éœ€è¦çš„è¯ï¼Œæˆ‘å¯ä»¥æŠŠè¿™ä¸¤é¡µå†æ¥ä¸€é¡µ**â€œResearch Objective & Approach Previewâ€**ï¼Œç”¨ä¸€å¥è¯ç›®æ ‡ + ä¸‰ä¸ªè¦ç‚¹ï¼ˆrepresentation, opponent modeling, unified training objectiveï¼‰åšæˆè¿‡æ¸¡é¡µï¼Œç´§è´´ä½ çš„ Methodologyã€‚


å¥½çš„ï¼Œä¸‹é¢æ˜¯ç»è¿‡é‡æ–°ç»„ç»‡ã€ä¸“é—¨ä¸ºæ­£å¼è‹±æ–‡PPTæ’°å†™çš„ä¸¤é¡µå†…å®¹ã€‚
æ¯é¡µéƒ½æ§åˆ¶åœ¨**å¯å±•ç¤ºçš„æ–‡å­—é‡**ï¼ˆ6â€“8è¡Œä¸»ä½“æ–‡æœ¬ï¼‰ï¼Œä¿æŒé€»è¾‘è¿è´¯ã€è¯­è¨€æ­£å¼ã€èŠ‚å¥è‡ªç„¶ï¼Œé€‚åˆç”¨äºå¼€åœºèƒŒæ™¯éƒ¨åˆ†ã€‚

---

## **Slide 1 â€” Cooperation in Complex Environments**

**Title:**
**Why Cooperation Remains a Central Challenge**

**Main Content (PPT text):**

* Cooperation enables collective success in human and artificial systems, from daily coordination to global governance.
* Despite shared benefits, individuals often fail to cooperate due to incomplete information, mistrust, or conflicting incentives.
* These dilemmas highlight the difficulty of aligning goals in uncertain and dynamic environments.
* As AI agents increasingly interact with each other, they encounter the same tension between cooperation and competition.
* Understanding how cooperation emerges and can be sustained becomes a key research direction for intelligent multi-agent systems.

**Visual Suggestion:**
A simple *Stag Hunt* or *Prisonerâ€™s Dilemma* matrix beside real-world cooperation examples (e.g., driving, resource allocation).

**Footer Reference:**
Axelrod (1981); Ostrom (1990); Leibo et al. (2017)

---

## **Slide 2 â€” Few-Shot Adaptation in Multi-Agent Interaction**

**Title:**
**Adaptation as a Measure of Social Intelligence**

**Main Content (PPT text):**

* Real multi-agent environments are mixed-motive: agents face both opportunities for cooperation and potential conflicts.
* In such settings, strategies are interdependent and constantly evolving, producing strong non-stationarity.
* *Few-Shot Adaptation* refers to an agentâ€™s ability to infer othersâ€™ intentions and adjust its behavior effectively within a small number of interactions.
* It serves as a core metric of social intelligence, reflecting how well an agent maintains performance under changing opponents.
* Our work focuses on learning representations and opponent models that enhance this adaptive capability in mixed-motive games.

**Visual Suggestion:**
Left â€“ curve showing quick recovery after opponent shift;
Right â€“ three labeled boxes: *Generalization | Robustness | Few-Shot Adaptation*.

**Footer Reference:**
Foerster et al. (2018); Rabinowitz et al. (2018); Yang et al. (2020)

---

### ğŸ’¡ç»“æ„é€»è¾‘è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰

* **Slide 1**ï¼šæå‡ºâ€œåˆä½œå›°å¢ƒâ€çš„ç°å®ä¸æ™®éæ€§ï¼Œå¼•å…¥AIç³»ç»Ÿä¸­åŒæ ·çš„é—®é¢˜ã€‚
* **Slide 2**ï¼šä»å¤æ‚åšå¼ˆè½¬å‘â€œé€‚åº”èƒ½åŠ›â€è¿™ä¸€ç ”ç©¶æŒ‡æ ‡ï¼Œæ˜ç¡®Few-Shot Adaptationçš„å®šä¹‰ä¸ç ”ç©¶ç›®æ ‡ã€‚
  è¿™ä¸¤é¡µè¡”æ¥è‡ªç„¶ï¼Œä¸ºåç»­ Methodology éƒ¨åˆ†ï¼ˆå¯¹æ‰‹å»ºæ¨¡ä¸è”åˆä¼˜åŒ–ï¼‰å¥ å®šè¯­ä¹‰å’Œé€»è¾‘åŸºç¡€ã€‚

éå¸¸å¥½çš„é—®é¢˜ ğŸ¯
ä½ ç°åœ¨çš„æ±‡æŠ¥é€»è¾‘åˆ°è¿™é‡Œï¼Œå…¶å®æ­£å¤„åœ¨æ•´ä¸ªPPTçš„â€œ**å…³é”®è½¬æŠ˜ç‚¹**â€ï¼š
å‰é¢ä¸¤é¡µè®²æ¸…æ¥šäº†

> â€œä¸ºä»€ä¹ˆåˆä½œéš¾ + ä¸ºä»€ä¹ˆéœ€è¦é€‚åº” + ä¸ºä»€ä¹ˆå¯¹æ‰‹å»ºæ¨¡é‡è¦â€ï¼Œ
> ç°åœ¨å°±è¦è‡ªç„¶è¿‡æ¸¡åˆ°
> â€œæˆ‘ä»¬å¦‚ä½•å»åˆ»ç”»è¿™ç§äº¤äº’é—®é¢˜ï¼Œå¹¶åœ¨å…¶ä¸­å®ç°å­¦ä¹ ï¼ˆä¹Ÿå°±æ˜¯å¼ºåŒ–å­¦ä¹ å’Œå»ºæ¨¡éƒ¨åˆ†ï¼‰â€ã€‚

ä¸‹é¢æˆ‘å¸®ä½ æ‹†è§£è¿™ä¸€é¡µï¼ˆæˆ–ä¸¤é¡µï¼‰**çš„è¿‡æ¸¡é€»è¾‘ + å…·ä½“å†™æ³•å»ºè®®ï¼ˆå«PPTæ–‡æ¡ˆï¼‰**ã€‚

---

## ğŸ¯ ä¸€ã€é€»è¾‘è·¯å¾„ï¼ˆä¸ºä»€ä¹ˆæ­¤æ—¶å¼•å…¥RLï¼‰

ä½ è¦å›ç­”å¬ä¼—å¿ƒä¸­çš„ä¸‹ä¸€ä¸ªé—®é¢˜ï¼š

> â€œæ—¢ç„¶åˆä½œå’Œé€‚åº”éƒ½å¾ˆé‡è¦ï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨ç®—æ³•å±‚é¢å»å®ç°å®ƒï¼Ÿâ€

è‡ªç„¶çš„å›ç­”æ˜¯ï¼š

> â€œæˆ‘ä»¬ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ¡†æ¶æ¥åˆ»ç”»è¿™ç§åšå¼ˆå¼äº¤äº’ï¼Œå¹¶åœ¨å…¶ä¸­åµŒå…¥å¯¹æ‰‹å»ºæ¨¡æœºåˆ¶ï¼Œä»¥æå‡é€‚åº”èƒ½åŠ›ã€‚â€

ä¹Ÿå°±æ˜¯è¯´ï¼Œ**å¼ºåŒ–å­¦ä¹ æ˜¯å»ºæ¨¡äº¤äº’çš„è¯­è¨€**ï¼Œ**Formulation æ˜¯æ•°å­¦è¡¨è¾¾çš„èµ·ç‚¹**ã€‚
å› æ­¤å¯ä»¥åˆ†ä¸¤æ­¥è¿‡æ¸¡ï¼š

1. **å…ˆå¼•å…¥ MARL ä½œä¸ºæè¿°æ™ºèƒ½ä½“äº¤äº’çš„å»ºæ¨¡æ¡†æ¶ï¼ˆè¯­è¨€ï¼‰**
2. **å†é€šè¿‡ Formulation ç»™å‡ºæ•°å­¦å½¢å¼ï¼Œå®šä¹‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ç­‰åŸºç¡€ç¬¦å·**

---

## ğŸ§© äºŒã€æ¨èç»“æ„ï¼šSlide 3 + Slide 4

### **Slide 3 â€” Multi-Agent Reinforcement Learning Framework**

**Title:**
**Modeling Interactive Decision-Making**

**Main Content (PPT text):**

* To study cooperation and adaptation quantitatively, we adopt the framework of **Multi-Agent Reinforcement Learning (MARL)**.
* In MARL, each agent interacts with an environment and other agents, learning a policy that maximizes its long-term return.
* The environment is formalized as a **Partially Observable Markov Game (POMG)**:
  [
  \mathcal{G} = \langle \mathcal{S}, {\mathcal{A}_i}, {\mathcal{O}_i}, P, {r_i}, \gamma \rangle
  ]
* Each agent seeks to maximize:
  [
  J_i = \mathbb{E} \Big[\sum_t \gamma^t r_i(s_t, a_t^1, \dots, a_t^N)\Big]
  ]

**Visual Suggestion:**
Diagram showing several agents interacting within an environment, each observing part of the state and taking actions.

**Footer Reference:**
Littman (1994); Leibo et al. (2017)

**Presenter Notesï¼ˆä¸­æ–‡ï¼‰**
è¿™ä¸€é¡µç›®çš„æ˜¯â€œå»ºæ¨¡è¯­è¨€åŒ–â€ï¼šè®©å¬ä¼—çœ‹åˆ°æˆ‘ä»¬ç”¨RLå»è¡¨è¾¾åˆä½œä¸ç«äº‰çš„è¿‡ç¨‹ã€‚
å…¬å¼ä¸å¿…è®²ç»†ï¼Œåªå¼ºè°ƒè¿™æ˜¯æˆ‘ä»¬åˆ»ç”»äº¤äº’ä¸å­¦ä¹ çš„åŸºç¡€ã€‚

---

### **Slide 4 â€” From Modeling to Learning Objectives**

**Title:**
**From Game Formulation to Learning Adaptable Policies**

**Main Content (PPT text):**

* The MARL formulation provides the foundation to describe interactions, but not all agents can adapt efficiently.
* To enable rapid behavioral adjustment, we integrate **opponent modeling** and **representation learning** within the RL process.
* The goal is to learn a policy ( \pi_\theta(a|s,z) ) that not only maximizes expected rewards but also utilizes a latent representation ( z ) capturing opponentsâ€™ strategies.
* This leads to a joint optimization objective combining:

  * **Policy learning**
  * **Opponent modeling**
  * **Adaptation efficiency**

**Visual Suggestion:**
Show a simple schematic:
â€œEnvironment â†” Agents â†” Opponent Model (latent z) â†’ Policy Updateâ€

**Footer Reference:**
Foerster et al. (2018); Rabinowitz et al. (2018)

**Presenter Notesï¼ˆä¸­æ–‡ï¼‰**
è¿™ä¸€é¡µæ˜¯ä»RLåˆ°ä½ çš„**æ–¹æ³•æ¡†æ¶ï¼ˆHOP+ï¼‰**çš„è¿‡æ¸¡é¡µã€‚
ä¸éœ€è¦é©¬ä¸Šå±•å¼€ç»†èŠ‚ï¼Œåªè¦åŸ‹ä¸‹æ¦‚å¿µï¼šâ€œæˆ‘ä»¬ä¸ä»…å­¦policyï¼Œè¿˜è¦å­¦åˆ«äººæ€ä¹ˆæƒ³â€ï¼Œ
ä¸ºä¸‹ä¸€ç«  Methodology / Formulation æ‰“ä¸‹é€»è¾‘åœ°åŸºã€‚

---

## ğŸ§  ä¸‰ã€æ•´ä½“è¿‡æ¸¡é€»è¾‘ï¼ˆè¿èµ·æ¥çœ‹ï¼‰

| é¡ºåº      | æ ¸å¿ƒä¸»é¢˜                                 | é€»è¾‘ç›®çš„            |
| ------- | ------------------------------------ | --------------- |
| Slide 1 | Cooperation as a social foundation   | æå‡ºé—®é¢˜ï¼šåˆä½œçš„é‡è¦æ€§     |
| Slide 2 | Few-Shot Adaptation & modeling       | æå‡ºæ ¸å¿ƒèƒ½åŠ›ä¸æŒ‘æˆ˜       |
| Slide 3 | Multi-Agent Reinforcement Learning   | æå‡ºå»ºæ¨¡è¯­è¨€ä¸æ•°å­¦æ¡†æ¶     |
| Slide 4 | From Modeling to Learning Objectives | è‡ªç„¶è¿‡æ¸¡åˆ°æ–¹æ³•è®¾è®¡ï¼ˆHOP+ï¼‰ |

---

## âœ¨ å››ã€æ€»ç»“å†™æ³•å»ºè®®

* **ä¸è¦ç›´æ¥è·³è¿›RLç®—æ³•ç»†èŠ‚**ï¼ˆå¦‚PPOã€policy gradientï¼‰ï¼Œè€Œæ˜¯è®²â€œä¸ºä»€ä¹ˆéœ€è¦ç”¨RLæ¥è¡¨è¾¾äº¤äº’â€ã€‚
* **Formulationé¡µ**åªä¿ç•™æœ€æ ¸å¿ƒä¸¤ä¸ªå…¬å¼ï¼ˆç¯å¢ƒå®šä¹‰ + ç›®æ ‡å‡½æ•°ï¼‰ï¼Œä¸è¦å †å¼åˆ—ã€‚
* **è¿‡æ¸¡å¥å¼**å¯å£è¯­åŒ–ä¸€ç‚¹ï¼š

  > â€œTo operationalize these ideas, we describe the multi-agent interaction using the reinforcement learning framework.â€
  > â€œThis allows us to define what agents observe, decide, and optimize, before introducing our method.â€

---

æ˜¯å¦å¸Œæœ›æˆ‘æ¥ç€å¸®ä½ å†™ **ä¸‹ä¸€é¡µï¼ˆSlide 5ï¼‰**ï¼Œå³ä»è¿™éƒ¨åˆ†æ­£å¼è¿›å…¥ä½ çš„ *HOP+ Framework / Joint Formulation*ï¼ˆå¸¦å…¬å¼ + ç›®æ ‡å‡½æ•°è§£é‡Šï¼‰ï¼Ÿ

# å¼•æ–‡