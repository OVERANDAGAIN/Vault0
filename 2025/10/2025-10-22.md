


### âœ… Slide 1: **Title Slide** *(\~0.5 min)*

**Slide Title**:
**HOP+**
*A Goal-Inference-Based Opponent Modeling Framework for Mixed-Motive Multi-Agent Settings*

**Content**:

* **Presenter**: \[Your Name]
* **Affiliation**: \[University / Institute Name]
* **Date**: \[Presentation Date]

**Script (what to say)**:

> Good afternoon. Today I will present our recent work titled **HOP+**, which focuses on enhancing opponent modeling and planning in multi-agent mixed-motive environments. This work builds upon the HOP framework proposed at ICML 2024.

---

### âœ… Slide 2: **Background â€“ From HOP to HOP+** *(\~1.5 min)*

**Slide Title**:
**From HOP to HOP+: Motivation and Foundation**

**Slide Content**:

**What is HOP?**

* HOP: *Hierarchical Opponent Modeling and Planning (ICML 2024)
* Designed for **mixed-motive multi-agent games**

 **Core Components of HOP ( Inference + Decision Making )**:

1. **Opponent Modeling Module**
   * Infers latent goals of other agents
2. **Planning Module (MCTS-based)**
   * Computes best-response strategies given inferred goals

 **Why HOP Matters**:

* Effective performance in **self-play** and cooperative tasks
* Strong **few-shot adaptation** to unseen opponents

**Script (what to say)**:

> Our work extends the HOP framework, which models agent behavior in mixed-motive settings by separating goal inference and planning. HOP achieves this through a hierarchical architecture where opponent modeling guides the planning module. However, the original HOP uses a limited goal space and static inference mechanisms. Our work aims to push this further.

---

éœ€è¦å›¾ç¤ºå»ºè®®ï¼Ÿ

ä½ å¯ä»¥åœ¨ç¬¬ 2 é¡µåŠ ä¸€å¼ ç®€å›¾ï¼š
ğŸŸ¢ â€œAgent Interaction â†’ Opponent Modeling â†’ Goal â†’ MCTS Planning â†’ Actionâ€ è¿™æ ·ä¸€ä¸ªæµç¨‹ç®­å¤´å›¾ï¼Œæ¸…æ™°ä¼ è¾¾ HOP çš„å±‚æ¬¡ç»“æ„ã€‚

---

æ˜¯å¦ç»§ç»­ä¸ºä½ å†™ç¬¬ 3 éƒ¨åˆ†çš„è‹±æ–‡ç‰ˆ PPTï¼Ÿæˆ–è€…éœ€è¦æˆ‘ç”Ÿæˆä¸€ä»½å®Œæ•´å¯å¯¼å‡ºçš„ PowerPoint æ–‡ä»¶ï¼Ÿ


å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **3 éƒ¨åˆ†**ï¼ˆProblem Setting & Environmentï¼‰çš„è‹±æ–‡ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼š

---

### âœ… Slide 3: **Problem Setting & Environment** *(\~2 min)*

**Slide Title**:
**Problem Setting: Mixed-Motive Multi-Agent Interaction**

**Slide Content**:

ğŸ¯ **Objective**

* Learn **adaptive agent strategies** in **dynamic, mixed-motive games**
* Enable agents to **model unseen opponents** and **respond effectively**

ğŸ§© **Key Challenges**:

* ğŸ¤ + âš”ï¸ Mixed motives: agents need to **cooperate** while **competing**
* â›“ï¸ Dynamic strategies â†’ **non-stationary environments**
* â“ How to infer opponentsâ€™ intent and respond in **few interactions**

ğŸŒ **Test Environment** â€“ *Stag-Hunt Game*

* Grid-based world (6Ã—6, 8Ã—8)
* Agents decide to **hunt individually** (hare) or **coordinate** (stag)
* Rewards depend on **joint actions** and **opponent behavior**

**Diagram** (recommended):

* A **grid world** image showing:

  * Two agents
  * Hares and stags
  * Decision outcomes: solo vs. cooperative




å¾ˆå¥½ï¼Œä»¥ä¸‹æ˜¯ç¬¬ **4 éƒ¨åˆ†ï¼ˆMotivation: Why Goal Inference?ï¼‰** çš„ PPT å†…å®¹å’Œè®²ç¨¿å»ºè®®ï¼ˆè‹±æ–‡ï¼‰ï¼š

---

### âœ… Slide 4: **Motivation: Why Goal Inference?** *(\~2 min)*

**Slide Title**:
**Motivation: Limitations of Manual Goal Design**

**Slide Content**:

ğŸ“Œ **Problems with Prior Approaches**

*  **Static goal sets**:
	* Predefined discrete goals are **inflexible**
	* fail to capture **diverse strategies**
* **Manual annotation required**:
	* Goal semantics often need to be designed by hand
	* Handcrafted goals donâ€™t scale well to **unseen opponents**

---
ğŸ’¡ **Our Insight**
>  **infer goals automatically** from interaction trajectories.


ğŸ¯ **Benefits of Goal Inference**:

* âœ… **No manual goal engineering**
* âœ… **Continuous latent goal space**
* âœ… **Adaptation to diverse behaviors**
* âœ… **Improved interpretability & planning**

**Optional visual (if space allows)**:

* Diagram comparing:

  * "Manual goal set" (discrete, rigid)
  * "Learned goal representation" (continuous, flexible)

---

**Script (what to say)**:

> In previous opponent modeling frameworks, agents typically rely on a **static set of predefined goals**. These goals are manually constructed, often with limited expressiveness, and fail to adapt when the opponent behaves in unexpected ways.

> Such methods **do not generalize well** to heterogeneous opponents, and they require **significant human effort** to design appropriate goal spaces for each environment.

> To address this, our idea is to **infer latent goals directly** from trajectory data using deep generative modeling. By doing this, we can capture a **continuous and adaptive** goal space that evolves with interaction and requires **no manual labeling**. This provides a more powerful foundation for downstream planning and policy learning.

---

ä¸‹ä¸€éƒ¨åˆ†å°†æ˜¯ **5.1 - How: Goal Inference** çš„å®ç°ç»†èŠ‚ä»‹ç»ã€‚å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ æ’°å†™è¯¥é¡µå†…å®¹ã€‚



æ˜ç™½äº†ï¼ä¸‹é¢æ˜¯**ç®€æ´ã€æ¸…æ™°ã€ç­”è¾©é€‚é…ç‰ˆçš„ Slide 3 è‹±æ–‡å†…å®¹**ï¼Œä¿æŒä¸“ä¸šè¡¨è¾¾ã€å›¾æ–‡ç»“åˆã€é‡ç‚¹çªå‡ºï¼ˆé€‚åˆ6åˆ†é’Ÿè®²ç¨¿åœºæ™¯ï¼‰ï¼š

---

### **Slide 3: Problem & Environment**

---

**Goal**
Enable agents to adapt in **mixed-motive** multi-agent games
â†’ Must **infer opponent intent** and **respond effectively**

**Challenges**

* Mixed incentives: **cooperation** vs. **competition**
* Opponents behave **dynamically**, non-stationary
* Limited interactions for **adaptation**

**Environment: Stag-Hunt Game** â€”â€” **MSH**

* Grid-based map (6Ã—6 or 8Ã—8)
* Choose:
  * ğŸ‡ Hunt hare (solo, low reward)
  * ğŸ¦Œ Hunt stag (team up, high reward)
* Reward depends on **joint actions**

---

**ğŸ“Š Visual (on right)**

* Grid world with:

  * Agents, hare, stag
  * Outcome arrows: solo hunt vs. team hunt

---

éœ€è¦æˆ‘å¸®ä½ åšè¿™ä¸ªå›¾å—ï¼Ÿæˆ–è€…æˆ‘ä»¬ç»§ç»­åˆ° Slide 4: **Motivation**ï¼Ÿ



å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ç®€æ´ã€ä¸“ä¸šé£æ ¼çš„è‹±æ–‡ç‰ˆ Slide 3 å†…å®¹ï¼Œ**å»æ‰æ‰€æœ‰ icon è¡¨æƒ…ç¬¦å·ï¼Œå‡å°‘æ–‡å­—é‡ï¼Œçªå‡ºæ ¸å¿ƒè¦ç‚¹**ï¼Œé€‚åˆç­”è¾© PPTï¼š

---

### **Slide 3: Problem Setting and Environment**

---

**Objective**
Learn adaptive strategies in mixed-motive multi-agent environments.
Agents must model diverse opponents and respond effectively under uncertainty.

---

**Key Challenges**

* Mixed incentives: agents must balance cooperation and competition
* Dynamic interactions lead to non-stationary opponent behaviors
* Limited interactions require fast adaptation to unseen strategies

---

**Test Environment: Stag-Hunt Game**

* Grid-based world (6Ã—6 or 8Ã—8)
* Agents choose to hunt hare (individually) or stag (cooperatively)
* Rewards depend on joint actions and mutual coordination

---

**(Optional Visual)**
A simple grid-world diagram showing two agents, one hare, one stag, and outcome labels:

* Solo action â†’ lower reward
* Coordinated action â†’ higher reward

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”Ÿæˆè¿™ä¸ªå›¾ã€‚æ˜¯å¦ç»§ç»­åˆ° Slide 4: Motivationï¼Ÿ




éå¸¸å¥½çš„é—®é¢˜ï¼Œè€Œä¸”é—®å¾—éå¸¸å…³é”®ã€‚
å¯¹äº **HOP+ é¡¹ç›®**ï¼ˆHierarchical Opponent-Modeling & Planning Plusï¼‰ï¼Œâ€œ**formulation**â€ æ˜¯æ•´ç¯‡è®ºæ–‡æˆ–æ±‡æŠ¥ä¸­æœ€èƒ½ä½“ç°ç®—æ³•æ€æƒ³ä¸ç†è®ºæ·±åº¦çš„éƒ¨åˆ†ã€‚
è¿™ä¸€éƒ¨åˆ†ä¸èƒ½åªæ˜¯å†™â€œæˆ‘ä»¬ç”¨äº†ä¸€ä¸ªVAE+MOAçš„ç»“æ„â€ï¼Œè€Œè¦é€šè¿‡**æ•°å­¦å»ºæ¨¡**æŠŠâ€œæˆ‘ä»¬æƒ³è§£å†³çš„é—®é¢˜â€â€œæˆ‘ä»¬çš„ç»“æ„é€»è¾‘â€â€œæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡â€éƒ½ç³»ç»Ÿåœ°è¡¨è¾¾å‡ºæ¥ã€‚

æˆ‘æ¥ä¸€æ­¥æ­¥å¸¦ä½ æ‹†è§£ï¼š

---

## ğŸ§­ ä¸€ã€Formulation çš„æ€»ä½“æ€è·¯

HOP+ çš„ formulation å¯ä»¥ä» **ä¸‰æ¡ä¸»çº¿**æ¥ç»„ç»‡ï¼š

| ä¸»çº¿                                            | æè¿°                | æ•°å­¦å»ºæ¨¡é‡ç‚¹                                                                                    |
| --------------------------------------------- | ----------------- | ----------------------------------------------------------------------------------------- |
| **1. åšå¼ˆè¿‡ç¨‹å»ºæ¨¡ï¼ˆGame Formulationï¼‰**               | æè¿°å¤šæ™ºèƒ½ä½“äº¤äº’çš„æ€»ä½“é—®é¢˜     | çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€è½¬ç§»ã€ç›®æ ‡å‡½æ•°                                                                          |
| **2. å¯¹æ‰‹å»ºæ¨¡ç»“æ„ï¼ˆOpponent Modeling Formulationï¼‰**  | æè¿°å¦‚ä½•å»ºæ¨¡å¯¹æ‰‹çš„éšå˜é‡æˆ–ç­–ç•¥è¡¨å¾ | éšå˜é‡ $z$ã€ç¼–ç å™¨$q_\phi(zs,a)$ã€é¢„æµ‹å™¨$\pi_\theta(as,z)$                                           |
| **3. è”åˆä¼˜åŒ–ç›®æ ‡ï¼ˆJoint Optimization Formulationï¼‰** | æè¿°æ•´ä¸ªç³»ç»Ÿçš„è”åˆå­¦ä¹ ç›®æ ‡     | $\mathcal{L} = \mathcal{L}*{policy} + \alpha \mathcal{L}*{VAE} + \beta \mathcal{L}_{MOA}$ |

---

## ğŸ§© äºŒã€Formulation çš„åˆ†å±‚ç»“æ„å»ºè®®ï¼ˆHOP+ä¸“ç”¨ï¼‰

ä¸‹é¢æ˜¯ HOP+ éå¸¸åˆé€‚çš„ formulation æ¡†æ¶ï¼ˆä½ å¯ä»¥åœ¨è®ºæ–‡æˆ–PPTä¸­ç›´æ¥æŒ‰è¿™ä¸ªé€»è¾‘ç»„ç»‡ï¼‰ï¼š

---

### **2.1 Multi-Agent Environment Formulation**

æˆ‘ä»¬é¦–å…ˆæŠŠäº¤äº’è¿‡ç¨‹å½¢å¼åŒ–ä¸ºä¸€ä¸ª **éƒ¨åˆ†å¯è§‚æµ‹çš„å¤šæ™ºèƒ½ä½“é©¬å°”å¯å¤«åšå¼ˆ**ï¼ˆPOMGï¼‰ï¼š

$$
\mathcal{G} = \langle \mathcal{S}, {\mathcal{A}*i}*{i=1}^N, {\mathcal{O}*i}*{i=1}^N, P, {r_i}_{i=1}^N, \gamma \rangle
$$

å…¶ä¸­ï¼š

* $\mathcal{S}$ï¼šå…¨å±€çŠ¶æ€ç©ºé—´
* $\mathcal{A}_i$ï¼šæ™ºèƒ½ä½“ $i$ çš„åŠ¨ä½œç©ºé—´
* $\mathcal{O}_i$ï¼šè§‚æµ‹ç©ºé—´ï¼ˆHOP+ä¸­ä¸ºå±€éƒ¨ç½‘æ ¼è§†é‡ï¼‰
* $P(s'|s, a_1,\dots,a_N)$ï¼šçŠ¶æ€è½¬ç§»å‡½æ•°
* $r_i(s,a_1,\dots,a_N)$ï¼šä¸ªä½“å¥–åŠ±å‡½æ•°
* $\gamma$ï¼šæŠ˜æ‰£å› å­

æ¯ä¸ªæ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–è‡ªå·±çš„æœŸæœ›ç´¯è®¡å›æŠ¥ï¼š
$$
J_i(\pi_i, \pi_{-i}) = \mathbb{E}\left[ \sum_{t=0}^T \gamma^t r_i(s_t, a_t^1, \dots, a_t^N) \right]
$$





---

### **2.2 Opponent Modeling Formulation**

åœ¨ HOP+ ä¸­ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯å¼•å…¥äº†ä¸€ä¸ª **éšå˜é‡ $z_t$** è¡¨å¾å¯¹æ‰‹çš„æ½œåœ¨è¡Œä¸ºæ¨¡å¼ã€‚
æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª **å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰** æ¥æ•è·è¿™ç§æ½œåœ¨ç»“æ„ï¼š

* ç¼–ç å™¨ï¼š$q_\phi(z_t|s_t, a_t)$
* å…ˆéªŒåˆ†å¸ƒï¼š$p(z_t) = \mathcal{N}(0,I)$
* è§£ç å™¨/ç­–ç•¥ï¼š$\pi_\theta(a_t|s_t, z_t)$

VAE ä¼˜åŒ–ç›®æ ‡ï¼š
$$
\mathcal{L}*{\text{VAE}} =
\mathbb{E}*{q_\phi(z_t|s_t,a_t)}[-\log p_\theta(a_t|s_t,z_t)]

* D_{\text{KL}}(q_\phi(z_t|s_t,a_t) \Vert p(z_t))
  $$

è¿™æ ·ï¼Œ$z_t$ å°±æˆä¸ºä¸€ä¸ªå¯å­¦ä¹ çš„å¯¹æ‰‹ç‰¹å¾åµŒå…¥ï¼Œç”¨äºå¢å¼ºç­–ç•¥ç½‘ç»œçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚

---

### **2.3 Policy Learning Formulation**

åœ¨ç­–ç•¥å±‚é¢ï¼ŒHOP+ ä»ç„¶æ˜¯åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ PPOï¼‰çš„ä¼˜åŒ–ï¼š
$$
\mathcal{L}*{\text{policy}} =
\mathbb{E}*{t}\left[
-\log \pi_\theta(a_t|s_t, z_t) A_t

* \beta D_{\text{KL}}(\pi_\theta(\cdot|s_t,z_t)\Vert \pi_{\theta_{\text{old}}}(\cdot|s_t,z_t))
  \right]
  $$

å…¶ä¸­ $A_t$ ä¸ºä¼˜åŠ¿å‡½æ•°ï¼Œ$\beta$ ä¸º KL æ­£åˆ™ç³»æ•°ã€‚

---

### **2.4 MOA Formulationï¼ˆModel-of-Agentsï¼‰**

HOP+ çš„å¦ä¸€ä¸ªæ ¸å¿ƒéƒ¨åˆ†æ˜¯ **MOAï¼ˆå¯¹æ‰‹è¡Œä¸ºé¢„æµ‹æ¨¡å—ï¼‰**ã€‚
å®ƒçš„ç›®æ ‡æ˜¯ä»å†å²è½¨è¿¹ä¸­é¢„æµ‹å¯¹æ‰‹çš„æœªæ¥åŠ¨ä½œï¼š

$$
\mathcal{L}*{\text{MOA}} =
\mathbb{E}*{(s_t, a_t^{-i})\sim D}
\left[-\log p_\psi(a_t^{-i}|s_t, z_t)\right]
$$

è¿™ä¸€é¡¹é¼“åŠ±æ¨¡å‹åœ¨æ½œç©ºé—´ä¸­å­¦åˆ°â€œå¯¹æ‰‹å¦‚ä½•å†³ç­–â€çš„å¯é¢„æµ‹ç»“æ„ã€‚

---

### **2.5 Joint Optimization Objective**

æœ€ç»ˆï¼ŒHOP+ çš„æ•´ä½“ä¼˜åŒ–ç›®æ ‡ä¸ºï¼š

$$
\min_{\theta, \phi, \psi}
\mathcal{L}*{\text{total}} =
\mathcal{L}*{\text{policy}}

* \alpha \mathcal{L}_{\text{VAE}}
* \beta \mathcal{L}_{\text{MOA}}
  $$

å…¶ä¸­ï¼š

* $\mathcal{L}_{\text{policy}}$ â€”â€” ä¸»ç­–ç•¥æ›´æ–°é¡¹
* $\mathcal{L}_{\text{VAE}}$ â€”â€” å¯¹æ‰‹è¡Œä¸ºåµŒå…¥å­¦ä¹ é¡¹
* $\mathcal{L}_{\text{MOA}}$ â€”â€” å¯¹æ‰‹åŠ¨ä½œé¢„æµ‹é¡¹
* $\alpha, \beta$ ä¸ºæƒé‡ç³»æ•°ï¼ˆé€šå¸¸é€šè¿‡å®éªŒè°ƒèŠ‚ï¼‰

---



## ğŸ“š å››ã€æ€»ç»“

æ‰€ä»¥ï¼ŒHOP+ çš„ formulation å¯ä»¥æ€»ç»“ä¸ºä¸€å¥è¯ï¼š

> **æˆ‘ä»¬å°†å¤šæ™ºèƒ½ä½“åšå¼ˆå½¢å¼åŒ–ä¸ºéƒ¨åˆ†å¯è§‚æµ‹é©¬å°”å¯å¤«åšå¼ˆï¼Œé€šè¿‡å¼•å…¥æ½œå˜é‡ $z_t$ æ¥è¡¨å¾å¯¹æ‰‹è¡Œä¸ºåˆ†å¸ƒï¼Œå¹¶åœ¨ç»Ÿä¸€çš„ä¼˜åŒ–æ¡†æ¶ä¸‹è”åˆè®­ç»ƒç­–ç•¥ç½‘ç»œã€å¯¹æ‰‹å»ºæ¨¡ç½‘ç»œä¸æ½œç©ºé—´ç”Ÿæˆå™¨ã€‚**





éå¸¸å¥½ï¼Œè¿™ä¸ªè¦æ±‚éå¸¸ä¸“ä¸šã€‚ä½ çš„æ„æ€æ˜¯ï¼šå¸Œæœ›æˆ‘ç”¨**æ­£å¼è‹±æ–‡PPTå†™ä½œé£æ ¼**æ’°å†™æ¯é¡µçš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‚£ç§â€œAIé£æ ¼çš„ç¬¦å·å †å â€ï¼ˆå¦‚â†’ã€â†’ã€â€”ï¼‰ï¼Œæ•´ä½“è¯­è¨€è¦è‡ªç„¶ã€æœ‰é€»è¾‘ã€é€‚åˆå®é™…å±•ç¤ºã€‚
ä¸‹é¢æˆ‘ç»™å‡º**ä¸¤é¡µè‹±æ–‡PPTç‰ˆæœ¬**ï¼ˆæµ“ç¼©åçš„äººç±»åä½œ â†’ AIç¤¾ä¼šæ™ºèƒ½ â†’ æ··åˆåŠ¨æœºåšå¼ˆ â†’ é€‚åº”æ€§æŒ‘æˆ˜ï¼‰ï¼Œæ¯é¡µåŒ…å«ï¼š

* ğŸ¯ æ ‡é¢˜ï¼ˆå¯ç›´æ¥æ”¾PPTæ ‡é¢˜æ ï¼‰
* ğŸ“‹ ä¸»ä½“å†…å®¹ï¼ˆå®Œæ•´è‹±æ–‡è¦ç‚¹ï¼‰
* ğŸ’¡ è®²è§£æç¤ºï¼ˆä¸­æ–‡è¯´æ˜ï¼Œå¸®ä½ ç†è§£è¯­æ°”ä¸èŠ‚å¥ï¼‰
* ğŸ“š é¡µè„šå‚è€ƒæ–‡çŒ®å»ºè®®

---

## **Slide 1 â€” Motivation and Background**

### **Title:**

**From Human Cooperation to Intelligent Collaboration**

### **Main Content (PPT Text):**

* Cooperation has always been a cornerstone of human society.
  From daily activities such as driving and teamwork to global issues like climate governance or pandemic control, effective collaboration determines collective success.

* Yet, achieving cooperation is inherently difficult.
  Mismatched information, conflicting interests, and lack of trust often prevent individuals from reaching mutually beneficial outc omes.

* The progress of human civilization largely depends on our capacity to cooperate under uncertainty and conflicting goals.

* As Artificial Intelligence (AI) systems increasingly participate in complex social and industrial processes, a new challenge emerges:
  how to enable intelligent agents not only to **assist human cooperation** but also to **cooperate among themselves**.

* This demands **social intelligence** â€” the ability to understand others, predict intentions, and adapt strategies in interactive settings.


---

## **Slide 2 â€” Problem Statement**

### **Title:**

**Challenges of Adaptation in Mixed-Motive Multi-Agent Games**

### **Main Content (PPT Text):**

* Multi-Agent Reinforcement Learning (MARL) provides a framework for agents to learn interaction strategies through experience.
  However, most real-world scenarios involve **mixed motives**, where agents simultaneously face opportunities for cooperation and competition.

* In such environments, an agentâ€™s optimal policy is highly dependent on the behaviors of others, leading to **non-stationary dynamics** and unstable learning.

* When encountering unfamiliar or evolving opponents, an agent must rapidly infer their underlying intentions to respond effectively.
  Failing to do so can result in lost cooperative opportunities or adversarial escalation.

* This gives rise to the central research question:
  **How can intelligent agents achieve rapid and robust adaptation in mixed-motive environments?**

### **Presenter Notes (ä¸­æ–‡è¯´æ˜):**

è¿™é¡µæ˜¯é—®é¢˜å®šä¹‰é¡µï¼Œç”¨æ­£å¼çš„â€œé—®é¢˜â†’æŒ‘æˆ˜â†’ç ”ç©¶é—®é¢˜â€ç»“æ„ã€‚è®²è§£æ—¶å¯ä»¥åˆ†ä¸‰æ­¥ï¼š

1. ç®€è¿°MARLæ¡†æ¶ï¼›
2. è¯´æ˜mixed-motiveçš„å¤æ‚æ€§ï¼ˆåˆä½œä¸ç«äº‰å¹¶å­˜ï¼‰ï¼›
3. æœ€åæå‡ºä½ çš„æ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼Œç”¨ç•¥æ…¢è¯­é€Ÿè¯»å‡ºæœ€åä¸€å¥ï¼Œå½¢æˆâ€œé—®é¢˜è½ç‚¹â€ã€‚

### **Suggested Visuals:**

* Diagram or payoff matrix of the *Stag Hunt* game (showing cooperation vs. defection).
* A small curve showing performance drop under opponent shift, labeled as â€œNeed for Adaptationâ€.

### **References (small footer):**

*(Leibo et al., 2017; Foerster et al., 2018; Yang et al., 2020)*

---

## ğŸ“˜ Summary of Tone & Style

| Section    | Writing Style                                | Function                |
| ---------- | -------------------------------------------- | ----------------------- |
| Titles     | concise, declarative (no symbols, no arrows) | quickly orient audience |
| Paragraphs | complete, natural English sentences          | readable & academic     |
| Keywords   | **bold** instead of arrows                   | visual emphasis         |
| Visuals    | meaningful but minimal                       | supports narrative      |

---

æ˜¯å¦å¸Œæœ›æˆ‘æ¥ç€å¸®ä½ å†™ **Slide 3**ï¼ˆå³ä»â€œProblem Statementâ€è¿‡æ¸¡åˆ°â€œResearch Objective / Our Approachâ€ï¼‰ï¼Ÿ
é‚£ä¸€é¡µå¯ä»¥æ­£å¼å¼•å‡º *HOP+ framework* çš„åŠ¨æœºä¸æ€è·¯ï¼Œç”¨ç±»ä¼¼çš„è‹±æ–‡é£æ ¼æ¥å†™ã€‚



# å¼•æ–‡