


### ✅ Slide 1: **Title Slide** *(\~0.5 min)*

**Slide Title**:
**HOP+**
*A Goal-Inference-Based Opponent Modeling Framework for Mixed-Motive Multi-Agent Settings*

**Content**:

* **Presenter**: \[Your Name]
* **Affiliation**: \[University / Institute Name]
* **Date**: \[Presentation Date]

**Script (what to say)**:

> Good afternoon. Today I will present our recent work titled **HOP+**, which focuses on enhancing opponent modeling and planning in multi-agent mixed-motive environments. This work builds upon the HOP framework proposed at ICML 2024.

---

### ✅ Slide 2: **Background – From HOP to HOP+** *(\~1.5 min)*

**Slide Title**:
**From HOP to HOP+: Motivation and Foundation**

**Slide Content**:

**What is HOP?**

* HOP: *Hierarchical Opponent Modeling and Planning (ICML 2024)
* Designed for **mixed-motive multi-agent games**

 **Core Components of HOP ( Inference + Decision Making )**:

1. **Opponent Modeling Module**
   * Infers latent goals of other agents
2. **Planning Module (MCTS-based)**
   * Computes best-response strategies given inferred goals

 **Why HOP Matters**:

* Effective performance in **self-play** and cooperative tasks
* Strong **few-shot adaptation** to unseen opponents

**Script (what to say)**:

> Our work extends the HOP framework, which models agent behavior in mixed-motive settings by separating goal inference and planning. HOP achieves this through a hierarchical architecture where opponent modeling guides the planning module. However, the original HOP uses a limited goal space and static inference mechanisms. Our work aims to push this further.

---

需要图示建议？

你可以在第 2 页加一张简图：
🟢 “Agent Interaction → Opponent Modeling → Goal → MCTS Planning → Action” 这样一个流程箭头图，清晰传达 HOP 的层次结构。

---

是否继续为你写第 3 部分的英文版 PPT？或者需要我生成一份完整可导出的 PowerPoint 文件？


好的，以下是第 **3 部分**（Problem Setting & Environment）的英文 PPT 内容和讲稿建议：

---

### ✅ Slide 3: **Problem Setting & Environment** *(\~2 min)*

**Slide Title**:
**Problem Setting: Mixed-Motive Multi-Agent Interaction**

**Slide Content**:

🎯 **Objective**

* Learn **adaptive agent strategies** in **dynamic, mixed-motive games**
* Enable agents to **model unseen opponents** and **respond effectively**

🧩 **Key Challenges**:

* 🤝 + ⚔️ Mixed motives: agents need to **cooperate** while **competing**
* ⛓️ Dynamic strategies → **non-stationary environments**
* ❓ How to infer opponents’ intent and respond in **few interactions**

🌍 **Test Environment** – *Stag-Hunt Game*

* Grid-based world (6×6, 8×8)
* Agents decide to **hunt individually** (hare) or **coordinate** (stag)
* Rewards depend on **joint actions** and **opponent behavior**

**Diagram** (recommended):

* A **grid world** image showing:

  * Two agents
  * Hares and stags
  * Decision outcomes: solo vs. cooperative




很好，以下是第 **4 部分（Motivation: Why Goal Inference?）** 的 PPT 内容和讲稿建议（英文）：

---

### ✅ Slide 4: **Motivation: Why Goal Inference?** *(\~2 min)*

**Slide Title**:
**Motivation: Limitations of Manual Goal Design**

**Slide Content**:

📌 **Problems with Prior Approaches**

*  **Static goal sets**:
	* Predefined discrete goals are **inflexible**
	* fail to capture **diverse strategies**
* **Manual annotation required**:
	* Goal semantics often need to be designed by hand
	* Handcrafted goals don’t scale well to **unseen opponents**

---
💡 **Our Insight**
>  **infer goals automatically** from interaction trajectories.


🎯 **Benefits of Goal Inference**:

* ✅ **No manual goal engineering**
* ✅ **Continuous latent goal space**
* ✅ **Adaptation to diverse behaviors**
* ✅ **Improved interpretability & planning**

**Optional visual (if space allows)**:

* Diagram comparing:

  * "Manual goal set" (discrete, rigid)
  * "Learned goal representation" (continuous, flexible)

---

**Script (what to say)**:

> In previous opponent modeling frameworks, agents typically rely on a **static set of predefined goals**. These goals are manually constructed, often with limited expressiveness, and fail to adapt when the opponent behaves in unexpected ways.

> Such methods **do not generalize well** to heterogeneous opponents, and they require **significant human effort** to design appropriate goal spaces for each environment.

> To address this, our idea is to **infer latent goals directly** from trajectory data using deep generative modeling. By doing this, we can capture a **continuous and adaptive** goal space that evolves with interaction and requires **no manual labeling**. This provides a more powerful foundation for downstream planning and policy learning.

---

下一部分将是 **5.1 - How: Goal Inference** 的实现细节介绍。如果你准备好了，我可以继续为你撰写该页内容。



明白了！下面是**简洁、清晰、答辩适配版的 Slide 3 英文内容**，保持专业表达、图文结合、重点突出（适合6分钟讲稿场景）：

---

### **Slide 3: Problem & Environment**

---

**Goal**
Enable agents to adapt in **mixed-motive** multi-agent games
→ Must **infer opponent intent** and **respond effectively**

**Challenges**

* Mixed incentives: **cooperation** vs. **competition**
* Opponents behave **dynamically**, non-stationary
* Limited interactions for **adaptation**

**Environment: Stag-Hunt Game** —— **MSH**

* Grid-based map (6×6 or 8×8)
* Choose:
  * 🐇 Hunt hare (solo, low reward)
  * 🦌 Hunt stag (team up, high reward)
* Reward depends on **joint actions**

---

**📊 Visual (on right)**

* Grid world with:

  * Agents, hare, stag
  * Outcome arrows: solo hunt vs. team hunt

---

需要我帮你做这个图吗？或者我们继续到 Slide 4: **Motivation**？



好的，以下是简洁、专业风格的英文版 Slide 3 内容，**去掉所有 icon 表情符号，减少文字量，突出核心要点**，适合答辩 PPT：

---

### **Slide 3: Problem Setting and Environment**

---

**Objective**
Learn adaptive strategies in mixed-motive multi-agent environments.
Agents must model diverse opponents and respond effectively under uncertainty.

---

**Key Challenges**

* Mixed incentives: agents must balance cooperation and competition
* Dynamic interactions lead to non-stationary opponent behaviors
* Limited interactions require fast adaptation to unseen strategies

---

**Test Environment: Stag-Hunt Game**

* Grid-based world (6×6 or 8×8)
* Agents choose to hunt hare (individually) or stag (cooperatively)
* Rewards depend on joint actions and mutual coordination

---

**(Optional Visual)**
A simple grid-world diagram showing two agents, one hare, one stag, and outcome labels:

* Solo action → lower reward
* Coordinated action → higher reward

---

如果你需要，我可以帮你生成这个图。是否继续到 Slide 4: Motivation？




非常好的问题，而且问得非常关键。
对于 **HOP+ 项目**（Hierarchical Opponent-Modeling & Planning Plus），“**formulation**” 是整篇论文或汇报中最能体现算法思想与理论深度的部分。
这一部分不能只是写“我们用了一个VAE+MOA的结构”，而要通过**数学建模**把“我们想解决的问题”“我们的结构逻辑”“我们的优化目标”都系统地表达出来。

我来一步步带你拆解：

---

## 🧭 一、Formulation 的总体思路

HOP+ 的 formulation 可以从 **三条主线**来组织：

| 主线                                            | 描述                | 数学建模重点                                                                                    |
| --------------------------------------------- | ----------------- | ----------------------------------------------------------------------------------------- |
| **1. 博弈过程建模（Game Formulation）**               | 描述多智能体交互的总体问题     | 状态、动作、奖励、转移、目标函数                                                                          |
| **2. 对手建模结构（Opponent Modeling Formulation）**  | 描述如何建模对手的隐变量或策略表征 | 隐变量 $z$、编码器$q_\phi(zs,a)$、预测器$\pi_\theta(as,z)$                                           |
| **3. 联合优化目标（Joint Optimization Formulation）** | 描述整个系统的联合学习目标     | $\mathcal{L} = \mathcal{L}*{policy} + \alpha \mathcal{L}*{VAE} + \beta \mathcal{L}_{MOA}$ |

---

## 🧩 二、Formulation 的分层结构建议（HOP+专用）

下面是 HOP+ 非常合适的 formulation 框架（你可以在论文或PPT中直接按这个逻辑组织）：

---

### **2.1 Multi-Agent Environment Formulation**

我们首先把交互过程形式化为一个 **部分可观测的多智能体马尔可夫博弈**（POMG）：

$$
\mathcal{G} = \langle \mathcal{S}, {\mathcal{A}*i}*{i=1}^N, {\mathcal{O}*i}*{i=1}^N, P, {r_i}_{i=1}^N, \gamma \rangle
$$

其中：

* $\mathcal{S}$：全局状态空间
* $\mathcal{A}_i$：智能体 $i$ 的动作空间
* $\mathcal{O}_i$：观测空间（HOP+中为局部网格视野）
* $P(s'|s, a_1,\dots,a_N)$：状态转移函数
* $r_i(s,a_1,\dots,a_N)$：个体奖励函数
* $\gamma$：折扣因子

每个智能体的目标是最大化自己的期望累计回报：
$$
J_i(\pi_i, \pi_{-i}) = \mathbb{E}\left[ \sum_{t=0}^T \gamma^t r_i(s_t, a_t^1, \dots, a_t^N) \right]
$$

---

### **2.2 Opponent Modeling Formulation**

在 HOP+ 中，核心创新是引入了一个 **隐变量 $z_t$** 表征对手的潜在行为模式。
我们使用一个 **变分自编码器（VAE）** 来捕获这种潜在结构：

* 编码器：$q_\phi(z_t|s_t, a_t)$
* 先验分布：$p(z_t) = \mathcal{N}(0,I)$
* 解码器/策略：$\pi_\theta(a_t|s_t, z_t)$

VAE 优化目标：
$$
\mathcal{L}*{\text{VAE}} =
\mathbb{E}*{q_\phi(z_t|s_t,a_t)}[-\log p_\theta(a_t|s_t,z_t)]

* D_{\text{KL}}(q_\phi(z_t|s_t,a_t) \Vert p(z_t))
  $$

这样，$z_t$ 就成为一个可学习的对手特征嵌入，用于增强策略网络的感知能力。

---

### **2.3 Policy Learning Formulation**

在策略层面，HOP+ 仍然是基于强化学习（如 PPO）的优化：
$$
\mathcal{L}*{\text{policy}} =
\mathbb{E}*{t}\left[
-\log \pi_\theta(a_t|s_t, z_t) A_t

* \beta D_{\text{KL}}(\pi_\theta(\cdot|s_t,z_t)\Vert \pi_{\theta_{\text{old}}}(\cdot|s_t,z_t))
  \right]
  $$

其中 $A_t$ 为优势函数，$\beta$ 为 KL 正则系数。

---

### **2.4 MOA Formulation（Model-of-Agents）**

HOP+ 的另一个核心部分是 **MOA（对手行为预测模块）**。
它的目标是从历史轨迹中预测对手的未来动作：

$$
\mathcal{L}*{\text{MOA}} =
\mathbb{E}*{(s_t, a_t^{-i})\sim D}
\left[-\log p_\psi(a_t^{-i}|s_t, z_t)\right]
$$

这一项鼓励模型在潜空间中学到“对手如何决策”的可预测结构。

---

### **2.5 Joint Optimization Objective**

最终，HOP+ 的整体优化目标为：

$$
\min_{\theta, \phi, \psi}
\mathcal{L}*{\text{total}} =
\mathcal{L}*{\text{policy}}

* \alpha \mathcal{L}_{\text{VAE}}
* \beta \mathcal{L}_{\text{MOA}}
  $$

其中：

* $\mathcal{L}_{\text{policy}}$ —— 主策略更新项
* $\mathcal{L}_{\text{VAE}}$ —— 对手行为嵌入学习项
* $\mathcal{L}_{\text{MOA}}$ —— 对手动作预测项
* $\alpha, \beta$ 为权重系数（通常通过实验调节）

---



## 📚 四、总结

所以，HOP+ 的 formulation 可以总结为一句话：

> **我们将多智能体博弈形式化为部分可观测马尔可夫博弈，通过引入潜变量 $z_t$ 来表征对手行为分布，并在统一的优化框架下联合训练策略网络、对手建模网络与潜空间生成器。**








# 引文