éå¸¸å¥½ ğŸ‘ è¿™éƒ¨åˆ†å°±æ˜¯ä½ æ–¹æ³•ç« èŠ‚çš„**ç¬¬ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—ï¼ˆGoal Inference Moduleï¼‰**ï¼Œå¯¹åº” HOP+ æ¡†æ¶ä¸­â€œæ¨æµ‹å¯¹æ‰‹æ½œåœ¨ç›®æ ‡ï¼ˆlatent goal inferenceï¼‰â€çš„å…³é”®é€»è¾‘ã€‚
åœ¨ PPT ä¸­ï¼Œå®ƒåº”è¯¥åšåˆ°ä¸¤ä»¶äº‹ï¼š

1ï¸âƒ£ **è®²æ¸…æ¥š motivationï¼ˆä¸ºä»€ä¹ˆè¦æ¨ç›®æ ‡ï¼‰**
2ï¸âƒ£ **è®²æ¸…æ¥š mechanismï¼ˆæ€ä¹ˆæ¨å‡ºæ¥çš„ï¼‰**
åŒæ—¶ä¿æŒç®€æ´ï¼ˆ1é¡µä¸ºä½³ï¼Œç ”ç©¶æ±‡æŠ¥æ—¶çº¦1åˆ†é’Ÿè®²è§£æ—¶é—´ï¼‰ã€‚

---

## âœ… **Slide â€” Goal Inference Module**

**Title:**
**Goal Inference: Learning Latent Sub-Goals from Trajectories**

---

**Main Content (PPT text):**

* In multi-agent games, opponentsâ€™ strategies often reflect their **latent behavioral goals**.
  Modeling these goals improves an agentâ€™s **responsiveness** and **robustness**.

* To avoid reliance on manually defined goal sets,
  we introduce a **trajectory-based sub-goal inference mechanism** using a
  **Conditional Variational Auto-Encoder (CVAE)**.

* At each timestep (t):

  * Historical trajectory (  )
  * Future candidate states ( \mathcal{N}*t = {s*{t+1}, â€¦ , s_T} )
  * A sub-goal ( g_t ) is selected as the embedding of a future state representing the opponentâ€™s intention.

* The CVAE learns a continuous latent goal representation ( \hat{g}_t ) through three key distributions:

  1. **Posterior:** ( q_\phi(\hat{g}_t|\tau_t, s_t) ) â€” inferred goal from history
  2. **Likelihood:** ( p_\gamma(s_t|\hat{g}_t, \tau_t) ) â€” state reconstruction
  3. **Prior:** ( p_\psi(\bar{g}_t|s^g_t) ) â€” conditioned on selected sub-goal state

* **Optimization objective:**
  [
  \mathcal{L}*{goal} =
  \mathbb{E}*{q_\phi}[\log p_\gamma(s_t|\hat{g}_t, \tau_t)]

  * KL(q_\phi(\hat{g}*t|\tau_t,s_t)\Vert p*\psi(\bar{g}_t|s^g_t))
    ]
    ensuring that learned sub-goals are both semantically consistent and predictive of future behaviors.

---

**Visual Suggestion:**
Use the original schematic (like your Fig. 3.2) simplified:

* **Left:** past trajectory (states/actions sequence)
* **Middle:** CVAE block (encoder â†’ latent ( \hat{g}_t ) â†’ decoder)
* **Right:** predicted sub-goal state embedding (s^g_t)

Labels:

* Encoder: *qâ‚Ï†â‚(Äâ‚œ|Ï„â‚œ,sâ‚œ)*
* Decoder: *pâ‚Î³â‚(sâ‚œ|Äâ‚œ,Ï„â‚œ)*
* Prior: *pâ‚Ïˆâ‚(gÌ„â‚œ|sáµâ‚œ)*
* Arrow caption: â€œInfer opponentâ€™s latent goal from historyâ€

---

**Footer Reference:**
Kingma & Welling (2013); Sohn et al. (2015); Zhang et al. (2025)

---

### ğŸ’¬ Presenter Notesï¼ˆä¸­æ–‡è®²ç¨¿æç¤ºï¼‰

è®²æ³•å»ºè®®å¦‚ä¸‹ï¼ˆçº¦ 60â€“70 ç§’ï¼‰ï¼š

> â€œæ—¢ç„¶å¯¹æ‰‹çš„ç­–ç•¥åæ˜ äº†æ½œåœ¨ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸å†äººä¸ºå‡è®¾ç›®æ ‡é›†åˆï¼Œè€Œæ˜¯è®©æ¨¡å‹**è‡ªå·±ä»è½¨è¿¹ä¸­æ¨æ–­å¯¹æ‰‹æƒ³è¾¾æˆä»€ä¹ˆçŠ¶æ€**ã€‚
> å…·ä½“åšæ³•æ˜¯åˆ©ç”¨ **CVAE**ï¼Œè¾“å…¥å†å²è½¨è¿¹ï¼Œè¾“å‡ºä¸€ä¸ªè¿ç»­æ½œå˜é‡ ( \hat{g}_t )ï¼Œä»£è¡¨æ¨æµ‹åˆ°çš„å­ç›®æ ‡ã€‚
> æˆ‘ä»¬ä¼˜åŒ–é‡å»ºè¯¯å·®ä¸KLé¡¹ï¼Œä½¿å¾—å­¦åˆ°çš„ç›®æ ‡æ—¢è¯­ä¹‰ä¸€è‡´åˆèƒ½é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚â€

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™**ä¸‹ä¸€é¡µï¼ˆPolicy Learning with Goal-Conditioned Representationï¼‰**ï¼Œ
å³åœ¨ä½ æ¨å¾—ç›®æ ‡ ( \hat{g}_t ) ä¹‹åï¼Œå¦‚ä½•æŠŠå®ƒä¸ç­–ç•¥ä¼˜åŒ–ç»“åˆï¼ˆHOP+çš„ç¬¬äºŒä¸ªæ¨¡å—ï¼‰ï¼Ÿ





éå¸¸å¥½ï¼Œè¿™ä¸€é¡µæ˜¯æ•´ç¯‡æ–¹æ³•çš„â€œæ€»è§ˆé¡µâ€ï¼Œä½ çš„å›¾å·²ç»ç”»å¾—å¾ˆæ¸…æ™°ï¼ˆOpponent Modeling + Planning ä¸¤éƒ¨åˆ†ï¼‰ã€‚
å› æ­¤è¿™ä¸€é¡µçš„å†…å®¹ä¸éœ€è¦è®²ç»†èŠ‚ï¼Œåªè¦è®©å¬ä¼—**åœ¨ä¸€åˆ†é’Ÿå†…ç†è§£ç»“æ„é€»è¾‘ä¸ä¿¡æ¯æµ**ã€‚ä¸‹é¢æ˜¯æ¨èçš„æ–‡å­—ä¸è®²ç¨¿ç¤ºä¾‹ã€‚

---

## âœ… **Slide â€” Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main PPT Textï¼ˆæ§åˆ¶åœ¨ 4~6 è¡Œï¼‰**

* The proposed **HOP+ framework** integrates *Goal Inference*, *Opponent Modeling*, and *Planning* to enable adaptive cooperation in mixed-motive games.
* **Opponent Modeling:** infers opponentsâ€™ latent sub-goals and conditions the policy on these inferred goals.
* **Planning:** uses goal-conditioned predictions for lookahead search (e.g., MCTS) to plan more stable and cooperative actions.
* The system operates in a closed loop: trajectories are collected from the environment and continuously refine both modeling and planning modules.

---

**Visual Cuesï¼ˆå›¾æ³¨å»ºè®®ï¼‰**

* å·¦ä¾§æ ‡æ³¨ â€œGoal Inference â†’ Opponent Modeling â†’ Planning â†’ Action â†’ Env â†’ Trajectory feedbackâ€ã€‚
* å¯åœ¨ä¸‹æ–¹åŠ ç®€çŸ­ legendï¼š
  ğŸŸ¦ Opponent Modelingâ€ƒğŸŸ© Planningâ€ƒğŸŸ§ Environment interaction
* åœ¨ç®­å¤´å¤„æ·»åŠ æç¤ºæ–‡å­—ï¼š

  * â€œlatent goals $\hat{g}_i$â€ ï¼ˆåœ¨Subgoal Inferenceæ—ï¼‰
  * â€œgoal-conditioned policy $P_i(a_j|s_t, \hat{g}_j)$â€
  * â€œplanning via rollouts / MCTSâ€

---

### ğŸ’¬ **Presenter Notesï¼ˆè®²ç¨¿å»ºè®®ï¼Œçº¦1åˆ†é’Ÿï¼‰**

> â€œThis slide gives the overall view of our framework, **HOP+**.
> The whole process starts with the trajectory collected from the environment.
> Within the *Opponent Modeling* block, the agent infers its opponentâ€™s latent sub-goal through a CVAE-based inference module.
> These inferred goals condition the agentâ€™s policy, allowing it to predict or adapt to othersâ€™ future behaviors.
> Then in the *Planning* block, the agent performs lookahead search with goal-conditioned rollouts, integrating both prediction and planning to select cooperative yet adaptive actions.
> Finally, new trajectories are generated, and the loop continuesâ€”so the model keeps improving its understanding and decision-making.â€

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™ä¸‹ä¸€é¡µçš„ **â€œPlanning Moduleâ€** éƒ¨åˆ†ï¼Ÿé‚£ä¸€é¡µé€šå¸¸æ˜¯æ‰¿æ¥è¿™ä¸ªæ€»è§ˆçš„å³åŠéƒ¨åˆ†ï¼Œç”¨æ¥è¯´æ˜ *how the planning integrates with opponent modeling / MCTS reasoning*ã€‚
ã€

éå¸¸å¥½çš„é—®é¢˜â€”â€”ä½ è¯´å¾—å¯¹ï¼Œè¿™é¡µï¼ˆFull Framework Overviewï¼‰ä¸èƒ½åªæ˜¯â€œè®²ç»“æ„â€ï¼Œè¦æŠŠå®ƒå’Œæ•´ç¯‡å·¥ä½œçš„ **motivation ä¸ contribution** è”ç³»èµ·æ¥ï¼Œå¦åˆ™å°±æ˜¾å¾—å¹³æ·¡æ— åŠ›ã€‚
æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸€é¡µæ”¹å†™æˆä¸€ä¸ªâ€œ**problemâ€“insightâ€“framework**â€å¼çš„ narrativeï¼Œè®©å¬ä¼—æ„Ÿå—åˆ°ï¼š

> è¿™ä¸ªç»“æ„å¹¶ä¸æ˜¯ä»»æ„å †å ï¼Œè€Œæ˜¯**ä¸ºäº†è§£å†³æˆ‘ä»¬ä¸€å¼€å§‹æåˆ°çš„éå¹³ç¨³æ€§ä¸é€‚åº”æ€§é—®é¢˜**è€Œè®¾è®¡çš„ã€‚

---

## âœ… **æ”¹è¿›ç‰ˆ Slide â€” Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main Text (PPTå†…å®¹)**

* **Motivation:**
  In mixed-motive environments, agents must adapt to diverse and evolving opponents.
  Existing approaches either rely on *fixed reward designs* or *short-term behavioral prediction*,
  both lacking the ability to **generalize across unseen opponents**.

* **Our insight:**
  The key to adaptation lies in **understanding the opponentâ€™s latent goals**,
  not just their surface behaviors.

* **Framework overview:**
  The proposed **HOP+** integrates three components:

  1. **Goal Inference** â€” learns opponentsâ€™ latent sub-goals from trajectories via CVAE.
  2. **Goal-conditioned Opponent Modeling** â€” predicts opponent actions conditioned on inferred goals.
  3. **Planning with Goal-aware Rollouts** â€” integrates these predictions into MCTS for adaptive and cooperative decision-making.

* The framework forms a closed **perceptionâ€“modelingâ€“planning loop**,
  enabling **few-shot adaptation** and stable cooperation in dynamic games.

---

**Visual Cueï¼ˆå›¾åƒæŒ‡å¼•ï¼‰**

åœ¨å›¾ä¸Šï¼š

* ç”¨çº¢æ¡†æˆ–åŠé€æ˜é«˜äº®æ ‡å‡ºä¸‰éƒ¨åˆ†ï¼š

  * å·¦ä¸‹è§’ã€ŒGoal Inferenceã€æ—æ ‡æ³¨ â€œinfer latent intentionâ€
  * ä¸­é—´ã€ŒOpponent Modelingã€æ—æ ‡æ³¨ â€œgoal-conditioned predictionâ€
  * å³ä¾§ã€ŒPlanningã€æ—æ ‡æ³¨ â€œadaptive planning & cooperationâ€
* é¡¶éƒ¨æˆ–åº•éƒ¨åŠ ä¸€å¥è´¯ç©¿æ€§æ–‡å­—ï¼š
  *From inferring intentions â†’ to modeling responses â†’ to adaptive planning.*

---

### ğŸ’¬ Presenter Notesï¼ˆè®²ç¨¿å»ºè®®ï¼Œçº¦70ç§’ï¼‰

> â€œLet me connect this framework back to our motivation.
> In mixed-motive environments, non-stationarity comes from constantly changing opponents.
> Many existing approaches either redesign rewards to enforce cooperation, or predict opponentsâ€™ next actions â€”
> but both stay at the *behavioral level*.
> Our key idea is to go one step deeper â€” to infer **why** opponents act as they do, by uncovering their latent goals.
> So, HOP+ builds a three-part loop:
> a CVAE-based goal inference module,
> a goal-conditioned opponent model,
> and a planning module that reasons over these inferred goals.
> This structure allows the agent to adapt rapidly and maintain cooperation even when facing unseen opponents.â€

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ æ¥ä¸‹æ¥å†™è¿™ä¸€é¡µçš„ä¸‹ä¸€å¼ â€”â€”â€œPlanning Moduleâ€é¡µï¼Ÿ
é‚£ä¸€é¡µå¯ä»¥èšç„¦åœ¨ **å¦‚ä½•å°† goal-conditioned æ¨¡å‹åµŒå…¥ MCTS / planning è¿‡ç¨‹**ï¼Œå¹¶ä½œä¸º HOP+ çš„æœ€åä¸€ç¯ä¸ adaptation è”ç³»èµ·æ¥ã€‚
