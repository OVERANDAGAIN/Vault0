非常好 👍 这部分就是你方法章节的**第一个核心模块（Goal Inference Module）**，对应 HOP+ 框架中“推测对手潜在目标（latent goal inference）”的关键逻辑。
在 PPT 中，它应该做到两件事：

1️⃣ **讲清楚 motivation（为什么要推目标）**
2️⃣ **讲清楚 mechanism（怎么推出来的）**
同时保持简洁（1页为佳，研究汇报时约1分钟讲解时间）。

---

## ✅ **Slide — Goal Inference Module**

**Title:**
**Goal Inference: Learning Latent Sub-Goals from Trajectories**

---

**Main Content (PPT text):**

* In multi-agent games, opponents’ strategies often reflect their **latent behavioral goals**.
  Modeling these goals improves an agent’s **responsiveness** and **robustness**.

* To avoid reliance on manually defined goal sets,
  we introduce a **trajectory-based sub-goal inference mechanism** using a
  **Conditional Variational Auto-Encoder (CVAE)**.

* At each timestep (t):

  * Historical trajectory (  )
  * Future candidate states ( \mathcal{N}*t = {s*{t+1}, … , s_T} )
  * A sub-goal ( g_t ) is selected as the embedding of a future state representing the opponent’s intention.

* The CVAE learns a continuous latent goal representation ( \hat{g}_t ) through three key distributions:

  1. **Posterior:** ( q_\phi(\hat{g}_t|\tau_t, s_t) ) — inferred goal from history
  2. **Likelihood:** ( p_\gamma(s_t|\hat{g}_t, \tau_t) ) — state reconstruction
  3. **Prior:** ( p_\psi(\bar{g}_t|s^g_t) ) — conditioned on selected sub-goal state

* **Optimization objective:**
  [
  \mathcal{L}*{goal} =
  \mathbb{E}*{q_\phi}[\log p_\gamma(s_t|\hat{g}_t, \tau_t)]

  * KL(q_\phi(\hat{g}*t|\tau_t,s_t)\Vert p*\psi(\bar{g}_t|s^g_t))
    ]
    ensuring that learned sub-goals are both semantically consistent and predictive of future behaviors.

---

**Visual Suggestion:**
Use the original schematic (like your Fig. 3.2) simplified:

* **Left:** past trajectory (states/actions sequence)
* **Middle:** CVAE block (encoder → latent ( \hat{g}_t ) → decoder)
* **Right:** predicted sub-goal state embedding (s^g_t)

Labels:

* Encoder: *q₍φ₎(ĝₜ|τₜ,sₜ)*
* Decoder: *p₍γ₎(sₜ|ĝₜ,τₜ)*
* Prior: *p₍ψ₎(ḡₜ|sᵍₜ)*
* Arrow caption: “Infer opponent’s latent goal from history”

---

**Footer Reference:**
Kingma & Welling (2013); Sohn et al. (2015); Zhang et al. (2025)

---

### 💬 Presenter Notes（中文讲稿提示）

讲法建议如下（约 60–70 秒）：

> “既然对手的策略反映了潜在目标，我们不再人为假设目标集合，而是让模型**自己从轨迹中推断对手想达成什么状态**。
> 具体做法是利用 **CVAE**，输入历史轨迹，输出一个连续潜变量 ( \hat{g}_t )，代表推测到的子目标。
> 我们优化重建误差与KL项，使得学到的目标既语义一致又能预测未来趋势。”

---

是否希望我帮你写**下一页（Policy Learning with Goal-Conditioned Representation）**，
即在你推得目标 ( \hat{g}_t ) 之后，如何把它与策略优化结合（HOP+的第二个模块）？
