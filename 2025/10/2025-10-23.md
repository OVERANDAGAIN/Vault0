éå¸¸å¥½ ğŸ‘ è¿™éƒ¨åˆ†å°±æ˜¯ä½ æ–¹æ³•ç« èŠ‚çš„**ç¬¬ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—ï¼ˆGoal Inference Moduleï¼‰**ï¼Œå¯¹åº” HOP+ æ¡†æ¶ä¸­â€œæ¨æµ‹å¯¹æ‰‹æ½œåœ¨ç›®æ ‡ï¼ˆlatent goal inferenceï¼‰â€çš„å…³é”®é€»è¾‘ã€‚
åœ¨ PPT ä¸­ï¼Œå®ƒåº”è¯¥åšåˆ°ä¸¤ä»¶äº‹ï¼š

1ï¸âƒ£ **è®²æ¸…æ¥š motivationï¼ˆä¸ºä»€ä¹ˆè¦æ¨ç›®æ ‡ï¼‰**
2ï¸âƒ£ **è®²æ¸…æ¥š mechanismï¼ˆæ€ä¹ˆæ¨å‡ºæ¥çš„ï¼‰**
åŒæ—¶ä¿æŒç®€æ´ï¼ˆ1é¡µä¸ºä½³ï¼Œç ”ç©¶æ±‡æŠ¥æ—¶çº¦1åˆ†é’Ÿè®²è§£æ—¶é—´ï¼‰ã€‚

---

## âœ… **Slide â€” Goal Inference Module**

**Title:**
**Goal Inference: Learning Latent Sub-Goals from Trajectories**

---

**Main Content (PPT text):**

* In multi-agent games, opponentsâ€™ strategies often reflect their **latent behavioral goals**.
  Modeling these goals improves an agentâ€™s **responsiveness** and **robustness**.

* To avoid reliance on manually defined goal sets,
  we introduce a **trajectory-based sub-goal inference mechanism** using a
  **Conditional Variational Auto-Encoder (CVAE)**.

* At each timestep (t):

  * Historical trajectory (  )
  * Future candidate states ( \mathcal{N}*t = {s*{t+1}, â€¦ , s_T} )
  * A sub-goal ( g_t ) is selected as the embedding of a future state representing the opponentâ€™s intention.

* The CVAE learns a continuous latent goal representation ( \hat{g}_t ) through three key distributions:

  1. **Posterior:** ( q_\phi(\hat{g}_t|\tau_t, s_t) ) â€” inferred goal from history
  2. **Likelihood:** ( p_\gamma(s_t|\hat{g}_t, \tau_t) ) â€” state reconstruction
  3. **Prior:** ( p_\psi(\bar{g}_t|s^g_t) ) â€” conditioned on selected sub-goal state

* **Optimization objective:**
  [
  \mathcal{L}*{goal} =
  \mathbb{E}*{q_\phi}[\log p_\gamma(s_t|\hat{g}_t, \tau_t)]

  * KL(q_\phi(\hat{g}*t|\tau_t,s_t)\Vert p*\psi(\bar{g}_t|s^g_t))
    ]
    ensuring that learned sub-goals are both semantically consistent and predictive of future behaviors.

---

**Visual Suggestion:**
Use the original schematic (like your Fig. 3.2) simplified:

* **Left:** past trajectory (states/actions sequence)
* **Middle:** CVAE block (encoder â†’ latent ( \hat{g}_t ) â†’ decoder)
* **Right:** predicted sub-goal state embedding (s^g_t)

Labels:

* Encoder: *qâ‚Ï†â‚(Äâ‚œ|Ï„â‚œ,sâ‚œ)*
* Decoder: *pâ‚Î³â‚(sâ‚œ|Äâ‚œ,Ï„â‚œ)*
* Prior: *pâ‚Ïˆâ‚(gÌ„â‚œ|sáµâ‚œ)*
* Arrow caption: â€œInfer opponentâ€™s latent goal from historyâ€

---

**Footer Reference:**
Kingma & Welling (2013); Sohn et al. (2015); Zhang et al. (2025)

---

### ğŸ’¬ Presenter Notesï¼ˆä¸­æ–‡è®²ç¨¿æç¤ºï¼‰

è®²æ³•å»ºè®®å¦‚ä¸‹ï¼ˆçº¦ 60â€“70 ç§’ï¼‰ï¼š

> â€œæ—¢ç„¶å¯¹æ‰‹çš„ç­–ç•¥åæ˜ äº†æ½œåœ¨ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸å†äººä¸ºå‡è®¾ç›®æ ‡é›†åˆï¼Œè€Œæ˜¯è®©æ¨¡å‹**è‡ªå·±ä»è½¨è¿¹ä¸­æ¨æ–­å¯¹æ‰‹æƒ³è¾¾æˆä»€ä¹ˆçŠ¶æ€**ã€‚
> å…·ä½“åšæ³•æ˜¯åˆ©ç”¨ **CVAE**ï¼Œè¾“å…¥å†å²è½¨è¿¹ï¼Œè¾“å‡ºä¸€ä¸ªè¿ç»­æ½œå˜é‡ ( \hat{g}_t )ï¼Œä»£è¡¨æ¨æµ‹åˆ°çš„å­ç›®æ ‡ã€‚
> æˆ‘ä»¬ä¼˜åŒ–é‡å»ºè¯¯å·®ä¸KLé¡¹ï¼Œä½¿å¾—å­¦åˆ°çš„ç›®æ ‡æ—¢è¯­ä¹‰ä¸€è‡´åˆèƒ½é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚â€

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™**ä¸‹ä¸€é¡µï¼ˆPolicy Learning with Goal-Conditioned Representationï¼‰**ï¼Œ
å³åœ¨ä½ æ¨å¾—ç›®æ ‡ ( \hat{g}_t ) ä¹‹åï¼Œå¦‚ä½•æŠŠå®ƒä¸ç­–ç•¥ä¼˜åŒ–ç»“åˆï¼ˆHOP+çš„ç¬¬äºŒä¸ªæ¨¡å—ï¼‰ï¼Ÿ





éå¸¸å¥½ï¼Œè¿™ä¸€é¡µæ˜¯æ•´ç¯‡æ–¹æ³•çš„â€œæ€»è§ˆé¡µâ€ï¼Œä½ çš„å›¾å·²ç»ç”»å¾—å¾ˆæ¸…æ™°ï¼ˆOpponent Modeling + Planning ä¸¤éƒ¨åˆ†ï¼‰ã€‚
å› æ­¤è¿™ä¸€é¡µçš„å†…å®¹ä¸éœ€è¦è®²ç»†èŠ‚ï¼Œåªè¦è®©å¬ä¼—**åœ¨ä¸€åˆ†é’Ÿå†…ç†è§£ç»“æ„é€»è¾‘ä¸ä¿¡æ¯æµ**ã€‚ä¸‹é¢æ˜¯æ¨èçš„æ–‡å­—ä¸è®²ç¨¿ç¤ºä¾‹ã€‚

---

## âœ… **Slide â€” Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main PPT Textï¼ˆæ§åˆ¶åœ¨ 4~6 è¡Œï¼‰**

* The proposed **HOP+ framework** integrates *Goal Inference*, *Opponent Modeling*, and *Planning* to enable adaptive cooperation in mixed-motive games.
* **Opponent Modeling:** infers opponentsâ€™ latent sub-goals and conditions the policy on these inferred goals.
* **Planning:** uses goal-conditioned predictions for lookahead search (e.g., MCTS) to plan more stable and cooperative actions.
* The system operates in a closed loop: trajectories are collected from the environment and continuously refine both modeling and planning modules.

---

**Visual Cuesï¼ˆå›¾æ³¨å»ºè®®ï¼‰**

* å·¦ä¾§æ ‡æ³¨ â€œGoal Inference â†’ Opponent Modeling â†’ Planning â†’ Action â†’ Env â†’ Trajectory feedbackâ€ã€‚
* å¯åœ¨ä¸‹æ–¹åŠ ç®€çŸ­ legendï¼š
  ğŸŸ¦ Opponent Modelingâ€ƒğŸŸ© Planningâ€ƒğŸŸ§ Environment interaction
* åœ¨ç®­å¤´å¤„æ·»åŠ æç¤ºæ–‡å­—ï¼š

  * â€œlatent goals $\hat{g}_i$â€ ï¼ˆåœ¨Subgoal Inferenceæ—ï¼‰
  * â€œgoal-conditioned policy $P_i(a_j|s_t, \hat{g}_j)$â€
  * â€œplanning via rollouts / MCTSâ€

---

### ğŸ’¬ **Presenter Notesï¼ˆè®²ç¨¿å»ºè®®ï¼Œçº¦1åˆ†é’Ÿï¼‰**

> â€œThis slide gives the overall view of our framework, **HOP+**.
> The whole process starts with the trajectory collected from the environment.
> Within the *Opponent Modeling* block, the agent infers its opponentâ€™s latent sub-goal through a CVAE-based inference module.
> These inferred goals condition the agentâ€™s policy, allowing it to predict or adapt to othersâ€™ future behaviors.
> Then in the *Planning* block, the agent performs lookahead search with goal-conditioned rollouts, integrating both prediction and planning to select cooperative yet adaptive actions.
> Finally, new trajectories are generated, and the loop continuesâ€”so the model keeps improving its understanding and decision-making.â€

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™ä¸‹ä¸€é¡µçš„ **â€œPlanning Moduleâ€** éƒ¨åˆ†ï¼Ÿé‚£ä¸€é¡µé€šå¸¸æ˜¯æ‰¿æ¥è¿™ä¸ªæ€»è§ˆçš„å³åŠéƒ¨åˆ†ï¼Œç”¨æ¥è¯´æ˜ *how the planning integrates with opponent modeling / MCTS reasoning*ã€‚
