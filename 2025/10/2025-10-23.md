非常好 👍 这部分就是你方法章节的**第一个核心模块（Goal Inference Module）**，对应 HOP+ 框架中“推测对手潜在目标（latent goal inference）”的关键逻辑。
在 PPT 中，它应该做到两件事：

1️⃣ **讲清楚 motivation（为什么要推目标）**
2️⃣ **讲清楚 mechanism（怎么推出来的）**
同时保持简洁（1页为佳，研究汇报时约1分钟讲解时间）。

---

## ✅ **Slide — Goal Inference Module**

**Title:**
**Goal Inference: Learning Latent Sub-Goals from Trajectories**

---

**Main Content (PPT text):**

* In multi-agent games, opponents’ strategies often reflect their **latent behavioral goals**.
  Modeling these goals improves an agent’s **responsiveness** and **robustness**.

* To avoid reliance on manually defined goal sets,
  we introduce a **trajectory-based sub-goal inference mechanism** using a
  **Conditional Variational Auto-Encoder (CVAE)**.

* At each timestep (t):

  * Historical trajectory (  )
  * Future candidate states ( \mathcal{N}*t = {s*{t+1}, … , s_T} )
  * A sub-goal ( g_t ) is selected as the embedding of a future state representing the opponent’s intention.

* The CVAE learns a continuous latent goal representation ( \hat{g}_t ) through three key distributions:

  1. **Posterior:** ( q_\phi(\hat{g}_t|\tau_t, s_t) ) — inferred goal from history
  2. **Likelihood:** ( p_\gamma(s_t|\hat{g}_t, \tau_t) ) — state reconstruction
  3. **Prior:** ( p_\psi(\bar{g}_t|s^g_t) ) — conditioned on selected sub-goal state

* **Optimization objective:**
  [
  \mathcal{L}*{goal} =
  \mathbb{E}*{q_\phi}[\log p_\gamma(s_t|\hat{g}_t, \tau_t)]

  * KL(q_\phi(\hat{g}*t|\tau_t,s_t)\Vert p*\psi(\bar{g}_t|s^g_t))
    ]
    ensuring that learned sub-goals are both semantically consistent and predictive of future behaviors.

---

**Visual Suggestion:**
Use the original schematic (like your Fig. 3.2) simplified:

* **Left:** past trajectory (states/actions sequence)
* **Middle:** CVAE block (encoder → latent ( \hat{g}_t ) → decoder)
* **Right:** predicted sub-goal state embedding (s^g_t)

Labels:

* Encoder: *q₍φ₎(ĝₜ|τₜ,sₜ)*
* Decoder: *p₍γ₎(sₜ|ĝₜ,τₜ)*
* Prior: *p₍ψ₎(ḡₜ|sᵍₜ)*
* Arrow caption: “Infer opponent’s latent goal from history”

---

**Footer Reference:**
Kingma & Welling (2013); Sohn et al. (2015); Zhang et al. (2025)

---

### 💬 Presenter Notes（中文讲稿提示）

讲法建议如下（约 60–70 秒）：

> “既然对手的策略反映了潜在目标，我们不再人为假设目标集合，而是让模型**自己从轨迹中推断对手想达成什么状态**。
> 具体做法是利用 **CVAE**，输入历史轨迹，输出一个连续潜变量 ( \hat{g}_t )，代表推测到的子目标。
> 我们优化重建误差与KL项，使得学到的目标既语义一致又能预测未来趋势。”

---

是否希望我帮你写**下一页（Policy Learning with Goal-Conditioned Representation）**，
即在你推得目标 ( \hat{g}_t ) 之后，如何把它与策略优化结合（HOP+的第二个模块）？





非常好，这一页是整篇方法的“总览页”，你的图已经画得很清晰（Opponent Modeling + Planning 两部分）。
因此这一页的内容不需要讲细节，只要让听众**在一分钟内理解结构逻辑与信息流**。下面是推荐的文字与讲稿示例。

---

## ✅ **Slide — Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main PPT Text（控制在 4~6 行）**

* The proposed **HOP+ framework** integrates *Goal Inference*, *Opponent Modeling*, and *Planning* to enable adaptive cooperation in mixed-motive games.
* **Opponent Modeling:** infers opponents’ latent sub-goals and conditions the policy on these inferred goals.
* **Planning:** uses goal-conditioned predictions for lookahead search (e.g., MCTS) to plan more stable and cooperative actions.
* The system operates in a closed loop: trajectories are collected from the environment and continuously refine both modeling and planning modules.

---

**Visual Cues（图注建议）**

* 左侧标注 “Goal Inference → Opponent Modeling → Planning → Action → Env → Trajectory feedback”。
* 可在下方加简短 legend：
  🟦 Opponent Modeling 🟩 Planning 🟧 Environment interaction
* 在箭头处添加提示文字：

  * “latent goals $\hat{g}_i$” （在Subgoal Inference旁）
  * “goal-conditioned policy $P_i(a_j|s_t, \hat{g}_j)$”
  * “planning via rollouts / MCTS”

---

### 💬 **Presenter Notes（讲稿建议，约1分钟）**

> “This slide gives the overall view of our framework, **HOP+**.
> The whole process starts with the trajectory collected from the environment.
> Within the *Opponent Modeling* block, the agent infers its opponent’s latent sub-goal through a CVAE-based inference module.
> These inferred goals condition the agent’s policy, allowing it to predict or adapt to others’ future behaviors.
> Then in the *Planning* block, the agent performs lookahead search with goal-conditioned rollouts, integrating both prediction and planning to select cooperative yet adaptive actions.
> Finally, new trajectories are generated, and the loop continues—so the model keeps improving its understanding and decision-making.”

---

是否希望我帮你写下一页的 **“Planning Module”** 部分？那一页通常是承接这个总览的右半部分，用来说明 *how the planning integrates with opponent modeling / MCTS reasoning*。
、

非常好的问题——你说得对，这页（Full Framework Overview）不能只是“讲结构”，要把它和整篇工作的 **motivation 与 contribution** 联系起来，否则就显得平淡无力。
我们可以把这一页改写成一个“**problem–insight–framework**”式的 narrative，让听众感受到：

> 这个结构并不是任意堆叠，而是**为了解决我们一开始提到的非平稳性与适应性问题**而设计的。

---

## ✅ **改进版 Slide — Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main Text (PPT内容)**

* **Motivation:**
  In mixed-motive environments, agents must adapt to diverse and evolving opponents.
  Existing approaches either rely on *fixed reward designs* or *short-term behavioral prediction*,
  both lacking the ability to **generalize across unseen opponents**.

* **Our insight:**
  The key to adaptation lies in **understanding the opponent’s latent goals**,
  not just their surface behaviors.

* **Framework overview:**
  The proposed **HOP+** integrates three components:

  1. **Goal Inference** — learns opponents’ latent sub-goals from trajectories via CVAE.
  2. **Goal-conditioned Opponent Modeling** — predicts opponent actions conditioned on inferred goals.
  3. **Planning with Goal-aware Rollouts** — integrates these predictions into MCTS for adaptive and cooperative decision-making.

* The framework forms a closed **perception–modeling–planning loop**,
  enabling **few-shot adaptation** and stable cooperation in dynamic games.

---

**Visual Cue（图像指引）**

在图上：

* 用红框或半透明高亮标出三部分：

  * 左下角「Goal Inference」旁标注 “infer latent intention”
  * 中间「Opponent Modeling」旁标注 “goal-conditioned prediction”
  * 右侧「Planning」旁标注 “adaptive planning & cooperation”
* 顶部或底部加一句贯穿性文字：
  *From inferring intentions → to modeling responses → to adaptive planning.*

---

### 💬 Presenter Notes（讲稿建议，约70秒）

> “Let me connect this framework back to our motivation.
> In mixed-motive environments, non-stationarity comes from constantly changing opponents.
> Many existing approaches either redesign rewards to enforce cooperation, or predict opponents’ next actions —
> but both stay at the *behavioral level*.
> Our key idea is to go one step deeper — to infer **why** opponents act as they do, by uncovering their latent goals.
> So, HOP+ builds a three-part loop:
> a CVAE-based goal inference module,
> a goal-conditioned opponent model,
> and a planning module that reasons over these inferred goals.
> This structure allows the agent to adapt rapidly and maintain cooperation even when facing unseen opponents.”

---

是否希望我帮你接下来写这一页的下一张——“Planning Module”页？
那一页可以聚焦在 **如何将 goal-conditioned 模型嵌入 MCTS / planning 过程**，并作为 HOP+ 的最后一环与 adaptation 联系起来。
a_i^{t*}=\arg\max_{a_i^t}\ \mathbb{E}_{a_j^t\sim\hat\pi_j(\cdot|o^t)}\Big[r(o^t,a_i^t,a_j^t)+\gamma\,V(o^{t+1})\Big]

这个公式

$$
a_i^{t*}=\arg\max_{a_i^t}\ \mathbb{E}_{a_j^t\sim\hat\pi_j(\cdot|o^t)}\Big[r(o^t,a_i^t,a_j^t)+\gamma,V(o^{t+1})\Big]
$$

非常适合作为你 **Planning 模块（或整合页）** 的核心视觉表达。它体现了 HOP+ 的“**带有对手建模的规划决策机制**”。

下面是我建议你在 PPT 上如何用图形化方式去展示它（和你的前一页“Full Framework Overview”衔接）：

---

## ✅ **Slide — Goal-aware Planning Module**

**标题：**
**Goal-aware Planning: Acting via Modeled Opponent Responses**

---

### **上半部分：公式呈现**

居中展示上面那条公式（字体大、留白多）
下面用简洁注释分三条箭头指向公式的三个部分：

| 公式部分                                  | 图中标注                                        | 含义说明（可中英对照）                                                           |                 |
| ------------------------------------- | ------------------------------------------- | --------------------------------------------------------------------- | --------------- |
| $\mathbb{E}_{a_j^t\sim\hat\pi_j(\cdot | o^t)}$                                      | “Opponent modeling: predict co-player’s response under inferred goal” | 基于对手建模预测对手的响应策略 |
| $r(o^t,a_i^t,a_j^t)$                  | “Immediate interaction reward”              | 即时交互收益                                                                |                 |
| $\gamma V(o^{t+1})$                   | “Future value from lookahead search (MCTS)” | 规划模块基于未来价值的前瞻性搜索                                                      |                 |

---

### **下半部分：图形化流程**

可以画一个从左到右的信息流图（配色延续前页）：

```
Opponent Modeling (𝑠ₜ, âⱼ, ĝⱼ)
      ↓  (predicted opponent policy 𝜋̂ⱼ)
Planning (search over aᵢ)
      ↓  (evaluate r + γV)
Action aᵢ*
      ↓
Environment → Next observation o^{t+1}
```

* 在“Planning”方框中放一个小 MCTS 树图标（代表多步前瞻）；
* 箭头上方写 “Goal-conditioned rollouts”；
* 从 Opponent Modeling → Planning 的箭头上标 “$\hat{\pi}_j(a_j|o^t,\hat{g}_j)$”；
* 从 Planning → Environment 的箭头标 “$a_i^{t*}$”。

---

### **底部两行总结文字**

> * Integrates **opponent goal inference** into **forward planning**,
>   enabling the agent to anticipate others’ intentions.
> * Achieves **adaptive cooperation** and **robust policy improvement** under non-stationary opponents.

---

### 💬 Presenter Notes（中文讲稿建议，约1分钟）

> “前面我们讲了如何通过 CVAE 推断对手的潜在目标，现在我们把这个目标信息整合进了规划层。
> 在每个决策时刻，智能体不再假设对手是固定策略，而是利用我们推断出的 $\hat{\pi}_j$ 去预测对手可能的动作分布。
> 然后，通过期望算子结合奖励与未来价值函数 $V(o^{t+1})$，智能体选择最优动作 $a_i^{t*}$。
> 实际上，这一步就是在 MCTS 中做前瞻性推理，目标是让决策既能考虑短期收益，也能考虑长期合作稳定性。”

---

是否希望我帮你把 **Goal-aware Planning** 这一页和前面的 “Goal Inference” 以及 “Full Framework Overview” 三页，整理成一个衔接自然的 3 页方法部分（带过渡句与页脚文献引用）？
