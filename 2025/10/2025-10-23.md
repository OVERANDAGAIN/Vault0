非常好 👍 这部分就是你方法章节的**第一个核心模块（Goal Inference Module）**，对应 HOP+ 框架中“推测对手潜在目标（latent goal inference）”的关键逻辑。
在 PPT 中，它应该做到两件事：

1️⃣ **讲清楚 motivation（为什么要推目标）**
2️⃣ **讲清楚 mechanism（怎么推出来的）**
同时保持简洁（1页为佳，研究汇报时约1分钟讲解时间）。

---

## ✅ **Slide — Goal Inference Module**

**Title:**
**Goal Inference: Learning Latent Sub-Goals from Trajectories**

---

**Main Content (PPT text):**

* In multi-agent games, opponents’ strategies often reflect their **latent behavioral goals**.
  Modeling these goals improves an agent’s **responsiveness** and **robustness**.

* To avoid reliance on manually defined goal sets,
  we introduce a **trajectory-based sub-goal inference mechanism** using a
  **Conditional Variational Auto-Encoder (CVAE)**.

* At each timestep (t):

  * Historical trajectory (  )
  * Future candidate states ( \mathcal{N}*t = {s*{t+1}, … , s_T} )
  * A sub-goal ( g_t ) is selected as the embedding of a future state representing the opponent’s intention.

* The CVAE learns a continuous latent goal representation ( \hat{g}_t ) through three key distributions:

  1. **Posterior:** ( q_\phi(\hat{g}_t|\tau_t, s_t) ) — inferred goal from history
  2. **Likelihood:** ( p_\gamma(s_t|\hat{g}_t, \tau_t) ) — state reconstruction
  3. **Prior:** ( p_\psi(\bar{g}_t|s^g_t) ) — conditioned on selected sub-goal state

* **Optimization objective:**
  [
  \mathcal{L}*{goal} =
  \mathbb{E}*{q_\phi}[\log p_\gamma(s_t|\hat{g}_t, \tau_t)]

  * KL(q_\phi(\hat{g}*t|\tau_t,s_t)\Vert p*\psi(\bar{g}_t|s^g_t))
    ]
    ensuring that learned sub-goals are both semantically consistent and predictive of future behaviors.

---

**Visual Suggestion:**
Use the original schematic (like your Fig. 3.2) simplified:

* **Left:** past trajectory (states/actions sequence)
* **Middle:** CVAE block (encoder → latent ( \hat{g}_t ) → decoder)
* **Right:** predicted sub-goal state embedding (s^g_t)

Labels:

* Encoder: *q₍φ₎(ĝₜ|τₜ,sₜ)*
* Decoder: *p₍γ₎(sₜ|ĝₜ,τₜ)*
* Prior: *p₍ψ₎(ḡₜ|sᵍₜ)*
* Arrow caption: “Infer opponent’s latent goal from history”

---

**Footer Reference:**
Kingma & Welling (2013); Sohn et al. (2015); Zhang et al. (2025)

---

### 💬 Presenter Notes（中文讲稿提示）

讲法建议如下（约 60–70 秒）：

> “既然对手的策略反映了潜在目标，我们不再人为假设目标集合，而是让模型**自己从轨迹中推断对手想达成什么状态**。
> 具体做法是利用 **CVAE**，输入历史轨迹，输出一个连续潜变量 ( \hat{g}_t )，代表推测到的子目标。
> 我们优化重建误差与KL项，使得学到的目标既语义一致又能预测未来趋势。”

---

是否希望我帮你写**下一页（Policy Learning with Goal-Conditioned Representation）**，
即在你推得目标 ( \hat{g}_t ) 之后，如何把它与策略优化结合（HOP+的第二个模块）？





非常好，这一页是整篇方法的“总览页”，你的图已经画得很清晰（Opponent Modeling + Planning 两部分）。
因此这一页的内容不需要讲细节，只要让听众**在一分钟内理解结构逻辑与信息流**。下面是推荐的文字与讲稿示例。

---

## ✅ **Slide — Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main PPT Text（控制在 4~6 行）**

* The proposed **HOP+ framework** integrates *Goal Inference*, *Opponent Modeling*, and *Planning* to enable adaptive cooperation in mixed-motive games.
* **Opponent Modeling:** infers opponents’ latent sub-goals and conditions the policy on these inferred goals.
* **Planning:** uses goal-conditioned predictions for lookahead search (e.g., MCTS) to plan more stable and cooperative actions.
* The system operates in a closed loop: trajectories are collected from the environment and continuously refine both modeling and planning modules.

---

**Visual Cues（图注建议）**

* 左侧标注 “Goal Inference → Opponent Modeling → Planning → Action → Env → Trajectory feedback”。
* 可在下方加简短 legend：
  🟦 Opponent Modeling 🟩 Planning 🟧 Environment interaction
* 在箭头处添加提示文字：

  * “latent goals $\hat{g}_i$” （在Subgoal Inference旁）
  * “goal-conditioned policy $P_i(a_j|s_t, \hat{g}_j)$”
  * “planning via rollouts / MCTS”

---

### 💬 **Presenter Notes（讲稿建议，约1分钟）**

> “This slide gives the overall view of our framework, **HOP+**.
> The whole process starts with the trajectory collected from the environment.
> Within the *Opponent Modeling* block, the agent infers its opponent’s latent sub-goal through a CVAE-based inference module.
> These inferred goals condition the agent’s policy, allowing it to predict or adapt to others’ future behaviors.
> Then in the *Planning* block, the agent performs lookahead search with goal-conditioned rollouts, integrating both prediction and planning to select cooperative yet adaptive actions.
> Finally, new trajectories are generated, and the loop continues—so the model keeps improving its understanding and decision-making.”

---

是否希望我帮你写下一页的 **“Planning Module”** 部分？那一页通常是承接这个总览的右半部分，用来说明 *how the planning integrates with opponent modeling / MCTS reasoning*。
、

非常好的问题——你说得对，这页（Full Framework Overview）不能只是“讲结构”，要把它和整篇工作的 **motivation 与 contribution** 联系起来，否则就显得平淡无力。
我们可以把这一页改写成一个“**problem–insight–framework**”式的 narrative，让听众感受到：

> 这个结构并不是任意堆叠，而是**为了解决我们一开始提到的非平稳性与适应性问题**而设计的。

---

## ✅ **改进版 Slide — Full Framework Overview**

**Title:**
**Full Framework Overview**

---

**Main Text (PPT内容)**

* **Motivation:**
  In mixed-motive environments, agents must adapt to diverse and evolving opponents.
  Existing approaches either rely on *fixed reward designs* or *short-term behavioral prediction*,
  both lacking the ability to **generalize across unseen opponents**.

* **Our insight:**
  The key to adaptation lies in **understanding the opponent’s latent goals**,
  not just their surface behaviors.

* **Framework overview:**
  The proposed **HOP+** integrates three components:

  1. **Goal Inference** — learns opponents’ latent sub-goals from trajectories via CVAE.
  2. **Goal-conditioned Opponent Modeling** — predicts opponent actions conditioned on inferred goals.
  3. **Planning with Goal-aware Rollouts** — integrates these predictions into MCTS for adaptive and cooperative decision-making.

* The framework forms a closed **perception–modeling–planning loop**,
  enabling **few-shot adaptation** and stable cooperation in dynamic games.

---

**Visual Cue（图像指引）**

在图上：

* 用红框或半透明高亮标出三部分：

  * 左下角「Goal Inference」旁标注 “infer latent intention”
  * 中间「Opponent Modeling」旁标注 “goal-conditioned prediction”
  * 右侧「Planning」旁标注 “adaptive planning & cooperation”
* 顶部或底部加一句贯穿性文字：
  *From inferring intentions → to modeling responses → to adaptive planning.*

---

### 💬 Presenter Notes（讲稿建议，约70秒）

> “Let me connect this framework back to our motivation.
> In mixed-motive environments, non-stationarity comes from constantly changing opponents.
> Many existing approaches either redesign rewards to enforce cooperation, or predict opponents’ next actions —
> but both stay at the *behavioral level*.
> Our key idea is to go one step deeper — to infer **why** opponents act as they do, by uncovering their latent goals.
> So, HOP+ builds a three-part loop:
> a CVAE-based goal inference module,
> a goal-conditioned opponent model,
> and a planning module that reasons over these inferred goals.
> This structure allows the agent to adapt rapidly and maintain cooperation even when facing unseen opponents.”

---

是否希望我帮你接下来写这一页的下一张——“Planning Module”页？
那一页可以聚焦在 **如何将 goal-conditioned 模型嵌入 MCTS / planning 过程**，并作为 HOP+ 的最后一环与 adaptation 联系起来。
