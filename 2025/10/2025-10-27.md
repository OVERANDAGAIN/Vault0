
```ad-tip
* `next_obs_raw["actions"]` 的顺序是**全局顺序** `[player_1, player_2, player_3, player_4]`（且包含自己）。这是在 `__action__` 里按 `self.players` 顺序拼出来的。
* 但 `observation` 的玩家通道顺序**被旋转过**：`__obs_state__(id)` 会把**“自己”旋到通道 0**（ego-first），顺序变成 `[self, 下一个, …]`。
```

## World_Model

> 主要对 **world_model 的动作输入和模型容量**做了调整：
> 一是把动作输入从单一动作改为“自动作 + 对手动作”并统一为 **ego-first 格式**，同时使用 one-hot 编码方式；
> 二是 **增大了隐藏层维度和 latent 表征大小**


![[calibration_scatter 1.png]]


![[calibration_hist 1.png]]



![[triplet_ego_024.png]]

![[triplet_ego_005.png]]
![[triplet_ego_013.png]]




---



## 这份采集代码到底是怎么存的（关键点）

采集时你存了：



```python
obs_flat       = [ obs_dict_raw["actions"], obs_dict_raw["action_mask"], obs_dict_raw["observation"].flatten() ]
next_obs_flat  = [ next_obs_raw["actions"], next_obs_raw["action_mask"], next_obs_raw["observation"].flatten() ]
action         = action_batch[i][aid]        # 这名 agent 在时刻 t 选的动作（自动作）
reward_value   = reward_batch[aid][i]        # 这名 agent 的即时 reward（自回报）
t, aid
```

---

结论先说在前：**把所有对手的动作 embedding 简单做 `mean` 池化不够用**——它会抹平“人数”“组合”和“是谁在动”的关键信息，尤其在需要多人协同才能触发的事件（比如两人以上同格捕鹿）上，会直接伤到 world model 的精度，从而影响到你后续用 value/policy 做 planning 的效果。

为什么不够用（直觉+反例）

* **人数信息丢失**：`mean` 把 `[hunt, hunt]` 与 `[hunt, stay]` 变到很接近，甚至当 K 可变时更糟（平均后幅度变小）。但捕鹿是**阈值型联动**（≥2 人同格且做 hunt），人数差 1 的后果完全不同。
* **组合信息丢失**：`mean` 不能区分 “两个向上” 和 “一个向上一个向右”（平均后一团糊）。
* **身份/空间耦合丢失**：不同对手与我/猎物的**相对位置**和**动作**的组合决定了下一帧的格子状态；`mean` 不看“是谁在哪儿做了什么”。

> 这些信息恰好就是 world model 在捕猎瞬间需要“非常精确”建模的东西。





## 2Stag 4Hare
### player_1 个体奖励

![[Pasted image 20251027154253.png]]


### moa预测情况

![[Pasted image 20251027154307.png]]


