
你说得对，不能拍脑袋。把你给的三段代码串起来看，**根因非常明确**：

* `next_obs_raw["actions"]` 的顺序是**全局顺序** `[player_1, player_2, player_3, player_4]`（且包含自己）。这是在 `__action__` 里按 `self.players` 顺序拼出来的。
* 但 `observation` 的玩家通道顺序**被旋转过**：`__obs_state__(id)` 会把**“自己”旋到通道 0**（ego-first），顺序变成 `[self, 下一个, …]`。

所以在 `test_collect` 里，你把 `next_action`（全局顺序）直接和 `next_hwc`（ego-first 顺序）做对照，就会**对不齐**。这既可能让你误以为“动作顺序错了”，也会让你看到“显示 4 但图里在移动”的错觉（因为你拿的是 `obs_action`=t−1 的动作去对照 `obs_t→obs_{t+1}` 的位移）。

![[calibration_scatter 1.png]]


![[calibration_hist 1.png]]


---



## 这份采集代码到底是怎么存的（关键点）

采集时你存了：



```python
obs_flat       = [ obs_dict_raw["actions"], obs_dict_raw["action_mask"], obs_dict_raw["observation"].flatten() ]
next_obs_flat  = [ next_obs_raw["actions"], next_obs_raw["action_mask"], next_obs_raw["observation"].flatten() ]
action         = action_batch[i][aid]        # 这名 agent 在时刻 t 选的动作（自动作）
reward_value   = reward_batch[aid][i]        # 这名 agent 的即时 reward（自回报）
t, aid
```

---

结论先说在前：**把所有对手的动作 embedding 简单做 `mean` 池化不够用**——它会抹平“人数”“组合”和“是谁在动”的关键信息，尤其在需要多人协同才能触发的事件（比如两人以上同格捕鹿）上，会直接伤到 world model 的精度，从而影响到你后续用 value/policy 做 planning 的效果。

为什么不够用（直觉+反例）

* **人数信息丢失**：`mean` 把 `[hunt, hunt]` 与 `[hunt, stay]` 变到很接近，甚至当 K 可变时更糟（平均后幅度变小）。但捕鹿是**阈值型联动**（≥2 人同格且做 hunt），人数差 1 的后果完全不同。
* **组合信息丢失**：`mean` 不能区分 “两个向上” 和 “一个向上一个向右”（平均后一团糊）。
* **身份/空间耦合丢失**：不同对手与我/猎物的**相对位置**和**动作**的组合决定了下一帧的格子状态；`mean` 不看“是谁在哪儿做了什么”。

> 这些信息恰好就是 world model 在捕猎瞬间需要“非常精确”建模的东西。
