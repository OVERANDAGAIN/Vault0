在这一页中，你可以用一个简洁但能体现模型输入输出关系的公式来说明 MOA 模块的功能。建议写成如下形式（行间公式展示）：

$$
\hat{a}_{j}^{t} = f_{\text{MOA}}\big(o_{i}^{t},, g_{i}^{t}\big)
$$

其中：

* $o_{i}^{t}$ 表示智能体 $i$ 在时刻 $t$ 的观测（observation）；
* $g_{i}^{t}$ 表示智能体 $i$ 当前的目标（goal）；
* $f_{\text{MOA}}(\cdot)$ 表示 MOA 模块（Model of Agents）；
* 输出 $\hat{a}_{j}^{t}$ 表示 MOA 模块预测的对手 $j$ 在该时刻的下一步动作（predicted next action of opponent $j$）。

若想强调该模块基于轨迹学习得到，可以进一步写出数据驱动形式：

$$
\hat{a}*{j}^{t} = f*{\theta_{\text{MOA}}}\big(o_{i}^{t},, g_{i}^{t}\big),
\quad \text{where } \theta_{\text{MOA}} \text{ are learned from trajectories } {(o,a)}.
$$

在 PPT 上可以配合一句解释性文字：

> The MOA module learns a mapping from the agent’s observation and goal to the opponent’s next action through trajectory-based supervised learning.

如果你希望更强调预测精度，也可以在图表下加一句：

> The top-1 accuracy of the predicted opponent actions rapidly converges to ≈0.99, indicating high consistency between predicted and actual actions.




非常好，你说得对 —— 在我们的实现中，World Model 不仅预测下一状态表征 $\hat{s}*{t+1}$，同时也预测当前动作下的奖励 $\hat{r}*{t}$。
这样模型就能同时学习环境的**动态转移（state transition）**与**奖励机制（reward dynamics）**。

你可以在 PPT 中用如下数学形式统一表示这一点：

---

### 🌍 World Model 表示公式（完整版本）

$$
\begin{aligned}
\hat{s}_{t+1},, \hat{r}_{t}
&= f_{\text{world}}\big(s_t, a_t\big) 
\end{aligned}
  $$

---

其中：

* $f_{\text{world}}(\cdot)$：世界模型，用于联合建模状态转移与奖励函数；
* $\hat{s}_{t+1}$：预测的下一状态表征；
* $\hat{r}_t$：预测的即时奖励；
* $\lambda_r$：用于平衡状态预测损失与奖励预测损失的权重系数；
* $\mathcal{L}_{\text{world}}$：训练目标函数。

---

📘**PPT辅助说明文字建议：**

> The world model jointly predicts both the next-state representation and the corresponding reward, enabling consistent learning of transition and reward dynamics from trajectory data.

---

这种公式可以很好地配合你展示的“pixel-wise calibration”图（验证 $\hat{s}_{t+1}$）以及 reward learning 的对比曲线（若后续页展示）。
