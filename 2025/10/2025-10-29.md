åœ¨è¿™ä¸€é¡µä¸­ï¼Œä½ å¯ä»¥ç”¨ä¸€ä¸ªç®€æ´ä½†èƒ½ä½“ç°æ¨¡å‹è¾“å…¥è¾“å‡ºå…³ç³»çš„å…¬å¼æ¥è¯´æ˜ MOA æ¨¡å—çš„åŠŸèƒ½ã€‚å»ºè®®å†™æˆå¦‚ä¸‹å½¢å¼ï¼ˆè¡Œé—´å…¬å¼å±•ç¤ºï¼‰ï¼š

$$
\hat{a}_{j}^{t} = f_{\text{MOA}}\big(o_{i}^{t},, g_{i}^{t}\big)
$$

å…¶ä¸­ï¼š

* $o_{i}^{t}$ è¡¨ç¤ºæ™ºèƒ½ä½“ $i$ åœ¨æ—¶åˆ» $t$ çš„è§‚æµ‹ï¼ˆobservationï¼‰ï¼›
* $g_{i}^{t}$ è¡¨ç¤ºæ™ºèƒ½ä½“ $i$ å½“å‰çš„ç›®æ ‡ï¼ˆgoalï¼‰ï¼›
* $f_{\text{MOA}}(\cdot)$ è¡¨ç¤º MOA æ¨¡å—ï¼ˆModel of Agentsï¼‰ï¼›
* è¾“å‡º $\hat{a}_{j}^{t}$ è¡¨ç¤º MOA æ¨¡å—é¢„æµ‹çš„å¯¹æ‰‹ $j$ åœ¨è¯¥æ—¶åˆ»çš„ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆpredicted next action of opponent $j$ï¼‰ã€‚

è‹¥æƒ³å¼ºè°ƒè¯¥æ¨¡å—åŸºäºè½¨è¿¹å­¦ä¹ å¾—åˆ°ï¼Œå¯ä»¥è¿›ä¸€æ­¥å†™å‡ºæ•°æ®é©±åŠ¨å½¢å¼ï¼š

$$
\hat{a}*{j}^{t} = f*{\theta_{\text{MOA}}}\big(o_{i}^{t},, g_{i}^{t}\big),
\quad \text{where } \theta_{\text{MOA}} \text{ are learned from trajectories } {(o,a)}.
$$

åœ¨ PPT ä¸Šå¯ä»¥é…åˆä¸€å¥è§£é‡Šæ€§æ–‡å­—ï¼š

> The MOA module learns a mapping from the agentâ€™s observation and goal to the opponentâ€™s next action through trajectory-based supervised learning.

å¦‚æœä½ å¸Œæœ›æ›´å¼ºè°ƒé¢„æµ‹ç²¾åº¦ï¼Œä¹Ÿå¯ä»¥åœ¨å›¾è¡¨ä¸‹åŠ ä¸€å¥ï¼š

> The top-1 accuracy of the predicted opponent actions rapidly converges to â‰ˆ0.99, indicating high consistency between predicted and actual actions.




éå¸¸å¥½ï¼Œä½ è¯´å¾—å¯¹ â€”â€” åœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼ŒWorld Model ä¸ä»…é¢„æµ‹ä¸‹ä¸€çŠ¶æ€è¡¨å¾ $\hat{s}*{t+1}$ï¼ŒåŒæ—¶ä¹Ÿé¢„æµ‹å½“å‰åŠ¨ä½œä¸‹çš„å¥–åŠ± $\hat{r}*{t}$ã€‚
è¿™æ ·æ¨¡å‹å°±èƒ½åŒæ—¶å­¦ä¹ ç¯å¢ƒçš„**åŠ¨æ€è½¬ç§»ï¼ˆstate transitionï¼‰**ä¸**å¥–åŠ±æœºåˆ¶ï¼ˆreward dynamicsï¼‰**ã€‚

ä½ å¯ä»¥åœ¨ PPT ä¸­ç”¨å¦‚ä¸‹æ•°å­¦å½¢å¼ç»Ÿä¸€è¡¨ç¤ºè¿™ä¸€ç‚¹ï¼š

---

### ğŸŒ World Model è¡¨ç¤ºå…¬å¼ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰

$$
\begin{aligned}
\hat{s}_{t+1},, \hat{r}_{t}
&= f_{\text{world}}\big(s_t, a_t\big) 
\end{aligned}
  $$

---

å…¶ä¸­ï¼š

* $f_{\text{world}}(\cdot)$ï¼šä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºè”åˆå»ºæ¨¡çŠ¶æ€è½¬ç§»ä¸å¥–åŠ±å‡½æ•°ï¼›
* $\hat{s}_{t+1}$ï¼šé¢„æµ‹çš„ä¸‹ä¸€çŠ¶æ€è¡¨å¾ï¼›
* $\hat{r}_t$ï¼šé¢„æµ‹çš„å³æ—¶å¥–åŠ±ï¼›
* $\lambda_r$ï¼šç”¨äºå¹³è¡¡çŠ¶æ€é¢„æµ‹æŸå¤±ä¸å¥–åŠ±é¢„æµ‹æŸå¤±çš„æƒé‡ç³»æ•°ï¼›
* $\mathcal{L}_{\text{world}}$ï¼šè®­ç»ƒç›®æ ‡å‡½æ•°ã€‚

---

ğŸ“˜**PPTè¾…åŠ©è¯´æ˜æ–‡å­—å»ºè®®ï¼š**

> The world model jointly predicts both the next-state representation and the corresponding reward, enabling consistent learning of transition and reward dynamics from trajectory data.

---

è¿™ç§å…¬å¼å¯ä»¥å¾ˆå¥½åœ°é…åˆä½ å±•ç¤ºçš„â€œpixel-wise calibrationâ€å›¾ï¼ˆéªŒè¯ $\hat{s}_{t+1}$ï¼‰ä»¥åŠ reward learning çš„å¯¹æ¯”æ›²çº¿ï¼ˆè‹¥åç»­é¡µå±•ç¤ºï¼‰ã€‚
