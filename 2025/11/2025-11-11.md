好的，下面是对论文 **4.1 In-Context-Learning-based Pretraining** 这一节的**详细总结**，包括逻辑思路、数据构造、公式含义和直观理解。

---



---

## 三组件联合监督训练

在预训练阶段，模型学习三个目标：
$$
\max_{\theta}\ \mathbb{E}*{\mathcal{Q}*{t}^{k}}!\left[\log \pi_{\theta}!\left(a_{t}^{1,k,*}\mid s_{t},, D_{t}^{k}\right)\right]
$$

$$
\max_{\phi}\ \mathbb{E}*{\mathcal{Q}*{t}^{k}}!\left[\log \mu_{\phi}!\left(a_{t}^{-1,k}\mid s_{t},, D_{t}^{k}\right)\right]
$$

$$
\min_{\omega}\ \mathbb{E}*{\mathcal{Q}*{t}^{k}}!\left[\left(V_{\omega}!\left(s_{t},, D_{t}^{k}\right)-G_{t}^{1,k,*}\right)^{2}\right]
$$

解释：

* (3) 让 Actor 学会对每个对手的最优反应（即 BR 模仿学习）；
* (4) 让 Imitator 学会预测对手动作（MOA 角色）；
* (5) 让 Critic 学会估计当前状态-上下文下的价值。

三者**共享同一个 Transformer** 骨干网络，
通过注意力机制在输入序列中对不同对手风格形成可区分的表示，
这就是 **In-Context Representation Learning** 的核心。

---


---

## ✅ 小结表

| 模块                   | 输入             | 输出              | 学习目标   | 对应公式 |
| -------------------- | -------------- | --------------- | ------ | ---- |
| Actor πθ             | $(s_t, D_t^k)$ | 自方动作 $a_t^1$    | 模仿 BR  | (3)  |
| Opponent Imitator μϕ | $(s_t, D_t^k)$ | 对手动作 $a_t^{-1}$ | 模仿对手   | (4)  |
| Critic Vω            | $(s_t, D_t^k)$ | 价值估计            | 拟合 RTG | (5)  |

---

如果你愿意，我可以把这一节再整理成：

* ✅ “方法框架图 + 数据流动箭头”
* ✅ “公式推导简版 + 中文解释注释”

方便你放进论文方法章节或答辩 PPT。是否希望我帮你画出那张图？









好的，下面是论文 **4.2 Decision-Time Search with In-Context Components** 的完整总结，包含方法逻辑、关键公式以及直观解释。

---

# **4.2 Decision-Time Search with In-Context Components**

## **一、方法目的**

预训练阶段让模型学会“如何识别和应对不同对手”，但在测试时：

* 对手是 **未知 (unknown)** 且 **非平稳 (non-stationary)** 的；
* 即对手会在不同 episode 间切换策略 $\pi^{-1} \sim \Pi^{\text{test}}$。

因此，OMIS 在测试阶段引入 **Decision-Time Search (DTS)**：



