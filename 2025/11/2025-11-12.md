---
创建时间: 2025-十一月-14日  星期五, 10:05:05 上午
---
## TAO
#### **生成式损失 Generative Loss**
* 想让 embedding 真能“代表”某个对手的策略，就让模型根据 embedding 能够**重现该对手的行为分布**；
* 因此，$\mathcal{L}_{gen}$ 让 embedding 学会携带生成性信息：**从轨迹生成动作。**
* 
##### 公式

$$\mathcal{L}_{\text{gen}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau_i^{-1,k},\tau_j^{-1,k}\sim \mathcal{T}^{-1,k}}
\left[
\sum_{(o,a)\sim\tau_i^{-1,k}}
\log \pi_{\phi_d}\left(a \mid o,, \text{AP}\left(M_{\theta_e}(\tau_j^{-1,k})\right)\right)
\right]$$

##### 含义

* 对每个对手类型 $k$，采样两段属于该对手的轨迹 $\tau_i^{-1,k}$ 与 $\tau_j^{-1,k}$；
* 将其中一段 $\tau_j^{-1,k}$ 编码为平均轨迹表示 $\bar{z}^{-1,k} = \text{AP}(M_{\theta_e}(\tau_j^{-1,k}))$；
* 让解码器 $\pi_{\phi_d}$ 在观察 $o$ 与该表示的条件下，预测对手的动作 $a$；
* 本质是一个 **条件模仿学习（conditional imitation learning）** 任务。



---

#### **判别式损失 Discriminative Loss**
* 让同一个对手的 embedding **彼此靠近**；
* 不同对手的 embedding **彼此远离**；
* 从而 embedding 空间具有区分不同策略的能力。

##### 公式

$$\mathcal{L}_{\text{dis}}
= -\mathbb{E}_{\mathcal{T}^{-1}}
\left[
\frac{1}{B}
\sum_{i=1}^{B}
\frac{1}{|\mathcal{E}_i|-1}
\sum_{j\in\mathcal{E}_i,j\neq i}
\log
\frac{\exp(\bar{z}_i^{-1}\cdot\bar{z}_j^{-1}/p)}
{\sum_{l\in[B],l\neq i}\exp(\bar{z}_i^{-1}\cdot\bar{z}_l^{-1}/p)}
\right]$$

##### 含义

* 采样批量 $B$ 条轨迹，每条属于某个对手策略类型 $k$；
* 对每条轨迹计算 embedding $\bar{z}_i^{-1}$；
* 若两条轨迹来自同一对手类型，就视为**正样本**；不同对手类型则为**负样本**；
* 用 InfoNCE（对比学习）形式最大化正样本相似度、最小化负样本相似度。



#### 监督训练（in-context learning-based supervised pre-training）

##### 公式（Response Loss）

$$\mathcal{L}_{\text{res}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau^{1,k}\sim\mathcal{T}^{1,k}}
\left[
\sum_{(y_t,a_t)\sim\tau^{1,k}}
\log M_{\theta_d}\left(a_t \mid y_t;, M_{\theta_e}\left(\text{GetOffD}(\mathcal{T}^{-1,k})\right)\right)
\right]
\tag{5}$$

其中：

* $M_{\theta_e}$ 是对手表示提取器（OPE encoder）；
* $\text{GetOffD}(\mathcal{T}^{-1,k})$ 从对手轨迹集中采样 $H$ 个连续片段（更能体现对手风格）；
* $y_t = (G_0, o_0, a_0, ..., G_t, o_t)$ 为自方的观测序列与回报；
* 模型 $M_{\theta_d}$ 自回归地预测动作 $a_t$，目标是最大化真动作的似然。

---

##### 核心思想

* 使用 **In-Context Learning** 的方式进行监督训练；
* 模型通过 cross-attention 在对手轨迹嵌入 $z^{-1}$ 与自方轨迹之间建立联系；
* 从而**显式学习“对手策略 ↔ 响应策略”之间的映射关系**；
* 使得在测试阶段，给定新的对手嵌入，模型就能立即推断出相应的响应策略，而无需微调。




#### **In-context Opponent Adapting**


> 在部署阶段，模型不再进行参数更新，而是通过 **ICL（In-Context Learning）** 机制实时根据对手最新的行为轨迹进行策略适配。


---

##### **Opponent Context Window（OCW）**

OMIS 在部署阶段引入一个 **OCW（对手上下文窗口）**，记作 $\mathcal{W}$，用来缓存当前对手的最新轨迹数据。

* $\mathcal{W}$ 始终保存最近 $C$ 条对手轨迹：
  $$\mathcal{W} = {\tau_{l-C+1}^{-1}, \dots, \tau_{l-1}^{-1}, \tau_l^{-1}}$$
  其中 $l$ 是最近一次交互的索引。

* 每次交互后，$\mathcal{W}$ 会更新，**丢弃最旧、加入最新**，从而维持对手的短期记忆。

---

##### **策略公式**

$$a_t^1 \sim M_{\theta_d}(\cdot \mid y_t^1; M_{\theta_e}(\text{GetOnD}(\mathcal{W})))
\tag{6}$$

解释如下：

* $M_{\theta_e}$：OPE 编码器，用于从 $\mathcal{W}$ 中抽取代表当前对手策略的嵌入；
* $\text{GetOnD}(\mathcal{W})$：从 $\mathcal{W}$ 中采样若干连续片段（$H$ 段）并拼接，形成当前时刻的上下文数据；
* $M_{\theta_d}$：ICD 解码器（已训练好的响应策略）；
* $y_t^1$：自方的当前观测与历史轨迹；
* 输出：我方在时刻 $t$ 的动作分布 $a_t^1$。


---



---
## OMIS

## 1 In-Context-Learning-based Pretraining


### **构建 In-Context 数据 D**

为训练三组件（Actor、Opponent Imitator、Critic），
每个时刻 $t$ 构造 in-context 数据 $D_t^k$，由两部分组成：

$$D_t^k := (D^{\text{epi},k}, D^{\text{step},k}_t)$$

* **$D^{\text{epi},k}$（episode-wise context）**：
  描述对手在整个回合上的长期行为模式。
  由对手-自方在多局对战中生成的历史轨迹 ${(\tilde s_h,\tilde a_h)}^H_{h=1}$ 组成。

* **$D^{\text{step},k}_t$（step-wise context）**：
  当前局中从起始到 $t$ 时刻的对手-自方动作序列：
  $$D^{\text{step},k}*t=(s_0,a_0^{-1,k},…,s*{t-1},a_{t-1}^{1,k,*})$$
  表征了对手在当前局的局部行为特征。

→ 组合起来的 $D_t^k$ 提供了**关于对手风格的上下文信息**，
让模型在输入 $(s_t,D_t^k)$ 时即可“识别对手是谁”。

---

### **计算监督信号**

在每个时间步 $t$：

* 收集自方的 **Return-To-Go（RTG）**：
  $$G_{t}^{1,k,*}=\sum_{t’=t}^{T} \gamma^{t'-t} r_{t'}^1$$
* 同时记录该时刻的四元组数据：
  $$\mathcal{Q}_t^k = (s_t, D_t^k, a_t^{1,k,*}, a_t^{-1,k}, G_t^{1,k,*})
  \tag{2}$$

这些样本组成训练集，用于三任务监督学习。



> 在不更新参数的前提下，利用预训练好的三个 In-Context 组件 $(\pi_\theta, \mu_\phi, V_\omega)$，在执行前“脑内搜索”多个未来轨迹，挑选出最优动作。

---

## **DTS 的基本设定**

假设：

* 环境的真实转移函数 $\mathcal{P}$ 已知；
* 在每个时刻 $t$，针对自方每个合法动作 $\hat a_t^1$，执行 $M$ 次长度为 $L$ 的 rollout；
* 目标是估计该动作的价值 $\hat Q(s_t, \hat a_t^1)$。

---

### **上下文数据构造**

测试时的 in-context 数据 $D_t=(D^{\text{epi}}, D^{\text{step}}_t)$ 构造与 4.1 类似：

* $D^{\text{epi}}$：从最近 $C$ 局对手交互轨迹中抽取连续片段，描述对手的整体风格；
* $D^{\text{step}}_t$：当前局从起点到 $t$ 的状态-动作历史。

该上下文提供了**对手策略的隐式估计**，帮助预训练模型在 roll-out 时预测未来行为。

---

### **搜索过程**

1️⃣ **初始化**：选定当前状态 $s_t$ 与候选动作 $\hat a_t^1$；
对手动作由预训练的 **Opponent Imitator** 模拟生成：
$$\hat a_t^{-1} \sim \mu_\phi(\cdot|s_t, D_t)$$

2️⃣ **环境推进**：
$$s_{t+1}, r_t^1 = \mathcal{P}(s_t, \hat a_t^1, \hat a_t^{-1})$$
并更新 step-wise 上下文：
$$\hat D^{\text{step}}_{t+1} = D^{\text{step}}_t \cup (s_t, \hat a_t^{-1})$$

3️⃣ **循环 rollout（共 L 步）**
对每个未来时刻 $l\in[1,L]$：
$$\hat a_{t+l}^1 \sim \pi_\theta(\cdot|\hat s_{t+l}, \hat D_{t+l}), \quad
\hat a_{t+l}^{-1} \sim \mu_\phi(\cdot|\hat s_{t+l}, \hat D_{t+l})
\tag{6,7}$$
环境 $\mathcal{P}$ 推进到 $\hat s_{t+l+1}$ 并获得 $\hat r_{t+l}^1$；
更新上下文 $\hat D_{t+l+1} = (D^{\text{epi}}, \hat D^{\text{step}}_{t+l+1})$。

4️⃣ **终止时**（第 L 步后），用 Critic 估计末端价值：
$$\hat V_{t+L+1} = V_\omega(\hat s_{t+L+1}, \hat D_{t+L+1})$$

---

### **动作价值评估**

对每个候选动作 $\hat a_t^1$，将 $M$ 次 rollout 结果平均得到其搜索价值：
$$\hat Q(s_t,\hat a_t^1) =
\frac{1}{M}\sum_{m=1}^{M}
\left[
\sum_{t'=t}^{t+L}\gamma_{\text{search}}^{,t'-t}\hat r_{t'}^1
 \gamma_{\text{search}}^{,L+1}\hat V_{t+L+1}
  \right]
  \tag{8}$$
  其中 $\gamma_{\text{search}}$ 为搜索时的折扣因子。

然后选取搜索策略：
$$\pi_{\text{search}}(s_t) = \arg\max_{\hat a_t^1}\hat Q(s_t,\hat a_t^1)
\tag{9}$$

---

### **混合策略 (Mixing Policy) 机制**

由于：

* 对手建模 $\mu_\phi$ 与环境模拟存在误差；
* 完全依赖搜索结果可能引入噪声或高方差，

论文引入了阈值控制的混合策略：
$$\pi_{\text{mix}}(s_t) :=
\begin{cases}
\pi_{\text{search}}(s_t), &
|\hat Q(s_t,\pi_{\text{search}}(s_t))| > \varepsilon, \
\pi_\theta(\cdot|s_t, D_t), & \text{otherwise}.
\end{cases}
\tag{10}$$
解释：

* 当搜索结果“足够自信”时（$\hat Q$ 较高）→ 采用搜索动作；
* 否则退回原始 Actor 动作。





----


---


# Truncated Q-driven Instant Policy Refinement (TIPR)

---

## 一、总体思路

TIPR 是一个**通用的离线对手建模（Offline Opponent Modeling, OOM）改进框架**，旨在解决：

> 当离线数据集来自“次优策略”（suboptimal dataset）时，原始 OOM 方法性能显著退化。

为此，TIPR 引入两个核心步骤：

1. **学习一个截断的 Q 函数（Truncated Q）**：
   对离线轨迹进行局部回报建模，以减轻长时回报估计偏差；
2. **测试阶段即时策略精炼（Instant Policy Refinement, IPR）**：
   在运行时根据置信度判断是否对原策略进行“保守更新”，实现“高置信改进、低置信保持”。

TIPR 是 **plug-and-play 式的增强模块**，可与现有 OOM 算法（如 DRON、LIAM、TAO 等）结合使用，而无需重新训练策略网络。

---

## 二、Truncated Q 的设计与训练

### (1) 定义

Truncated Q 是一个 **horizon-truncated in-context action-value function**，表示：
$$
\check Q_h(o_t^1, a_t^1, a_t^{-1}, D) \approx \mathbb{E}\left[\sum_{k=0}^{h-1} \gamma^k r_{t+k}^1\right]
$$
其中：

* $h$ 是固定的截断长度；
* $D$ 是对手的上下文数据 $(o^{-1}*{1:m}, a^{-1}*{1:m})$；
* $\check Q_h$ 通过 Transformer 实现，具备 in-context learning 能力。

它的作用是：
**仅在有限 horizon 内建模回报**，以避免对长时间非平稳对手的误估计。

---

### (2) 训练目标

Truncated Q 模型包含两个输出头：

1. **Value Head ($Q_V$)** —— 预测未来每步即时奖励：
   $$
   L_{QV} = \frac{1}{H}\sum_{h=0}^{H-1} \left(\check Q_V^h(o_t^1,a_t^1,a_t^{-1},D) - r_{t+h}^1\right)^2
   $$

2. **Confidence Head ($Q_C$)** —— 预测未来是否会出现非零奖励：
   $$
   L_{QC} = \frac{1}{H}\sum_{h=0}^{H-1} \text{BCE}!\left(\check Q_C^h(o_t^1,a_t^1,a_t^{-1},D), \mathbf{1}{r_{t+h}^1\neq0}\right)
   $$

最终总损失：
$$
L = \alpha L_{QV} + \beta L_{QC}
$$
其中 $\alpha, \beta$ 为平衡系数。

---

## 三、模块 2：Instant Policy Refinement (IPR)

在测试阶段，TIPR 并不直接执行 $\bar\pi$（原策略），而是根据 Truncated Q 判断是否执行 refinement。

### (1) 计算累计置信度

将 $Q_C$ 的多步输出视为伯努利分布，定义：
$$
\check Q_C^{\Sigma}(o_t^1,a_t^1,a_t^{-1},D) = \sum_{h=0}^{H-1} Bern(\check Q_C^h)
$$
表示在未来 $H$ 步中获得非零回报的概率质量。

---

### (2) 定义策略改进条件 (Refinement Condition, RC)
![[Pasted image 20251114145409.png]]


* 若 $f_{RC}>0$ → 触发策略改进；
* 若 $f_{RC}=0$ → 保持原策略 $\bar\pi$。

此时 TIPR 使用 $Q_V$ 进行动作选择：
$$
a_t^{1*} = \arg\max_a \check Q_V(o_t^1,a,a_t^{-1},D_t)
$$



