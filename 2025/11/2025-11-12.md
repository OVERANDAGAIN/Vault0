
#### **生成式损失 Generative Loss**
* 想让 embedding 真能“代表”某个对手的策略，就让模型根据 embedding 能够**重现该对手的行为分布**；
* 因此，$\mathcal{L}_{gen}$ 让 embedding 学会携带生成性信息：**从轨迹生成动作。**
* 
##### 公式

$$\mathcal{L}_{\text{gen}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau_i^{-1,k},\tau_j^{-1,k}\sim \mathcal{T}^{-1,k}}
\left[
\sum_{(o,a)\sim\tau_i^{-1,k}}
\log \pi_{\phi_d}\left(a \mid o,, \text{AP}\left(M_{\theta_e}(\tau_j^{-1,k})\right)\right)
\right]$$

##### 含义

* 对每个对手类型 $k$，采样两段属于该对手的轨迹 $\tau_i^{-1,k}$ 与 $\tau_j^{-1,k}$；
* 将其中一段 $\tau_j^{-1,k}$ 编码为平均轨迹表示 $\bar{z}^{-1,k} = \text{AP}(M_{\theta_e}(\tau_j^{-1,k}))$；
* 让解码器 $\pi_{\phi_d}$ 在观察 $o$ 与该表示的条件下，预测对手的动作 $a$；
* 本质是一个 **条件模仿学习（conditional imitation learning）** 任务。



---

#### **判别式损失 Discriminative Loss**
* 让同一个对手的 embedding **彼此靠近**；
* 不同对手的 embedding **彼此远离**；
* 从而 embedding 空间具有区分不同策略的能力。

##### 公式

$$\mathcal{L}_{\text{dis}}
= -\mathbb{E}_{\mathcal{T}^{-1}}
\left[
\frac{1}{B}
\sum_{i=1}^{B}
\frac{1}{|\mathcal{E}_i|-1}
\sum_{j\in\mathcal{E}_i,j\neq i}
\log
\frac{\exp(\bar{z}_i^{-1}\cdot\bar{z}_j^{-1}/p)}
{\sum_{l\in[B],l\neq i}\exp(\bar{z}_i^{-1}\cdot\bar{z}_l^{-1}/p)}
\right]$$

##### 含义

* 采样批量 $B$ 条轨迹，每条属于某个对手策略类型 $k$；
* 对每条轨迹计算 embedding $\bar{z}_i^{-1}$；
* 若两条轨迹来自同一对手类型，就视为**正样本**；不同对手类型则为**负样本**；
* 用 InfoNCE（对比学习）形式最大化正样本相似度、最小化负样本相似度。



#### 监督训练（in-context learning-based supervised pre-training）

##### 🧮 公式（Response Loss）

$$\mathcal{L}_{\text{res}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau^{1,k}\sim\mathcal{T}^{1,k}}
\left[
\sum_{(y_t,a_t)\sim\tau^{1,k}}
\log M_{\theta_d}\left(a_t \mid y_t;, M_{\theta_e}\left(\text{GetOffD}(\mathcal{T}^{-1,k})\right)\right)
\right]
\tag{5}$$

其中：

* $M_{\theta_e}$ 是对手表示提取器（OPE encoder）；
* $\text{GetOffD}(\mathcal{T}^{-1,k})$ 从对手轨迹集中采样 $H$ 个连续片段（更能体现对手风格）；
* $y_t = (G_0, o_0, a_0, ..., G_t, o_t)$ 为自方的观测序列与回报；
* 模型 $M_{\theta_d}$ 自回归地预测动作 $a_t$，目标是最大化真动作的似然。

---

##### 💡 核心思想

* 使用 **In-Context Learning** 的方式进行监督训练；
* 模型通过 cross-attention 在对手轨迹嵌入 $z^{-1}$ 与自方轨迹之间建立联系；
* 从而**显式学习“对手策略 ↔ 响应策略”之间的映射关系**；
* 使得在测试阶段，给定新的对手嵌入，模型就能立即推断出相应的响应策略，而无需微调。



#### **In-context Opponent Adapting**


> 在部署阶段，模型不再进行参数更新，而是通过 **ICL（In-Context Learning）** 机制实时根据对手最新的行为轨迹进行策略适配。


---

##### **Opponent Context Window（OCW）**

OMIS 在部署阶段引入一个 **OCW（对手上下文窗口）**，记作 $\mathcal{W}$，用来缓存当前对手的最新轨迹数据。

* $\mathcal{W}$ 始终保存最近 $C$ 条对手轨迹：
  $$\mathcal{W} = {\tau_{l-C+1}^{-1}, \dots, \tau_{l-1}^{-1}, \tau_l^{-1}}$$
  其中 $l$ 是最近一次交互的索引。

* 每次交互后，$\mathcal{W}$ 会更新，**丢弃最旧、加入最新**，从而维持对手的短期记忆。

---

##### **策略公式**

$$a_t^1 \sim M_{\theta_d}(\cdot \mid y_t^1; M_{\theta_e}(\text{GetOnD}(\mathcal{W})))
\tag{6}$$

解释如下：

* $M_{\theta_e}$：OPE 编码器，用于从 $\mathcal{W}$ 中抽取代表当前对手策略的嵌入；
* $\text{GetOnD}(\mathcal{W})$：从 $\mathcal{W}$ 中采样若干连续片段（$H$ 段）并拼接，形成当前时刻的上下文数据；
* $M_{\theta_d}$：ICD 解码器（已训练好的响应策略）；
* $y_t^1$：自方的当前观测与历史轨迹；
* 输出：我方在时刻 $t$ 的动作分布 $a_t^1$。


---



---


