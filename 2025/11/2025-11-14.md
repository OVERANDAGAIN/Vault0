---
创建时间: 2025-十一月-14日  星期五, 9:51:57 上午
---
# 总结：

## 基于transfomer的representation learning

### OMIS
采用 **Transformer（Causal Transformer）** 架构，通过 **In-Context Learning（ICL）** 机制学习对手建模的通用表示。

* 训练输入为带有上下文的交互序列：状态 $s_t$、动作 $a_t^1, a_t^{-1}$、累计回报 $G_t$。
* Transformer 通过自注意力机制，在 token 序列中“读取”对手的历史行为，形成隐式的对手表示。
* 模型的三部分（Actor、Opponent Imitator、Critic）共享这一 Transformer backbone，分别利用该表示来生成动作、模拟对手、评估价值。
* 本质：Transformer 在此充当一个**跨任务的对手表征学习器**，能在不同对手之间迁移已学知识。

![[Pasted image 20251114102940.png]]

目标：

> 从离线轨迹中学习不同对手策略的潜在表示（embedding），使模型既能模仿这些对手，又能区分它们。

方法：

> 设计两个互补的损失函数 —— **生成式损失（Generative Loss）** 和 **判别式损失（Discriminative Loss）**。
> 前者让嵌入“像谁”，后者让嵌入“不混淆谁”。

---

#### **生成式损失 Generative Loss**
* 想让 embedding 真能“代表”某个对手的策略，就让模型根据 embedding 能够**重现该对手的行为分布**；
* 因此，$\mathcal{L}_{gen}$ 让 embedding 学会携带生成性信息：**从轨迹生成动作。**
* 
##### 公式

$$\mathcal{L}_{\text{gen}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau_i^{-1,k},\tau_j^{-1,k}\sim \mathcal{T}^{-1,k}}
\left[
\sum_{(o,a)\sim\tau_i^{-1,k}}
\log \pi_{\phi_d}\left(a \mid o,, \text{AP}\left(M_{\theta_e}(\tau_j^{-1,k})\right)\right)
\right]$$

##### 含义

* 对每个对手类型 $k$，采样两段属于该对手的轨迹 $\tau_i^{-1,k}$ 与 $\tau_j^{-1,k}$；
* 将其中一段 $\tau_j^{-1,k}$ 编码为平均轨迹表示 $\bar{z}^{-1,k} = \text{AP}(M_{\theta_e}(\tau_j^{-1,k}))$；
* 让解码器 $\pi_{\phi_d}$ 在观察 $o$ 与该表示的条件下，预测对手的动作 $a$；
* 本质是一个 **条件模仿学习（conditional imitation learning）** 任务。



---

#### **判别式损失 Discriminative Loss**
* 让同一个对手的 embedding **彼此靠近**；
* 不同对手的 embedding **彼此远离**；
* 从而 embedding 空间具有区分不同策略的能力。

##### 公式

$$\mathcal{L}_{\text{dis}}
= -\mathbb{E}_{\mathcal{T}^{-1}}
\left[
\frac{1}{B}
\sum_{i=1}^{B}
\frac{1}{|\mathcal{E}_i|-1}
\sum_{j\in\mathcal{E}_i,j\neq i}
\log
\frac{\exp(\bar{z}_i^{-1}!\cdot!\bar{z}_j^{-1}/p)}
{\sum_{l\in[B],l\neq i}\exp(\bar{z}_i^{-1}\cdot\bar{z}_l^{-1}/p)}
\right]$$

##### 含义

* 采样批量 $B$ 条轨迹，每条属于某个对手策略类型 $k$；
* 对每条轨迹计算 embedding $\bar{z}_i^{-1}$；
* 若两条轨迹来自同一对手类型，就视为**正样本**；不同对手类型则为**负样本**；
* 用 InfoNCE（对比学习）形式最大化正样本相似度、最小化负样本相似度。



#### 监督训练

##### 🧮 公式（Response Loss）

$$\mathcal{L}_{\text{res}}
= -\frac{1}{K} \sum_{k=1}^{K}
\mathbb{E}_{\tau^{1,k}\sim\mathcal{T}^{1,k}}
\left[
\sum_{(y_t,a_t)\sim\tau^{1,k}}
\log M_{\theta_d}\left(a_t \mid y_t;, M_{\theta_e}\left(\text{GetOffD}(\mathcal{T}^{-1,k})\right)\right)
\right]
\tag{5}$$

其中：

* $M_{\theta_e}$ 是对手表示提取器（OPE encoder）；
* $\text{GetOffD}(\mathcal{T}^{-1,k})$ 从对手轨迹集中采样 $H$ 个连续片段（更能体现对手风格）；
* $y_t = (G_0, o_0, a_0, ..., G_t, o_t)$ 为自方的观测序列与回报；
* 模型 $M_{\theta_d}$ 自回归地预测动作 $a_t$，目标是最大化真动作的似然。

---

##### 💡 核心思想

* 使用 **In-Context Learning** 的方式进行监督训练；
* 模型通过 cross-attention 在对手轨迹嵌入 $z^{-1}$ 与自方轨迹之间建立联系；
* 从而**显式学习“对手策略 ↔ 响应策略”之间的映射关系**；
* 使得在测试阶段，给定新的对手嵌入，模型就能立即推断出相应的响应策略，而无需微调。



#### **In-context Opponent Adapting**


> 在部署阶段，模型不再进行参数更新，而是通过 **ICL（In-Context Learning）** 机制实时根据对手最新的行为轨迹进行策略适配。


---

##### **Opponent Context Window（OCW）**

OMIS 在部署阶段引入一个 **OCW（对手上下文窗口）**，记作 $\mathcal{W}$，用来缓存当前对手的最新轨迹数据。

* $\mathcal{W}$ 始终保存最近 $C$ 条对手轨迹：
  $$\mathcal{W} = {\tau_{l-C+1}^{-1}, \dots, \tau_{l-1}^{-1}, \tau_l^{-1}}$$
  其中 $l$ 是最近一次交互的索引。

* 每次交互后，$\mathcal{W}$ 会更新，**丢弃最旧、加入最新**，从而维持对手的短期记忆。

---

##### **策略公式**

$$a_t^1 \sim M_{\theta_d}(\cdot \mid y_t^1; M_{\theta_e}(\text{GetOnD}(\mathcal{W})))
\tag{6}$$

解释如下：

* $M_{\theta_e}$：OPE 编码器，用于从 $\mathcal{W}$ 中抽取代表当前对手策略的嵌入；
* $\text{GetOnD}(\mathcal{W})$：从 $\mathcal{W}$ 中采样若干连续片段（$H$ 段）并拼接，形成当前时刻的上下文数据；
* $M_{\theta_d}$：ICD 解码器（已训练好的响应策略）；
* $y_t^1$：自方的当前观测与历史轨迹；
* 输出：我方在时刻 $t$ 的动作分布 $a_t^1$。











## 辅助学习一个moa
OMIS 的“Opponent Imitator”模块实质上扮演了一个 **MOA（Model of Opponent Agent）** 的角色：

* 模块 $\mu_\phi(a^{-1}|s, D)$ 学习预测对手的动作分布；
* 训练目标是最大化对手动作的似然：
  $$\max_\phi \ \mathbb{E}[\log \mu_\phi(a_t^{-1,k}|s_t, D_t^k)]$$
* 在搜索阶段（DTS）中，$\mu_\phi$ 用来在虚拟环境中**模拟对手反应**，从而支持策略的脑内演练；
* 因此，MOA 作为辅助学习器，使得 OMIS 能在无需真实对手反馈的情况下，预测对手策略变化并进行决策优化。










## 上下文学习ICL
ICL 是 OMIS 的核心思想：

> “让模型从历史上下文中自动理解对手是谁以及该如何行动。”

具体实现：

* **上下文定义**：由 episode 级历史 $D_{\text{epi}}$（反映整体风格）与 step 级历史 $D_{\text{step},t}$（反映当前局部趋势）组成；
* **机制**：模型不更新参数，而是根据输入的上下文片段自动调整策略输出；
* **作用**：

  * 在面对未知对手时，通过 ICL 的“few-shot 推理”机制识别相似对手；
  * 提高模型泛化性（generalization across opponents）；
  * 形成一种**无梯度更新的快速适应**机制。


## 离线对手建模-OOM
OMIS 属于 **Offline Opponent Modeling（离线对手建模）** 框架：

* 训练阶段完全基于离线数据（由 PPO 生成的 BR 轨迹），不依赖在线交互；
* 模型学习在给定对手轨迹分布下的最优反应；
* 理论部分（Thm 4.2）证明：

  * 对已见对手能收敛到最优响应；
  * 对未见对手会识别为最相似的已知对手（基于 KL 距离）。







## 离线数据集问题-OOD




## 策略响应策略



OMIS 的策略形成两层结构：

1. **预训练阶段**：学习最优响应（Best Response, BR）策略 $\pi^{1,k,*}$；
2. **测试阶段**：在面对未知对手 $\pi^{-1}$ 时，通过

   * **Actor πθ** 输出基础策略；
   * **Decision-Time Search (DTS)** 用 $\mu_\phi$ 模拟对手，$\omega$ 估价值，执行短期滚动；
   * 形成新的搜索策略 $\pi_{\text{search}} = \arg\max_{a} \hat{Q}(s,a)$；
   * 最终采用**混合策略**：
     $$\pi_{\text{mix}}(s) =
     \begin{cases}
     \pi_{\text{search}}(s), & |\hat{Q}| > \epsilon \
     \pi_\theta(s), & \text{otherwise}
     \end{cases}$$

* 理论保证（Thm 4.3）：该搜索策略至少不劣于原策略 πθ，具有单步改进性质。
* 这形成一种“策略响应策略”：

  > 在不同对手策略下，模型以 ICL 识别为输入，再以 DTS 动态调整响应，构成一个稳定的多层自适应响应闭环。
