# 总结：

## 基于transfomer的representation learning

OMIS 采用 **Transformer（Causal Transformer）** 架构，通过 **In-Context Learning（ICL）** 机制学习对手建模的通用表示。

* 训练输入为带有上下文的交互序列：状态 $s_t$、动作 $a_t^1, a_t^{-1}$、累计回报 $G_t$。
* Transformer 通过自注意力机制，在 token 序列中“读取”对手的历史行为，形成隐式的对手表示。
* 模型的三部分（Actor、Opponent Imitator、Critic）共享这一 Transformer backbone，分别利用该表示来生成动作、模拟对手、评估价值。
* 本质：Transformer 在此充当一个**跨任务的对手表征学习器**，能在不同对手之间迁移已学知识。
















## 辅助学习一个moa
OMIS 的“Opponent Imitator”模块实质上扮演了一个 **MOA（Model of Opponent Agent）** 的角色：

* 模块 $\mu_\phi(a^{-1}|s, D)$ 学习预测对手的动作分布；
* 训练目标是最大化对手动作的似然：
  $$\max_\phi \ \mathbb{E}[\log \mu_\phi(a_t^{-1,k}|s_t, D_t^k)]$$
* 在搜索阶段（DTS）中，$\mu_\phi$ 用来在虚拟环境中**模拟对手反应**，从而支持策略的脑内演练；
* 因此，MOA 作为辅助学习器，使得 OMIS 能在无需真实对手反馈的情况下，预测对手策略变化并进行决策优化。










## 上下文学习ICL
ICL 是 OMIS 的核心思想：

> “让模型从历史上下文中自动理解对手是谁以及该如何行动。”

具体实现：

* **上下文定义**：由 episode 级历史 $D_{\text{epi}}$（反映整体风格）与 step 级历史 $D_{\text{step},t}$（反映当前局部趋势）组成；
* **机制**：模型不更新参数，而是根据输入的上下文片段自动调整策略输出；
* **作用**：

  * 在面对未知对手时，通过 ICL 的“few-shot 推理”机制识别相似对手；
  * 提高模型泛化性（generalization across opponents）；
  * 形成一种**无梯度更新的快速适应**机制。


## 离线对手建模-OOM
OMIS 属于 **Offline Opponent Modeling（离线对手建模）** 框架：

* 训练阶段完全基于离线数据（由 PPO 生成的 BR 轨迹），不依赖在线交互；
* 模型学习在给定对手轨迹分布下的最优反应；
* 理论部分（Thm 4.2）证明：

  * 对已见对手能收敛到最优响应；
  * 对未见对手会识别为最相似的已知对手（基于 KL 距离）。







## 离线数据集问题-OOD




## 策略响应策略



OMIS 的策略形成两层结构：

1. **预训练阶段**：学习最优响应（Best Response, BR）策略 $\pi^{1,k,*}$；
2. **测试阶段**：在面对未知对手 $\pi^{-1}$ 时，通过

   * **Actor πθ** 输出基础策略；
   * **Decision-Time Search (DTS)** 用 $\mu_\phi$ 模拟对手，$\omega$ 估价值，执行短期滚动；
   * 形成新的搜索策略 $\pi_{\text{search}} = \arg\max_{a} \hat{Q}(s,a)$；
   * 最终采用**混合策略**：
     $$\pi_{\text{mix}}(s) =
     \begin{cases}
     \pi_{\text{search}}(s), & |\hat{Q}| > \epsilon \
     \pi_\theta(s), & \text{otherwise}
     \end{cases}$$

* 理论保证（Thm 4.3）：该搜索策略至少不劣于原策略 πθ，具有单步改进性质。
* 这形成一种“策略响应策略”：

  > 在不同对手策略下，模型以 ICL 识别为输入，再以 DTS 动态调整响应，构成一个稳定的多层自适应响应闭环。
