---
创建时间: 2025-十一月-14日  星期五, 9:51:57 上午
---
# 总结：

## 基于transfomer的representation learning

### OMIS

* 使用 **Opponent Policy Encoder (OPE)** 与 **In-context Control Decoder (ICD)** 的 Transformer 架构：OPE 将对手轨迹编码为嵌入 $z^{-1}$；ICD 以 $z^{-1}$ 为条件进行自回归控制。

* 训练输入为带有上下文的交互序列：状态 $s_t$、动作 $a_t^1, a_t^{-1}$、累计回报 $G_t$。
* Transformer 通过自注意力机制，在 token 序列中“读取”对手的历史行为，形成隐式的对手表示。
* 模型的三部分（Actor、Opponent Imitator、Critic）共享这一 Transformer backbone，分别利用该表示来生成动作、模拟对手、评估价值。
* 本质：Transformer 在此充当一个**跨任务的对手表征学习器**，能在不同对手之间迁移已学知识。

![[Pasted image 20251114102940.png]]

目标：

> 从离线轨迹中学习不同对手策略的潜在表示（embedding），使模型既能模仿这些对手，又能区分它们。

方法：

> 设计两个互补的损失函数 —— **生成式损失（Generative Loss）** 和 **判别式损失（Discriminative Loss）**。
> 前者让嵌入“像谁”，后者让嵌入“不混淆谁”。

---





---




## 辅助学习一个moa
### TAO

* TAO 的 **Opponent Policy Encoder (OPE)** 可以视为一种隐式的 MOA：它从离线数据中学习**对手策略嵌入（opponent embedding）**。
* OPE 同时优化两个目标：

  * **生成式损失（L_gen）**：重建对手动作分布；
  * **判别式损失（L_dis）**：区分不同对手策略。
* 这种联合目标让模型学到既能区分不同对手，又能生成其行为的潜在表示，本质上等价于“学习一个对手模型”。






OMIS 的“Opponent Imitator”模块实质上扮演了一个 **MOA（Model of Opponent Agent）** 的角色：

* 模块 $\mu_\phi(a^{-1}|s, D)$ 学习预测对手的动作分布；
* 训练目标是最大化对手动作的似然：
  $$\max_\phi \ \mathbb{E}[\log \mu_\phi(a_t^{-1,k}|s_t, D_t^k)]$$
* 在搜索阶段（DTS）中，$\mu_\phi$ 用来在虚拟环境中**模拟对手反应**，从而支持策略的脑内演练；
* 因此，MOA 作为辅助学习器，使得 OMIS 能在无需真实对手反馈的情况下，预测对手策略变化并进行决策优化。







---


## 上下文学习ICL


### TAO

#### 1) 离线第二阶段中的“ICL式”监督预训练

* 目标：学会**在给定对手上下文**时输出近似最优响应动作。损失：
  $$L_{\text{res}} = -\frac{1}{K}\sum_{k=1}^{K}
  \mathbb{E}_{\tau^{1,k}}\left[\sum_{\langle y_t,a_t\rangle\in\tau^{1,k}}
  \log M_{\theta_d}\big(a_t\mid y_t;, M_{\theta_e}(\mathrm{GetOffD}(\mathcal T^{-1,k}))\big)\right]$$
  其中 $y_t=(G_0,o_0,a_0,\dots,G_t,o_t)$，$\mathrm{GetOffD}$ 从对手离线轨迹中抽取 **连续片段** 拼接成“**对手上下文**”，再经 OPE 得到 $z^{-1}$；ICD 通过 **交叉注意力**在条件 $z^{-1}$ 下产生动作分布。
* **ICL 的影子**：仅通过“把对手轨迹片段作为**输入上下文**”来条件化决策；训练阶段让模型形成这种“看上下文就会切换响应风格”的能力。

#### 2) 部署阶段的“真 ICL”自适应

* 在线收集 **Opponent Context Window (OCW)**：$W={\tau^{-1}*{\ell-C+1},\dots,\tau^{-1}*\ell}$。
* 用 $\mathrm{GetOnD}(W)$ 抽取连续片段并编码；**在完全冻结参数 $\theta={\theta_e,\theta_d}$ 的情况下**，每步动作由
  $$a^1_t \sim M_{\theta_d}\left(\cdot \mid y^1_t;, M_{\theta_e}\big(\mathrm{GetOnD}(W)\big)\right)
  \tag{6}$$
  给出；适应性来自**上下文更新**而非梯度更新。







### OMIS
ICL 是 OMIS 的核心思想：

> “让模型从历史上下文中自动理解对手是谁以及该如何行动。”

具体实现：

* **上下文定义**：由 episode 级历史 $D_{\text{epi}}$（反映整体风格）与 step 级历史 $D_{\text{step},t}$（反映当前局部趋势）组成；
* **机制**：模型不更新参数，而是根据输入的上下文片段自动调整策略输出；
* **作用**：

  * 在面对未知对手时，通过 ICL 的“few-shot 推理”机制识别相似对手；
  * 提高模型泛化性（generalization across opponents）；
  * 形成一种**无梯度更新的快速适应**机制。


---



## 离线对手建模-OOM





* **Offline Opponent Modeling (OOM)** 是本文提出的新问题设定。
* 目标：利用已有的离线交互数据 $\mathcal{D}_{off}$，学习一个在**未知对手策略 $\pi^{-1, test}$** 下仍能自适应的策略模型。
* 离线数据包含多个对手策略下的轨迹：
  $$
  \mathcal{D}*{off} = {\mathcal{T}^{1,k}, \mathcal{T}^{-1,k}}*{k=1}^{K}
  $$
  其中 $k$ 表示对手类型。
* 模型在训练阶段学习对手分布及其行为特征，部署时通过 ICL 机制实现快速响应未知对手。






OMIS 属于 **Offline Opponent Modeling（离线对手建模）** 框架：

* 训练阶段完全基于离线数据（由 PPO 生成的 BR 轨迹），不依赖在线交互；
* 模型学习在给定对手轨迹分布下的最优反应；
* 理论部分（Thm 4.2）证明：

  * 对已见对手能收敛到最优响应；
  * 对未见对手会识别为最相似的已知对手（基于 KL 距离）。




---


## 离线数据集问题-OOD

### TAO
* 离线数据仅覆盖部分对手策略集合 $\Pi_{train}$，而测试时的 $\Pi_{test}$ 可能来自**分布外（out-of-distribution, OOD）** 的对手。
* 论文通过 OPE + Transformer 结合方式增强了泛化能力：

  * OPE 学习全局策略空间；
  * Transformer 在 ICL 阶段基于局部上下文动态适配。
* 因此，TAO 在 unseen 对手场景中（即分布外 OOD 测试）依旧能保持较好性能，这正是 OOM 的关键挑战。




---


## 策略响应策略

### TAO

$$a_t^1 \sim M_{\theta_d}(\cdot \mid y_t^1; M_{\theta_e}(\text{GetOnD}(\mathcal{W})))
\tag{6}$$



OMIS 的策略形成两层结构：

1. **预训练阶段**：学习最优响应（Best Response, BR）策略 $\pi^{1,k,*}$；
2. **测试阶段**：在面对未知对手 $\pi^{-1}$ 时，通过

   * **Actor πθ** 输出基础策略；
   * **Decision-Time Search (DTS)** 用 $\mu_\phi$ 模拟对手，$\omega$ 估价值，执行短期滚动；
   * 形成新的搜索策略 $\pi_{\text{search}} = \arg\max_{a} \hat{Q}(s,a)$；
   * 最终采用**混合策略**：
     $$\pi_{\text{mix}}(s) =
     \begin{cases}
     \pi_{\text{search}}(s), & |\hat{Q}| > \epsilon \
     \pi_\theta(s), & \text{otherwise}
     \end{cases}$$

* 理论保证（Thm 4.3）：该搜索策略至少不劣于原策略 πθ，具有单步改进性质。
* 这形成一种“策略响应策略”：

  > 在不同对手策略下，模型以 ICL 识别为输入，再以 DTS 动态调整响应，构成一个稳定的多层自适应响应闭环。


---


## Limitations


### TAO
1. 固定对手策略假设

	* 当前方法仅针对**固定策略的对手**进行建模，即假设对手在整个 episode 中使用不变的策略。
	* 然而，现实中对手可能会在游戏过程中动态改变或进化其策略。

2. 缺乏递归推理

	* 当前模型只考虑单向的 opponent modeling，即智能体建模对手，但不考虑对手也在建模自己。

3. 单对手建模限制

	* 实验中仅涉及**一个对手智能体**。

4. 仅限竞争场景

	* 本文实验仅在**竞争性（competitive）** 环境中进行。


### 

