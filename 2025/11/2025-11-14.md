---
创建时间: 2025-十一月-14日  星期五, 9:51:57 上午
---
# 总结：

## 动机

### TIPR
针对OOM(offline opponent modeling)问题中，如何使用一个次优离线数据集，达到在线的快速适应问题：

提出：
1. Truncated Q-value function(H horizon) with ICL $\Longrightarrow$ 学习一个额外的Q函数
2. Instant Policy Refinement(IPR) $\Longrightarrow$ 如何运用额外的Q函数来决定是否 切换策略

---
---

## 基于transfomer的representation learning

### TAO

* 使用 **Opponent Policy Encoder (OPE)** 与 **In-context Control Decoder (ICD)** 的 Transformer 架构：OPE 将对手轨迹编码为嵌入 $z^{-1}$；ICD 以 $z^{-1}$ 为条件进行自回归控制。


![[Pasted image 20251114102940.png]]

目标：

> 从离线轨迹中学习不同对手策略的潜在表示（embedding），使模型既能模仿这些对手，又能区分它们。

方法：

> 设计两个互补的损失函数 —— **生成式损失（Generative Loss）** 和 **判别式损失（Discriminative Loss）**。
> 前者让嵌入“像谁”，后者让嵌入“不混淆谁”。


---
### OMIS
#### 数据采集与标签

* 对训练集中每个对手 $\pi^{-1,k}$ 先用 PPO 充分训练求出其 BR：$\text{BR}(\pi^{-1,k})=\pi^{1,k,*}$。
* 采集轨迹：对手固定为 $\pi^{-1,k}$，我方用 $\pi^{1,k,*}$ 与之对战。
* 在每个时刻 $t$ 构造训练条目：
  $$D_t^k := \big(s_t,; D_t^k=(D_{\text{epi}}^k, D_{\text{step},t}^k),; a_{t}^{1,k,*},; a_t^{-1,k},; G_{t}^{1,k,*}\big)$$
  其中 $G_{t}^{1,k,*}$ 是从 t 时刻起的自方 RTG；$a_{t}^{1,k,*}$ 是 BR 动作标签；\(a_t^{-1,k}\) 是对手真动作标签。

####  三个监督目标（同一 Transformer 内分头）

* **Actor**：最大化 $\mathbb{E}\big[\log \pi_\theta(a_{t}^{1,k,*}!\mid s_t, D_t^k)\big]$ —— 模仿 BR。
* **Opponent Imitator**：最大化 $\mathbb{E}\big[\log \mu_\phi(a_t^{-1,k}!\mid s_t, D_t^k)\big]$ —— 模仿对手。
* **Critic**：最小化 $\mathbb{E}\big[\big(V_\omega(s_t, D_t^k)-G_{t}^{1,k,*}\big)^2\big]$ —— 拟合 RTG。




> **直觉**：让模型“看上下文就会：①像 BR 一样决策；②像对手一样决策；③评估局面值”。


![[Pasted image 20251114135610.png]]

---


### TIPR
类似于TAO的架构


![[Pasted image 20251114142514.png]]

---
---



## 辅助学习一个moa
### TAO

* TAO 的 **Opponent Policy Encoder (OPE)** 可以视为一种隐式的 MOA：它从离线数据中学习**对手策略嵌入（opponent embedding）**。
* OPE 同时优化两个目标：

  * **生成式损失（L_gen）**：重建对手动作分布；
  * **判别式损失（L_dis）**：区分不同对手策略。
* 这种联合目标让模型学到既能区分不同对手，又能生成其行为的潜在表示，本质上等价于“学习一个对手模型”。


---


### OMIS

OMIS 的“Opponent Imitator”模块实质上扮演了一个 **MOA（Model of Opponent Agent）** 的角色：

* 模块 $\mu_\phi(a^{-1}|s, D)$ 学习预测对手的动作分布；
* 训练目标是最大化对手动作的似然：
  $$\max_\phi \ \mathbb{E}[\log \mu_\phi(a_t^{-1,k}|s_t, D_t^k)]$$
* 在搜索阶段（DTS）中，$\mu_\phi$ 用来在虚拟环境中**模拟对手反应**，从而支持策略的脑内演练；
* 因此，MOA 作为辅助学习器，使得 OMIS 能在无需真实对手反馈的情况下，预测对手策略变化并进行决策优化。

### TIPR
* TIPR 的 Truncated Q 可以视为一种的 MOA：它从离线数据中学习**对手策略嵌入（opponent embedding）**

---
---

## 上下文学习ICL


### TAO

#### 1) 离线第二阶段中的“ICL式”监督预训练

* 目标：学会**在给定对手上下文**时输出近似最优响应动作。损失：
  $$L_{\text{res}} = -\frac{1}{K}\sum_{k=1}^{K}
  \mathbb{E}_{\tau^{1,k}}\left[\sum_{\langle y_t,a_t\rangle\in\tau^{1,k}}
  \log M_{\theta_d}\big(a_t\mid y_t;, M_{\theta_e}(\mathrm{GetOffD}(\mathcal T^{-1,k}))\big)\right]$$
  其中 $y_t=(G_0,o_0,a_0,\dots,G_t,o_t)$，$\mathrm{GetOffD}$ 从对手离线轨迹中抽取 **连续片段** 拼接成“**对手上下文**”，再经 OPE 得到 $z^{-1}$；ICD 通过 **交叉注意力**在条件 $z^{-1}$ 下产生动作分布。
* **ICL 的影子**：仅通过“把对手轨迹片段作为**输入上下文**”来条件化决策；训练阶段让模型形成这种“看上下文就会切换响应风格”的能力。

#### 2) 部署阶段的“真 ICL”自适应

* 在线收集 **Opponent Context Window (OCW)**：$W={\tau^{-1}*{\ell-C+1},\dots,\tau^{-1}*\ell}$。
* 用 $\mathrm{GetOnD}(W)$ 抽取连续片段并编码；**在完全冻结参数 $\theta={\theta_e,\theta_d}$ 的情况下**，每步动作由
  $$a^1_t \sim M_{\theta_d}\left(\cdot \mid y^1_t;, M_{\theta_e}\big(\mathrm{GetOnD}(W)\big)\right)
  \tag{6}$$
  给出；适应性来自**上下文更新**而非梯度更新。





---

### OMIS

> “让模型从历史上下文中自动理解对手是谁以及该如何行动。”

具体实现：

* **上下文定义**：由 episode 级历史 $D_{\text{epi}}$（反映整体风格）与 step 级历史 $D_{\text{step},t}$（反映当前局部趋势）组成；
* **机制**：模型不更新参数，而是根据输入的上下文片段自动调整策略输出；
* **作用**：

  * 在面对未知对手时，通过 ICL 的“few-shot 推理”机制识别相似对手；
  * 提高模型泛化性（generalization across opponents）；
  * 形成一种**无梯度更新的快速适应**机制。

---

### TIPR
TIPR 承接了 Jing 等人 2024 年的 OOM 框架，其中自适应策略 $\pi(a^1 | o^1, D)$ 通过**In-Context Learning (ICL)** 实现：

* 离线阶段通过 Transformer 模型学习“如何根据对手历史 $D$ 生成适应性行为”；
* 测试阶段模型无需额外训练，仅利用当前对手的交互片段 $(o_{1:m}^{-1}, a_{1:m}^{-1})$ 即可动态调整策略；
* 这类方法本质上让 Transformer 充当一个“元学习器 (meta-learner)”——**在上下文中学习学习算法本身**，从而实现快速对手适应。



---
---


## 离线对手建模-OOM



### TAO

* **Offline Opponent Modeling (OOM)** 是本文提出的新问题设定。
* 目标：利用已有的离线交互数据 $\mathcal{D}_{off}$，学习一个在**未知对手策略 $\pi^{-1, test}$** 下仍能自适应的策略模型。
* 离线数据包含多个对手策略下的轨迹：
  $$
  \mathcal{D}*{off} = {\mathcal{T}^{1,k}, \mathcal{T}^{-1,k}}*{k=1}^{K}
  $$
  其中 $k$ 表示对手类型。
* 模型在训练阶段学习对手分布及其行为特征，部署时通过 ICL 机制实现快速响应未知对手。


---


### OMIS
OMIS 属于 **Offline Opponent Modeling（离线对手建模）** 框架：

在训练阶段，OMIS 先针对训练集中每个对手 $\pi^{-1,k}$ 离线求解其 Best Response：
$$\text{BR}(\pi^{-1,k})=\pi^{1,k,*}(a|s)$$
然后用 $π^{-1,k}, π^{1,k,}$ 配对产生离线轨迹 ${(s_t,a^{1,k,*}_t,a^{-1,k}_t,G^{1,k,*}_t)}$，
供三组件监督学习。


---

### TIPR
Offline Opponent Modeling（OOM）旨在**仅使用离线数据**训练能够动态适应对手的自智能体策略。

* 关键假设：不再与环境或对手交互；
* 训练目标：学习一个 adaptive policy，使其在测试时可根据对手历史行为自适应；
* 原始 OOM（Jing et al., 2024a）假定数据集为最优轨迹，而 TIPR 针对**次优离线数据（suboptimal dataset）**进行了改进；
* TIPR 在原有 OOM 框架上引入 Truncated Q 模块与 Instant Policy Refinement (IPR)，在测试阶段实时判断并微调策略，从而缓解数据次优问题。


---
---

## 离线数据集问题-OOD

### TAO
* 离线数据仅覆盖部分对手策略集合 $\Pi_{train}$，而测试时的 $\Pi_{test}$ 可能来自**分布外（out-of-distribution, OOD）** 的对手。

The expert data is derived from the interactions between the opponent policy π−1,k and its approximate best response policy π1,k,∗.

assume that Dk, k ∈ [K] comprises a larger proportion, η, of expert data and a smaller proportion, 1 − η, of noisy data
and η to 0.9





### OMIS

Best Responses (BR) against different opponent policies


---


### TIPR
TIPR 关注的核心问题是：离线 OOM 数据集通常来源于不完美（次优）策略交互，导致分布偏移（OOD）和性能退化。

* OOM 的性能严重依赖于离线数据质量；
* TIPR 通过学习 **Truncated Q（截断 Q）** 来增强对奖励信号的局部建模能力，减轻长时回报估计误差；
* 在次优数据集上，TIPR 的“保守式改进”机制（参照 OCL 思想）确保更新仅在高置信度区域进行，避免因 OOD 样本导致的错误改进；
* 实验中通过设定不同 “Optimal Ratio ρ” 的数据集模拟次优程度，并验证 TIPR 的鲁棒提升效果。





---
---

## 策略响应策略

### TAO

* 训练得到的策略本质是一个**对手感知的条件响应策略**：
  $$M_\theta(a^1\mid o^1;,D),\quad D=\text{对手上下文（轨迹片段）}.$$
* **离线阶段**：用近似最优响应轨迹（$\pi^{1,k,*}$ 生成，含噪声）对 ICD 进行监督，让其在不同 $z^{-1}$ 条件下输出**近似最佳响应**。
* **部署阶段**：**完全冻结参数 $\theta$**，只不断更新 OCW；动作按式(6)由“**当前上下文→OPE 编码→ICD 解码**”产生，实现**无梯度的快速自适应**。
* 直观理解：**OPE 负责识别，**ICD 负责响应**；识别与响应都通过上下文馈入完成，而非在线微调。

---

### OMIS
* 面对未知、还会换策略的对手：先用最近历史**拼上下文 D**；
* 对每个可行动作，做**L 步脑内演练（rollout）**：

  * 自己的动作用 Actor 采样；
  * 对手的动作用 Imitator 采样；
  * 环境用真模型/学到的模型推进；
  * 末端用 Critic 估价值；
* 汇总每个候选动作的估计回报，挑最优；
* 若搜索结果“信心不够”，就退回原始 Actor 的动作（一个**混合策略**的开关保证稳定）。

---

### TIPR
TIPR 引入 **Instant Policy Refinement (IPR)**，在测试阶段即时修正策略响应：

1. **阶段一**：使用 Transformer 结构训练 **Truncated Q** 函数，估计未来固定 horizon 内的期望回报；
2. **阶段二**：在测试时，计算 “置信门控条件 (Refinement Condition)”：

   * 若累计置信 $Q_C^{\Sigma}>0$（即未来可能获得 reward），则触发 refinement；
   * 否则保持原策略；
3. **阶段三**：当 refinement 触发时，TIPR 使用 $Q_V$（value head）选择当前最优动作，实现即时策略微调；

这种设计的意义在于：

* 将传统的 offline conservative learning（OCL）的“训练期保守更新”迁移到“测试期即时修正”；
* 从策略响应角度看，相当于动态平衡了**策略稳定性（保守执行）与适应性（即时响应）**。



---
---

## Limitations

### TAO
1. 固定对手策略假设

	* 当前方法仅针对**固定策略的对手**进行建模，即假设对手在整个 episode 中使用不变的策略。
	* 然而，现实中对手可能会在游戏过程中动态改变或进化其策略。

2. 缺乏递归推理

	* 当前模型只考虑单向的 opponent modeling，即智能体建模对手，但不考虑对手也在建模自己。

3. 单对手建模限制

	* 实验中仅涉及**一个对手智能体**。

4. 仅限竞争场景

	* 本文实验仅在**竞争性（competitive）** 环境中进行。

---

## OMIS
1. 固定对手策略假设
2. perfect information games

---

## TIPR

1. 目前的Truncated Q-value function 的Horizon确定：如何寻找最优的horizon大小
2. 在在线测试阶段，对手的策略是固定不变的，并没有面对learning-based的opponents去做在线适应
