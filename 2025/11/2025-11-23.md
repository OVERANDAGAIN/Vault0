---
创建时间: 2025-十一月-22日  星期六, 11:44:41 晚上
---

HOP+ 4\*4 self-play 效果：

![[Pasted image 20251122234624.png]]

![[Pasted image 20251122234444.png]]



前20min : value_loss 的并没有上升很高，


![[Pasted image 20251123001733.png]]












我先直接给一个「博士生级别排雷方案大纲」，一步步从**现象 → 假设 → 实验设计 → 代码检查点**来走，你可以按这个路线慢慢推进。后面每一步我们都可以细化成具体实验和改代码的建议。

---

## 0. 先把问题「科学地描述清楚」

现象：

* 4 个独立学习的 MBRL_Policy，self-play、对称环境。
* 多次实验中，稳定出现：
  **$R_1 > R_2 > R_3 > R_4$**（按 agent 序号严格排序）。

**第一件事：**先确认这是“结构性的偏差”，不是噪声。建议你先做：

* 画出每个 agent 的：

  * 平均 episode return 随训练步数的曲线；
  * 多个 seed（比如 3～5 个 seed），看排序是不是稳。
* 统计训练中后期（比如最后 20% training steps）的：

  * $mean(R_1), \dots, mean(R_4)$；
  * 把不同 seed 的结果做个箱线图或均值±方差。

> 如果多 seed 下排序都不乱，就可以认为是**系统性偏差**，值得认真排查。

---

## 1. 假设列表：可能导致「按编号排队」的来源

先不碰代码，逻辑上列出几种大概率原因：

1. **环境本身不对称**

   * 初始位置、视野、墙、stag/hare 分布、tie-breaking 规则，对 player_1 更友好；
   * 或者 reward 分配逻辑里不小心用了 `player_id` 顺序。

2. **训练流程上的不对称**

   * 训练循环里对 agent 的处理顺序固定（总是先更新 player_1，再 player_2...），且更新逻辑有依赖；
   * buffer 写入 / 采样，对某些 agent 更有利（样本更多 / 更高质量）。

3. **模型与配置上的不对称**

   * 不同 agent 的 config 不完全一样（learning rate、gamma、ToM_config、my_id 等）；
   * subgoal / MOA / WM 中隐含使用 `my_id` 导致「第一个 my_id=1 的 agent」被更好地建模。

4. **日志或奖励统计 bug**

   * reward 写错了：例如所有人 reward 都记录到 `agent_1` 上；
   * 或者 `agent_k` 的 reward 被缩放 / mask 掉一部分。

**大纲目标**：逐类排除。优先从「环境对称性」和「训练 pipeline 对称性」开始。

---

## 2. 阶段一：环境对称性检查（不训练，只跑随机）

**目标**：先验证纯环境 + 随机策略下，4 个 agent 的期望收益是不是相等。

### 2.1 固定随机策略，禁用学习

实验设计：

* 写一个小脚本：

  * 所有 4 个 agent 都用「独立的随机策略」（即每个 agent 的动作是 i.i.d. uniform over legal actions）；
  * 完全不更新参数；
  * 跑很多 episode（比如 10k episode，或者至少每个 agent 收到几万次 reward）。
* 统计每个 agent 的平均回报 $R_i^{random}$。

**预期：**

* 如果环境真的是对称的，理论上 $R_1^{random} \approx R_2^{random} \approx R_3^{random} \approx R_4^{random}$，差别只在统计噪声里；
* 如果此时就出现稳定的 $R_1^{random} > R_2^{random} > \dots$，说明**环境/奖励机制本身就是不对称的**，训练只是放大了它。

### 2.2 agent index 随机重命名 / 打乱 ID

再做一个小实验：

* 在 reset 环节，对环境里玩家的 ID 做一个随机 permutation：

  * 即每次 episode，把「真正的物理玩家」随机映射到 `player_1 ~ player_4` 这几个名字上；
* 继续使用随机策略，或使用你当前的学习策略，但每次重新开局，ID 乱序。

观察：
如果 reward 排序跟着映射走（比如真实最优的那个 agent，总是映射到 `player_1` 时 reward 最高），那说明「reward 与**物理位置/角色**关联」，不是单纯和「名字」相关。

> 这一步可以帮你判断，**问题是出在环境角色本身（位置、视野、stag 距离）不对称，还是出在代码中硬编码了 player_1 的优势**。

---

## 3. 阶段二：训练 pipeline 的对称性检查

假设第 1 阶段发现：随机策略下环境是对称的，那问题就更可能在**训练逻辑**。

### 3.1 配置对比：cfg_by_agent 是否完全一致

检查：

* `cfg_by_agent["player_1"]` 到 `"player_4"`：

  * 除了 `ToM_config["my_id"]` 这种必须不同的字段外，其他是否完全一致？
  * 特别是：`learning_rate`, `gamma`, `omg.args.buffer_size`, `batch_size`, `world_model` 配置。

如果有差别（比如写配置时 copy/paste 时不小心改了一个 agent 的参数），会直接导致训练效果按 index 分层。

### 3.2 训练顺序与更新频率

在 `run` 脚本里，你是这么训练的：

```python
for aid in agent_ids:
    policy_dict[aid].train(agent_trajectories=agent_trajectories, agent_ids=agent_ids)
```

**检查：**

* 每个 agent 的 `train()` 内部：

  * 是否都调用了 `_push_episode_to_buffers`（应该是每个 agent 各推一份）；
  * 是否都执行了 `_maybe_train_wm / _maybe_train_moa / _maybe_train_omg / _maybe_train_policy`；
  * 有没有某些逻辑是「只在 my_id=1 或 my_id_zero=0 时才运行」；
* `_maybe_train_*` 里使用的是 **全局的 `learn_count`**，还是每个 agent 有自己的计数？

  * 如果 `learn_count` 是成员变量，那么 4 个 policy 各自维护，更新次数相同；
  * 如果有任何地方用到**全局变量**或者只某个 agent 调用了 `learn_count += 1`，就可能导致「某个 agent train 次数更多」。

实验建议：

* 在 `MBRL_Policy.train()` 里加日志：

  * 打印 `my_id`、`learn_count`、各个 `_maybe_train_*` 是否被触发；
  * 训练若干 step，统计每个 agent 在同一 global step 下实际执行了几次 update。

---

## 4. 阶段三：数据分布对称性检查（buffer 层面）

从你贴的代码可以看到，**每个 agent 有自己的：**

* `real_ppo_buffer / imagined_ppo_buffer`（在各自的 policy 对象里）；
* `wm_buffer`；
* `model_buffer`（MOA）。

问题：**这些 buffer 收到的数据是否“公平”？**

### 4.1 `_push_episode_to_buffers` 中 my_id 的使用

关键片段：

```python
my_name = f"player_{self.my_id}"
my_idx = agent_ids.index(my_name)

subgoal_T = self._gather_subgoal_series(env_id, t_valid).to(device)
subg_pad = torch.zeros(nA, MAX_T, D, dtype=torch.float32, device=device)
subg_pad[my_idx, :t_valid, :] = subgoal_T
...
my_tmap = per_agent.get(my_name, {})
...
for t in range(steps):
    tr_my = my_tmap.get(t)
    ...
    # 推入 wm_buffer & moa_buffer
```

对每个 policy（my_id = 1,2,3,4）来说：

* `my_name` 和 `my_idx` 不同；
* 对应的 `my_tmap` 就是 **它视角下自己的轨迹**；
* 理论上，这应该是对称的：每个 policy 都用同一份 env_episodes，只是从不同的 agent 视角抽数据。

但可能出现两类问题：

1. **某些 agent 的 subgoal 日志 `_logs[key]` 不完整**

   * `_gather_subgoal_series` 依赖于你在 `get_action()` 中缓存的 `self._logs[(env_id, self.my_id)]`；
   * 如果在并行环境中、某些 agent 的 `get_action` 没有被正常调用（比如中途死亡、env_alive_agents 处理问题），那 `_logs` 对这个 agent 就是空的；
   * 于是它的 `subgoal_T` 总是零向量，等价于「不会用到 subgoal」，导致训练质量变差。

2. **某些 agent 在 _push_episode_to_buffers 里被提前 `return`**

   * 比如 `my_tmap` 为空或者 `T==0` 时直接 return；
   * 如果因为数据结构构造的小错误（比如 env_id 类型不一致）导致某个 agent 的 `my_tmap` 老是空，那它的 buffer 就永远很 sparse，学习自然最差。

**建议实验：**

* 在 `_push_episode_to_buffers` 最后，加一些统计，按 agent 打印：

  * 每次调用中，`my_id`、`my_name`、`len(my_tmap)`、`steps`；
  * 每个 `MBRL_Policy` 对象在一段训练期间累计：

    * push 进 `wm_buffer` 的样本数；
    * `model_buffer` 的样本数；
    * `real_ppo_buffer` 的 episode 数。

你可以在训练跑一段时间后，打印类似：

> player_1: wm_buffer size = 5000, real_ppo_episodes = 800
> player_2: wm_buffer size = 3000, real_ppo_episodes = 600
> ...

如果这里就已经不平衡，那 reward 排序完全可以解释为 **数据分布不对称 → 学到的策略强度不对称**。

---

## 5. 阶段四：控制变量实验（逐个关掉模块）

因为你的 MBRL_Policy 结构很复杂（PPO + OMG + MOA + WM + imagined rollouts），要查问题可以采用“逐个关掉”的策略：

### 5.1 关掉 WM & imagined rollouts，只保留 PPO + OMG + MOA

* 在 `train()` 里先注释掉：

  * `self._maybe_train_wm()`
  * `self._generate_model_rollouts()` 和 imagined buffer 的使用；
* 保留真实数据上的 PPO / OMG / MOA；

如果此时 reward 排序消失或明显减弱，说明问题很可能出在：

* imagined data 的生成；
* imagined episode 被推回 buffer 时的逻辑（env_id、agent_id 对齐问题）；
* 或者 imagined 的分布对某些 agent 更有利。

### 5.2 再关掉 OMG 和 MOA，只保留「纯 PPO」

* 让 `get_action()` 里 subgoal 相关的都变成常数零向量；
* `train()` 里只调用 `_maybe_train_policy()`，不调用 `_maybe_train_omg/_maybe_train_moa`。

观察 reward 排序：

* 如果纯 PPO self-play 下 4 agent 是对称的（奖励差不多），那说明问题在高阶模块（OMG/MOA/WM）；
* 如果纯 PPO 也有排序，那很可能是环境 / buffer / PPO 实现细节的问题。

这一套实验做完，你就能把问题锁定到「哪个模块」上。

---

## 6. 阶段五：对照实验：共享参数 vs 独立参数 self-play

虽然你现在**想要 4 套独立的 model**，但作为一个诊断工具：

* 你可以跑一版「单一 shared policy」的 self-play（所有 agent 引用同一个 policy 对象）；
* 比较：

  * shared policy 下：4 个 agent 的平均 reward 是否还呈严格排序？
  * 独立 policy 下：排序是否更严重？

**解释：**

* 如果 shared policy 下，各 agent reward 几乎一样，那说明：

  * 训练 pipeline（整体更新逻辑）是 OK 的；
  * 问题更可能是 **独立参数 + 数据不均衡** 导致的；
* 如果 shared policy 下仍然有强烈排序，那更倾向于：

  * 环境本身不对称；
  * 或者 reward 记账 / logging 出错。

---

## 7. 阶段六：最后是「解释」而不是「瞎修」

如果以上步骤一轮走下来，你发现：

* 环境随机策略下是对称的；
* buffer 中各 agent 的数据量是相近的；
* 在简化版本（纯 PPO / 去除 imagined / 共享 policy）下，reward 排序消失；

那你就可以比较有把握地说：

> **当前 4 个独立 MBRL_Policy 在复杂结构（OMG+MOA+WM+imagined）下的训练 pipeline，存在 agent 视角特定的数据偏差 / subgoal logging 不对齐的问题**。

这时候我们再一起去具体看：

* `_logs[(env_id, self.my_id)]` 在多环境并行 + self-play 下是不是对某些 my_id 不完整；
* imagined episode 的构造中，“虚拟 env_id” 和真实 agent_id 映射是否被正确处理；
* MOA 的 `model_buffer[i]` 针对不同 my_id 推入的数据中，样本是否均衡。

---

## 总结：完整的大体排查方案大纲

你可以按这个 checklist 来推进：

1. **现象刻画**

   * 多 seed 曲线 + 均值/方差；
   * 确认 reward 排序是系统性的。

2. **环境对称性验证**

   * 随机策略 + 禁用学习，统计 $R_i^{random}$；
   * 引入 agent id permutation，看排序是否跟着 id 走。

3. **训练 pipeline 对称性检查**

   * diff `cfg_by_agent[player_k]` 是否相同；
   * 打 log 检查 `train()` 调用频率、`learn_count` 和各 `_maybe_train_*` 的触发次数是否对称。

4. **buffer / 数据分布检查**

   * 对每个 MBRL_Policy 统计 `real_ppo_buffer`、`imagined_ppo_buffer`、`wm_buffer`、`model_buffer` 的大小随时间走势；
   * 检查 `_push_episode_to_buffers` 中 `my_tmap`、`subgoal_T` 是否对某些 my_id 总是为空或长度显著更短。

5. **控制变量实验**

   * 关闭 WM & imagined rollouts；
   * 再关闭 OMG & MOA，只留 PPO；
   * 看 reward 排序在哪一级开始出现。

6. **shared policy 对照实验**

   * 共享一套参数 self-play，对比 reward 排序现象；
   * 帮助区分「环境/logging 问题」和「独立策略+数据偏差」问题。

7. **定位 / 修改**

   * 根据上面定位到的模块（环境、buffer、OMG、MOA、WM），再做针对性的 code review 和修复。

---

你可以先从**第 2 步（随机策略环境检查）**开始，我强烈建议先做一下这个实验，我们就能先排除一半的可能性。做完之后，把统计结果（比如 4 个 agent 的平均 reward），或者你画的图贴给我，我们再一起往下走下一阶段。
