先说人话版结论，再给你一段可以直接改成自己代码的 PyTorch 实现。

---

## 1. “L-BFGS 训练至收敛”在干什么？

这句话一般意味着：

1. **先用 Adam 训练固定步数**（这里是 100K iterations），把参数推到一个“还不错”的区域；
2. 然后**切换到 L-BFGS 做精修**，不再规定 多少个 epoch，而是：

   * 每次 full-batch 计算 loss+grad；
   * L-BFGS 内部做线搜索、更新拟牛顿矩阵，减少 loss；
   * 直到「收敛条件」满足，比如：

     * 损失函数变化 $|\Delta L| < 10^{-9}$，**或者**
     * 梯度范数 $|\nabla L| < 10^{-9}$，
   * 满足就停，不再多跑无意义的小步。

**为什么可以不用固定训练轮数？**

因为在**确定的目标函数 + 确定的数据集**上，优化的目标不是“跑 100 个 epoch”，而是“把 loss 优化到足够小 / 梯度接近 0”。
L-BFGS 属于二阶（拟牛顿）方法，每一步都比较“贵”，通常就让它**自动停在收敛阈值附近**更合理，而不是瞎定一个 epoch 数：

* 收敛得快：提前停，省算力；
* 收敛很慢：如果还没到阈值，就继续迭代，避免“训练轮数不够”的尴尬。

论文里写的 **tolerance = 1e-9**，就是一个**收敛精度**：

> “当 loss/梯度的变化已经小到 $10^{-9}$ 级别时，我们认为已经收敛，不必再继续优化。”

---

## 2. PyTorch 里的 L-BFGS 怎么写？

PyTorch 的 `torch.optim.LBFGS` 比较特殊，有两点要注意：

1. **需要 closure 函数**：

   * 这个函数要清零梯度、前向、计算 loss、反向求梯度，并返回 loss；
   * L-BFGS 内部会多次调用这个 closure 做线搜索。
2. **适合 full-batch / 小数据集**：

   * 每一步都要用到整个数据集；
   * 和 SGD/Adam 不一样，不太适合超大规模 mini-batch 随机优化。

下面给一段完整结构：

* 先用 Adam 训练 100K 步；
* 再用 L-BFGS “直到收敛（tol=1e-9 或达到 max_epochs）”。

你只需要替换 `MyModel`、`X_train`、`y_train` 为自己的东西即可直接跑。

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ===== 1. 一个简单的模型示例 =====
class MyModel(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.net(x)

# ===== 2. 假设已经有训练数据 X_train, y_train =====
# 注意：L-BFGS 通常用 full-batch，所以这里直接是整批 tensor
# 你可以根据需要改成自己的数据加载方式
device = "cuda" if torch.cuda.is_available() else "cpu"

# 示例数据（你自己换成真实数据）
N, in_dim, hidden_dim, out_dim = 1024, 20, 64, 1
X_train = torch.randn(N, in_dim, device=device)
y_train = torch.randn(N, out_dim, device=device)

model = MyModel(in_dim, hidden_dim, out_dim).to(device)
criterion = nn.MSELoss()

# ===== 3. 第一阶段：Adam 训练 100K iter =====
adam = optim.Adam(model.parameters(), lr=1e-3)
num_adam_iters = 100_000

model.train()
for step in range(num_adam_iters):
    adam.zero_grad()
    pred = model(X_train)
    loss = criterion(pred, y_train)
    loss.backward()
    adam.step()

    if (step + 1) % 5000 == 0:
        print(f"[Adam] step {step+1}/{num_adam_iters}, loss = {loss.item():.6e}")

# ===== 4. 第二阶段：L-BFGS 直到收敛 =====
# 这里用 tolerance_change 模拟论文里的 1e-9 收敛阈值
lbfgs = optim.LBFGS(
    model.parameters(),
    lr=1.0,               # 通常配合 line_search_fn 使用较大的 lr
    max_iter=50,          # 每次 step 内部最多迭代 50 次
    max_eval=None,
    history_size=10,
    line_search_fn="strong_wolfe",
    tolerance_grad=1e-9,   # 梯度范数收敛阈值（可调）
    tolerance_change=1e-9  # loss 变化收敛阈值（可调）
)

max_lbfgs_epochs = 100      # “外层”最多跑多少次 LBFGS step
global_tol = 1e-9           # 我们自己再加一层 loss 变化的停止条件

prev_loss = None

for epoch in range(max_lbfgs_epochs):
    # closure 会被 LBFGS 内部多次调用
    def closure():
        lbfgs.zero_grad()
        pred = model(X_train)
        loss = criterion(pred, y_train)
        loss.backward()
        return loss

    loss = lbfgs.step(closure)   # 返回的是 closure 的最终 loss 值（float）

    loss_val = loss.item()
    # 监控 loss 变化，如果变化已经非常小，就认为收敛
    if prev_loss is not None:
        delta = abs(prev_loss - loss_val)
        print(f"[L-BFGS] epoch {epoch+1}, loss = {loss_val:.12e}, |ΔL| = {delta:.12e}")
        if delta < global_tol:
            print(f"[L-BFGS] Converged: |ΔL| < {global_tol}")
            break
    else:
        print(f"[L-BFGS] epoch {epoch+1}, loss = {loss_val:.12e}")

    prev_loss = loss_val
else:
    print("[L-BFGS] Reached max_lbfgs_epochs without hitting global_tol")
```

---

## 3. 这段代码里“直到收敛”的具体逻辑

你可以把「收敛」理解为两层判断：

1. **PyTorch 自带的判断（内部）：**

   在 `LBFGS` 构造函数内部：

   * `tolerance_grad=1e-9`：
     如果当前梯度范数已经小于这个值，认为已经没必要继续更新；
   * `tolerance_change=1e-9`：
     如果每一步参数更新导致的 loss 变化已经小于这个值，也认为没必要继续。

   只要满足这些条件之一，**本次 `optimizer.step(closure)` 内部就会提前停止迭代**。

2. **外层我们自己加的判断：**

   ```python
   delta = abs(prev_loss - loss_val)
   if delta < global_tol:
       break
   ```

   这保证了：即使内部还没有触发收敛（比如你把内部的 tolerance 设得比较宽），只要我们观测到 epoch 级别的 loss 基本不变了，也认为“已经优化到头了”。

---

## 4. 总结一下要点（方便你写到自己的笔记 / 报告里）

* L-BFGS 不是像 SGD/Adam 那样依赖大步数、小学习率，而是**少步数 + 每步很“精细”**；
* “训练至收敛”就是：loss 或梯度的变化已经小到阈值以下（比如 $10^{-9}$），继续走下去收益几乎为 0；
* 因为有这样的**收敛判据**，就不需要提前规定“训练多少轮”，让优化器自己决定什么时候停更合理；
* 在 PyTorch 中：

  * 通过 `tolerance_grad` / `tolerance_change` 控制内部收敛；
  * 通过自己的外层循环 + loss 监控再加一层“保险”。

如果你愿意，我也可以帮你把这套逻辑写成一个函数 `train_with_lbfgs(model, loss_fn, data, tol=1e-9)`，方便直接丢进你现有项目里用。
