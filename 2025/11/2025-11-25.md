ä¸‹é¢ç»§ç»­è¿›è¡Œ **PPT ç¬¬ 2 é¡µ** çš„åˆ¶ä½œã€‚
è¿™ä¸€é¡µé‡ç‚¹æ˜¯ï¼š**åœ¨æ·±å…¥å…¬å¼ä¹‹å‰ï¼Œè®©å¬ä¼—ç›´è§‚ç†è§£ SMï¼ˆState Modellingï¼‰å’Œ AMï¼ˆAgent Modellingï¼‰åˆ°åº•åœ¨åšä»€ä¹ˆã€‚**

æˆ‘ä¼šç»™ä½ ï¼š

* **PPTé¡µé¢çš„å†…å®¹ï¼ˆç®€çŸ­æ¸…æ™°ï¼‰**
* **Presenter Notesï¼ˆé€å¥å£è¯­ç¨¿ï¼‰**
* **é…å›¾å»ºè®®ï¼ˆå¿…è¦æ—¶æˆ‘å¯ä»¥ç›´æ¥ç”Ÿæˆï¼‰**

---

# ğŸŸ¦ **ğŸ“PPT ç¬¬ 2 é¡µï¼šIntuition â€” State Modelling & Agent Modelling**

---

# ğŸ¨ **PPT é¡µé¢å†…å®¹ï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰**

**Intuition: What does SMPEÂ² try to learn?**

**State Modelling (SM)**

* Each agent observes only a small part of the environment.
* Goal: infer a **belief embedding** ( z^i ) that summarizes
  â€œwhat the global state might beâ€
  based only on local observation ( o^i ).

**Agent Modelling (AM)**

* Other agents also observe valuable information.
* But most of their observation features are
  **irrelevant, redundant, or not inferable**.
* AM Filters ( w^i ) select **only the useful and inferable parts**
  of othersâ€™ observations.

**Key idea of SMPEÂ²**

* Learn **belief** ( z^i ) that encodes global state information.
* Learn **filters** ( w^i ) that preserve only
  policy-relevant & reconstructible features.
* Couple representation learning directly with **value learning** â†’
  better cooperation + better exploration.

---

# ğŸ“ **Presenter Notesï¼ˆæ¼”è®²ç¨¿ï¼Œçº¦ 1 åˆ†é’Ÿï¼‰**


---

# ğŸ¯ **å»ºè®®é…å›¾ï¼ˆå¯é€‰ï¼‰**

* å·¦ä¾§ç”»ä¸€ä¸ª agent çœ‹åˆ°å±€éƒ¨åŒºåŸŸ â†’ è¾“å‡º zá¶¦ï¼ˆstate beliefï¼‰
* å³ä¾§ç”»å¤šä¸ª agentï¼ŒAM filter é€‰æ‹©éƒ¨åˆ† featuresï¼Œä¼ ç»™ encoder
* æ•´ä½“å½¢æˆâ€œè§‚æµ‹ â†’ z â†’ ç­–ç•¥ & explorationâ€çš„æµç¨‹ç®­å¤´

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ç”¨ image_gen ç”Ÿæˆé€‚ç”¨äº PPT çš„å›¾ç¤ºã€‚

---

å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬ç»§ç»­è¿›å…¥ **PPT ç¬¬ 3 é¡µï¼šæ–¹æ³•æ¡†æ¶å›¾ï¼ˆFigure 1ï¼‰æ•´ä½“ç»“æ„è§£æ**ã€‚
