è®ºæ–‡ï¼š Enhancing Cooperative Multi-Agent Reinforement Learning with State modeling and Adversarial Exploration
[[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration]]


å…ˆæŠŠç›®å‰**å·²ç»çŸ¥é“çš„äº‹å®**æ‹ä¸€éï¼Œç„¶åæˆ‘ç›´æ¥ä»ä½ è´´å‡ºçš„è¿™ä¸¤æ®µä»£ç é‡Œâ€œæ‰¾çªç ´å£â€ï¼Œç»™ä½ ä¸€ä¸ªæ¯”è¾ƒç¡¬çš„ debug æ–¹æ¡ˆã€‚

---

## 0. ç›®å‰å®éªŒç°è±¡ï¼ˆæˆ‘ä»¬å·²ç»é”å®šçš„ä¸œè¥¿ï¼‰

1. **ç¯å¢ƒä¾§ï¼š**

   * 4 Ã— Randomã€4 Ã— NS çš„å®éªŒé‡Œï¼Œ4 ä¸ª agent çš„å¹³å‡å›æŠ¥éƒ½å·®ä¸å¤šï¼Œæ²¡æœ‰ç¨³å®šçš„
     $R_1 > R_2 > R_3 > R_4$ è¿™ç§åºå·åç½®ã€‚
   * è¯´æ˜ï¼š**ç¯å¢ƒæœ¬èº« + VecEnv + logging è¿™æ¡é“¾è·¯åŸºæœ¬æ˜¯å¯¹ç§°çš„**ã€‚

2. **PPO ä¾§ï¼š**

   * â€œ4 ä¸ª PPO è‡ªåšå¼ˆâ€æ—¶ï¼Œå¹³å‡ reward æœ€åéƒ½èƒ½åˆ° 2.8 å·¦å³ï¼Œä½†ï¼š

     * player_1 çš„å››ä¸ª loss æŒ‡æ ‡ï¼ˆentropy, pg, total, valueï¼‰éƒ½æ¯”è¾ƒæ­£å¸¸ã€æ”¶æ•›å¹³æ»‘ï¼›
     * player_2/3/4 çš„ reward è™½ç„¶ä¹Ÿæ”¶æ•›ï¼Œä½†

       * entropy ä¸€ç›´åœ¨ 0.5â€“0.7 é™„è¿‘ä¹±è·³ç”šè‡³ä¸Šå‡ï¼›
       * value_loss é¢‘ç¹åœ¨å‡ ç™¾ç”šè‡³ä¸Šåƒã€‚
   * â€œ3NS + 1PPOï¼Œå¹¶è½®æµæŠŠ PPO ç»‘æˆ player_1/2/3/4â€ï¼š

     * 4 ç»„çš„ **reward éƒ½èƒ½æ”¶æ•›åˆ° 2.9 å·¦å³**ï¼›
     * ä½† **åªæœ‰å½“ PPO æ˜¯ player_1 æ—¶ loss æ­£å¸¸ï¼Œå…¶ä½™ä¸‰ä¸ª player_k çš„ value_loss éƒ½æå¤§ä¸”éœ‡è¡ã€‚**

=> è¿™ä¸€ç»„å¯¹æ¯”å·²ç»éå¸¸æ˜ç¡®åœ°å‘Šè¯‰æˆ‘ä»¬ï¼š

> **ç¯å¢ƒå‡ ä¹è‚¯å®šæ²¡é—®é¢˜ï¼ŒçœŸæ­£æœ‰é—®é¢˜çš„æ˜¯â€œPPO è®­ç»ƒç®¡çº¿é‡Œå’Œ agent ç´¢å¼•ç›¸å…³çš„ä¸€æ®µä»£ç â€ã€‚**
> æ›´å…·ä½“ä¸€ç‚¹ï¼šå½“ `my_id â‰  1` æ—¶ï¼ŒPPO çš„ value head / advantage è®¡ç®—æ–¹å¼å’Œ `my_id = 1` ä¸ä¸€æ ·ï¼ˆæˆ–è€…ç”¨äº†ä¸å±äºè‡ªå·±çš„è½¨è¿¹ï¼‰ã€‚

---

## 1. ä»ç¯å¢ƒä»£ç å‡ºå‘ï¼šç¡®è®¤â€œç¯å¢ƒæ²¡æœ‰ç»“æ„æ€§åç½®â€

ä½ æŠŠ `StagHuntEnv` è´´å‡ºæ¥å…¶å®æ˜¯å¾ˆå¥½çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ check é‡Œé¢æ‰€æœ‰å’Œ `id` / `player_i` ç›¸å…³çš„é€»è¾‘ï¼Œçœ‹æœ‰æ²¡æœ‰ç»“æ„æ€§ä¸å¯¹ç§°ã€‚

### 1.1 è§‚æµ‹é‡æ’ `__obs_state__`

```python
def __obs_state__(self, id):
    mystate = self.state.copy()
    mystate[:, :, :self.player_num - id] = self.state[:, :, id:self.player_num]
    mystate[:, :, self.player_num - id:self.player_num] = self.state[:, :, :id]
    return mystate
```

å¯¹ 4 ä¸ªç©å®¶ï¼š

* id=0 â†’ [0,1,2,3]
* id=1 â†’ [1,2,3,0]
* id=2 â†’ [2,3,0,1]
* id=3 â†’ [3,0,1,2]

å³ï¼š**æ¯ä¸ª agent çœ‹ä¸–ç•Œæ—¶ï¼Œè‡ªå·±çš„ channel å§‹ç»ˆåœ¨ index 0ï¼Œå…¶ä»–äººè¢«å¾ªç¯å¹³ç§»ã€‚**
æ‰€ä»¥ä»è§‚æµ‹ç»“æ„ä¸Šçœ‹ï¼Œ4 ä¸ªä½ç½®æ˜¯å®Œå…¨å¯¹ç§°çš„ã€‚

### 1.2 å¥–åŠ±ä¸ç»ˆæ­¢

æ‰€æœ‰ reward / done æ“ä½œéƒ½åœ¨ `for id in avail_players_id` çš„å¾ªç¯é‡Œï¼Œå®Œå…¨æŒ‰ index èµ°ï¼Œæ²¡æœ‰ä»»ä½• â€œå¦‚æœ id==0 å°±æ€æ ·â€ çš„åˆ†æ”¯ï¼›
`player_strength` é»˜è®¤ `[1,1,1,1]` ä¹Ÿæ²¡æœ‰ç ´åå¯¹ç§°æ€§ã€‚
`prosocial` ä¸º False æ—¶é‚£æ®µå¥‡æ€ªçš„

```python
for i in ['player_1', 'player_2', 'player_4']:
```

ä¸ä¼šè¢«æ‰§è¡Œï¼Œæ‰€ä»¥ç°åœ¨ä¹Ÿä¸å½±å“ã€‚

ç»¼åˆæ¥çœ‹ï¼š**ä½ çš„ç¯å¢ƒåœ¨ 4NS å’Œ random å®éªŒä¸­è¡¨ç°å¯¹ç§°ï¼Œæœ¬èº«æ²¡å‘ç°ç»“æ„æ€§çš„ agent_idx_biasã€‚**
æ‰€ä»¥å†ç»§ç»­å¾€ç¯å¢ƒé‡ŒæŠ  bug çš„æ”¶ç›Šä¼šå¾ˆå°ã€‚

---

## 2. çœŸæ­£å€¼å¾—æ€€ç–‘çš„é“¾è·¯ï¼šPPO è®­ç»ƒè¿™æ¡

ç°åœ¨é‡ç‚¹å°±åªå‰©ä¸€æ¡è·¯ï¼š**`HOPPlusPolicy.train()` é‡Œé¢ç”¨äº†â€œé”™è¯¯çš„ agent è½¨è¿¹ / é”™è¯¯çš„ç´¢å¼•â€ï¼Œè€Œè¿™è·Ÿ `player_1` / `player_2/3/4` æ°å¥½å¼ºç›¸å…³ã€‚**

ä½ è´´å‡ºçš„ä¸»è®­ç»ƒè„šæœ¬é‡Œæœ‰å‡ ä¸ªå…³é”®ç‚¹ï¼š

### 2.1 è½¨è¿¹çš„ç»“æ„

ä½ åœ¨ rollout ç»“æŸåæ„é€ çš„æ˜¯ï¼š

```python
Transition = namedtuple("Transition", [
    "obs_dict", "next_obs_dict", "reward", "done",
    "time_step", "env_id", "action", "log_prob", "value",
])

agent_trajectories = {aid: [] for aid in agent_ids}

# æ¯ä¸ª step é‡Œ
agent_trajectories[aid].append(
    Transition(
        obs_dict=obs_dict_raw,
        next_obs_dict=next_obs_raw,
        reward=torch.tensor(...),
        done=done_batch[aid][i],
        time_step=t,
        env_id=i,
        action=action_batch[i][aid],
        log_prob=item["logprob"],
        value=item["value"],
    )
)
```

è¿™é‡Œæœ¬èº«æ²¡ä»€ä¹ˆé—®é¢˜ï¼Œå¹¶ä¸” 3NS+1PPO çš„å››ç»„å®éªŒå·²ç»é—´æ¥è¯æ˜ï¼š**é‡‡æ ·é€»è¾‘å¯¹ 4 ä¸ªä½ç½®æ˜¯ä¸€æ ·çš„**ï¼Œä¸ç„¶ reward æœ¬èº«å°±ä¸ä¼šéƒ½æ”¶æ•›åˆ° 2.9ã€‚

### 2.2 è®­ç»ƒè°ƒç”¨æ–¹å¼

åœ¨ â€œå• PPO + 3NSâ€ çš„è„šæœ¬é‡Œä½ æ˜¯è¿™æ ·è°ƒç”¨è®­ç»ƒçš„ï¼š

```python
for aid in agent_ids:
    if aid != ppo_aid:
        continue
    policy_dict[aid].train(
        agent_trajectories=agent_trajectories,
        agent_ids=agent_ids,
    )
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

* **ä¸ç®¡ PPO æ˜¯ player_1 / 2 / 3 / 4ï¼Œ`train()` æ‹¿åˆ°çš„ `agent_trajectories` å­—å…¸ç»“æ„éƒ½æ˜¯ä¸€æ ·çš„ï¼š**

  * `agent_trajectories["player_1"]`ï¼šNS æˆ– PPO çš„è½¨è¿¹ï¼›
  * `agent_trajectories["player_2"]`ï¼šNS æˆ– PPO çš„è½¨è¿¹ï¼›
  * ... ä¾æ­¤ç±»æ¨ã€‚
* å”¯ä¸€å˜åŒ–çš„æ˜¯ï¼š**å“ªä¸€ä¸ª key å¯¹åº”çš„è½¨è¿¹æ˜¯â€œæˆ‘è‡ªå·±ï¼ˆPPOï¼‰â€çš„è½¨è¿¹ï¼Œè¿™ä¾èµ– `HOPPlusPolicy.train()` é‡Œé¢çš„ç´¢å¼•æ–¹å¼ã€‚**

> æ¢å¥è¯è¯´ï¼š
> ğŸ‘‰ **å¦‚æœ `HOPPlusPolicy` å†…éƒ¨å§‹ç»ˆé»˜è®¤ç”¨ `"player_1"` é‚£ä¸€æ¡è½¨è¿¹æ¥ç®— lossï¼Œé‚£ä¹ˆï¼š**
>
> * å½“ PPO ç»‘å®šåœ¨ player_1 ä¸Šæ—¶ï¼Œå®ƒåˆšå¥½è®­ç»ƒçš„æ˜¯è‡ªå·± â†’ loss æ­£å¸¸ï¼›
> * å½“ PPO ç»‘å®šåœ¨ player_2/3/4 ä¸Šæ—¶ï¼Œå®ƒä»ç„¶åœ¨ç”¨ `agent_trajectories["player_1"]` çš„ reward / done / value æ¥ç®— lossï¼Œå†æ‹¿è¿™ä¸ª loss å›å¤´æ›´æ–°â€œè‡ªå·±ï¼ˆplayer_kï¼‰â€è¿™å¥—å‚æ•° â†’
>   reward è¿˜èƒ½é  self-play æ…¢æ…¢æ··åˆ° 2.9ï¼Œä½† **value head å½»åº•å¤±é…ï¼Œæ‰€ä»¥ value_loss/entropy å…¨ä¹±äº†ã€‚**

è¿™ä¸ªâ€œå‡è®¾â€å…¶å®å·²ç»è¢«ä½ é‚£ 4 ç»„å• PPO å®éªŒ**å¼ºçƒˆæ”¯æŒ**äº†ï¼Œè€Œä¸”å®ƒæ˜¯**å”¯ä¸€åŒæ—¶è§£é‡Šï¼š**

* ç¯å¢ƒå¯¹ç§°ï¼›
* é‡‡æ ·é€»è¾‘å¯¹ç§°ï¼›
* reward éƒ½èƒ½æ”¶æ•›ï¼›
* ä½† â€œPPO åªåœ¨ç»‘ player_1 æ—¶ loss æ­£å¸¸â€ è¿™ä»¶äº‹ã€‚

æ¥ä¸‹æ¥å°±ä¸æ˜¯â€œççŒœâ€ï¼Œè€Œæ˜¯ **è®¾è®¡æœ€å°çš„ä»£ç å®éªŒï¼Œç›´æ¥è¯ä¼ª / è¯å®è¿™ä¸ªå‡è®¾ã€‚**

---

## 3. å…·ä½“çš„ä»£ç çº§ debug æ­¥éª¤ï¼ˆå»ºè®®ä¸€æ¬¡æ€§åšå®Œï¼‰

> ç›®æ ‡ï¼šç¡®å®š `HOPPlusPolicy` åœ¨ `train()` é‡Œåˆ°åº•ç”¨äº†å“ªä¸€æ¡è½¨è¿¹ã€‚

### 3.1 åœ¨ `HOPPlusPolicy.__init__` é‡ŒåŠ ä¸Šä¸€ä¸ªæ˜ç¡®çš„ self id

åœ¨ `policy_PPO.py` é‡Œï¼ˆHOPPlusPolicy å®šä¹‰å¤„ï¼‰ï¼Œå…ˆæ‰¾åˆ°æ„é€ å‡½æ•°ï¼Œè¡¥ä¸¤è¡Œï¼š

```python
class HOPPlusPolicy:
    def __init__(self, config, env_creator, model_core, device="cuda"):
        self.config = config
        self.device = device
        ...
        # >>> åŠ ä¸Šè¿™ä¸¤è¡Œï¼Œæ˜ç¡®è®°å½•â€œæˆ‘æ˜¯è°â€
        # æ¯”å¦‚ make_hop_config é‡Œæœ¬æ¥å°±æœ‰ "agent_id" / "agent_name"
        self.my_aid = config.get("agent_id", None)
        self.my_idx = config.get("agent_idx", None)
        print("[HOPPlusPolicy init] my_aid =", self.my_aid, ", my_idx =", self.my_idx)
```

ç„¶åé‡æ–°è·‘ä¸€æ¬¡ 3NS+1PPOï¼ˆä»»æ„ä¸€ä¸ª ppo_player_idï¼‰ï¼Œç¡®è®¤ï¼š

* `player_1` å¯¹åº”çš„ policy å®ä¾‹æ‰“å°å‡ºæ¥çš„ `my_aid` / `my_idx` æ˜¯ä»€ä¹ˆï¼›
* å½“ä½ æŠŠ `ppo-player-id` æ”¹æˆ 2/3/4 æ—¶ï¼Œè¿™ä¸ª `my_aid` / `my_idx` æœ‰æ²¡æœ‰å˜åŒ–ã€‚

**å¦‚æœå®ƒå§‹ç»ˆæ˜¯ `"player_1"` æˆ– `0`ï¼Œé‚£å°±ç›´æ¥æŠ“åˆ°ç¬¬ä¸€ä¸ªå¤§ bug äº†ã€‚**

---

### 3.2 åœ¨ `HOPPlusPolicy.train()` çš„å¼€å¤´ï¼Œæ‰“å°å®ƒå®é™…ç”¨çš„è½¨è¿¹ key

åœ¨ `HOPPlusPolicy.train()` ä¸€å¼€å¤´åŠ å‡ è¡Œâ€œæš´åŠ›â€çš„è°ƒè¯•ä»£ç ï¼š

```python
def train(self, agent_trajectories, agent_ids, *args, **kwargs):
    # ---- Debug: å®é™…æ‹¿å“ªæ¡è½¨è¿¹åœ¨è®­ç»ƒï¼Ÿ----
    # 1) æ‰“å°è¿™æ¬¡å‡½æ•°è°ƒç”¨æ—¶ä¼ è¿›æ¥çš„ key
    if hasattr(self, "_debug_printed") is False:
        print("[HOPPlusPolicy.train] my_aid =", self.my_aid,
              ", available keys:", list(agent_trajectories.keys()))
        self._debug_printed = True

    # 2) ä½ ç°åœ¨åœ¨ä»£ç ä¸­æ˜¯æ€ä¹ˆå–å‡ºâ€œè‡ªå·±çš„è½¨è¿¹â€çš„ï¼Ÿ
    #    å‡è®¾åŸæ¥ç±»ä¼¼è¿™æ ·ï¼š
    # my_trajs = agent_trajectories[self.my_aid]
    # æˆ–è€… agent_trajectories["player_1"] ä¹‹ç±»
    # è¯·åœ¨è¿™ä¸€è¡Œä¹‹å‰åŠ ä¸Šä¸€ä¸ª assertï¼š

    assert self.my_aid in agent_trajectories, \
        f"my_aid={self.my_aid} not in agent_trajectories keys {list(agent_trajectories.keys())}"
```

ç„¶åè·‘ä¸¤ç»„å®éªŒåšå¯¹æ¯”ï¼š

* åªè·‘ 100â€“200 ä¸ª update å°±å¤Ÿäº†ï¼š

  * ä¸€æ¬¡ `--ppo-player-id=1`
  * ä¸€æ¬¡ `--ppo-player-id=2`

æŸ¥çœ‹ç»ˆç«¯è¾“å‡ºï¼š

1. `my_aid` æ˜¯å¦éšç€ `ppo-player-id` æ”¹å˜ï¼Ÿ
2. `my_trajs = agent_trajectories[???]` è¿™é‡Œç”¨çš„ key æ˜¯ä»€ä¹ˆï¼Ÿ

å¦‚æœä½ çœ‹åˆ°ç±»ä¼¼ï¼š

```text
[HOPPlusPolicy init] my_aid = player_1, my_idx = 0
[HOPPlusPolicy.train] my_aid = player_1, available keys: ['player_1', 'player_2', 'player_3', 'player_4']
```

è€Œä¸ç®¡ `ppo-player-id` è®¾æˆå‡ éƒ½æ˜¯è¿™æ ·ï¼Œé‚£åŸºæœ¬å¯ä»¥ 99% ç¡®è®¤ï¼š

> **ä½ çš„ PPO å§‹ç»ˆåœ¨ç”¨ `player_1` çš„è½¨è¿¹åšè®­ç»ƒã€‚**

è¿™å°±æ˜¯ agent_idx_bias çš„æ ¹ã€‚

---

### 3.3 ä¸€æ—¦ç¡®è®¤é—®é¢˜ï¼Œå°±ç»™å‡º minimal ä¿®å¤æ–¹æ¡ˆ

å‡è®¾ä¸Šé¢çš„çŒœæµ‹è¢«éªŒè¯äº†ï¼Œé‚£ä¹ˆä¿®å¤æ–¹å¼å…¶å®éå¸¸ç®€å•ï¼š

1. **ä¿è¯ `make_hop_config` ä¸ºæ¯ä¸ª agent ç”Ÿæˆçš„ `cfg_by_agent[aid]` é‡Œé¢ï¼Œæœ‰æ­£ç¡®çš„ `agent_id` æˆ– `my_aid` å­—æ®µã€‚**

   æ¯”å¦‚åœ¨ `make_hop_config` é‡Œï¼š

   ```python
   for i, aid in enumerate(agent_ids):
       cfg = {...}
       cfg["agent_id"] = aid        # "player_1" / "player_2" ...
       cfg["agent_idx"] = i         # 0 / 1 / 2 / 3
       cfg_by_agent[aid] = cfg
   ```

2. **åœ¨ `HOPPlusPolicy.__init__` ç”¨è¿™ä¸ªå­—æ®µï¼Œè€Œä¸è¦å†™æ­»ã€‚**

   ```python
   self.my_aid = config["agent_id"]
   self.my_idx = config["agent_idx"]
   ```

3. **åœ¨ `train()` é‡Œï¼Œå–è½¨è¿¹æ—¶ç»Ÿä¸€ç”¨ï¼š**

   ```python
   my_trajs = agent_trajectories[self.my_aid]
   ```

   è€Œä¸æ˜¯ï¼š

   ```python
   my_trajs = agent_trajectories["player_1"]  # âŒ
   my_trajs = agent_trajectories[agent_ids[0]]  # âŒ
   ```

åšå®Œè¿™ä¸€æ­¥ä¹‹åï¼Œå†è·‘ä¸€é 3NS+1PPO çš„ 4 ç»„å®éªŒï¼Œä½ æœŸæœ›çœ‹åˆ°çš„æ˜¯ï¼š

* å››ä¸ª run çš„ reward æ›²çº¿ä¾ç„¶éƒ½èƒ½æ”¶æ•›åˆ° 2.8â€“2.9ï¼›
* æ›´å…³é”®ï¼š**å››ä¸ª run çš„ entropy / value_loss å½¢çŠ¶éƒ½ç±»ä¼¼ player_1 é‚£ç»„ï¼Œä¸å†å‡ºç°â€œplayer_1 æ­£å¸¸ï¼Œå…¶ä»–çˆ†ç‚¸â€çš„ç°è±¡ã€‚**

---

## 4. è¿™ä¸€æ­¥ä¹‹åçš„ä¸‹ä¸€æ­¥

å¦‚æœä½ æŒ‰ç…§ä¸Šé¢çš„ debug æ­¥éª¤ï¼Œå‘ç°ï¼š

* `my_aid` æ¯æ¬¡éƒ½è®¾ç½®æ­£ç¡®ï¼›
* `train()` ä¹Ÿç¡®å®åœ¨ç”¨å¯¹åº” key çš„è½¨è¿¹ï¼›

ä½†ä»ç„¶å­˜åœ¨æ˜æ˜¾çš„ agent_idx_biasï¼Œé‚£ä¹ˆä¸‹ä¸€æ­¥å†å¾€å†…æ”¶ç¼©ï¼š

1. åœ¨ `train()` ä¸­ï¼Œæ‰“å° / ç”¨ TensorBoard è®°å½• **value target ä¸ predicted value çš„åˆ†å¸ƒ**ï¼ŒæŒ‰ agent åˆ†å¼€ï¼š

   * `mean(return_t)`, `std(return_t)`
   * `mean(value_t)`, `std(value_t)`
   * `corr(return_t, value_t)`

2. çœ‹çœ‹ï¼š

   * player_1 çš„ value æ˜¯å¦åœ¨é€æ¸å¯¹é½ targetsï¼›
   * player_2/3/4 æ˜¯å¦ value ä¸€ç›´åœ¨æŸä¸ªâ€œå¥‡æ€ªçš„åŒºé—´â€é‡Œæ‰“è½¬ï¼ˆæ¯”å¦‚å…¨æ˜¯ 0 æˆ–å…¨æ˜¯æŸä¸ªå¸¸æ•°ï¼‰ã€‚

è¿™ä¸€æ­¥å°±è¦çœŸæ­£æ‰“å¼€ `HOPPlusPolicy` é‡Œçš„ GAE / value_loss è®¡ç®—éƒ¨åˆ†äº†ï¼ˆå¯èƒ½éœ€è¦ä½ æŠŠé‚£æ®µä»£ç å†è´´ä¸€æ¬¡ï¼‰ã€‚

---

## å°ç»“ï¼ˆç»™ä½ è®°ç¬”è®°ç”¨çš„â€œé˜¶æ®µç»“è®ºâ€ï¼‰

ä½ ç°åœ¨å¯ä»¥åœ¨ä½ çš„ debug ç¬”è®°é‡Œè¿™æ ·å†™ä¸€å°æ®µï¼š

---

ä½ å…ˆæŒ‰ç…§ 3.1 + 3.2 é‚£ä¸¤æ­¥æŠŠ `HOPPlusPolicy` é‡Œçš„çœŸå®è¡Œä¸ºâ€œæ›å…‰â€å‡ºæ¥ï¼Œè·‘ä¸¤ç»„ï¼ˆppo_player=1 å’Œ 2ï¼‰çœ‹çœ‹ logï¼›è·‘å®ŒæŠŠç›¸å…³ print / å…³é”®ç‰‡æ®µè´´ç»™æˆ‘ï¼Œæˆ‘ä»¬å†ç»§ç»­å¾€é‡Œæ”¶ç¼©ã€‚
