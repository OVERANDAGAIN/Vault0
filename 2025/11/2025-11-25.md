ä¸‹é¢æ ¹æ®ä½ æä¾›çš„ **3.2 Adversarial Count-based Intrinsic Exploration** å…¨æ®µå†…å®¹ï¼Œæˆ‘ä¸ºä½ åˆ¶ä½œ **ä¸€é¡µ PPTï¼ˆç»“æ„ç´§å‡‘ä½†å†…å®¹å®Œæ•´ï¼‰**ï¼Œå¹¶é™„ä¸Š Presenter Notesï¼ˆå£è¯­ç¨¿ï¼‰ã€‚

è¿™éƒ¨åˆ†ç¯‡å¹…è™½ç„¶çŸ­ï¼Œä½†æ€æƒ³éå¸¸å·§å¦™ï¼Œæˆ‘ä¼šå¸®ä½ æŠŠæ ¸å¿ƒç²¾åæç‚¼å‡ºæ¥ï¼Œä¿è¯å¬ä¼—ä¸€é¡µå°±ç†è§£ã€‚

---

# ğŸŸ¦ **ğŸ“PPT ç¬¬ X é¡µï¼šAdversarial Count-based Intrinsic Explorationï¼ˆæ¢ç´¢ç­–ç•¥ï¼‰**

---

## ğŸ¨ **ï¼ˆå¯ç›´æ¥ç²˜è´´ï¼‰PPT é¡µé¢å†…å®¹**

### **Adversarial Count-based Intrinsic Exploration**

**Goal:**
Encourage agents to explore **novel & high-value** parts of the belief space,
even under **sparse rewards**.

---

### **Key Idea: Explore Novel Belief States**

* Belief embedding $z^i$ captures rich global-state information
* Exploring **novel (z^i)** â‡’ discovering **novel observations** $o^i$
* Intrinsic reward:
  $$\hat{r}^i = \frac{1}{p \cdot n(\mathrm{SH}(z^i))}$$
  where SH is SimHash and $n(\cdot)$ counts visits

---

### **Adversarial Exploration Effect**

* Novel $o^i$ â†’ novel $z^i$
* These novel $z^i$ are **unseen targets** for other agentsâ€™ ED reconstruction
* This **increases** their reconstruction loss â†’ forces them to
  learn **better state beliefs**
  â†’ emergent **cooperative adversarial exploration**

---

### **Modified Reward**

$$\tilde{r}_t^i = r_t^i + \beta \hat{r}_t^i$$

Î² controls intrinsic vs. extrinsic reward weight.

---

### **Stability Consideration**

* SimHash domain depends on encoder parameters
* To keep intrinsic reward stable, periodically **hard update**
  encoder/decoder parameters (Ï‰áµ¢, Ï†áµ¢)

---

## ğŸ“ **Presenter Notesï¼ˆ1 åˆ†é’Ÿå£è¯­ç¨¿ï¼‰**

è¿™é¡µè®² SMPEÂ² çš„æ¢ç´¢æ–¹æ³•ã€‚
æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**æ¢ç´¢ novel çš„ latent belief çŠ¶æ€ zâ±ï¼Œè€Œä¸æ˜¯æ¢ç´¢ raw observations**ã€‚

å› ä¸º zâ± å·²ç»åŒ…å«äº†è¿‡æ»¤åçš„ã€å¯¹åˆä½œç­–ç•¥è‡³å…³é‡è¦çš„ä¿¡æ¯ï¼Œæ‰€ä»¥æ¢ç´¢ zâ± èƒ½æ›´æœ‰æ•ˆåœ°å‘ç°ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å…³é”®çŠ¶æ€ã€‚

å…·ä½“åšæ³•æ˜¯ä½¿ç”¨ SimHashï¼Œå¯¹æ¯ä¸ª zâ± åš hashingï¼Œç»Ÿè®¡è®¿é—®æ¬¡æ•°ï¼Œç„¶åå®šä¹‰ count-based intrinsic rewardï¼š

$$\hat{r}^i = \frac{1}{n(\mathrm{SH}(z^i))}$$

ä¹Ÿå°±æ˜¯ zâ± è¶Šå°‘å‡ºç°ï¼Œagent å¾—åˆ°çš„ intrinsic reward è¶Šé«˜ã€‚

è¿™åˆ›é€ äº†ä¸€ä¸ªå¾ˆç‰¹åˆ«çš„ç°è±¡ï¼š

**å½“ agent i æ‰¾åˆ° novel zâ± æ—¶ï¼Œå®ƒä¼šç”Ÿæˆ novel reconstruction targetsï¼Œ
ä»è€Œ adversarial åœ°æé«˜å…¶ä»– agents çš„ reconstruction lossï¼Œ
è¿«ä½¿å®ƒä»¬å­¦ä¹ æ›´å¼ºçš„ belief è¡¨ç¤ºã€‚**

æœ€ç»ˆå…¨ä½“ agent éƒ½ä¼šè¢«æ¨åŠ¨åˆ°ï¼š

* better global-state understanding
* better coordination
* better policies under sparse reward

è®ºæ–‡è¿˜æŒ‡å‡ºï¼Œä¸ºäº†è®© intrinsic reward ç¨³å®šï¼Œencoder/decoder ä¼šå‘¨æœŸæ€§ hard updateã€‚

---

# ğŸ¯ ä¸ºä»€ä¹ˆè¿™ä¸€é¡µè¶³å¤Ÿï¼Ÿï¼ˆæ–¹æ³•å®Œæ•´è¡¨è¾¾ï¼‰

è¿™ä¸€é¡µå·²ç»æ¸…æ¥šè§£é‡Šäº†ï¼š

* **æ¢ç´¢çš„è¾“å…¥æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆzâ± è€Œé oâ±ï¼‰**
* **ä¸ºä»€ä¹ˆèƒ½æ¢ç´¢ï¼Ÿï¼ˆSimHash count-based intrinsic rewardï¼‰**
* **ä¸ºä»€ä¹ˆå« adversarialï¼Ÿï¼ˆæé«˜åˆ«äºº reconstruction éš¾åº¦ï¼‰**
* **ä¸ºä»€ä¹ˆèƒ½æé«˜åˆä½œï¼Ÿï¼ˆbelief å¼ºåŒ– + shared improvementï¼‰**
* **å¦‚ä½•ä¿æŒç¨³å®šï¼Ÿï¼ˆç¡¬æ›´æ–° EDï¼‰**

è€Œä¸”ä¸€é¡µå°±è®²é€äº†ï¼ŒèŠ‚å¥ç¨³ã€é€»è¾‘æ¸…æ™°ã€‚

---

å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­åšä¸‹ä¸€èŠ‚ï¼š

ğŸ‘‰ **PPTï¼šç®—æ³•çš„å®Œæ•´ lossï¼ˆæ•´åˆ actor + critic + encodingï¼‰é¡µé¢**

æˆ–è€…ä½ æƒ³å¼€å§‹åšå®éªŒéƒ¨åˆ†ä¹Ÿå¯ä»¥ã€‚
