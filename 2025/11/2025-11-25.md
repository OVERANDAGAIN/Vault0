ä¸‹é¢æˆ‘æ ¹æ®ä½ æä¾›çš„ **Preliminaries â†’ 2.2 State Modelling Optimization Framework** éƒ¨åˆ†ï¼Œä¸ºä½ åˆ¶ä½œ **PPT ç¬¬ 2 é¡µï¼ˆæ–¹æ³•ç›´è§‰ + SM æ¡†æ¶çš„ç†è®ºèƒŒæ™¯æ”¯æ’‘ï¼‰** çš„å†…å®¹ã€‚

è¿™é¡µçš„ä½œç”¨å¾ˆå…³é”®ï¼š

* è®©å¬ä¼—ç†è§£ï¼š**æœ¬æ–‡çš„åˆ›æ–°ä¸æ˜¯å‡­ç©ºå‡ºç°çš„ï¼Œè€Œæ˜¯ä»â€œçŠ¶æ€å»ºæ¨¡ï¼ˆState Modellingï¼‰â€ç†è®ºæ¡†æ¶å‡ºå‘çš„**
* å°†â€œlatent beliefâ€ ä¸ â€œpolicy optimizationâ€ è”ç³»èµ·æ¥
* å¼•å‡ºåé¢çš„ SMPEÂ² æ–¹æ³•ï¼ˆELBO + AM Filters + Dual Criticsï¼‰

æˆ‘ä¼šåšæˆ **æ›´é¥±æ»¡ã€å¯è®² 1.5 åˆ†é’Ÿçš„ç‰ˆæœ¬**ã€‚

---

# ğŸŸ¦ **ğŸ“PPT ç¬¬ 2 é¡µï¼šState Modelling Optimization Frameworkï¼ˆç†è®ºåŸºç¡€ï¼‰**

---

# ğŸ¨ **PPT å†…å®¹ï¼ˆå¯ç›´æ¥ç²˜è´´ï¼‰**

### **2. State Modelling Optimization Framework**

**Goal:**
Agents should infer **latent beliefs** ( z^i ) about the unobserved global state
**using only their own local observations**.

---

### **Why State Modelling?**

Real-world multi-agent states contain:

* **Redundant features**
  â†’ irrelevant for agent (i)â€™s reward
* **Non-inferable features**
  â†’ cannot be predicted from (o^i) due to partial observability

Therefore, agent (i) only needs a belief representation ( z^i ) that:

1. Contains **informative** features for policy optimization
2. Contains **only** features that are **predictable** from its own observation

---

### **Formal Definition**

Belief distribution:
[
z^i \sim p_{\omega_i}(z \mid o^i)
]

State Modelling objective:
[
V^*_{SM}

\max_{\omega, \psi}
;
V_{SM}(\omega,\psi)


\max_{\omega, \psi}
;
\mathbb{E}*{z \sim p*{\omega}}
\Big[
\mathbb{E}*{a \sim \pi*\psi,, s' \sim P,, o \sim F}
\sum_{t=0}^{\infty}
\gamma^t r_t
\Big]
\tag{1}
]

---

### **Key Theoretical Result**

**Proposition 2.1:**
[
V^*_{SM} = V^*
]

* The optimal value under the state-modelling formulation
  equals the optimal value of the original Dec-POMDP.
* Meaning:
  â†’ Learning latent beliefs (z^i) **does not restrict** the optimal policy space
  â†’ Belief-based policies can be **as good as** full-state optimal policies

---

### **Takeaway**

* SM forms the **theoretical foundation** of SMPEÂ²
* It establishes the connection between:
  **representation learning â†” policy optimization**
* Sets the stage for learning:

  * latent beliefs ( z^i )
  * filtered observations (AM)
  * value-aligned representations

---

# ğŸ“ **Presenter Notesï¼ˆå£è¯­ç¨¿ï¼Œçº¦ 1.5 åˆ†é’Ÿï¼‰**

è¿™ä¸€é¡µæˆ‘ä»¬ä»‹ç»è®ºæ–‡æ–¹æ³•èƒŒåçš„ç†è®ºåŸºç¡€ï¼šState Modelling Optimization Frameworkã€‚

ä½œè€…é¦–å…ˆè®¨è®ºäº†çœŸå®å¤šæ™ºèƒ½ä½“ä»»åŠ¡çš„ä¸€ä¸ªé‡è¦äº‹å®ï¼š
å…¨å±€ state ä¸­å¾€å¾€åŒ…å«å¤§é‡å†—ä½™ã€å™ªå£°ã€å¯¹æŸä¸ª agent å®Œå…¨æ— å…³çš„ç‰¹å¾ï¼›
æ›´å…³é”®çš„æ˜¯ï¼Œ**å¾ˆå¤šç‰¹å¾æ ¹æœ¬ä¸å¯èƒ½ä»è‡ªèº«è§‚å¯Ÿæ¨æ–­å‡ºæ¥**ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­ã€‚

å› æ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªå…³é”®å‡è®¾ï¼š
Agent ä¸éœ€è¦æ¢å¤åŸå§‹ stateï¼›
å®ƒåªéœ€è¦å­¦åˆ°ä¸€ä¸ª **å¯¹ç­–ç•¥æœ€é‡è¦ã€ä¸”å¯ä»è‡ªèº«è§‚å¯Ÿä¸­æ¨æ–­çš„ latent belief è¡¨ç¤º**ã€‚

åŸºäºè¿™ä¸ªå‡è®¾ï¼Œä½œè€…å®šä¹‰äº†ä¸€ä¸ªçŠ¶æ€å»ºæ¨¡ç›®æ ‡ï¼š
æ¯ä¸ª agent å­¦ä¸€ä¸ªæ¡ä»¶åˆ†å¸ƒ
[
p_{\omega_i}(z^i | o^i)
]
è®© latent belief (z^i) èƒ½å¸®åŠ© agent å®ç°æœ€é«˜çš„æœŸæœ›å›æŠ¥ã€‚

è¿™åœ¨å…¬å¼ï¼ˆ1ï¼‰ä¸­å¾—åˆ°äº†å½¢å¼åŒ–ï¼š
æ•´ä½“ä¼˜åŒ–ç›®æ ‡æ˜¯å¯¹ encoder å‚æ•° (\omega) å’Œ policy å‚æ•° (\psi) çš„è”åˆæœ€å¤§åŒ–ã€‚

éå¸¸å…³é”®çš„æ˜¯ Proposition 2.1ï¼š
å®ƒè¡¨æ˜ï¼Œåœ¨æœ€ä¼˜æƒ…å†µä¸‹ï¼Œè¿™ä¸ª state modelling ç›®æ ‡å’ŒåŸå§‹ Dec-POMDP çš„æœ€ä¼˜å€¼æ˜¯ç­‰ä»·çš„ã€‚
æ¢å¥è¯è¯´ï¼Œ**å¼•å…¥ latent belief ä¸ä¼šé™ä½å¯è¾¾çš„æœ€ä¼˜æ€§èƒ½**ï¼Œ
åè€Œæä¾›äº†ä¸€ç§æœºåˆ¶ï¼Œè®©æˆ‘ä»¬ç†è§£ agent å¦‚ä½•åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­å½¢æˆ meaningful beliefsã€‚

è¿™ä¸º SMPEÂ² æä¾›äº†æ˜ç¡®çš„ç†è®ºæ”¯æ’‘ï¼Œä¹Ÿä¸ºåç»­çš„

* encoderâ€“decoder
* AM filters
* dual-critic coupling
  å¥ å®šäº†åŸºç¡€ã€‚

---

# ğŸ¯ ä¸ºä»€ä¹ˆè¿™ä¸€é¡µå¾ˆå…³é”®ï¼Ÿ

å› ä¸ºï¼š

* å®ƒè®©å¬ä¼—çŸ¥é“ä½ ä¸æ˜¯åœ¨å±•ç¤ºä¸€ä¸ªâ€œå·¥ç¨‹å‹â€ trickï¼Œè€Œæ˜¯æœ‰ç†è®ºæ”¯æ’‘
* å®ƒè‡ªç„¶å¼•å‡ºã€Œä¸ºä»€ä¹ˆ SMPEÂ² è¦å­¦ä¹  latent beliefã€
* å®ƒè§£é‡Šäº†â€œä¸ºä»€ä¹ˆä¸æ˜¯é‡å»ºå…¨å±€çŠ¶æ€ï¼Œè€Œæ˜¯é€‰æ‹©æ€§å»ºæ¨¡â€
* å®ƒå¸®åŠ©å¬ä¼—ç†è§£ AM Filter çš„åŠ¨æœºï¼ˆinformative + inferable featuresï¼‰

---

å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬ç»§ç»­ï¼š

ğŸ‘‰ **PPT ç¬¬ 3 é¡µï¼šSMPEÂ² æ•´ä½“ç»“æ„å›¾ï¼ˆFigure 1ï¼‰å®Œæ•´è§£æç‰ˆ**
