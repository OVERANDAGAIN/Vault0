ä¸‹é¢æ ¹æ®ä½ ç»™å‡ºçš„ **3.1 Self-supervised State Modelling** å…¨æ®µå†…å®¹ï¼Œæˆ‘ä¸ºä½ è®¾è®¡ **ä¸€å¥—å®Œæ•´çš„ PPT æ–¹æ³•éƒ¨åˆ†ï¼ˆå»ºè®® 3ï½4 é¡µï¼‰**ï¼Œæ¯ä¸€é¡µéƒ½åšåˆ°ï¼š

* **ç»“æ„æ¸…æ™°**
* **ä¿¡æ¯ä¸æ¼**
* **å…¬å¼ä¸å †ç Œä½†é€»è¾‘è¿è´¯**
* **å¬ä¼—ä¸€é¡µå°±èƒ½ç†è§£ä¸€ä¸ªå…³é”®æ€æƒ³**
* **Presenter Notesï¼ˆå£è¯­ç¨¿ï¼‰ 1â€“1.5 åˆ†é’Ÿ**

è¿™ä¸€éƒ¨åˆ†å†…å®¹æ˜¯æ•´ç¯‡è®ºæ–‡æœ€æ ¸å¿ƒçš„æŠ€æœ¯éƒ¨åˆ†ï¼Œæˆ‘ä¼šå¸®ä½ ç»„ç»‡æˆ**æœ€å¥½è®²ã€ä¹Ÿæœ€å¥½å¬**çš„ç»“æ„ã€‚

---

# ğŸŸ¦ **PPT ç¬¬ 4 é¡µï¼šSelf-supervised State Modelling â€” Why & What?ï¼ˆç›´è§‰ + åŠ¨æœºï¼‰**

---

## ğŸ¨ **PPT å†…å®¹ï¼ˆç®€æ´ç‰ˆï¼‰**

### **Why do we need self-supervised state modelling?**

* Each agent only sees **local** observations (o^i)
* But cooperation requires **reasoning about others** and latent global state
* Directly using full state or joint observations â†’
  **redundant & non-informative**, harms policy learning

---

### **Goal**

Learn a latent belief embedding:

$$z^i \sim q_{\omega_i}(z \mid o^i)$$

that contains only:

1. **Informative** features for agent $i$'s future rewards
2. **Inferable** features given partial observations

---

### **How to learn zâ±?**

* Use **variational encoderâ€“decoder (ED)**
  $$q_{\omega_i},; p_{\phi_i}$$
* Reconstruct **informative parts** of othersâ€™ observations
* Filter non-informative features via AM filters $w^i$

---

### **Key idea**

> **Self-supervised reconstruction + feature filtering
> makes zâ± a meaningful latent belief under partial observability.**

---

## ğŸ“ **Presenter Notes**

è¿™ä¸€é¡µè®²çš„æ˜¯ SMPEÂ² ä¸ºä»€ä¹ˆè¦åš self-supervised state modellingã€‚

ç”±äº agent åªèƒ½çœ‹åˆ°å±€éƒ¨è§‚å¯Ÿï¼Œå®ƒæ— æ³•è·å¾—å…¨å±€ stateã€‚è€Œ naive åœ°æŠŠ full state æˆ– o^{-i} å…¨å¡ç»™ encoder ä¼šå¯¼è‡´ä¸¥é‡å†—ä½™ï¼šæœ‰äº›ä¿¡æ¯æ—¢ä¸ reward æ— å…³ï¼Œä¹Ÿæ— æ³•è¢«ä»å±€éƒ¨è§‚å¯Ÿæ¨æ–­å‡ºæ¥ï¼›åŠ å…¥è¿™äº›ä¿¡æ¯åè€Œè®© policy å­¦å¾—æ›´ç³Ÿã€‚

å› æ­¤ï¼Œè®ºæ–‡æå‡ºä¸€ä¸ªå…³é”®æƒ³æ³•ï¼š

**ç”¨ reconstructive self-supervisionï¼Œè®© agent å­¦åˆ°â€œå®ƒèƒ½æ¨æ–­åˆ°çš„ã€å¯¹ç­–ç•¥æœ‰ç”¨çš„â€ latent beliefã€‚**

æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå˜åˆ† encoderâ€“decoder æ¥å­¦ä¹ éšè—å˜é‡ zâ±ï¼›ä½†æ˜¯ reconstruct çš„å¯¹è±¡ä¸æ˜¯å®Œæ•´çš„ o^{-i}ï¼Œè€Œæ˜¯**è¢« AM filter è¿‡æ»¤åçš„æœ‰ç”¨éƒ¨åˆ†**ã€‚

è¿™æ ·ï¼Œagent å­¦åˆ°çš„ zâ± å°±æ˜¯ä¸€ä¸ªå¯æ¨æ–­ã€å¯ç”¨äºåˆä½œçš„ latent belief representationã€‚

---

---

# ğŸŸ¦ **PPT ç¬¬ 5 é¡µï¼šAM Filters â€” Select Only Useful & Inferable Features**

---

## ğŸ¨ **PPT å†…å®¹**

### **Problem: Not all joint observation features are useful**

Redundant features:

* Do not help maximize future rewards
* Cannot be inferred from (o^i) under partial observability
  â†’ would pollute zâ± if reconstructed

---

### **Solution: Agent Modelling (AM) Filters**

Each agent learns a soft mask:

$$w^i = \sigma(\phi_i^w(o^{-i}))$$

Where:

* $\phi_i^w$ is an MLP
* sigmoid ensures $w^i \in [0,1]$
* one weight for each agent/feature dimension
* interpreted as **importance of each feature**

---

### **Filtered state**

$$\hat{s}=
o^i \oplus (w^i \cdot o^{-i})$$

Only useful + inferable parts of othersâ€™ observations are preserved.

---

### **Benefits**

* Removes redundant information
* Ensures ED reconstructs only meaningful targets
* Ensures zâ± supports policy optimization (value-aligned)

---

## ğŸ“ **Presenter Notes**

è¿™é¡µè®² AM Filtersã€‚

ä¸ºä»€ä¹ˆéœ€è¦ wâ±ï¼Ÿå› ä¸º o^{-i} ä¸­å¤§é‡ç‰¹å¾å¯¹ agent i æ¯«æ— ç”¨å¤„ï¼š

* å¯èƒ½ä¸ future rewards ä¸ç›¸å…³
* æˆ–è€… agent æ ¹æœ¬æ¨æ–­ä¸åˆ°è¿™äº›ç‰¹å¾

å¦‚æœéƒ½é€è¿› encoderï¼Œä¼šè®© zâ± å˜å¾— noisyã€ä¸ç¨³å®šã€‚

è§£å†³æ–¹æ³•æ˜¯ï¼š
æ¯ä¸ª agent å­¦ä¸€ä¸ª soft gate wâ±ï¼Œç”¨äº**è¿‡æ»¤æ‰æ— å…³æˆ–ä¸å¯æ¨æ–­çš„ç»´åº¦**ã€‚wâ± è¶Šæ¥è¿‘ 1ï¼Œå°±è¶Šä»£è¡¨è¯¥ç‰¹å¾é‡è¦ä¸”å¯ä»¥æ¨æ–­ã€‚

è¿‡æ»¤åçš„çŠ¶æ€ï¼š

$$\hat{s} = o^i \oplus (w^i \cdot o^{-i})$$

è¿™ä¸€æ­¥æ˜¯æ•´ä¸ª SMPEÂ² çš„å…³é”®åˆ›æ–°ï¼Œä½¿å¾— zâ± å­¦åˆ°çš„æ˜¯â€œå¯¹åˆä½œçœŸæ­£æœ‰æ„ä¹‰çš„ latent beliefâ€ã€‚

---

---

# ğŸŸ¦ **PPT ç¬¬ 6 é¡µï¼šVariational Inference + ELBO Objective**

---

## ğŸ¨ **PPT å†…å®¹**

### **Variational ED model**

Encoder:
$$z^i \sim q_{\omega_i}(z \mid o^i)$$

Decoder reconstructs filtered observations:

$$w^i \cdot o^{-i}$$

---

### **Combined optimization objective**

$$\max_{\omega_i, \phi_i, \psi_i}
\quad
V_{SM}(\omega, \psi)
+
\lambda \cdot \mathrm{ELBO}(\omega_i, \phi_i)$$

---

### **ELBO term**

$$\mathrm{ELBO}


\mathbb{E}*{z_i \sim q*{\omega_i}}
[\log p_{\phi_i}(w^i \cdot o^{-i} \mid z_i)]

\mathrm{KL}(q_{\omega_i}(z_i \mid o^i) \Vert p(z_i))$$

ä½œç”¨ï¼š

* Reconstruction term â†’ å¼ºè¿« zâ± æ•è·â€œå¯æ¨æ–­çš„â€æœ‰ç”¨ç‰¹å¾
* KL term â†’ è®© zâ± ç¨³å®šã€å…·å¤‡æ³›åŒ–èƒ½åŠ›
* Î» æ§åˆ¶è¡¨å¾ä¸ç­–ç•¥ç›®æ ‡çš„æƒé‡

---

### **æ ¸å¿ƒ insight**

> ELBO è®© zâ± æœ‰èƒ½åŠ›æ¢å¤å…¨å±€çŠ¶æ€ä¸­â€œå¯¹åˆä½œæœ‰ç”¨ + agent èƒ½æ¨æ–­â€çš„éƒ¨åˆ†ã€‚

---

## ğŸ“ **Presenter Notes**

è¿™ä¸€é¡µè®² variational inference å’Œ ELBOã€‚

ED æ¨¡å‹çš„ reconstruction ç›®æ ‡æ˜¯ï¼š
$$w^i \cdot o^{-i}$$
è€Œä¸æ˜¯å®Œæ•´çš„ o^{-i}ã€‚

å› æ­¤ encoder å¿…é¡»å­¦ä¹ å¯¹è¿™äº›å¯æ¨æ–­ç»´åº¦æ•æ„Ÿçš„ latent beliefã€‚

ELBO çš„ç¬¬ä¸€é¡¹é‡å»ºæœ‰ç”¨ç‰¹å¾ï¼Œç¬¬äºŒé¡¹ KL è®©è¡¨ç¤ºåˆ†å¸ƒå¹³æ»‘ï¼Œé¿å… overfittingã€‚

è¿™ä¸ªç»“æ„ç¡®ä¿ï¼š

zâ± ä¸æ˜¯ generic embedding
è€Œæ˜¯**ä¸ºåˆä½œç­–ç•¥ä¼˜åŒ–è€Œå­¦çš„ latent belief**ã€‚

è¿™æ˜¯æ¯” LIAM/SIDE æ›´å¼ºã€æ›´ç¨³å®šã€æ›´ä»»åŠ¡ç›¸å…³çš„è¡¨ç¤ºå­¦ä¹ æ–¹å¼ã€‚

---

---

# ğŸŸ¦ **PPT ç¬¬ 7 é¡µï¼šFull State-Modelling Loss**

---

## ğŸ¨ **PPT å†…å®¹**

### **Reconstruction loss**

$$L_{rec}
| w_e^i \cdot o^{-i} - w^i \cdot \hat{o}^{-i} |^2$$

Target filter (w_e^i) stabilizes training.

---

### **Filter regularization**

$$L_{norm} = -| w^i |^2$$

Prevents wâ± from collapsing to all zeros.

---

### **KL loss**

$$L_{KL}
\mathrm{KL}(q_{\omega_i}(z_i | o_i) \Vert \mathcal{N}(0,I))$$

---

### **Filtered critic loss**

$$L_{critic}^w

(r_t + V_{k'}(\hat{s}_{t+1})
- V_k(\hat{s}_t))^2$$

Couples zâ± and wâ± to **value function learning**.

---

### **Final encoding loss**

$$L_{encodings}


L_{critic}^w
+
\lambda_{rec} L_{rec}
+
\lambda_{norm} L_{norm}
+
\lambda_{KL} L_{KL}$$

---

## ğŸ“ **Presenter Notes**

è¿™ä¸€é¡µæ€»ç»“æ‰€æœ‰ lossã€‚

å…¶ä¸­æœ€å…³é”®çš„æ˜¯ï¼š

* **Filtered critic loss**
  è¿™æ˜¯ SMPEÂ² å’Œä»¥å¾€æ–¹æ³•çš„æœ¬è´¨åŒºåˆ«ã€‚

ä»¥å¾€çš„ AM æˆ– VAE-based è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œè¡¨ç¤ºæ˜¯ä¸ç­–ç•¥è§£è€¦çš„ï¼Œå› æ­¤å®¹æ˜“å‡ºç° distribution mismatchã€‚

ä½†æ˜¯åœ¨ SMPEÂ² ä¸­ï¼Œfiltered critic ä½¿ç”¨ï¼š

$$\hat{s} = o^i \oplus (w^i \cdot o^{-i})$$

è¿™æ · zâ± å’Œ wâ± çš„å­¦ä¹ æ˜¯ç›´æ¥å¯¹ TD error è´Ÿè´£çš„ï¼Œå› æ­¤ latent belief ç›´æ¥ä¸ä»·å€¼å‡½æ•°å¯¹é½ï¼Œä½¿å¾—è¡¨ç¤ºçœŸæ­£å¸®åŠ©ç­–ç•¥å­¦ä¹ ã€‚

---

# ğŸ”š æ€»ç»“

ä»¥ä¸Š 4 é¡µå†…å®¹å°† **3.1 Self-supervised State Modelling** è®²å¾—éå¸¸æ¸…æ™°ã€ç»“æ„åˆ†æ˜ã€æ˜“äºå¬ä¼—ç†è§£ï¼Œè€Œä¸”é€»è¾‘é€’è¿›ï¼š

1. **ä¸ºä»€ä¹ˆè¦å»ºæ¨¡ stateï¼Ÿ**
2. **ä¸ºä»€ä¹ˆéœ€è¦è¿‡æ»¤å™¨ï¼Ÿ**
3. **å¦‚ä½•ç”¨ VAE ç»“æ„å­¦ä¹  latent beliefï¼Ÿ**
4. **å¦‚ä½•é€šè¿‡ critic æŠŠ z ä¸ç­–ç•¥è€¦åˆï¼Ÿ**

è¿™æ˜¯ä½ ç»„ä¼š/è¯»è®ºæ–‡åˆ†äº«ä¼šæƒ³è¦çš„æœ€ç†æƒ³ç»“æ„ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­ä¸‹ä¸€èŠ‚ï¼š

ğŸ‘‰ **PPTï¼š3.2 Adversarial Explorationï¼ˆSMPEÂ² çš„æ¢ç´¢ç­–ç•¥ï¼‰**
