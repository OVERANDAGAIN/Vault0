下面逐条、清晰、专业地回答你的 **第 8～12 个问题**。这些都是你在组会上可能会被问到的点，我会给你最准确的解释。

---

# ✔ **第八个问题：什么是 “25–75% confidence intervals”？**

原文：

> We report **25–75% confidence intervals** (as in Zheng et al., 2021).

📌 **首先，它不是传统意义上的“95% 置信区间”**。
它指的是：

### 👉 **展示 6 次实验（6 seeds）的 reward 分布的 25% 分位数到 75% 分位数**

也就是所谓的 **IQR（Interquartile Range，四分位区间）**。

### 为什么不用 std，而用 IQR？

在 RL 中，每个 seed 差异可能非常大：

* early collapse
* exploration stagnation
* reward variance 过高

方差（std）会很不稳定，而 **IQR 更稳健**，不会受极端值影响。

所以曲线上的“阴影区域”代表：

> 6 个 seeds 的 reward，其 25% 和 75% 分位数之间的区域。

不是传统统计意义上的置信区间，而是一种 **robust variability measure**。

---

# ✔ **第九个问题：ED（Encoder-Decoder）和 AE（Autoencoder）的关系**

ED **是 AE 的一种形式**，但是论文中的 ED 与传统 AE 有几个明显区别。

### 🟢 共同点（AE 家族）

两者都属于 **autoencoder 类架构**：

* Encoder：$o^i \rightarrow z^i$
* Decoder：$z^i \rightarrow \hat{o}^{-i}$

目标都是通过重构学习一个压缩表示。

---

### 🔴 不同点（本文 ED = 更复杂的 AE）

| 项              | 传统 AE         | 本文 ED                          |
| -------------- | ------------- | ------------------------------ |
| 重构目标           | 重构自己的输入 $o^i$ | 重构 **其他 agent 的观测** $o^{-i}$   |
| Encoder 输入     | $o^i$         | $o^i$                          |
| Decoder 输出     | $\hat o^i$    | $\hat o^{-i}$（concat + filter） |
| 是否 variational | 可选            | ✔必须（VAE）                       |
| 是否与 RL 联动      | 否             | ✔critic 引导表示学习                 |
| 是否带 filter     | 否             | ✔AM Filter                     |

---

### 📌 因此：

> 本文的 ED = **VAE + multi-target reconstruction + RL-driven gradient**
> 本质是一个带隐变量的、面向 MARL 状态建模的 Autoencoder。

---

# ✔ **第十个问题：什么是 “single learnable controller”？**

文中多次说到：

> a single learnable controller setting

它指的是 MARL 中一种 **参数共享（parameter sharing）** 或 **集中式控制器（single policy controlling all agents）** 的设定。

### 📌 意思是：

**所有 agent 共享同一个 policy 网络（controller）**，输入不同，输出各自动作。

这在 MPE/LBF/RWARE 常用：

* 环境是对称的
* 每个 agent 的角色一样
* 参数共享提高效率 & 稳定性

### 你也可以这样理解：

> “我们不是每个agent训练一个独立policy，而是所有 agent 共享一个 policy 参数，像一个 controller 控制多个机器人。”

---

# ✔ **第十一个问题：MPE 环境为何是离散动作？不是连续空间吗？**

MPE（Multi-Agent Particle Environment）确实是一个 **连续 2D 空间**。

但原始 MPE 允许 **两种 action space 设计**：

| Action type | 描述                                        |
| ----------- | ----------------------------------------- |
| continuous  | 2D 力/速度控制，例如 (-1~1)^2                     |
| discrete    | 5 actions：stay / up / down / left / right |

论文选用的是：

### 👉 **离散动作版本（标准 MARL benchmark 版本）**

原因：

1. 离散动作更适合 A2C / COMA / QMIX 等 baseline
2. 避免连续控制带来的 policy variance
3. MPE 的协作性质不依赖精细力学，离散动作已足够体现 coordination

---

### 📌 **MPE 并不是网格，只是 actions 是离散方向移动，本质仍是连续 2D 坐标环境**。

例如：执行 “move right” = 以固定速度向右移动 Δx。

这个动作空间设计是文献中最常用版本。

---

# ✔ **第十二个问题：为何 observation 是一维张量？如何理解其结构？**

文中的 observation 是一维向量，例如：

* MPE: `[24]`
* LBF: `[11]`
* RWARE: `[71]`

### 📌 这些都是 **展平后的 feature vector**（flattened feature vector）

环境内部通常有结构化信息：

#### MPE

* 自己的位置信息 (2)
* 目标的位置（2）
* 每个 agent 的位置（2 × #agents）
* 是否碰撞等 flags
* 障碍物信息
  因此最终 flatten 成一个 24 维的 vector。

---

### LBF（Level-Based Foraging）

观测一般包括：

* 自己位置 (x,y)
* grid 范围内物品 / agent的 relative position
* 当前携带物品量
  最后一般 flatten 成 `[11]` 的结构。

---

### RWARE（Robot Warehouse）

包含：

* local egocentric map encoding（多通道 one-hot grid）
* 自己的当前位置、携物状态
* 任务类型
* distance-to-target
  总维度非常高，例如 `[71]`

---

### 🟢 关键点

这些 observation 本质上是：

> 把环境中局部观测、地图 patch、物体属性、队友属性全部 encode 成一个 flattened 1D 向量。

**不是表格、不是按维度分块，是“序列式的 feature vector”。**

---

# 🎁 全部问题总结（你可以在组会上按此回答）

| 问题                | 简明回答                                   |
| ----------------- | -------------------------------------- |
| 25–75% CI         | 展示 6 seeds reward 的 IQR，比 std 更稳健      |
| ED vs AE          | 本文 ED 是有 KL + RL 引导的 VAE 型 Autoencoder |
| single controller | 所有 agent 共享一个 policy（参数共享）             |
| MPE 为什么离散动作       | 用离散控制方向移动，但空间仍是连续的                     |
| observation 1D    | flatten 的特征向量，合并所有局部信息                 |

---

如果你愿意，我能继续给你：

👉 **这篇论文所有方法模块（z, w, ED, dual critic）的流程图**
👉 **10 分钟报告 PPT 框架（含需展示的关键图）**
👉 **问答环节可能会被问到的问题及标准回答**

你更需要哪一项？
