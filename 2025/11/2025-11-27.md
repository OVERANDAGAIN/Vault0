
## 一句结论先说清楚

* 你 TB 里看到的 `policy/value_loss` **只来自 PPO critic 的 MSE（带 value clipping）**，**没有直接把 WM/MOA 的 loss 加进去**。
* 但是：在 `_maybe_train_policy()` 里，critic 的 target `returns` 是在 **“真轨迹 + 想象轨迹（WM 生成）”** 混在一起的 batch 上计算的，而**想象轨迹的 reward 完全来自 world_model 的预测**。
  当 WM 还不准、或者 MOA 预测的对手动作很离谱时，这些 “model-based rewards” 会严重偏离真实尺度，直接把 TD 误差放大，所以你看到的 **value_loss 会比纯 PPO baseline 高很多**。
* 所以，问题不是「用到了对手轨迹就炸了」，而是「**把不成熟的 world model 输出的 reward 当成 critic target 的一部分**」，再叠加长 horizon 的 MC-return，导致 value_loss 偏大。

下面我分几步把路径摊开。

---

## 1. value_loss 在哪里算的？确实只是 critic loss

在 `_maybe_train_policy()` 里，核心几步是：

```python
# 取出我的 agent 的数据
obs_hwC = batch["obs"][:, :, self.my_id_zero] ...
avail   = batch["avail_actions"][:, :, self.my_id_zero] ...
acts    = batch["actions"][:, :, self.my_id_zero, 0] ...
rewards = batch["reward"][:, :, 0, 0]  # 注意这里，用的是 reward 的第 0 维
old_logp = batch["old_logp"][:, :, self.my_id_zero, 0]
old_val  = batch["old_value"][:, :, self.my_id_zero, 0]
...
# 计算 returns（纯 MC）
G = torch.zeros(B, device=device)
for t in reversed(range(T)):
    G = rewards[:, t] + gamma * G * (1.0 - term[:, t])
    returns[:, t] = G
...
adv = returns - old_val
...
logits, _ = self.model.forward(...)
values = self.model.value_function().reshape(BT)
...
# clipped value loss
v_clipped = old_val_f + (values - old_val_f).clamp(-clip_eps, clip_eps)
v1 = (values - ret_flat) ** 2
v2 = (v_clipped - ret_flat) ** 2
v_loss = torch.max(v1, v2)
v_loss = (v_loss * valid_w).sum() / valid_w.sum()
...
total_loss = pg_loss + vf_coef * v_loss - ent_coef * ent
```

可以看到：

* `v_loss` 只用到了 `values`（当前 critic）、`ret_flat`（MC return）和 `old_val_f`（旧 critic 输出），标准 clipped value loss；
* `wm` 的 `loss`, `moa` 的 `ce_loss` 都是单独 `_stat("wm/...")`, `_stat("moa/...")` 记日志，并没有加进 `total_loss`。

所以 **数值上 value_loss 大，是真正 TD 误差大，而不是因为你把 WM/MOA 的 loss 叠在一起了**。

---

## 2. 真正“污染” value target 的，是想象数据

关键是：这个 PPO 是在「真 + 想象」混合 buffer 上训的。

### 2.1 真实轨迹怎么入 buffer？

`train()` 一开始：

```python
env_episodes = self._assemble_episodes_from_transitions(agent_trajectories, agent_ids)
for env_ep in env_episodes:
    self._push_episode_to_buffers(env_ep, agent_ids, is_imagined=False)
```

`_push_episode_to_buffers(..., is_imagined=False)` 里：

* 从 `per_agent[aid][t]` 里读真实 `obs_dict["observation"]`, `action_mask`, `tr.action`, `tr.reward`, `tr.log_prob`, `tr.value`；
* 构建 `EpisodeBatch epb` 后：

```python
if is_imagined:
    target_buffer = self.imagined_ppo_buffer
else:
    target_buffer = self.real_ppo_buffer

target_buffer.insert_episode_batch(epb)
```

所以 **real buffer 里的 reward 就是环境给的真 reward（0、1、10 那些）**。

### 2.2 想象轨迹是怎么来的？

`train()` 之后紧接着：

```python
self._maybe_train_wm()
self._maybe_train_moa()
self._generate_model_rollouts()  # ← 生成 imagined data
self._maybe_train_policy()
```

在 `_generate_model_rollouts()` 中：

1. 从 `wm_buffer`（只存真实 $(s,a^{self},a^{opp},s',r)$）里采样 B 个真实 state：

   ```python
   real_transitions = self.wm_buffer.sample(B)
   obs_t_hwc = real_transitions[0].to(self.device)  # [B,H,W,C]
   ```

2. 用 OMG + 当前策略 + MOA 在 world_model 里 rollout H 步：

   ```python
   # 先用 OMG 算 subgoal_my（利用 pre_actions、obs_last_chw, obs_t_chw）
   subgoal_b_n_d = self.model_goal.cal_subgoal_batched(...)
   subgoal_my = subgoal_b_n_d[:, self.my_id_zero, :]  # [B,D]

   # 再用当前 policy 输出自 agent 动作
   logits, _ = self.model.forward({"obs_flat": obs_in}, None, [1], t_b, subgoal_my)
   dist = Categorical(softmax(logits))
   self_action = dist.sample()
   self_value = self.model.value_function().reshape(B)

   # 再用 MOA 输出对手动作 opp_actions_b_k
   priors = self.model_list[i].forward(...)
   opp_act = Categorical(logits=priors).sample()
   ...
   # 最后用 world_model 预测 obs_{t+1}, reward_pred
   obs_t1_pred_chw, reward_pred = self.world_model(obs_t_chw, self_action.unsqueeze(-1), opp_actions_b_k)
   ```

3. 把这些 **完全来自模型预测** 的 $(s_t, a_t, r^{model}*t, s*{t+1}^{model}, \text{done}^{model})$ 存成 `generated_transitions`，再组装成假 episode，最后调用：

```python
self._push_episode_to_buffers(env_ep, agent_ids=..., is_imagined=True)
```

进入 `imagined_ppo_buffer`。

> 对 imagined episode 来说，`reward` 是 `reward_pred[j].item()`，而不是环境真 reward。

### 2.3 PPO 训练时混合 real + imagined

在 `_maybe_train_policy()`：

```python
bs = self.config['omg']['args']['batch_size']
real_bs = int(bs * self.ppo_real_ratio)
imagined_bs = bs - real_bs

if real_bs > 0: real_batch = self.real_ppo_buffer.sample(real_bs)
if imagined_bs > 0: imagined_batch = self.imagined_ppo_buffer.sample(imagined_bs)

# 如果都有，就把两者沿 batch 维度 cat 出一个大的 EpisodeBatch
batch = EpisodeBatch(..., bs, ..., data=new_data, device=self.device)
...
max_ep_t = batch.max_t_filled()
batch = self._refine_batch(batch[:, :max_ep_t])
...
rewards = batch["reward"][:, :, 0, 0].to(device).float()
...
# 用 rewards 纯 MC 算 returns，再做 value_loss
```

也就是说：

* **value_loss 的 target `returns` 是在 real + imagined 混合 batch 上计算的**；
* imagined 部分的 reward 和状态完全由「WM + MOA + 当前 policy + OMG」生成；
* 如果 WM/MOA 还没学好，或者 subgoal 模型不稳定，那这些 imagined rewards 很容易：

  * 量纲跑飞（例如直接输出 5、10 之外的值，甚至负的大数/小数）；
  * 或者分布和真实环境 reward 完全不一致。

在这种情况下，你要让 critic 同时去拟合：

* 一部分是真实环境的 returns（还算健康）；
* 一部分是基于“坏世界模型 + 坏对手模型”的 returns（非常 noisy / biased）。

**TD 误差自然会大很多，value_loss 就被拉高了。**

---

## 3. MOA / World Model “用对手轨迹”的方式，如何影响 value_loss？

从代码可以看出：

* **MOA 和 WM 的 loss 自己一套：**

  ```python
  self._stat("wm/loss", loss.item())
  self._stat("moa/.../ce_loss", loss.item())
  ```

  并不写入 `policy/value_loss`。

* 它们影响 value_loss 的唯一途径是**改变 imagined data 的质量**：

  1. **MOA** 决定在想象 rollout 中对手动作 $a^{-i}$ 如何采样：

     ```python
     priors = self.model_list[i].forward(state_opp_b, t_b.squeeze(-1), target_b)
     opp_dist = Categorical(logits=priors)
     opp_act = opp_dist.sample()
     ```

     如果 MOA 模型还没学会对手行为，给 world_model 喂进去的 $(s_t, a^{self}_t, a^{opp}_t)$ 就偏离真实分布。

  2. **world_model** 在这种输入下输出的 $(s_{t+1}^{model}, r_t^{model})$ 也就很不靠谱。

  3. 这些 “假 reward” 被当成和真实 reward 一样处理，参与 critic 的 target 构造。

这就是你直觉里说的那句：

> “会不会是因为里面使用的 moa，world_model 等需要使用对手的一些轨迹数据，导致 value loss 大？”

——**是的，但不是直接把 MOA/WM 的 loss 加进来，而是间接通过低质量的 imagined rewards 污染了 critic 的训练数据。**

---

## 4. 还有几个容易被忽略的细节

### 4.1 imagined 轨迹的 subgoal = 0

在 `_gather_subgoal_series()` 中：

```python
if isinstance(env_id, str) and env_id.startswith("imagined_"):
    # imagined data 没有 logs，直接返回全 0
    return torch.zeros(T, self.subgoal_dim, ...)
```

所以：

* 想象数据中，critic 的输入 subgoal 是全 0；
* 而真实 rollouts 里，subgoal 是 OMG 计算出来的向量。

也就是说，critic 在训练时一半时间看到 `(obs, pre_actions, subgoal=0)`，一半时间看到 `(obs, pre_actions, subgoal≈某种模式)`，但是在执行时 `get_action()` 总是用 **真实 subgoal**。
虽然这不一定直接把 value_loss 撑爆，但会让输入分布更不稳定，critic 拟合变难，对 loss 也有放大的作用。

### 4.2 MC-return + 长 horizon 的基础尺度

你的 return 构造是纯 MC（没 GAE）：

```python
G = rewards[:, t] + gamma * G * (1 - term[:, t])
```

在 StagHunt 这种：

* hare_reward=1, stag_reward=10，final_time=35；
* 如果一只 stag 在后面被捕获，前面一大串 reward=0，最后几步 reward=10；
* 初始 critic 输出通常是 ~0 附近。

那么早期训练阶段，单步 TD 目标 $(ret - v)^2$ 的尺度会在 $10^2=100$ 级别是很正常的（baseline PPO 如果只用 hare 或较小 reward 时，v_loss 可能明显更小）。
这本身就会让 value_loss 看起来“偏大”；再叠加上想象数据的噪声，就更夸张。

---

## 5. 建议的 debug / 消融实验（非常关键）

为了确认 “value_loss 大” 真的是 WM/MOA + imagined data 引入的，而不是别的 bug，我建议你直接做两三个极简消融：

### 实验 1：只用真实数据训 PPO

在运行脚本中直接改参数：

```bash
--ppo-real-ratio 1.0
```

同时暂时禁用想象 rollout：

```python
def _generate_model_rollouts(self):
    return  # 先完全跳过
```

或者简单点，在 `train()` 里把第 4 步注释掉：

```python
# self._generate_model_rollouts()
```

只保留：

```python
1. _push_episode_to_buffers(..., is_imagined=False)
2. _maybe_train_wm()     # 可以留着，不会影响 PPO
3. _maybe_train_moa()    # 同上
4. _maybe_train_policy() # 现在只会从 real_ppo_buffer 采样
```

**预期：**

* 这时的 value_loss 大致应该回到和你 PPO baseline 同一个量级（略大一些可以接受，因为输入多了 pre_actions 和 subgoal）。
* 如果确实显著下降，基本就坐实问题是 “imagined reward 污染”。

### 实验 2：保留 rollout，但不要让 imagined reward 参与 critic

比如简单粗暴一点：在 `_push_episode_to_buffers()` 中，当 `is_imagined=True` 时，强行把 reward 清零：

```python
if is_imagined:
    # imagined episode: 用 0 reward，只让它参与 policy gradient，而不过分拉 critic
    reward_pad.zero_()
```

或者在 `_maybe_train_policy()` 里区分 real/imagined batch，critic 只用 real 的：

* 用 real batch 算 `v_loss`；
* real + imagined 一起算 `pg_loss`。

这样可以验证：

* 是不是 “actor 还能从 imagined data 里学到点东西，但 critic 不能承受其噪声” 这一点。

### 实验 3：log 出 real / imagined 的 reward 分布

在 `_maybe_train_policy()` 里：

* 给 batch 增加一个标记（比如在 EpisodeBatch 的 episode_data 里加个 `is_imagined` 标志），然后分别统计：

  * `rewards[real]` 的均值/方差；
  * `rewards[imagined]` 的均值/方差；
  * 对应的 `returns` 和 `old_val` 的分布。

如果你看到 imagined 部分的 reward/returns 明显偏离 [0,10] 范围，比如有大量 >20、<-10 等，大概率就是 WM 还不稳时给 critic 喂了炸弹。

---

## 6. 回答你原本的问题

> “会不会是因为里面使用的 moa，world_model 等需要使用对手的一些轨迹数据（ppo只需要自己的）导致的？”

结合代码看，可以分两层回答：

1. **直接层面：**

   * `value_loss` 的数学定义里 **没有直接出现 WM/MOA 的 loss**；
   * 它只是 critic 对 MC-return 的 MSE，形式和你改写的 PPO baseline 很像。

2. **间接层面（更重要的真实原因）：**

   * 你现在的 PPO 是 **真 + 想象混合训练**；
   * 想象数据完全依赖 WM + MOA + OMG 得到 $(s^{model}, r^{model})$；
   * 对手轨迹只是用来训练 MOA，再被用来生成 imagined opponent actions，最后进入 world_model；
   * 当 WM/MOA 还不准时，这些 imagined rewards 会严重偏离真实分布，直接造成 critic target 的偏移和方差上升；
   * 所以 **确实是“multi-model + 对手建模 + 想象 rollouts” 这一套联合导致了 value_loss 偏大**，但机制是 “污染训练数据”，不是 “把一堆 loss 生硬加起来”。

---

如果你愿意下一步继续优化，我建议的顺序是：

1. 先做上面 **Experiment 1**，确认只用 real data 时 value_loss 是否回归正常；
2. 再考虑如何“温和”地引入 imagined data（比如先只给 actor 用，或者给 critic 用但降低权重 / 截断 reward ），逐步观察 value_loss 的变化，而不是一上来就 0.5 real + 0.5 imagined 全权重塞给 critic。

你如果跑完实验 1 的曲线，可以把 TB 截图（especially `policy/value_loss`, `wm/loss`, `moa/...`）扔给我，我们可以一起再看下一步要不要调 `ppo_real_ratio`、reward clipping 或者修改 MC-return/GAE 的构造。
