
## 一句结论先说清楚

* 你 TB 里看到的 `policy/value_loss` **只来自 PPO critic 的 MSE（带 value clipping）**，**没有直接把 WM/MOA 的 loss 加进去**。
* 但是：在 `_maybe_train_policy()` 里，critic 的 target `returns` 是在 **“真轨迹 + 想象轨迹（WM 生成）”** 混在一起的 batch 上计算的，而**想象轨迹的 reward 完全来自 world_model 的预测**。
  当 WM 还不准、或者 MOA 预测的对手动作很离谱时，这些 “model-based rewards” 会严重偏离真实尺度，直接把 TD 误差放大，所以你看到的 **value_loss 会比纯 PPO baseline 高很多**。
* 所以，问题不是「用到了对手轨迹就炸了」，而是「**把不成熟的 world model 输出的 reward 当成 critic target 的一部分**」，再叠加长 horizon 的 MC-return，导致 value_loss 偏大。


---

## 4. 还有几个容易被忽略的细节

### 4.1 imagined 轨迹的 subgoal = 0

在 `_gather_subgoal_series()` 中：

```python
if isinstance(env_id, str) and env_id.startswith("imagined_"):
    # imagined data 没有 logs，直接返回全 0
    return torch.zeros(T, self.subgoal_dim, ...)
```

所以：

* 想象数据中，critic 的输入 subgoal 是全 0；
* 而真实 rollouts 里，subgoal 是 OMG 计算出来的向量。

也就是说，critic 在训练时一半时间看到 `(obs, pre_actions, subgoal=0)`，一半时间看到 `(obs, pre_actions, subgoal≈某种模式)`，但是在执行时 `get_action()` 总是用 **真实 subgoal**。
虽然这不一定直接把 value_loss 撑爆，但会让输入分布更不稳定，critic 拟合变难，对 loss 也有放大的作用。


### 4.2 MC-return + 长 horizon 的基础尺度

你的 return 构造是纯 MC（没 GAE）：

```python
G = rewards[:, t] + gamma * G * (1 - term[:, t])
```

在 StagHunt 这种：

* hare_reward=1, stag_reward=10，final_time=35；
* 如果一只 stag 在后面被捕获，前面一大串 reward=0，最后几步 reward=10；
* 初始 critic 输出通常是 ~0 附近。

那么早期训练阶段，单步 TD 目标 $(ret - v)^2$ 的尺度会在 $10^2=100$ 级别是很正常的（baseline PPO 如果只用 hare 或较小 reward 时，v_loss 可能明显更小）。
这本身就会让 value_loss 看起来“偏大”；再叠加上想象数据的噪声，就更夸张。

---

## 5. 建议的 debug / 消融实验（非常关键）

为了确认 “value_loss 大” 真的是 WM/MOA + imagined data 引入的，而不是别的 bug，我建议你直接做两三个极简消融：

### 实验 1：只用真实数据训 PPO

在运行脚本中直接改参数：

```bash
--ppo-real-ratio 1.0
```

同时暂时禁用想象 rollout：

```python
def _generate_model_rollouts(self):
    return  # 先完全跳过
```


### 实验 2：保留 rollout，但不要让 imagined reward 参与 critic

比如简单粗暴一点：在 `_push_episode_to_buffers()` 中，当 `is_imagined=True` 时，强行把 reward 清零：

```python
if is_imagined:
    # imagined episode: 用 0 reward，只让它参与 policy gradient，而不过分拉 critic
    reward_pad.zero_()
```

或者在 `_maybe_train_policy()` 里区分 real/imagined batch，critic 只用 real 的：

* 用 real batch 算 `v_loss`；
* real + imagined 一起算 `pg_loss`。

这样可以验证：

* 是不是 “actor 还能从 imagined data 里学到点东西，但 critic 不能承受其噪声” 这一点。

### 实验 3：log 出 real / imagined 的 reward 分布

在 `_maybe_train_policy()` 里：

* 给 batch 增加一个标记（比如在 EpisodeBatch 的 episode_data 里加个 `is_imagined` 标志），然后分别统计：

  * `rewards[real]` 的均值/方差；
  * `rewards[imagined]` 的均值/方差；
  * 对应的 `returns` 和 `old_val` 的分布。

如果你看到 imagined 部分的 reward/returns 明显偏离 [0,10] 范围，比如有大量 >20、<-10 等，大概率就是 WM 还不稳时给 critic 喂了炸弹。

---
