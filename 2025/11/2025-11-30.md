---
创建时间: 2025-十一月-30日  星期日, 11:41:10 中午
---
>以下所有都是在MBRL代码框架下更改的

# HOP+-self-play

```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_4hop_selfplay__2__1764398229"
```

4个agent reward在1 左右（10h)

# PPO_train(update_from_transtions)
```python
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764501961"
```
![[Pasted image 20251130193217.png]]

# episode_buffer+PPO_train
![[Pasted image 20251130114410.png]]

# episode_buffer+ppo_train(debug)

## buffer=20000
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764474035"
```


![[Pasted image 20251130191852.png]]![[Pasted image 20251130192055.png]]
## buffer=5000
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764475827"
```

![[Pasted image 20251130192034.png]]
![[Pasted image 20251130192045.png]]



## 优化GAE计算
```python
# 在 with torch.no_grad() 里算两份 value：V_t 和 V_{t+1}

with torch.no_grad():
    BT = B * T
    input_dict = {"obs_flat": obs_in.reshape(BT, -1)}
    t_input = times.reshape(BT, 1)
    sub_input = subgoal.reshape(BT, self.subgoal_dim)

    logits, _ = self.model.forward(input_dict, None, [1], t_input, sub_input)
    values_t = self.model.value_function().reshape(B, T)  # V(s_t)

    # 利用一个小 trick：用 value_t 的“后一格”来近似 V(s_{t+1})
    values_tp1 = torch.zeros_like(values_t)
    values_tp1[:, :-1] = values_t[:, 1:]           # shift 一步
    # 对最后一个时间步，V_{T} 的下一个就设为 0（终止）
    values_tp1[:, -1] = 0.0

# GAE
advantages = torch.zeros_like(rewards)
returns = torch.zeros_like(rewards)

for b in range(B):
    gae = 0.0
    for t in reversed(range(T)):
        if filled[b, t] < 0.5:
            continue

        # 标准 delta 形式：r + γ V_{t+1} - V_t
        delta = (
            rewards[b, t]
            + gamma * values_tp1[b, t] * (1 - term[b, t])
            - values_t[b, t]
        )
        gae = delta + gamma * lam * (1 - term[b, t]) * gae

        advantages[b, t] = gae
        # 这里 baseline 用 old_val（和 PPO 的 value clipping 对齐）
        returns[b, t] = gae + old_val[b, t]

```

```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764502637"
```
![[Pasted image 20251130200839.png]]![[Pasted image 20251201011308.png]]



## HOP+ (no image buffer)
### 修改完bug
```python
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764529511"
```

![[Pasted image 20251201042254.png]]


### subgoal == 0 (get_action ; may_train_policy)

```python
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764530519"
```


![[Pasted image 20251201042531.png]]



### self-play

