行，那我就用“框框 + 箭头”的方式，把这块 **offline VAE + MOA 多智能体框架**画成简单线条版。

---

### 1. 整体：从多智能体环境到离线 AM 模块

```text
┌─────────────────────────── 多智能体环境 StagHuntEnv ────────────────────────────┐
│                                                                               │
│  每个时间步 t：                                                               │
│    所有 agent 交互 → 生成 (o_t, a_t, r_t, env_id)                            │
│                                                                               │
└───────────────────────────▲────────────────────────────────────────────────────┘
                            │  采样 & 收集轨迹
                            │  (只用 o_t, a_t, t, env_id 做 AM 训练)
                            │
                            ▼
                 ┌──────────────────────────────┐
                 │   Offline 数据文件 all_collect_samples.npy   │
                 │   [(state=o_t, action=a_t, time=t, env_id),…] │
                 └──────────────────────────────┘
```

---

### 2. OfflineVAETrainer 内部：VAE + MOA 联合训练

```text
┌──────────────────────────── OfflineVAETrainer.train() ───────────────────────────┐
│                                                                                │
│  从 .npy 读出： states = o_t, actions = a_t, times = t                          │
│           ↓                                                                    │
│    按 batch 采样 (batch_state, batch_action, batch_time)                       │
│           ↓                                                                    │
│  ┌───────────────────────────────────────────────────────────────────────────┐ │
│  │                     AMLearner.train(states, actions, times)              │ │
│  │                                                                           │ │
│  │  ① VAE：OMG_AM.vae                                                        │ │
│  │     obs = 从 state 中裁掉前 7 维 action_mask 后的 o_t                     │ │
│  │         ┌─────────────────────────────┐                                    │ │
│  │         │   VanillaVAE (CNN Encoder)  │                                    │ │
│  │         │   输入：obs(o_t)           │                                    │ │
│  │         │   输出：μ(o_t), logσ²(o_t) │                                    │ │
│  │         └──────────────┬──────────────┘                                    │ │
│  │                        │  重参数化                                       │ │
│  │                 z_t = μ + σ⊙ε                                             │ │
│  │                        ▼                                                 │ │
│  │                  (subgoal / mode embedding z_t)                           │ │
│  │                                                                           │ │
│  │  ② MOA：MOAModel                                                          │ │
│  │     输入：                                                                │ │
│  │       - 完整 state (含 action_mask + 网格 o_t)                            │ │
│  │       - time t                                                            │ │
│  │       - z_t (来自 VAE)                                                    │ │
│  │     经 CNN + FC → 得到：                                                  │ │
│  │         logits(a | o_t, t, z_t)  + action_mask                            │ │
│  │         ↓ Softmax                                                         │ │
│  │         π̂(a_t | o_t, t, z_t)                                             │ │
│  │     同时： V(o_t, t, z_t)                                                 │ │
│  │                                                                           │ │
│  │  ③ 损失：                                                                 │ │
│  │     policy_loss  = CrossEntropy(π̂, a_t)                                  │ │
│  │     kld_loss     = KL(q(z|o_t) || N(0,I))                                 │ │
│  │     total_loss   = policy_loss + omg_vae_alpha * kld_loss                 │ │
│  │                                                                           │ │
│  │  ④ 反向传播，更新：                                                       │ │
│  │     - VAE 参数 (表示学习)                                                 │ │
│  │     - MOA 参数 (行为克隆，对手建模)                                       │ │
│  └───────────────────────────────────────────────────────────────────────────┘ │
│                                                                                │
│  每个 epoch 结束：                                                            │
│    - 保存 VAE 权重 vae.th                                                     │
│    - 保存通用 MOA 权重 player__to_.pth                                       │
│    - evaluate_moa_and_log() 评估 top1/top3、混淆矩阵写入 TensorBoard         │
└────────────────────────────────────────────────────────────────────────────────┘
```

---

### 3. 多智能体视角的 MOA 复制阶段

```text
        离线训练完毕后（共享参数 MOA）:
                      ┌──────────────────────────┐
                      │  通用 MOA: player__to_.pth │
                      └───────────┬──────────────┘
                                  │  文件复制 n(n-1) 份
                                  ▼
    ┌────────────────────────────────────────────────────────────────┐
    │ player_1_to_2.pth, player_1_to_3.pth, ..., player_i_to_j.pth  │
    │                                                              │
    │ → 在线 HOP+/Self-play 时：                                   │
    │    “从 i 的视角预测 j 的动作”就加载对应的 player_i_to_j.pth  │
    └────────────────────────────────────────────────────────────────┘
```

---

如果你后面要画“再往右边接上 RNN policy / MCTS / self-play”的整体大图，我们可以在这条线右侧再接一段，比如：

`(o_t, z_t, MOA 预测) → HOP+ / MCTS 规划 → 自身 action a_t`，到时候可以一起拼成完整 pipeline。
