行，那我就用“框框 + 箭头”的方式，把这块 **offline VAE + MOA 多智能体框架**画成简单线条版。

---

### 1. 整体：从多智能体环境到离线 AM 模块

```text
┌─────────────────────────── 多智能体环境 StagHuntEnv ────────────────────────────┐
│                                                                               │
│  每个时间步 t：                                                               │
│    所有 agent 交互 → 生成 (o_t, a_t, r_t, env_id)                            │
│                                                                               │
└───────────────────────────▲────────────────────────────────────────────────────┘
                            │  采样 & 收集轨迹
                            │  (只用 o_t, a_t, t, env_id 做 AM 训练)
                            │
                            ▼
                 ┌──────────────────────────────┐
                 │   Offline 数据文件 all_collect_samples.npy   │
                 │   [(state=o_t, action=a_t, time=t, env_id),…] │
                 └──────────────────────────────┘
```

---

### 2. OfflineVAETrainer 内部：VAE + MOA 联合训练

```text
┌──────────────────────────── OfflineVAETrainer.train() ───────────────────────────┐
│                                                                                │
│  从 .npy 读出： states = o_t, actions = a_t, times = t                          │
│           ↓                                                                    │
│    按 batch 采样 (batch_state, batch_action, batch_time)                       │
│           ↓                                                                    │
│  ┌───────────────────────────────────────────────────────────────────────────┐ │
│  │                     AMLearner.train(states, actions, times)              │ │
│  │                                                                           │ │
│  │  ① VAE：OMG_AM.vae                                                        │ │
│  │     obs = 从 state 中裁掉前 7 维 action_mask 后的 o_t                     │ │
│  │         ┌─────────────────────────────┐                                    │ │
│  │         │   VanillaVAE (CNN Encoder)  │                                    │ │
│  │         │   输入：$obs(o_t)$           │                                    │ │
│  │         │   输出：μ(o_t), logσ²(o_t) │                                    │ │
│  │         └──────────────┬──────────────┘                                    │ │
│  │                        │  重参数化                                       │ │
│  │                 z_t = μ + σ⊙ε                                             │ │
│  │                        ▼                                                 │ │
│  │                  (subgoal / mode embedding z_t)                           │ │
│  │                                                                           │ │
│  │  ② MOA：MOAModel                                                          │ │
│  │     输入：                                                                │ │
│  │       - 完整 state (含 action_mask + 网格 o_t)                            │ │
│  │       - time t                                                            │ │
│  │       - z_t (来自 VAE)                                                    │ │
│  │     经 CNN + FC → 得到：                                                  │ │
│  │         logits(a | o_t, t, z_t)  + action_mask                            │ │
│  │         ↓ Softmax                                                         │ │
│  │         π̂(a_t | o_t, t, z_t)                                             │ │
│  │     同时： V(o_t, t, z_t)                                                 │ │
│  │                                                                           │ │
│  │  ③ 损失：                                                                 │ │
│  │     policy_loss  = CrossEntropy(π̂, a_t)                                  │ │
│  │     kld_loss     = KL(q(z|o_t) || N(0,I))                                 │ │
│  │     total_loss   = policy_loss + omg_vae_alpha * kld_loss                 │ │
│  │                                                                           │ │
│  │  ④ 反向传播，更新：                                                       │ │
│  │     - VAE 参数 (表示学习)                                                 │ │
│  │     - MOA 参数 (行为克隆，对手建模)                                       │ │
│  └───────────────────────────────────────────────────────────────────────────┘ │
│                                                                                │
│  每个 epoch 结束：                                                            │
│    - 保存 VAE 权重 vae.th                                                     │
│    - 保存通用 MOA 权重 player__to_.pth                                       │
│    - evaluate_moa_and_log() 评估 top1/top3、混淆矩阵写入 TensorBoard         │
└────────────────────────────────────────────────────────────────────────────────┘
```

---

### 3. 多智能体视角的 MOA 复制阶段

```text
        离线训练完毕后（共享参数 MOA）:
                      ┌──────────────────────────┐
                      │  通用 MOA: player__to_.pth │
                      └───────────┬──────────────┘
                                  │  文件复制 n(n-1) 份
                                  ▼
    ┌────────────────────────────────────────────────────────────────┐
    │ player_1_to_2.pth, player_1_to_3.pth, ..., player_i_to_j.pth  │
    │                                                              │
    │ → 在线 HOP+/Self-play 时：                                   │
    │    “从 i 的视角预测 j 的动作”就加载对应的 player_i_to_j.pth  │
    └────────────────────────────────────────────────────────────────┘
```

---

如果你后面要画“再往右边接上 RNN policy / MCTS / self-play”的整体大图，我们可以在这条线右侧再接一段，比如：

`(o_t, z_t, MOA 预测) → HOP+ / MCTS 规划 → 自身 action a_t`，到时候可以一起拼成完整 pipeline。


下面我把 **多智能体 Offline-VAE-MOA 框架图**重新写一遍，
并且把所有 **核心模块的输入/输出都用公式表示**，
所有 **损失函数用数学公式（LaTeX）写在公式块**，
结构仍然是“框图 + 箭头”的简洁版本。

---

# **多智能体 Offline VAE + MOA 框架图（含完整数学公式）**

---

## **1. 离线数据收集阶段**

```text
多智能体环境 StagHuntEnv
        │ 生成轨迹
        ▼
```

[
D = {(o_t^{(k)}, a_t^{(k)}, t^{(k)}, \text{env_id})}_{k=1}^N
]

其中：

* ( o_t )：flatten 的全局观测（含 action mask + 网格特征）
* ( a_t \in {0,1,\dots,6} )：动作
* ( t )：时间步

数据被存为：

```text
all_collect_samples.npy
```

---

## **2. OfflineVAETrainer（训练循环）**

```text
o_t, a_t, t  →  batch 化 → 进入 AMLearner.train(...)
```

---

# **3. 关键模块的公式表示**

---

## **3.1 VAE：从观测中抽取 latent/subgoal**

### **Encoder**

[
(\mu_t, \log \sigma_t^2) = \mathrm{Enc}_\phi(o_t)
]

### **Reparameterization**

[
z_t = \mu_t + \sigma_t \odot \epsilon,\qquad \epsilon \sim \mathcal{N}(0,I)
]

### **Decoder**

[
\hat{o}*t = \mathrm{Dec}*\phi(z_t)
]

### **VAE 损失（AM 版本）**

重构损失（用于训练表示，但不进入 total）：
[
\mathcal{L}_{\text{recon}}(o_t,\hat{o}_t)
= |o_t - \hat{o}_t|_2^2
]

KL 项：
[
\mathcal{L}*{\text{KL}}
= -\frac{1}{2} \sum*{i=1}^d
\left( 1 + \log\sigma_{t,i}^2 - \mu_{t,i}^2 - \sigma_{t,i}^2 \right)
]

> **但在该 offline 框架中，最终优化不包含重构，只保留 KL 作为正则项。**

---

## **3.2 MOA：对手动作预测模型**

输入：

[
(o_t,\ t,\ z_t)
]

通过 CNN + MLP 得到动作 logits：

[
\ell_t = f_\theta(o_t,\ t,\ z_t)
]

Action mask 处理：

[
\tilde{\ell}_t = \ell_t + \log(\text{mask}_t)
]

预测动作分布：

[
\hat{\pi}*t(a) = \frac{\exp(\tilde{\ell}*{t,a})}
{\sum_{b=1}^7 \exp(\tilde{\ell}_{t,b})}
]

---

# **4. 损失函数（最终训练用）**

## **4.1 行为克隆（BC / MOA）损失**

[
\mathcal{L}_{\text{BC}}
= -\log \hat{\pi}_t(a_t)
]

## **4.2 KL 正则（来自 VAE）**

[
\mathcal{L}*{\text{KL}}
= \mathrm{KL}\left(q*\phi(z_t|o_t),|,\mathcal{N}(0,I)\right)
]

## **4.3 最终 loss（offline AM 训练）**

[
\boxed{
\mathcal{L}_{\text{total}}
==========================

\mathcal{L}*{\text{BC}}
+
\alpha*{\text{KL}} \cdot \mathcal{L}_{\text{KL}}
}
]

其中：

* (\alpha_{\text{KL}} = \text{omg_vae_alpha}) 会随着 step 动态增加（KL warm-up）

---

# **5. 最终框图（带公式）**

```text
                   Offline 轨迹 D
                    (o_t, a_t, t)
                           │
                           ▼
                 ┌──────────────────────┐
                 │   VAE Encoder        │
                 │  (CNN → μ_t, σ_t² )  │
                 └───────┬─────────────┘
                         │
          z_t = μ_t + σ_t ⊙ ε
                         ▼
              ┌───────────────────┐
              │       MOA         │
              │ fθ(o_t, t, z_t)   │
              │ logits → softmax  │
              └─────────┬─────────┘
                        │
          predicted π̂(a|o_t,t,z_t)
                        │
                        ▼
            ┌─────────────────────────────┐
            │      损失函数（总）         │
            │  BC:  -log π̂(a_t)          │
            │  KL:  KL(q(z|o)‖N(0,I))      │
            │                              │
            │ L_total = L_BC + α L_KL      │
            └─────────────────────────────┘
                        │
                        ▼
              反向传播 → 更新{VAE, MOA}
```

---

# **6. 区分“损失函数符号”和“模型符号”的规则**

你问到“如何区分 loss 和模型表示”，下面是统一规范：

| 类型                | 符号                                                   | 例子                                                 |
| ----------------- | ---------------------------------------------------- | -------------------------------------------------- |
| **概率分布 / 策略**     | (\pi(\cdot)), (\hat{\pi}(\cdot))                     | (\hat{\pi}_t(a_t))                                 |
| **模型/网络**         | (f_\theta), (\mathrm{Enc}*\phi), (\mathrm{Dec}*\phi) | (z_t = \mathrm{Enc}_\phi(o_t))                     |
| **隐变量 / subgoal** | (z_t)                                                | 低维 latent                                          |
| **损失函数**          | (\mathcal{L}_{\text{xxx}})                           | (\mathcal{L}*{\text{BC}}, \mathcal{L}*{\text{KL}}) |

所以：

* (\pi(a|o,z)) 表示 **模型输出的动作概率**
* (\mathcal{L}_{\mathrm{CE}}) 或 (-\log \pi(a)) 表示 **训练时的损失**

---

如果你需要“最终论文版本的图 + 全套公式 + 箭头解释”，我可以继续帮你整理成 Method 子章节格式。
