---
创建时间: 2025-十二月-9日  星期二, 8:20:43 晚上
---


## 3HOP+ 1NS
### GOAL
```
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1765191672"
```
![[Pasted image 20251211161958.png]]
### noGOAL
```
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1765191667"
```



## 2HOP+ 2NS
### GOAL
```
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1765184637"
```
![[Pasted image 20251211162226.png]]

### noGOAL

```
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1765184635"
```


# Time


![[Pasted image 20251211163721.png]]
以图中数据为例： 1.7+3.3=5 （rollout+train = total)


![[Pasted image 20251211163742.png]]以图中数据为例： 0.67+1.8=2.5（rollout+train = total)




![[Pasted image 20251211164205.png]]
0.3\*4 = 1.2 ？

| 算法/说明               | rollout | train | total |
| ------------------- | ------- | ----- | ----- |
| **Goal-noWM (16D)** | 1.7     | 3.3   | 5     |
| **NoGoal**          | 0.67    | 1.8   | 2.5   |
| 差值                  | 1       | 1.5   | 2.5   |

在 self-play 场景中，Goal-noWM（16D）比 NoGoal 明显更耗时，主要原因==可能==是 每个智能体都额外执行了一次 OMG 模块的前向与训练步骤。
由于 self-play 中同时存在 4 个智能体，这意味着 OMG 的计算成本会被 按智能体数量成倍叠加，因此 rollout 阶段与训练阶段的总体耗时均显著增加。
