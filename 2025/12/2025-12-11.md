---
创建时间: 2025-十二月-11日  星期四, 5:24:52 凌晨
---
# 表

## **表 1：Self-Play 结果
(曲线平稳时的steps,time)

|         算法          | Steps        | Time |               aver_Reward                |               Value Loss                |              Subgoal Loss               |
| :-----------------: | ------------ | :--: | :--------------------------------------: | :-------------------------------------: | :-------------------------------------: |
|     **NoGoal**      | 2.25 M steps | 10 h | ![[Pasted image 20251211151738.png]]2.88 | ![[Pasted image 20251211151645.png]]0.1 |                   ——                    |
| **Goal-noWM (16D)** | 2.25 M steps | 17 h | ![[Pasted image 20251211151844.png]]2.88 | ![[Pasted image 20251211151919.png]]0.2 | ![[Pasted image 20251211151941.png]]1.0 |
| **Goal-noWM (8D)**  | 2.45 steps   | 17 h | ![[Pasted image 20251211140433.png]]2.88 | ![[Pasted image 20251211153710.png]]0.2 | ![[Pasted image 20251211140446.png]]1.0 |
>三类算法在 self-play 中均能相同步数（2.25M)达到相同最优策略，但包含 subgoal 模块（Goal 系列）的算法训练耗时更长。

---

## **表 2：Adaptation vs 3 NS

|        算法         |                  Reward                  |               Value Loss                |              Subgoal Loss               |
|:-------------------:|:----------------------------------------:|:---------------------------------------:|:---------------------------------------:|
|     **NoGoal**      | ![[Pasted image 20251211152744.png]]2.85 | ![[Pasted image 20251211152759.png]]0.2 |                   ——                    |
| **Goal-noWM (16D)** | ![[Pasted image 20251211152641.png]]2.85 | ![[Pasted image 20251211152717.png]]0.2 | ![[Pasted image 20251211152704.png]]0.7 |
| **Goal-noWM (8D)**  | ![[Pasted image 20251211153412.png]]2.85 | ![[Pasted image 20251211153431.png]]0.2 | ![[Pasted image 20251211153422.png]]1.0 |
>在adaptation三名 NS 对手的过程中，三种算法均能达到几乎相同的 reward 与价值收敛水平；Goal 系列的 subgoal 模块保持正常工作，但不会带来额外性能提升，也不会造成性能下降。

---

## **表 3：Adaptation vs 3 NH

|         算法          |                  Reward                  |                Value Loss                |              Subgoal Loss               |
| :-----------------: | :--------------------------------------: | :--------------------------------------: | :-------------------------------------: |
|     **NoGoal**      | ![[Pasted image 20251211164832.png]]0.92 | ![[Pasted image 20251211164901.png]]0.02 |                   ——                    |
| **Goal-noWM (16D)** | ![[Pasted image 20251211164715.png]]0.99 | ![[Pasted image 20251211164920.png]]7e-3 | ![[Pasted image 20251211164754.png]]0.5 |
| **Goal-noWM (8D)**  | ![[Pasted image 20251211164950.png]]0.92 | ![[Pasted image 20251211165016.png]]0.01 | ![[Pasted image 20251211165004.png]]1.5 |

>对 3 个 NH 对手的适应中，三种算法皆能提升收益，Goal-noWM(16D) 在 reward、value loss 与 subgoal 稳定性上均较佳。

---

## **表 4：Adaptation vs 3 Random

|        算法         |                 Reward                  |               Value Loss                |              Subgoal Loss               |
|:-------------------:|:---------------------------------------:|:---------------------------------------:|:---------------------------------------:|
|     **NoGoal**      | ![[Pasted image 20251211152519.png]]1.8 | ![[Pasted image 20251211152526.png]]1.2 |                   ——                    |
| **Goal-noWM (16D)** | ![[Pasted image 20251211152410.png]]1.8 | ![[Pasted image 20251211152441.png]]1.2 | ![[Pasted image 20251211152450.png]]2.7 |
| **Goal-noWM (8D)**  | ![[Pasted image 20251211152551.png]]1.8 | ![[Pasted image 20251211152613.png]]1.2 | ![[Pasted image 20251211152606.png]]5.1 |
>对随机对手的适应中，三类算法的最终收益一致；但由于轨迹噪声极高，value 学习精度降低，而 Goal 系列的 subgoal loss 显著升高，特别是 8D，更难在随机环境中学习到有意义的结构。


---
---

# 图

## noGoal
### √self-play


```
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__20251209_221033"
```

```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/msh_MBRL_1hop_3ns____2__20251210_014104"
```

---
### adaptation
#### NS
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/MSH_noGoal_HOPp-NS-NS-NS__2__20251211_145516"
```

#### NH
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/MSH_noGoal_HOPp-NH-NH-NH__2__20251211_152849"
```

#### Random
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/MSH_noGoal_HOPp-RAND-RAND-RAND__2__20251211_133855"
```


---
---

## Goal_noWM （subgoal_dim=16)
### √self-play
```
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__20251209_221042" 
```

---
### adaptation
#### NS
```
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_Goal_noWM_HOPp-NS-NS-NS__2__20251211_145502" 
```

#### NH
```
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_Goal_noWM_HOPp-NH-NH-NH__2__20251211_152830"
```

#### Random
```
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_Goal_noWM_HOPp-RAND-RAND-RAND__2__20251211_134634"
```



---
---


## Goal_noWM （subgoal_dim=8)
### √self-play

```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/msh_MBRL_1hop_3ns____2__20251210_014158"
```

---
### adaptation
#### NS
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/msh_Goal_noWM_HOPp-NS-NS-NS__2__20251211_145520"
```

#### NH
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/msh_Goal_noWM_HOPp-NH-NH-NH__2__20251211_152853"
```

#### Random
```
tensorboard --logdir="/home/fengxue/projects/hopplus/cleanRL/runs/msh_Goal_noWM_HOPp-RAND-RAND-RAND__2__20251211_133912"
```







# Doing:
1. World_model已加上预测done
2. 加上world_model 训练


# Q:


##  multi-step loss 对 imagined trajectory ？


### 3.1 one-step loss 的典型问题：误差累积

你现在训练的是标准 one-step：

* 数据里只监督 $p(s_{t+1}, r_t, done_t \mid s_t, a_t)$
* 但是在 rollout 里，你会做：
  $$s_{t+1} = f(s_t, a_t),\quad s_{t+2} = f(s_{t+1}, a_{t+1}),\dots$$


### 3.2 multi-step loss 的作用

multi-step loss 的核心目的就是：

> 不仅惩罚第一步 $(t→t+1)$ 的错误，也惩罚 model 自己 unroll 多步后的状态错误。

一种简单的实现思路：

1. 在 DataLoader 里改为输出 **短序列**：$(s_t, a_t, r_t, done_t, s_{t+1}, a_{t+1}, \dots, s_{t+K})$
2. 训练时，从真实 $s_t$ 出发，在 latent / state 上用你的 world_model unroll K 步：

   * 得到 $\hat{s}_{t+1}, \hat{s}_{t+2}, \dots, \hat{s}_{t+K}$
3. 对每一步都加 reconstruction / reward / done 的 loss，或者用折扣权重：
   $$\sum_{k=1}^K \gamma^{k-1} \cdot \ell\big(\hat{s}_{t+k}, s_{t+k}\big)$$

这样训练出来的 model 对多步 unroll 更鲁棒一些，**特别适合你这种要在 model 里 rollout H 步产生 debug 轨迹的使用场景**。
