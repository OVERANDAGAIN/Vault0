Sample-Efficient Reinforcement Learning via Conservative Model-Based Actor-Critic	2022	AAAI Conference on Artificial Intelligence	Zhihai Wang; Jie Wang; Qi Zhou; Bin Li; Houqiang Li	CMBAC improves model-based RL sample efficiency by approximating a posterior over Q-values via ensemble models and multi-head critics, and mitigating model-error-induced overestimation through conservative policy optimization using trimmed (bottom-k) Q estimates.	Inspires uncertainty-aware yet non-pessimistic model-based RL: using distributional Q estimates and robust aggregation (e.g., trimmed mean) to avoid exploiting model errors without resorting to worst-case pessimism, which is applicable to world-model-based and multi-agent settings. :contentReference[oaicite:0]{index=0}
