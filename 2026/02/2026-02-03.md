---
创建时间: 2026-二月-3日  星期二, 1:43:28 下午
---
所用步数，而不是时长

哪些模块的问题， planning模块少了

不同对手组合，random策略少用
混淆对手策略不同，但是轨迹类似的 是否分辨

adaptation看world model是否想象很多？
self-play是否见到多样的polixy和应对的策略

vae的t-sne是否能看出来
cvae是去学vae的，是否学准。

world_model准不准，决定adaptation是否快不快

面对固定的对手策略，如果判断 在那一个团里面，就可以不用真实轨迹了



# Contents
1. 在 MSD 环境（4×4，6×6） 下测试了 PPO 与 HOP+ 的 self-play 与 adaptation 表现；实验结果显示：PPO 与 HOP+ 的整体性能接近，尚未观察到明显差异
2. 对于实验结果的分析：
	1. 首先统计的横轴是与环境交互的真实步数，没有计入world_model的轨迹数，因此可以改变world_model所生成轨迹的占比，跑多次实验
	2. 分析各模块对self_play及adaptation的作用：
		1. 与HOP相比，HOP+不包含显式的 planning 过程
		2. 关于CVAE模块，使用T-SNE可视化算法对某一种对手策略组合下的subgoal表征，目前难以观察到清晰的聚类结构
		3. 从self-play角度看，希望policy在这一阶段见到多样的policy和其对应的应对策略，
		4. 从adaptation角度看，希望算法能够快速地识别响应，因此第一步需要goal-inference模块正确识别对手的策略类型，然后world_model模块正确生成很多虚拟轨迹，以供快速学习。
3. 验证：
	1. 验证vae：从预训练的离线数据表征中，看vae是否把不同的状态可以表现出一些簇的概念
	2. 验证cvae：由于cvae会去靠近vae的分布，所以看cvae是否学准
	3. 验证world_model：即重建的质量，以及moa的准确性（这两个前期已经测过了）
4. 关于Overcooked的环境：上周的环境比较简单：只有一个菜谱，总目标是尽可能多地提交菜数，但是overcooked环境有很多变种，一些做adaptation的工作中，会魔改环境，比如加上更多的菜谱数，然后adaptation阶段对手的偏好随机，所以在这种下，self-play可能效果不太行，因为self-play阶段固定就进行某种菜谱提交，因此不会有很多多样性的策略。所以在这种情况下，可以先与设定一个对手策略集，然后训练和策略阶段从中随机采样，这样来测试，即不要self-play这阶段


# Todos
1. 改变超参数（radios,seed等），尽可能多次实验
2. 验证vae，cvae等各模块的效果
3. 采取多种不同的对手策略组合，看subgoal是否有表现一点东西



---

1. 在 MSD 环境（4×4，6×6） 下测试了 PPO 与 HOP+ 的 self-play 与 adaptation 表现；实验结果显示：PPO 与 HOP+ 的整体性能接近，尚未观察到明显差异

2. 对于实验结果的分析：