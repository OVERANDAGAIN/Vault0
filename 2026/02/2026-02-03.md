---
创建时间: 2026-二月-3日  星期二, 1:43:28 下午
---
所用步数，而不是时长

哪些模块的问题， planning模块少了

不同对手组合，random策略少用
混淆对手策略不同，但是轨迹类似的 是否分辨

adaptation看world model是否想象很多？
self-play是否见到多样的polixy和应对的策略

vae的t-sne是否能看出来
cvae是去学vae的，是否学准。

world_model准不准，决定adaptation是否快不快

面对固定的对手策略，如果判断 在那一个团里面，就可以不用真实轨迹了



# Contents
1. 在 MSD 环境（4×4，6×6） 下测试了 PPO 与 HOP+ 的 self-play 与 adaptation 表现；实验结果显示：PPO 与 HOP+ 的整体性能接近，尚未观察到明显差异
2. 对于实验结果的分析：
	1. 首先统计的横轴是与环境交互的真实步数，没有计入world_model的轨迹数，因此可以改变world_model所生成轨迹的占比，跑多次实验
	2. 分析各模块对self_play及adaptation的作用：
		1. 与HOP相比，HOP+不包含显式的 planning 过程
		2. 关于CVAE模块，使用T-SNE可视化算法对某一种对手策略组合下的subgoal表征，目前难以观察到清晰的聚类结构
		3. 从self-play角度看，希望policy在这一阶段见到多样的policy和其对应的应对策略，
		4. 从adaptation角度看，希望算法能够快速地识别响应，因此第一步需要goal-inference模块正确识别对手的策略类型，然后world_model模块正确生成很多虚拟轨迹，以供快速学习。
3. 验证：
	1. 验证vae：从预训练的离线数据表征中，观察其是否能够在状态表征中体现一定的聚类结构
	2. 验证cvae：由于 CVAE 会对齐 VAE 的潜在分布，重点验证其是否能够准确学习条件分布
	3. 验证world_model：即重建的质量，以及moa的准确性（以上指标在MSH实验中已进行过初步验证，后续将结合新环境继续观察）
4. 关于Overcooked的环境：
	1. 上周测试的 Overcooked 环境设置较为简单：单一菜谱, 总目标为尽可能多地提交菜品。但在部分 adaptation 相关工作中：环境会引入 多菜谱设置，adaptation 阶段对手的偏好随机变化
	2. 在该设置下：单纯的 self-play 可能难以产生足够多样的策略。一个可行思路是：预先设定一组对手策略集合，在训练与测试阶段从中随机采样，以此替代传统 self-play，更有针对性地评估 adaptation 能力

# Todos
1. 改变超参数（radio,seed等），尽可能多次实验
2. 验证vae，cvae等各模块的效果
3. 采取多种不同的对手策略组合，进一步分析 subgoal 表征是否呈现可区分结构
4. 继续推进overcooked环境下实验


