---
创建时间: 2025-九月-23日  星期二, 8:33:49 晚上
---
[[Alice in Differentiable Wonderland]]
p15
![[Pasted image 20250923203425.png]]

好的，我来解释一下这段代码和下面的注释。

---

### 代码解析

```python
import torch
x = torch.randn((4, 1))   # 形状是 (4,1)，类似于“列向量”
y = torch.randn((4,))     # 形状是 (4,)，一维张量
print((x + y).shape)
# 结果: torch.Size([4, 4])
```

* **`x`**：形状是 `(4, 1)`，4 行 1 列。
* **`y`**：形状是 `(4,)`，一维向量。

当执行 `x + y` 时，PyTorch 会触发 **广播机制 (broadcasting)** 来让形状对齐：

1. `y` 的形状 `(4,)` 会在前面补一个维度 → `(1, 4)`。
2. 此时 `x` 是 `(4, 1)`，`y` 是 `(1, 4)`。
3. 广播规则会扩展它们，得到 `(4, 4)` 的结果。

所以最终结果是一个 **4×4 的矩阵**，而不是直觉上以为的长度为 4 的向量。

---

### 为什么作者说“probably incorrect broadcasting”？

在很多情况下，我们想做的是**逐元素加法**（即两个“长度为 4 的向量”相加 → 结果还是长度为 4 的向量）。
但是这里因为 `x` 的形状是 `(4,1)`，`y` 的形状是 `(4,)`，导致广播成了 **矩阵相加**。

换句话说：

* 如果你本来是想算

  $$
  (x_1 + y_1, \; x_2 + y_2, \; x_3 + y_3, \; x_4 + y_4)
  $$

  结果应该是 `(4,)`。
* 但实际上 PyTorch 给你算了

  $$
  \begin{bmatrix}
  x_1+y_1 & x_1+y_2 & x_1+y_3 & x_1+y_4 \\
  x_2+y_1 & x_2+y_2 & x_2+y_3 & x_2+y_4 \\
  x_3+y_1 & x_3+y_2 & x_3+y_3 & x_3+y_4 \\
  x_4+y_1 & x_4+y_2 & x_4+y_3 & x_4+y_4
  \end{bmatrix}
  $$

  结果是 `(4,4)`。

---

### 怎么避免？

如果你只是想做**逐元素相加**，需要让两边形状匹配：

```python
x = torch.randn((4,))    # 直接定义成 1D 向量
y = torch.randn((4,))
print((x + y).shape)     # (4,)
```

或者如果 `x` 必须是 `(4,1)`，那就显式改变 `y` 的形状：

```python
(x + y.view(4,1)).shape   # (4,1)，逐元素相加
(x.squeeze() + y).shape   # (4,)
```

---

👉 总结：
这里展示了一个“**形状不匹配但广播强行扩展**”的情况，虽然不会报错，但结果可能不是你想要的。这个例子提醒大家在 PyTorch、NumPy、TensorFlow、JAX 等框架里做张量运算时，要注意 **维度和广播规则**，否则容易得到意外的结果。

要不要我帮你画一张 **(4,1) + (4,) → (4,4)** 的广播过程图，直观展示矩阵是怎么“展开”的？
