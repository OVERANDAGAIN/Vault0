---
创建时间: 2025-九月-23日  星期二, 8:47:58 晚上
---
[[Alice in Differentiable Wonderland]]

p17

### 原文大意

这里讨论了一个矩阵的结构：

* 矩阵 $X$ 有 **$n$ 行** 和 **$d$ 列**。
* 可以把矩阵看作是 **$n$ 个向量的堆叠 (stack)**，每个向量的维度是 $d$。
* 这些向量是 **按行排列**的：

  $$
  X = 
  \begin{bmatrix}
  x_1^T \\
  \vdots \\
  x_n^T
  \end{bmatrix}
  $$

这里的 $x_i \in \mathbb{R}^d$，而 $x_i^T$ 表示转置成行向量。

所以矩阵 $X$ 可以理解为：**一批 (batch) 数据向量的集合**。

---

### 为什么这么表示？

1. **机器学习中的 batch 概念**
   在深度学习和机器学习里，我们通常不是一次只处理一个样本，而是一次处理 **一批样本 (batch)**。

   * 每个样本是一个 **向量** $x_i$。
   * 多个样本叠在一起就形成了一个 **矩阵** $X$。

   比如：

   * 图像分类时，一个样本是一个展平的图像向量（例如 784 维的 MNIST 图像）。
   * 一次输入 64 张图像，就形成了一个 **形状 (64, 784)** 的矩阵。

2. **方便做矩阵运算**

   * 如果只用一个样本，你可能需要单独计算损失和梯度。
   * 如果把多个样本堆叠成矩阵，就能一次性用 **矩阵乘法** 来并行计算，大大提升效率。

---

### 举个例子

假设我们有 3 个二维样本：

$$
x_1 = [1,2], \quad x_2 = [3,4], \quad x_3 = [5,6]
$$

堆叠成矩阵就是：

$$
X = 
\begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6
\end{bmatrix}
$$

* 行数 $n=3$ → batch size = 3
* 列数 $d=2$ → 每个样本的特征维度 = 2

这个矩阵就表示了一批数据。
