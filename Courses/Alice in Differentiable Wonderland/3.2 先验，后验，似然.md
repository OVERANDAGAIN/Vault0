---
创建时间: 2025-九月-25日  星期四, 12:07:24 凌晨
---
[[Alice in Differentiable Wonderland]]
p48

# 问题
The likelihood, and the update is given by **Bayes’ theorem**:

$$
p(f \mid \mathcal{S}_n) = \frac{p(\mathcal{S}_n \mid f)\,p(f)}{p(\mathcal{S}_n)} \tag{E.3.7}
$$

* **Prior** (before observing the dataset)
* **Posterior** (after observing the dataset)

The term $p(f \mid \mathcal{S}_n)$ is called the **posterior distribution function**, while the term $p(\mathcal{S}_n)$ in the denominator is called the **evidence** and it is needed to ensure that the posterior is properly normalized.

Assume for now that we have access to the posterior. Differently from before, **the distribution can encode preference for more than a single function $f$, which may provide better predictive power.**

Given an input $x$, we can make a prediction by averaging all possible models based on their posterior’s weight:

$$
p(y \mid x) = \int_f p(y \mid f(x)) \, p(f \mid \mathcal{S}_n) \, df 
\;\;\;\; \approx \;\;\;\; \frac{1}{k} \sum_{i=1}^k p(y \mid f_i(x)) \, p(f_i \mid \mathcal{S}_n) \tag{E.3.8}
$$
![[Pasted image 20250925000837.png]]



# 回答

好，这个问题非常关键，其实就是 **Bayesian prediction** 和 **加权平均** 的思想。我们来逐步拆开：

---

### 1. 目标是什么？

我们想要预测 $p(y \mid x)$，但是 **我们并不知道哪个函数 $f$ 才是正确的模型**。

* 在频率学派（MLE）里：直接选择一个最优的 $f^*$，然后用它预测：

  $$
  p(y \mid x) \approx p(y \mid f^*(x)).
  $$
* 在贝叶斯学派里：不选择单个 $f$，而是认为 **所有可能的 $f$** 都有一定的可信度（posterior weight），因此预测应该是它们的加权平均。

---

### 2. 为什么会有乘 $p(f \mid \mathcal{S}_n)$？

公式是：

$$
p(y \mid x) = \int p(y \mid f(x)) \, p(f \mid \mathcal{S}_n) \, df
$$

* **$p(y \mid f(x))$**：如果我只相信某个函数 $f$，它会给出预测分布。
* **$p(f \mid \mathcal{S}_n)$**：这个函数 $f$ 在观测数据 $\mathcal{S}_n$ 下的可信度（posterior 权重）。
* 乘起来就是：**给每个函数的预测打上权重**，最后把它们积分起来。

直观理解：

* 就像投票，每个函数 $f$ 给出一个预测，但它的“发言权”取决于它的 posterior 概率。
* 如果某个 $f$ 很可能（posterior 大），它对结果的贡献就大；如果它几乎不可能（posterior 小），它对结果的贡献就小。

---

### 3. 会不会改变预测？

不会。反而更合理：

* 如果你只用一个 $f^*$，预测可能会偏（过拟合风险）。
* 用所有 $f$ 的 posterior 加权平均，可以自然地引入 **模型不确定性**：

  * posterior 集中在一个 $f^*$ 附近 → 结果接近频率派预测。
  * posterior 分布比较分散 → 结果会融合多个可能模型，表达出不确定性。

---

### 4. Monte Carlo 近似

实际积分 $\int p(y \mid f(x)) p(f \mid \mathcal{S}_n) df$ 很难算，所以用采样近似：

$$
p(y \mid x) \approx \frac{1}{k} \sum_{i=1}^k p(y \mid f_i(x)) \, p(f_i \mid \mathcal{S}_n)
$$

这就是式子 (E.3.8)。

---

✅ **总结**

* 乘 $p(f \mid \mathcal{S}_n)$ 是因为我们要“加权所有可能的函数”。
* 它不会破坏预测，反而让预测更合理：把不确定性纳入考虑。
* 本质上：**单一模型预测** → **加权集成预测**，就是贝叶斯预测的核心优势。



