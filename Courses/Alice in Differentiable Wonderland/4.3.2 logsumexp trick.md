[[Alice in Differentiable Wonderland]]
p67

很好的问题 👍，这其实是在解释 **交叉熵损失（cross-entropy loss）** 里 softmax+log 的一个 **数值稳定性问题**。我帮你拆解一下：

---

## 1. 原始形式

交叉熵的一部分是

$$
-\log \left( \frac{\exp(p_i)}{\sum_j \exp(p_j)} \right)
$$

其中 $p_i$ 是 logits（模型的原始输出，还没经过 softmax）。

---

## 2. 为什么会有数值问题？

这里有两个原因：

1. **logits $p_i$ 可以非常大或非常小（无界）**

   * 比如 $p_i = 1000$，那么 $\exp(1000)$ 会 **溢出**，数值变成 $\infty$。
   * 如果 $p_i = -1000$，那么 $\exp(-1000)$ 会接近 **0**，可能导致 **下溢**（underflow）。

2. **指数和对数的组合很敏感**

   * $\exp(p_i)$ 会放大数值范围，而再取 $\log$ 会让差距被进一步放大或缩小。
   * 在计算机的浮点数精度下，很容易失真。

👉 所以，这里说的 **issues** 指的就是：**数值不稳定**（overflow / underflow）。

---

## 3. 怎么解决？

作者把它改写成：

$$
-\log \left( \frac{\exp(p_i)}{\sum_j \exp(p_j)} \right)
= -p_i + \log \left( \sum_j \exp(p_j) \right)
$$

* $\log \left( \sum_j \exp(p_j) \right)$ 有专门的稳定实现（log-sum-exp trick）。
* 计算时会先减去一个常数（通常是 $\max(p_j)$），避免指数太大或太小：

$$
\log \left( \sum_j \exp(p_j) \right)
= \max_j(p_j) + \log \left( \sum_j \exp(p_j - \max_j(p_j)) \right)
$$

这样所有指数都在 $[0,1]$ 附近，避免溢出。

---

## 4. 📌 总结

* **问题**：logits 无界 → exp 可能导致数值溢出 / 下溢 → log-softmax 不稳定。
* **解决**：把 softmax + log 改写成 `-p_i + logsumexp(p)`，然后用 **log-sum-exp trick** 做数值稳定化。

---

要不要我给你写一个 **小的 Python 数值例子**，展示为什么原始公式会溢出，而 log-sum-exp trick 可以避免？
