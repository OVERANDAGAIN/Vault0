[[Alice in Differentiable Wonderland]]
p70




# 📑 模型校准（Calibration）与 Reliability Diagram 总结

## 1. 基本概念
- **Confidence（置信度）**：模型对自己预测的自我“信心”，通常取 softmax 最大值，如 `[0.2,0.7,0.1] → 0.7`。
- **Accuracy（准确率）**：真实正确的比例，即预测对的样本数 / 总样本数。
- **校准（Calibration）**：衡量模型的预测概率与实际正确率是否一致。

👉 好的模型应该满足：  
$$
\text{Accuracy} \approx \text{Confidence}
$$

---

## 2. bin 的作用
- 将区间 [0,1] 按置信度划分为若干 **bin（桶/区间）**。
- 每个 bin 内部统计：
  - **平均 confidence**：该区间预测的平均概率。
  - **平均 accuracy**：该区间预测的实际正确率。
- 对比两者 → 观察模型是否过度自信或过度保守。

📌 常见做法：把 [0,1] 等宽划分为 10 个区间（deciles），兼顾分辨率与统计稳定性。  
也可以用等频划分，让每个 bin 样本数相近。

---

## 3. Reliability Diagram（可靠性图）
- 横轴：平均 confidence  
- 纵轴：平均 accuracy  
- 对角线：完美校准（模型预测多少，现实就发生多少）。
  - 点落在 **对角线上** → 模型“诚实”。  
  - 点落在 **下方** → 模型过度自信（吹牛）。  
  - 点落在 **上方** → 模型过于保守（谦虚）。

![[Pasted image 20251006155313.png]]
---

## 4. Expected Calibration Error (ECE)
量化指标：  
$$
\text{ECE} = \sum_{i=1}^b \frac{|\mathcal{B}_i|}{n} \, |a_i - p_i|
$$
- $\mathcal{B}_i$：第 $i$ 个 bin 的样本集合  
- $|\mathcal{B}_i| / n$：权重（该 bin 占比）  
- $a_i$：平均准确率  
- $p_i$：平均置信度  

👉 ECE 越小，说明模型的置信度与真实准确率越接近。

---

## 5. 直观理解
- **Confidence** = 模型说“我有多大概率对”。  
- **Accuracy** = 现实中“它到底对了多少”。  
- bin = “把同样自信度的预测放在一起看”。  
- **对角线的意义**：模型的自我报告与现实匹配，像天气预报员说“70% 会下雨”，真的有 70% 的概率下雨。

---

📌 **总结一句话**  
模型校准不是追求“预测概率越大越好”，而是追求 **预测概率与实际正确率一致**。  
Reliability Diagram 提供了直观可视化，而 ECE 给出了数值化的误差度量。
