[[Alice in Differentiable Wonderland]]
p70


好的 👍 我帮你把这一页的核心内容整理一下，并回答你关于 **confidence 和 accuracy** 的计算问题。

---

# 📑 知识点归纳：Calibration Error

## 1. **为什么要校准 (Calibration)**

* 现代模型（尤其是深度学习分类器）可能 **预测概率分布不可靠**。

  * 例：模型说某类概率是 0.9，但实际上正确率可能只有 0.7 → **过度自信**。
* Calibration 用来衡量模型预测概率与真实准确率是否一致。

---

## 2. **Reliability Diagram (可靠性图)**

* 将所有预测分到 $b$ 个区间（bins），每个区间大小为 $1/b$。
* 对每个 bin：

  * **平均置信度** $p_{i}$：预测属于该 bin 的样本的平均预测概率。
  * **平均准确率** $a_{i}$：这些样本中，实际预测正确的比例。
* 作图：$(a_i, p_i)$

  * 完美情况：所有点在对角线上 ($a_i = p_i$)。
  * 偏离对角线：说明模型 **过度自信** 或 **过度保守**。

---

## 3. **Expected Calibration Error (ECE)**

* 量化模型校准误差的指标：

$$
\text{ECE} = \sum_{i} \frac{|\mathcal{B}_i|}{n} , |a_i - p_i|
$$

其中：

* $\mathcal{B}_i$ = 落在第 $i$ 个 bin 的样本集合
* $|\mathcal{B}_i| / n$ = 权重（该 bin 占整个验证集的比例）
* $a_i$ = 平均准确率
* $p_i$ = 平均置信度

👉 直观意义：**平均而言，预测概率和实际准确率差多大**。

---

## 4. **改进方法**

* 如果模型不够校准，可以做：

  * **Temperature scaling**：对 logits 除以一个温度参数，调整概率分布平滑度。
  * **Focal loss 等替代损失函数**：重新训练时对 hard/uncertain 样本加权。

---

## 5. **Conformal Prediction (保形预测)**

* 一种替代方法：不输出单个类别，而是输出一个 **类别集合** $\mathcal{C}(x)$。
* 定义：

$$
\mathcal{C}(x) = { i \mid [f(x)]_{i} > \gamma }
$$

即：所有预测概率大于阈值 $\gamma$ 的类都包含在集合里。

* 通过选择合适的 $\gamma$，可以保证预测集合包含正确类别的概率大于某个水平 $\alpha$。

👉 好处：可以避免模型过度自信，提供 **置信区间** 风格的输出。

---

# ❓ 你的问题：**confidence 和 accuracy 怎么算？**

* **Confidence（置信度 $p_i$）：**

  * 在一个 bin 内，取所有样本预测概率（通常是 softmax 后的最大值），求平均。
  * 例：bin 内 3 个样本预测概率 = [0.8, 0.75, 0.9] → confidence = (0.8+0.75+0.9)/3 = 0.816。

* **Accuracy（准确率 $a_i$）：**

  * 在一个 bin 内，计算预测正确的样本比例。
  * 例：3 个样本预测 [cat, dog, cat]，真实标签 [cat, dog, horse] → 正确 2 个，accuracy = 2/3 = 0.667。

---

📌 **总结一句话**：

* **Confidence = 模型说“我有多确定”**
* **Accuracy = 实际上它有多正确**
* 两者的差值反映了 **校准程度**。

---

要不要我帮你画一个 **ECE + Reliability Diagram** 的小图例？比如展示“过度自信”和“完美校准”的区别，会更直观。
