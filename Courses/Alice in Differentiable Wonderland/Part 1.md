---
åˆ›å»ºæ—¶é—´: 2025-ä¹æœˆ-23æ—¥  æ˜ŸæœŸäºŒ, 8:42:54 æ™šä¸Š
---
[[Alice in Differentiable Wonderland]]



We close with two additional observations that will be useful. First, we can write the sum of the elements of a vector as its dot product with a vector 1 composed entirely of ones:

$$
\langle x, 1 \rangle = \sum_{i=1}^{d} x_i
$$

Second, the distance between two vectors can also be written in terms of their dot products:

$$
\|x - y\|^2 = \langle x, x \rangle + \langle y, y \rangle - 2 \langle x, y \rangle
$$

The case $y = 0$ gives us $\|x\|^2 = \langle x, x \rangle$. Both equations can be useful when writing equations or in the code.

---

---

**Definition D.2.4 (Hadamard multiplication)** *The Hadamard multiplication of two matrices of the same shape is done element-wise:*

$$
[X \odot Y]_{ij} = X_{ij} Y_{ij}
$$

While Hadamard multiplication does not have all the interesting algebraic properties of standard matrix multiplication, it is commonly used in differentiable models for performing masking operations (e.g., setting some elements to zero) or scaling operations. Multiplicative interactions have also become popular in some recent families of models, as we will see next.

---
---

```python
X = torch.randn((5, 5))  
X = torch.exp(X) # Element-wise exponential 
X = torch.linalg.matrix_exp(X) # Matrix exponential
```
>Difference between the element-wise exponential of a matrix and the matrix exponential as defined in linear algebra textbooks. Specialized linear algebra operations are generally encapsulated in their own sub-package.

```ad-hint
å¥½é—®é¢˜ ğŸ‘ï¼Œè¿™ä¸ªå…¬å¼æ˜¯åœ¨çº¿æ€§ä»£æ•°å’Œæ·±åº¦å­¦ä¹ é‡Œéå¸¸é‡è¦çš„ **çŸ©é˜µæŒ‡æ•° (matrix exponential)** å®šä¹‰ã€‚æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥ç†è§£ï¼š

---

## 1. å›å¿†æ ‡é‡çš„æŒ‡æ•°å‡½æ•°

å¯¹äºä¸€ä¸ªå®æ•° $x$ï¼ŒæŒ‡æ•°å‡½æ•°å®šä¹‰ä¸ºå¹‚çº§æ•°ï¼š

$$
e^x = \sum_{k=0}^\infty \frac{x^k}{k!}
$$

è¿™æ˜¯å¾®ç§¯åˆ†é‡Œæœ€åŸºæœ¬çš„å±•å¼€å¼ã€‚

---

## 2. æ¨å¹¿åˆ°çŸ©é˜µ

å¦‚æœæŠŠæ ‡é‡ $x$ æ¢æˆçŸ©é˜µ $X$ï¼ˆæ–¹é˜µï¼‰ï¼Œé‚£ä¹ˆå®šä¹‰ **çŸ©é˜µæŒ‡æ•°** ä¸ºï¼š

$$
\text{mat-exp}(X) = e^X = \sum_{k=0}^\infty \frac{1}{k!} X^k
$$

å…¶ä¸­ï¼š

* $X^0 = I$ï¼ˆå•ä½çŸ©é˜µï¼‰
* $X^1 = X$ï¼Œ$X^2 = XX$ï¼Œä»¥æ­¤ç±»æ¨
* æ¯ä¸€é¡¹éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼ˆä¸æ˜¯ Hadamard é€å…ƒç´ ä¹˜æ³•ï¼ï¼‰

---

## 3. ä¸ºä»€ä¹ˆè¿™æ ·å®šä¹‰ï¼Ÿ

è¿™æ ·å®šä¹‰çš„åŸå› æ˜¯ï¼š

* ä¿è¯çŸ©é˜µæŒ‡æ•°å’Œæ ‡é‡æŒ‡æ•°æœ‰ç±»ä¼¼çš„ä»£æ•°æ€§è´¨ã€‚
* ç‰¹åˆ«é‡è¦çš„æ˜¯ï¼š

  * å¦‚æœ $X$ æ˜¯ä¸€ä¸ª **çº¿æ€§ç®—å­çŸ©é˜µ**ï¼Œé‚£ä¹ˆ $e^X$ è¡¨ç¤ºâ€œè¿ç»­æ—¶é—´æ¼”åŒ–â€çš„è§£ã€‚
  * æ¯”å¦‚åœ¨çº¿æ€§å¾®åˆ†æ–¹ç¨‹ $\frac{d}{dt}x(t) = Ax(t)$ ä¸­ï¼Œè§£å°±æ˜¯ï¼š

    $$
    x(t) = e^{At} x(0)
    $$
  * è¿™åœ¨ **åŠ¨åŠ›ç³»ç»Ÿã€ç‰©ç†ã€æ·±åº¦å­¦ä¹ ï¼ˆå¦‚è¿ç»­æ—¶é—´æ¨¡å‹ã€ODE-RNNï¼‰** é‡Œéå¸¸å¸¸è§ã€‚

---

## 4. ä¸¾ä¸ªä¾‹å­

è®¾

$$
X = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}
$$

è¿™æ˜¯ä¸€ä¸ª **æ—‹è½¬ç®—å­çŸ©é˜µ**ã€‚é‚£ä¹ˆï¼š

$$
e^{X\theta} = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}
$$

è¿™å°±è§£é‡Šäº†ä¸ºä»€ä¹ˆæŒ‡æ•°çŸ©é˜µè‡ªç„¶å‡ºç°äº† **æ—‹è½¬ã€æ¼”åŒ–** è¿™æ ·çš„æ“ä½œã€‚

  ```



---

---

ing lower and upper indices, in which case we assume that the summation runs along the full axis:

$$
\sum_i x_i = \sum_{i=1}^n x_i
$$

In PyTorch and other frameworks, reduction operations correspond to methods having an **axis** argument:

```python
r = X.sum(axis=1)
```


---

---

## ğŸ”¹ Asymptotic Analysis of Matrix Multiplication

1. **Naive implementation**

   * Matrix multiplication $Z = XY$, where

     * $X$ has shape $(a, b)$
     * $Y$ has shape $(b, c)$.
   * To compute each element $Z_{ij}$, we need **b multiplications and b additions**.
   * There are $a \times c$ such elements.
   * Hence the total cost is:

     $$
     O(a \cdot b \cdot c)
     $$

   So the runtime grows linearly with respect to all three parameters.

---

2. **Special case: square matrices**

   * If both matrices are $(n, n)$:

     $$
     O(n \cdot n \cdot n) = O(n^3)
     $$
   * This is why we say **naive matrix multiplication is cubic in the input dimension**.

---

3. **Beyond naive multiplication**

   * Strassenâ€™s algorithm: $O(n^{2.81})$
   * Coppersmithâ€“Winograd (and successors): $O(n^{2.37})$ (theoretical, ==not practical for most deep learning==).**(æ­¤äº‹åœ¨Aliceä¸­äº¦æœ‰è®°è½½)**
   * In practice, most frameworks (PyTorch, TensorFlow, NumPy) rely on optimized BLAS/LAPACK kernels (like Intel MKL, cuBLAS), which are still essentially $O(n^3)$ but with heavy optimizations (parallelization, cache efficiency, GPU acceleration).

---

---

As an example of the former, consider two tensors $X \sim (n,a,b)$ and $Y \sim (n,b,c)$.
**Batched matrix multiplication (BMM)** is defined as:

$$
[\text{BMM}(X,Y)]_i = X_i Y_i \sim (n,a,c) \tag{E.2.9}
$$

Operations in most frameworks operate transparently on batched versions of their arguments, which are assumed like in this case to be **leading dimensions** (the first dimensions). For example, batched matrix multiplication in PyTorch is the same as standard matrix multiplication, see Box C.2.3.
