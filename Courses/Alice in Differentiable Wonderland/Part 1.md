---
åˆ›å»ºæ—¶é—´: 2025-ä¹æœˆ-23æ—¥  æ˜ŸæœŸäºŒ, 8:42:54 æ™šä¸Š
---
[[Alice in Differentiable Wonderland]]



We close with two additional observations that will be useful. First, we can write the sum of the elements of a vector as its dot product with a vector 1 composed entirely of ones:

$$
\langle x, 1 \rangle = \sum_{i=1}^{d} x_i
$$

Second, the distance between two vectors can also be written in terms of their dot products:

$$
\|x - y\|^2 = \langle x, x \rangle + \langle y, y \rangle - 2 \langle x, y \rangle
$$

The case $y = 0$ gives us $\|x\|^2 = \langle x, x \rangle$. Both equations can be useful when writing equations or in the code.

---

---

**Definition D.2.4 (Hadamard multiplication)** *The Hadamard multiplication of two matrices of the same shape is done element-wise:*

$$
[X \odot Y]_{ij} = X_{ij} Y_{ij}
$$

While Hadamard multiplication does not have all the interesting algebraic properties of standard matrix multiplication, it is commonly used in differentiable models for performing masking operations (e.g., setting some elements to zero) or scaling operations. Multiplicative interactions have also become popular in some recent families of models, as we will see next.

---
---

```python
X = torch.randn((5, 5))  
X = torch.exp(X) # Element-wise exponential 
X = torch.linalg.matrix_exp(X) # Matrix exponential
```
>Difference between the element-wise exponential of a matrix and the matrix exponential as defined in linear algebra textbooks. Specialized linear algebra operations are generally encapsulated in their own sub-package.

```ad-hint
å¥½é—®é¢˜ ğŸ‘ï¼Œè¿™ä¸ªå…¬å¼æ˜¯åœ¨çº¿æ€§ä»£æ•°å’Œæ·±åº¦å­¦ä¹ é‡Œéå¸¸é‡è¦çš„ **çŸ©é˜µæŒ‡æ•° (matrix exponential)** å®šä¹‰ã€‚æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥ç†è§£ï¼š

---

## 1. å›å¿†æ ‡é‡çš„æŒ‡æ•°å‡½æ•°

å¯¹äºä¸€ä¸ªå®æ•° $x$ï¼ŒæŒ‡æ•°å‡½æ•°å®šä¹‰ä¸ºå¹‚çº§æ•°ï¼š

$$
e^x = \sum_{k=0}^\infty \frac{x^k}{k!}
$$

è¿™æ˜¯å¾®ç§¯åˆ†é‡Œæœ€åŸºæœ¬çš„å±•å¼€å¼ã€‚

---

## 2. æ¨å¹¿åˆ°çŸ©é˜µ

å¦‚æœæŠŠæ ‡é‡ $x$ æ¢æˆçŸ©é˜µ $X$ï¼ˆæ–¹é˜µï¼‰ï¼Œé‚£ä¹ˆå®šä¹‰ **çŸ©é˜µæŒ‡æ•°** ä¸ºï¼š

$$
\text{mat-exp}(X) = e^X = \sum_{k=0}^\infty \frac{1}{k!} X^k
$$

å…¶ä¸­ï¼š

* $X^0 = I$ï¼ˆå•ä½çŸ©é˜µï¼‰
* $X^1 = X$ï¼Œ$X^2 = XX$ï¼Œä»¥æ­¤ç±»æ¨
* æ¯ä¸€é¡¹éƒ½æ˜¯çŸ©é˜µä¹˜æ³•ï¼ˆä¸æ˜¯ Hadamard é€å…ƒç´ ä¹˜æ³•ï¼ï¼‰

---

## 3. ä¸ºä»€ä¹ˆè¿™æ ·å®šä¹‰ï¼Ÿ

è¿™æ ·å®šä¹‰çš„åŸå› æ˜¯ï¼š

* ä¿è¯çŸ©é˜µæŒ‡æ•°å’Œæ ‡é‡æŒ‡æ•°æœ‰ç±»ä¼¼çš„ä»£æ•°æ€§è´¨ã€‚
* ç‰¹åˆ«é‡è¦çš„æ˜¯ï¼š

  * å¦‚æœ $X$ æ˜¯ä¸€ä¸ª **çº¿æ€§ç®—å­çŸ©é˜µ**ï¼Œé‚£ä¹ˆ $e^X$ è¡¨ç¤ºâ€œè¿ç»­æ—¶é—´æ¼”åŒ–â€çš„è§£ã€‚
  * æ¯”å¦‚åœ¨çº¿æ€§å¾®åˆ†æ–¹ç¨‹ $\frac{d}{dt}x(t) = Ax(t)$ ä¸­ï¼Œè§£å°±æ˜¯ï¼š

    $$
    x(t) = e^{At} x(0)
    $$
  * è¿™åœ¨ **åŠ¨åŠ›ç³»ç»Ÿã€ç‰©ç†ã€æ·±åº¦å­¦ä¹ ï¼ˆå¦‚è¿ç»­æ—¶é—´æ¨¡å‹ã€ODE-RNNï¼‰** é‡Œéå¸¸å¸¸è§ã€‚

---

## 4. ä¸¾ä¸ªä¾‹å­

è®¾

$$
X = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}
$$

è¿™æ˜¯ä¸€ä¸ª **æ—‹è½¬ç®—å­çŸ©é˜µ**ã€‚é‚£ä¹ˆï¼š

$$
e^{X\theta} = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}
$$

è¿™å°±è§£é‡Šäº†ä¸ºä»€ä¹ˆæŒ‡æ•°çŸ©é˜µè‡ªç„¶å‡ºç°äº† **æ—‹è½¬ã€æ¼”åŒ–** è¿™æ ·çš„æ“ä½œã€‚

  ```



---

---

ing lower and upper indices, in which case we assume that the summation runs along the full axis:

$$
\sum_i x_i = \sum_{i=1}^n x_i
$$

In PyTorch and other frameworks, reduction operations correspond to methods having an **axis** argument:

```python
r = X.sum(axis=1)
```


---

---

## ğŸ”¹ Asymptotic Analysis of Matrix Multiplication

1. **Naive implementation**

   * Matrix multiplication $Z = XY$, where

     * $X$ has shape $(a, b)$
     * $Y$ has shape $(b, c)$.
   * To compute each element $Z_{ij}$, we need **b multiplications and b additions**.
   * There are $a \times c$ such elements.
   * Hence the total cost is:

     $$
     O(a \cdot b \cdot c)
     $$

   So the runtime grows linearly with respect to all three parameters.

---

2. **Special case: square matrices**

   * If both matrices are $(n, n)$:

     $$
     O(n \cdot n \cdot n) = O(n^3)
     $$
   * This is why we say **naive matrix multiplication is cubic in the input dimension**.

---

3. **Beyond naive multiplication**

   * Strassenâ€™s algorithm: $O(n^{2.81})$
   * Coppersmithâ€“Winograd (and successors): $O(n^{2.37})$ (theoretical, ==not practical for most deep learning==).**(æ­¤äº‹åœ¨Aliceä¸­äº¦æœ‰è®°è½½)**
   * In practice, most frameworks (PyTorch, TensorFlow, NumPy) rely on optimized BLAS/LAPACK kernels (like Intel MKL, cuBLAS), which are still essentially $O(n^3)$ but with heavy optimizations (parallelization, cache efficiency, GPU acceleration).

---

---

As an example of the former, consider two tensors $X \sim (n,a,b)$ and $Y \sim (n,b,c)$.
**Batched matrix multiplication (BMM)** is defined as:

$$
[\text{BMM}(X,Y)]_i = X_i Y_i \sim (n,a,c) \tag{E.2.9}
$$

Operations in most frameworks operate transparently on batched versions of their arguments, which are assumed like in this case to be **leading dimensions** (the first dimensions). For example, batched matrix multiplication in PyTorch is the same as standard matrix multiplication, see Box C.2.3.
```python
X = torch.randn((4, 5, 2)) Y = torch.randn((4, 2, 3)) (torch.matmul(X, Y)).shape # Or X @ Y # [Out]: (4, 5, 3)
```

````ad-hint
å¥½é—®é¢˜ ğŸ‘ï¼Œä½ æ­£å¥½è§¦åˆ°äº† **PyTorch/NumPy ç­‰æ¡†æ¶å¯¹ `matmul` çš„ç»´åº¦å¤„ç†è§„åˆ™**ã€‚æˆ‘ç»™ä½ ç³»ç»Ÿè§£é‡Šä¸€ä¸‹ï¼š

---

## 1. `torch.matmul` çš„æ ¸å¿ƒè§„åˆ™

åœ¨ PyTorchï¼ˆå’Œ NumPy `@` ç±»ä¼¼ï¼‰é‡Œï¼š

* **å¦‚æœè¾“å…¥æ˜¯ 2D å¼ é‡**ï¼šå°±æ˜¯æ™®é€šçš„çŸ©é˜µä¹˜æ³•ã€‚
* **å¦‚æœè¾“å…¥æ˜¯ 1D å‘é‡**ï¼šæŒ‰è§„åˆ™é€€åŒ–æˆå‘é‡ä¹˜æ³•ï¼ˆç‚¹ç§¯ã€çŸ©é˜µ-å‘é‡ä¹˜æ³•ç­‰ï¼‰ã€‚
* **å¦‚æœè¾“å…¥æ˜¯é«˜ç»´å¼ é‡**ï¼š**åªå¯¹æœ€åä¸¤ä¸ªç»´åº¦åšçŸ©é˜µä¹˜æ³•**ï¼Œå‰é¢çš„ç»´åº¦ä¼šè¢«å½“ä½œ **batch ç»´åº¦** è‡ªåŠ¨å¹¿æ’­ã€‚

---

## 2. ä¸ºä»€ä¹ˆä¾‹å­é‡Œç¬¬ä¸€ç»´æ˜¯ batchï¼Ÿ

ä½ çš„ä¾‹å­ï¼š

```python
X = torch.randn((4, 5, 2))  # shape (batch=4, m=5, k=2)
Y = torch.randn((4, 2, 3))  # shape (batch=4, k=2, n=3)
Z = torch.matmul(X, Y)      # shape (4, 5, 3)
```

* æœ€åä¸¤ç»´ `(5,2) @ (2,3) â†’ (5,3)`
* å‰é¢çš„ç»´åº¦ `4` è¢«å½“ä½œ batch â†’ ä¿ç•™ä¸‹æ¥
* æ‰€ä»¥ç»“æœæ˜¯ `(4, 5, 3)`

è¿™å°±æ˜¯â€œç¬¬ä¸€ç»´é»˜è®¤æ˜¯ batchâ€çš„åŸå› ï¼šå› ä¸º `matmul` åªå…³å¿ƒæœ€åä¸¤ä¸ªç»´åº¦ã€‚

---

## 3. å¦‚æœæœ‰æ›´å¤šç»´ä¼šæ€æ ·ï¼Ÿ

å‡è®¾ï¼š

```python
A = torch.randn(10, 4, 5, 2)
B = torch.randn(10, 4, 2, 3)
C = A @ B
```

* è¿™é‡Œ `A.shape = (10, 4, 5, 2)`
* `B.shape = (10, 4, 2, 3)`
* æœ€åä¸¤ç»´ `(5,2) @ (2,3) â†’ (5,3)`
* å‰é¢ `(10,4)` ä¿ç•™ä½œä¸º batch ç»´
* ç»“æœï¼š`C.shape = (10, 4, 5, 3)`

---

## 4. æ›´ä¸€èˆ¬çš„è§„åˆ™

åœ¨ PyTorch æ–‡æ¡£é‡Œï¼Œ`matmul` çš„è§„åˆ™æ˜¯ï¼š

* å¦‚æœ `A.shape = (*batch, m, k)`ï¼Œ`B.shape = (*batch, k, n)`ï¼Œ
  é‚£ä¹ˆ

  $$
  C = A @ B \quad \Rightarrow \quad C.shape = (*batch, m, n)
  $$

* `*batch` è¡¨ç¤ºå¯ä»¥æœ‰ä»»æ„å¤šä¸ª batch ç»´ï¼ˆç”šè‡³å¯ä»¥å¹¿æ’­ï¼‰ã€‚

---

## 5. å¹¿æ’­ä¾‹å­

```python
X = torch.randn(3, 5, 2)   # (3,5,2)
Y = torch.randn(2, 3)      # (2,3) â†’ ä¼šè‡ªåŠ¨å¹¿æ’­æˆ (3,2,3)
Z = X @ Y                  # (3,5,3)
```

è¿™é‡Œ Y æœ¬æ¥æ˜¯ 2Dï¼Œä½†ä¼šå¹¿æ’­åˆ° batch ç»´åº¦ `(3,2,3)`ï¼Œæ‰èƒ½å’Œ X åŒ¹é…ã€‚

---

âœ… **æ€»ç»“**

* `matmul` **åªè€ƒè™‘æœ€åä¸¤ç»´åšçŸ©é˜µä¹˜æ³•**ã€‚
* å…¶ä½™ç»´åº¦éƒ½æ˜¯ **batch ç»´åº¦**ï¼Œä¼šè‡ªåŠ¨ä¿ç•™ä¸‹æ¥æˆ–å¹¿æ’­ã€‚
* æ‰€ä»¥ `(4,5,2) @ (4,2,3)` â†’ `(4,5,3)`ã€‚
* å¦‚æœæœ‰æ›´å¤šç»´ï¼Œå°±ä¼šæœ‰æ›´å¤š batch ç»´ã€‚


````

---
---


æ–‡ä¸­å®šä¹‰äº†ä¸€ä¸ªå¹¿ä¹‰ç‚¹ç§¯ (**Generalized Dot Product, GDT**)ï¼š

$$
\text{GDT}(X, Y) = \sum_{i,j,k} [X \odot Y]_{ijk}
$$

* è¿™é‡Œçš„ $\odot$ è¡¨ç¤º **Hadamard ä¹˜æ³•ï¼ˆé€å…ƒç´ ä¹˜æ³•ï¼‰**ã€‚
* $[X \odot Y]_{ijk} = X_{ijk} Y_{ijk}$ã€‚
* æœ€åå†å¯¹æ‰€æœ‰ç´¢å¼• $i,j,k$ è¿›è¡Œæ±‚å’Œã€‚

æ¢å¥è¯è¯´ï¼š

$$
\text{GDT}(X, Y) = \sum_{i,j,k} X_{ijk} Y_{ijk}
$$

> ç›´è§‚ç†è§£:è¿™å…¶å®å°±æ˜¯ã€ŒæŠŠä¸¤ä¸ªå¼ é‡å±•å¹³æˆå‘é‡ï¼Œå†åšä¸€æ¬¡æ™®é€šçš„ç‚¹ç§¯ã€ã€‚


---
---

![[Pasted image 20250923235448.png]]

---
---


````ad-note
éå¸¸å¥½ ğŸ‘ï¼Œè¿™é¡µè®²çš„æ˜¯ **Jacobianï¼ˆé›…å¯æ¯”çŸ©é˜µï¼‰**ï¼Œå®ƒæ˜¯æ¢¯åº¦åœ¨é«˜ç»´å‡½æ•°ä¸­çš„è‡ªç„¶æ¨å¹¿ã€‚æˆ‘å¸®ä½ ç³»ç»Ÿè§£é‡Šä¸€ä¸‹ï¼š

---

## 1. èƒŒæ™¯ï¼šå‡½æ•°è¾“å…¥è¾“å‡ºçš„ç»´åº¦

* **æ ‡é‡å‡½æ•°**ï¼š
  è¾“å…¥ $x \in \mathbb{R}^d$ï¼Œè¾“å‡º $y \in \mathbb{R}$ã€‚

  * æ¢¯åº¦ $\nabla f(x)$ æ˜¯ä¸€ä¸ª **$d$-ç»´å‘é‡**ã€‚

* **å‘é‡å‡½æ•°**ï¼š
  è¾“å…¥ $x \in \mathbb{R}^d$ï¼Œè¾“å‡º $y \in \mathbb{R}^o$ã€‚

  * æ¯ä¸ªè¾“å‡ºåˆ†é‡ $y_i$ éƒ½æœ‰è‡ªå·±çš„æ¢¯åº¦ï¼ˆå¯¹è¾“å…¥çš„åå¯¼ï¼‰ã€‚
  * æŠŠè¿™äº›æ¢¯åº¦å †å èµ·æ¥ï¼Œå°±å¾—åˆ°ä¸€ä¸ª **çŸ©é˜µ** â€”â€” é›…å¯æ¯”çŸ©é˜µ (Jacobian)ã€‚

---

## 2. å®šä¹‰ï¼šJacobian çŸ©é˜µ

$$
\partial f(x) =
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_d} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_o}{\partial x_1} & \cdots & \frac{\partial y_o}{\partial x_d}
\end{bmatrix}
\quad \sim (o,d)
$$

* çŸ©é˜µå¤§å°æ˜¯ $(o, d)$ã€‚
* ç¬¬ $i$ è¡Œæ˜¯è¾“å‡º $y_i$ å¯¹è¾“å…¥æ‰€æœ‰åˆ†é‡çš„æ¢¯åº¦ã€‚

ç‰¹æ®Šæƒ…å†µï¼š

* å½“ $o=1$ï¼šJacobian å°±é€€åŒ–æˆæ¢¯åº¦å‘é‡ã€‚
* å½“ $d=1$ï¼šJacobian å°±æ˜¯æ™®é€šçš„ä¸€ç»´å¯¼æ•°ã€‚

---

## 3. é‡è¦æ€§è´¨ï¼šé“¾å¼æ³•åˆ™ (Composition)

å‡è®¾æœ‰å¤åˆå‡½æ•° $f(g(x))$ï¼Œé‚£ä¹ˆå®ƒçš„ Jacobian æ˜¯ï¼š

$$
\partial [f(g(x))] = [\partial f(\cdot)] \, \partial g(x)
$$

è¿™è¯´æ˜ **å¤åˆå‡½æ•°çš„å¯¼æ•°å°±æ˜¯ä¸¤ä¸ª Jacobian çš„çŸ©é˜µä¹˜æ³•**ã€‚
â†’ è¿™å°±æ˜¯å¤šå…ƒå‡½æ•°çš„ **é“¾å¼æ³•åˆ™**ã€‚

åœ¨æ·±åº¦å­¦ä¹ é‡Œï¼Œè¿™æ­£æ˜¯ **åå‘ä¼ æ’­ (backpropagation)** çš„æ•°å­¦åŸºç¡€ï¼šæ¯ä¸€å±‚çš„æ¢¯åº¦ç”± Jacobian é“¾å¼ç›¸ä¹˜å¾—åˆ°ã€‚

---

## 4. å‡ ä½•æ„ä¹‰

Jacobian å¯ä»¥çœ‹ä½œæ˜¯å‡½æ•°åœ¨æŸç‚¹é™„è¿‘çš„ **æœ€ä½³çº¿æ€§è¿‘ä¼¼**ï¼š

$$
\tilde{f}(x) \approx f(x_0) + \partial f(x_0) (x - x_0)
$$

* è¿™å…¶å®å°±æ˜¯ **Taylor ä¸€é˜¶å±•å¼€**ã€‚
* æ„æ€æ˜¯ï¼šåœ¨ä¸€ä¸ªå°é‚»åŸŸé‡Œï¼Œå‡½æ•° $f$ çœ‹èµ·æ¥å°±åƒä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œè€Œè¿™ä¸ªå˜æ¢ç”± Jacobian çŸ©é˜µç»™å‡ºã€‚

---

## 5. ä¸¾ä¸ªä¾‹å­

è®¾

$$
f(x,y) =
\begin{bmatrix}
x^2 + y \\
\sin(x) \\
y^2
\end{bmatrix}
$$

é‚£ä¹ˆ Jacobian æ˜¯ï¼š

$$
J =
\begin{bmatrix}
2x & 1 \\
\cos(x) & 0 \\
0 & 2y
\end{bmatrix}
\quad (3 \times 2 \text{ çŸ©é˜µ})
$$

è§£é‡Šï¼š

* æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè¾“å‡ºçš„æ¢¯åº¦ã€‚
* Jacobian æ•æ‰äº†å‡½æ•°åœ¨è¾“å…¥ç©ºé—´ $(x,y)$ åˆ°è¾“å‡ºç©ºé—´çš„ã€Œå±€éƒ¨çº¿æ€§å…³ç³»ã€ã€‚

---

âœ… **æ€»ç»“**

* Jacobian æ˜¯ã€Œå¤šè¾“å‡ºå‡½æ•°çš„æ¢¯åº¦ã€ï¼Œå½¢çŠ¶ä¸º $(è¾“å‡ºç»´åº¦, è¾“å…¥ç»´åº¦)$ã€‚
* å®ƒæ¨å¹¿äº†åå¯¼ã€æ¢¯åº¦çš„æ¦‚å¿µã€‚
* Jacobian çš„é“¾å¼æ³•åˆ™æœ¬è´¨ä¸Šå°±æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œè¿™ä¹Ÿæ˜¯åå‘ä¼ æ’­çš„æ ¸å¿ƒã€‚
* å®ƒç»™å‡ºäº†å‡½æ•°åœ¨æŸç‚¹é™„è¿‘çš„æœ€ä½³çº¿æ€§è¿‘ä¼¼ï¼ˆTaylor ä¸€é˜¶å±•å¼€ï¼‰ã€‚
**PyTorch ä»£ç **ï¼Œç®—ä¸€ä¸ªäºŒç»´å‡½æ•°çš„ Jacobianï¼Œå¹¶éªŒè¯é“¾å¼æ³•åˆ™ï¼Ÿ

````


---

---

````ad-note
éå¸¸å¥½ ğŸ‘ï¼Œè¿™ä¸€é¡µä¸»è¦æ˜¯åœ¨æé†’ä½ ä¸€ä¸ªç»å¸¸å®¹æ˜“å¿½è§†çš„é—®é¢˜ï¼š**Jacobian çš„ç»´åº¦ä¼šæ ¹æ®ä½ æŠŠè°å½“ä½œè¾“å…¥è€Œä¸åŒ**ã€‚

æˆ‘å¸®ä½ æ‹†å¼€è§£é‡Šï¼š

---

## 1. å‡½æ•°ä¾‹å­

ç»™å®šä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼š

$$
y = Wx
$$

* $W \in \mathbb{R}^{o \times d}$
* $x \in \mathbb{R}^d$
* è¾“å‡º $y \in \mathbb{R}^o$

---

## 2. æŠŠ $x$ å½“ä½œè¾“å…¥æ—¶

* è¾“å‡ºæ˜¯ $o$-ç»´ï¼Œè¾“å…¥æ˜¯ $d$-ç»´ã€‚
* Jacobian çš„å½¢çŠ¶å°±æ˜¯ $(o, d)$ã€‚
* å®é™…ä¸Šï¼Œ$\partial_x [Wx] = W$ã€‚

  * ç›´è§‚ç†è§£ï¼šçº¿æ€§å‡½æ•°å¯¹è¾“å…¥ $x$ çš„å¯¼æ•°å°±æ˜¯æƒé‡çŸ©é˜µæœ¬èº«ã€‚

---

## 3. æŠŠ $W$ å½“ä½œè¾“å…¥æ—¶

* è¿™æ—¶å€™è¾“å…¥æ˜¯çŸ©é˜µ $W$ï¼Œç»´åº¦æ˜¯ $(o, d)$ã€‚
* Jacobian å½¢çŠ¶ä¼šæ˜¯ $(o, o, d)$ã€‚

  * å› ä¸ºæ¯ä¸ªè¾“å‡ºåˆ†é‡ $y_i$ å¯¹ $W$ çš„æ¯ä¸ªåˆ†é‡ $W_{jk}$ éƒ½æœ‰åå¯¼ã€‚

ä½†è¿™æ ·å†™å¤ªå¤æ‚äº†ã€‚äºæ˜¯ä¹¦é‡Œæåˆ°ï¼š

* æˆ‘ä»¬å¯ä»¥æŠŠ $W$ â€œå±•å¹³â€ä¸ºä¸€ä¸ªå‘é‡ $\text{vec}(W) \in \mathbb{R}^{od}$ã€‚
* é‚£ä¹ˆ Jacobian å°±æ˜¯ä¸€ä¸ª $(o, od)$ çš„çŸ©é˜µã€‚

è¿™æ ·æ›´æ–¹ä¾¿å†™å…¬å¼ï¼Œä¹Ÿé¿å…äº†å¤šé‡ä¸‹æ ‡çš„æ··ä¹±ã€‚

---

## 4. ä½œè€…çš„é‡ç‚¹æé†’

* åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸ç›´æ¥æŠŠè¾“å…¥å’Œè¾“å‡º **çœ‹ä½œå‘é‡**ï¼Œè¿™åœ¨è®°å·ä¸Šå°±è¶³å¤Ÿäº†ã€‚
* ä½†æ˜¯ï¼š

  * çœŸæ­£çš„ Jacobian å°ºå¯¸è¿˜è¦è€ƒè™‘ batch ç»´åº¦å’ŒçŸ©é˜µå±•å¼€ï¼Œå¯èƒ½æ›´å¤§ã€‚
  * æ‰€ä»¥åœ¨å®é™…æ¨å¯¼æ—¶ï¼Œè¦å°å¿ƒè¿™äº›â€œéšè—çš„ç»´åº¦â€ã€‚
* åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šæ˜¾å¼æ„é€ åºå¤§çš„ Jacobianï¼ˆè®¡ç®—å’Œå­˜å‚¨ä»£ä»·å¤ªå¤§ï¼‰ï¼Œè€Œæ˜¯ç”¨ **vector-Jacobian product (VJP)** æ¥é«˜æ•ˆè®¡ç®—éœ€è¦çš„æ¢¯åº¦ã€‚

  * è¿™æ­£æ˜¯è‡ªåŠ¨å¾®åˆ†æ¡†æ¶ï¼ˆPyTorchã€TensorFlowï¼‰çš„åšæ³•ã€‚

---

## 5. æ€»ç»“ä¸€å¥è¯

è¿™ä¸€é¡µæ˜¯åœ¨è¯´æ˜ï¼š

* Jacobian çš„â€œå½¢çŠ¶â€å–å†³äºä½ æŠŠè°å½“ä½œè¾“å…¥ã€‚
* å¯¹ $x$ æ¥è¯´ï¼ŒJacobian å°±æ˜¯ $W$ï¼Œå½¢çŠ¶ $(o,d)$ã€‚
* å¯¹ $W$ æ¥è¯´ï¼ŒJacobian æ˜¯æ›´é«˜ç»´çš„ï¼Œå¯ä»¥é€šè¿‡ **å±•å¼€ (vectorization)** å˜æˆ $(o,od)$ã€‚
* åœ¨æ·±åº¦å­¦ä¹ é‡Œï¼Œæˆ‘ä»¬ä¸ä¼šçœŸçš„å†™å‡ºåºå¤§çš„ Jacobianï¼Œè€Œæ˜¯é€šè¿‡ **å‘é‡-é›…å¯æ¯”ç§¯ (VJP)** æ¥åšé«˜æ•ˆè®¡ç®—ï¼Œè¿™ä¹Ÿæ˜¯åå‘ä¼ æ’­çš„æ ¸å¿ƒæŠ€å·§ã€‚

---

è¦ä¸è¦æˆ‘ç»™ä½ å†™ä¸ª **PyTorch å°å®éªŒ**ï¼šåˆ†åˆ«è®¡ç®— $\partial y / \partial x$ å’Œ $\partial y / \partial W$ï¼Œç„¶åå±•ç¤º Jacobian çš„å½¢çŠ¶å·®å¼‚ï¼Ÿ

````

---
---


````ad-note
å¥½ ğŸ‘Œï¼Œè¿™ä¸€é¡µå…¶å®æ˜¯æŠŠå‰é¢è®²çš„ **æ–¹å‘å¯¼æ•° (directional derivative)** åº”ç”¨åˆ°ä¼˜åŒ–é—®é¢˜ä¸Šï¼Œè§£é‡Šäº†ä»€ä¹ˆæ˜¯ **ä¸‹é™æ–¹å‘ (descent direction)**ï¼Œä»¥åŠä¸ºä»€ä¹ˆâ€œè´Ÿæ¢¯åº¦æ–¹å‘â€æ˜¯æœ€é™¡ä¸‹é™çš„æ–¹å‘ã€‚

æˆ‘åˆ†æˆå‡ æ­¥æ¥è§£é‡Šï¼š

---

## 1. ä»€ä¹ˆæ˜¯ä¸‹é™æ–¹å‘ï¼Ÿ

æˆ‘ä»¬æƒ³æœ€å°åŒ–ä¸€ä¸ªå‡½æ•° $f(x)$ã€‚

* åœ¨ç‚¹ $x_{t-1}$ï¼Œé€‰æ‹©ä¸€ä¸ªç§»åŠ¨æ–¹å‘ $p_t$ã€‚
* å¦‚æœåœ¨è¿™ä¸ªæ–¹å‘ä¸Šï¼Œå‡½æ•°å€¼çš„ç¬æ—¶å˜åŒ–ç‡æ˜¯ **éæ­£çš„**ï¼ˆå°äºç­‰äº 0ï¼‰ï¼Œå°±è¯´æ˜å¾€è¿™ä¸ªæ–¹å‘èµ°èƒ½è®©å‡½æ•°ä¸‹é™ã€‚

æ•°å­¦è¡¨è¾¾ï¼š

$$
p_t \text{ is a descent direction } \;\; \iff \;\; D_{p_t} f(x_{t-1}) \le 0
$$

è¿™é‡Œç”¨åˆ°äº†æ–¹å‘å¯¼æ•°çš„å®šä¹‰ã€‚

---

## 2. ç”¨æ–¹å‘å¯¼æ•°å…¬å¼å±•å¼€

æ ¹æ®ä¹‹å‰çš„å…¬å¼ï¼š

$$
D_{p_t} f(x_{t-1}) = \langle \nabla f(x_{t-1}), \; p_t \rangle
$$

* ==å³æ¢¯åº¦å’Œæ–¹å‘å‘é‡çš„ç‚¹ç§¯ã€‚==

è¿›ä¸€æ­¥å†™æˆï¼š

$$
D_{p_t} f(x_{t-1}) = \|\nabla f(x_{t-1})\| \, \|p_t\| \cos(\alpha)
$$

å…¶ä¸­ $\alpha$ æ˜¯ $p_t$ å’Œæ¢¯åº¦ $\nabla f(x_{t-1})$ çš„å¤¹è§’ã€‚

---

## 3. å“ªäº›æ–¹å‘æ˜¯ä¸‹é™æ–¹å‘ï¼Ÿ

* å¦‚æœ $\cos(\alpha) > 0$ï¼Œç‚¹ç§¯æ˜¯æ­£çš„ â†’ ä¸Šå‡æ–¹å‘ã€‚
* å¦‚æœ $\cos(\alpha) < 0$ï¼Œç‚¹ç§¯æ˜¯è´Ÿçš„ â†’ ä¸‹é™æ–¹å‘ã€‚

æ‰€ä»¥åªè¦ $p_t$ å’Œæ¢¯åº¦çš„å¤¹è§’åœ¨ $[\pi/2, 3\pi/2]$ ä¹‹é—´ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªä¸‹é™æ–¹å‘ã€‚

æ¢å¥è¯è¯´ï¼š**ä»»ä½•ä¸æ¢¯åº¦æ–¹å‘ç›¸å·®å¤§äº 90Â° çš„æ–¹å‘ï¼Œéƒ½æ˜¯ä¸‹é™æ–¹å‘ã€‚**

---

## 4. æœ€é™¡ä¸‹é™æ–¹å‘

åœ¨æ‰€æœ‰ä¸‹é™æ–¹å‘é‡Œï¼Œå“ªä¸ªä¸‹é™å¾—æœ€å¿«ï¼Ÿ

* ç‚¹ç§¯æœ€å°ï¼ˆè´Ÿå¾—æœ€å¤§ï¼‰çš„æ—¶å€™ï¼Œä¹Ÿå°±æ˜¯ $\cos(\alpha) = -1$ã€‚
* è¿™å¯¹åº”ç€ $\alpha = \pi$ï¼Œå³æ–¹å‘ $p_t = -\nabla f(x_{t-1})$ã€‚

å› æ­¤ï¼š

$$
p_t = - \nabla f(x_{t-1})
$$

å°±æ˜¯æ‰€è°“çš„ **æœ€é™¡ä¸‹é™æ–¹å‘ (steepest descent direction)**ã€‚

---

## 5. ç›´è§‚ç†è§£

* æ¢¯åº¦æ–¹å‘ï¼šå‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚
* è´Ÿæ¢¯åº¦æ–¹å‘ï¼šå‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘ã€‚
* å…¶ä»–ä¸‹é™æ–¹å‘ï¼šä¹Ÿèƒ½è®©å‡½æ•°å˜å°ï¼Œä½†ä¸å¦‚è´Ÿæ¢¯åº¦æ–¹å‘é«˜æ•ˆã€‚

---

âœ… **æ€»ç»“**

* ä¸‹é™æ–¹å‘å®šä¹‰ï¼šæ–¹å‘å¯¼æ•° $\le 0$ã€‚
* æ•°å­¦æ¡ä»¶ï¼šä¸æ¢¯åº¦å¤¹è§’åœ¨ $90Â°$ åˆ° $270Â°$ ä¹‹é—´ã€‚
* æœ€é™¡ä¸‹é™æ–¹å‘ï¼šè´Ÿæ¢¯åº¦æ–¹å‘ã€‚
* è¿™å°±æ˜¯ **æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent)** çš„æ•°å­¦ä¾æ®ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€ä¸ªäºŒç»´å‡½æ•°ç­‰é«˜çº¿å›¾ï¼Œæ ‡å‡ºæ¢¯åº¦æ–¹å‘ã€ä¸‹é™æ–¹å‘åŒºé—´ï¼Œä»¥åŠæœ€é™¡ä¸‹é™æ–¹å‘ï¼Ÿè¿™æ ·ä½ èƒ½ç›´è§‚çœ‹åˆ°è¿™éƒ¨åˆ†å†…å®¹ã€‚
![[Pasted image 20250924004031.png]]
````

```ad-help
## 2. ä»»æ„æ–¹å‘çš„ç§»åŠ¨ = å„åæ ‡æ–¹å‘çš„ç»„åˆ

å‡è®¾æˆ‘ä»¬æƒ³èµ°çš„æ–¹å‘æ˜¯ $p = (p_x, p_y)$ï¼Œé‚£å®é™…ä¸Šæ˜¯ï¼š

* åœ¨ $x$ æ–¹å‘èµ°äº† $p_x$ å•ä½ï¼Œ
* åœ¨ $y$ æ–¹å‘èµ°äº† $p_y$ å•ä½ã€‚

è¿™å°±æ˜¯ä¸€ä¸ªâ€œçº¿æ€§ç»„åˆâ€ï¼šä½ é€‰çš„æ–¹å‘ï¼Œæœ¬è´¨ä¸Šæ˜¯ **æ²¿ç€åæ ‡è½´æ–¹å‘çš„åŠ æƒç§»åŠ¨**ã€‚

---

## 3. å‡½æ•°çš„å˜åŒ–ç‡ = å„åæ ‡å˜åŒ–ç‡çš„åŠ æƒå’Œ

é‚£ä¹ˆå‡½æ•°å€¼çš„å˜åŒ–ç‡ï¼Œè‡ªç„¶å°±æ˜¯ï¼š

$$
\Delta f \approx \frac{\partial f}{\partial x} \cdot p_x + \frac{\partial f}{\partial y} \cdot p_y
$$

è¿™å…¶å®å°±æ˜¯æ¢¯åº¦å’Œæ–¹å‘çš„ç‚¹ç§¯ï¼š

$$
\nabla f(x,y) \cdot p
$$

```



---
---

````ad-note
##  ä¸‹åŠéƒ¨åˆ†ï¼šéå‡¸æ¨¡å‹å’Œå‡¸æ¨¡å‹çš„å¯¹æ¯”

> *â€œnon-trivial model is non-convex â€¦ This is in contrast to â€¦ support vector machines, which maintain non-linearity while allowing for convex optimization.â€*

è§£é‡Šï¼š

* **éå¹³å‡¡çš„æ¨¡å‹ï¼ˆæ¯”å¦‚ç¥ç»ç½‘ç»œï¼‰**ï¼šå¤§å¤šæ•°éƒ½æ˜¯ **éå‡¸çš„**ï¼Œå› æ­¤ä¼˜åŒ–ä¸­ä¼šæœ‰å¤šä¸ªåœç‚¹ï¼ˆå±€éƒ¨æœ€å°å€¼ã€éç‚¹ï¼‰ã€‚
* **å‡¸ä¼˜åŒ–çš„ä¼˜åŠ¿**ï¼š

  * æ¯”å¦‚ SVMï¼Œè™½ç„¶æ¨¡å‹å¯ä»¥å¼•å…¥éçº¿æ€§ï¼ˆé€šè¿‡æ ¸å‡½æ•°ï¼‰ï¼Œä½†å®ƒçš„ä¼˜åŒ–é—®é¢˜ä»ç„¶æ˜¯ **å‡¸ä¼˜åŒ–**ã€‚
  * å‡¸ä¼˜åŒ–åªæœ‰ä¸€ä¸ªå…¨å±€æœ€ä¼˜ç‚¹ï¼Œæ‰€ä»¥æ›´å®¹æ˜“è§£ã€‚
* **ç°å®ä¸­çš„ç»éªŒ**ï¼š

  * å°½ç®¡éå‡¸é—®é¢˜å¾ˆéš¾ä¿è¯å…¨å±€æœ€ä¼˜ï¼Œ
  * ä½†åœ¨å®è·µä¸­ï¼ˆæ¯”å¦‚æ·±åº¦å­¦ä¹ ï¼‰ï¼Œåªè¦åˆå§‹åŒ–åˆç†ï¼Œä¼˜åŒ–ç®—æ³•ï¼ˆSGDã€Adamï¼‰å¾€å¾€è¿˜æ˜¯èƒ½æ”¶æ•›åˆ°ä¸€ä¸ª **æ•ˆæœä¸é”™çš„ç‚¹**ï¼Œåœ¨ç»éªŒæ€§èƒ½ä¸Šè¶³å¤Ÿå¥½ã€‚
````

---
---

###  Momentum æ¢¯åº¦ä¸‹é™ç¬”è®°

* **é—®é¢˜**ï¼šæ™®é€šæ¢¯åº¦ä¸‹é™æ›´æ–°æ–¹å‘å˜åŒ–å¤§ï¼Œå®¹æ˜“â€œæŠ–åŠ¨â€ï¼ˆå°¤å…¶åœ¨å‡¸å‡½æ•°ä¸‹ï¼Œè¿ç»­ä¸¤æ­¥æ¢¯åº¦ç”šè‡³å¯èƒ½æ­£äº¤ï¼‰ã€‚
* **åŠ¨é‡æ€æƒ³**ï¼šä¿ç•™ä¸€éƒ¨åˆ†ä¸Šä¸€æ¬¡çš„æ›´æ–°æ–¹å‘ï¼Œä½¿è¿­ä»£æ›´å¹³æ»‘ã€‚
* **æ›´æ–°å…¬å¼**ï¼š

  $$
  g_t = -\eta_t \nabla f(x_{t-1}) + \lambda g_{t-1}
  $$

  $$
  x_t = x_{t-1} + g_t
  $$
* **ç›´è§‚ç±»æ¯”**ï¼šåƒå°çƒä¸‹å±±æ—¶å¸¦æœ‰æƒ¯æ€§ï¼Œä¸ä¼šå› æ¯ä¸€æ­¥çš„å±€éƒ¨å¡åº¦å‰§çƒˆæ”¹å˜æ–¹å‘ã€‚
* **æ•ˆæœ**ï¼šå‡å°‘éœ‡è¡ã€åŠ å¿«æ”¶æ•›ã€‚


## Adam (Adaptive Moment Estimation)

* **æ€æƒ³**ï¼šç»“åˆ **Momentum**ï¼ˆä¸€é˜¶åŠ¨é‡ï¼Œæ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼‰å’Œ **RMSProp**ï¼ˆäºŒé˜¶åŠ¨é‡ï¼Œå¹³æ–¹æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡ï¼‰ã€‚
* **å…¬å¼**ï¼š

  * ä¸€é˜¶åŠ¨é‡ï¼ˆç±»ä¼¼åŠ¨é‡æ³•ï¼‰ï¼š

    $$
    m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla f(x_t)
    $$
  * äºŒé˜¶åŠ¨é‡ï¼ˆç±»ä¼¼RMSPropï¼‰ï¼š

    $$
    v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla f(x_t))^2
    $$
  * åå·®ä¿®æ­£ï¼š

    $$
    \hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}
    $$
  * å‚æ•°æ›´æ–°ï¼š

    $$
    x_t = x_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
    $$
* **ç‰¹ç‚¹**ï¼š

  * è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆä¸åŒå‚æ•°å¯æœ‰ä¸åŒæ­¥é•¿ï¼‰
  * æ”¶æ•›æ›´å¿«ï¼Œè°ƒå‚è¾ƒå°‘
  * é»˜è®¤å‚æ•°ï¼ˆ$\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$ï¼‰æ•ˆæœå·²å¾ˆå¥½
* **ç›´è§‚ç±»æ¯”**ï¼šåœ¨â€œæœ‰æƒ¯æ€§çš„å°çƒâ€åŸºç¡€ä¸Šï¼Œè¿˜ä¼šæ ¹æ®å†å²æ¢¯åº¦å¤§å°è°ƒæ•´æ¯ä¸ªæ–¹å‘çš„â€œæ­¥å¹…â€ã€‚



---
---


````ad-note

### 3. ä¸ KL æ•£åº¦çš„å…³ç³»

äº‹å®ä¸Šï¼Œå¯ä»¥è¯æ˜ï¼š

$$
s^* = \arg\max_s \mathbb{E}_{x \sim p(x)} [\log f(x; s)]
$$

ç­‰ä»·äºï¼š

$$
s^* = \arg\min_s D_{\text{KL}}(p(x) \, \| \, f(x; s)),
$$

ä¹Ÿå°±æ˜¯ï¼š**æå¤§ä¼¼ç„¶ä¼°è®¡ç­‰ä»·äºæœ€å°åŒ–çœŸå®åˆ†å¸ƒå’Œæ¨¡å‹åˆ†å¸ƒçš„ KL æ•£åº¦**ã€‚

æ¢å¥è¯è¯´ï¼ŒMLE é€‰å‡ºçš„åˆ†å¸ƒå‚æ•°ï¼Œæ˜¯åœ¨ KL æ„ä¹‰ä¸‹æœ€æ¥è¿‘çœŸå®åˆ†å¸ƒçš„é‚£ä¸ªã€‚

---
æ¨å¯¼å¾ˆçŸ­ä¹Ÿå¾ˆå…³é”®ã€‚è®¾çœŸå®åˆ†å¸ƒä¸º $p(x)$ï¼Œæ¨¡å‹æ—ä¸º $f(x;s)$ã€‚å®šä¹‰ **KL æ•£åº¦**ï¼ˆç¦»æ•£/è¿ç»­åªå·®ç§¯åˆ†å·ï¼‰ï¼š

$$
D_{\mathrm{KL}}\!\left(p \,\|\, f(\cdot;s)\right)
= \mathbb{E}_{x\sim p}\!\left[\log \frac{p(x)}{f(x;s)}\right]
= \mathbb{E}_{p}[\log p(x)] - \mathbb{E}_{p}[\log f(x;s)] .
$$

æ³¨æ„ä¸¤ç‚¹ï¼š

1. $\mathbb{E}_{p}[\log p(x)]$ **ä¸å‚æ•° $s$ æ— å…³**ï¼›
2. ä¸ºä½¿ KL æœ‰é™ï¼Œéœ€è¦å‡è®¾ï¼šåªè¦ $p(x)>0$ åˆ™ $f(x;s)>0$ï¼ˆå³ $\mathrm{supp}(p)\subseteq \mathrm{supp}(f)$ï¼‰ã€‚

å› æ­¤ï¼Œå¯¹ä»»æ„å¯è¡Œ $s$,

$$
\arg\min_{s} D_{\mathrm{KL}}\!\left(p \,\|\, f(\cdot;s)\right)
= \arg\min_{s}\Big(\mathbb{E}_{p}[\log p(x)]-\mathbb{E}_{p}[\log f(x;s)]\Big)
= \arg\max_{s}\mathbb{E}_{p}[\log f(x;s)] .
$$

è¿™å°±å¾—åˆ°

$$
s^*=\arg\max_{s}\mathbb{E}_{x\sim p}[\log f(x;s)]
\quad \Longleftrightarrow \quad
s^*=\arg\min_{s} D_{\mathrm{KL}}\!\left(p \,\|\, f(\cdot;s)\right).
$$

---

### ä¸ MLE çš„å…³ç³»ï¼ˆæ ·æœ¬è¿‘ä¼¼ï¼‰

æœ‰ $n$ ä¸ª i.i.d. æ ·æœ¬ $\{x_i\}_{i=1}^n\sim p$ã€‚ç”±å¤§æ•°å®šå¾‹ï¼Œ

$$
\mathbb{E}_{p}[\log f(x;s)]
\approx \frac{1}{n}\sum_{i=1}^n \log f(x_i;s) .
$$

äºæ˜¯æœ€å¤§åŒ–æœŸæœ›å¯¹æ•°ä¼¼ç„¶ç­‰ä»·äºæœ€å¤§åŒ–**ç»éªŒå¯¹æ•°ä¼¼ç„¶**ï¼ˆå³ MLEï¼‰ï¼›è€Œè¿™åˆç­‰ä»·äºæœ€å°åŒ– $D_{\mathrm{KL}}(p\|f(\cdot;s))$ çš„ç»éªŒè¿‘ä¼¼ã€‚

````

---
---

