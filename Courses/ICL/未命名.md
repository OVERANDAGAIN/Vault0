#courses


ICL relies entirely on pretrained language models and keeps their parameters unchanged.[^1]


# How
>At its core, in-context learning works by conditioning a large language model (LLM) on a prompt that includes a set of examples (input/output pairs or in-context examples) typically written in natural language as part of the input sequence. These examples, often drawn from a dataset, are not used to retrain the model but are fed directly into its context window. 

![[image_1035707217.png]]

![[traditional-llm-training-architecture.png]]


---









# FootNote

[^1]: [What is In-Context Learning (ICL)? \| IBM](https://www.ibm.com/think/topics/in-context-learning)