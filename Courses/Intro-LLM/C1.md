---
创建时间: 2025-七月-23日  星期三, 3:53:25 下午
---
[[Intro-LLM]]

# QAs
# 如何理解这里的“数据稀疏性”？

>n 元语言模型从整体上看与训练数据规模和模型的阶数(考虑上下文的数量)有较大的关系,  不同的平滑算法在不同情况下的表现有较大的差距。虽然平滑算法较好地解决了零概率问题,但  是基于稀疏表示的 n 元语言模型仍然有以下三个较为明显的缺点。
>  (1)无法对长度超过 n 的上下文建模。  
>  (2)依赖人工设计规则的平滑技术。  
>  (3)==当 n 增大时,数据的稀疏性随之增大==,模型的参数量更是呈指数级增加,受数据稀疏问  题的影响,其参数难以被准确学习。

### simply：训练数据少


这里的“稀疏”指的是**数据稀疏性（data sparsity）**，在 n 元语言模型（n-gram language model）中，它通常有如下含义：

---

### ✅ **定义：**
在语言模型中，“稀疏”意味着：**在训练语料中，大多数可能的 n 元短语（n-grams）根本没有出现，或者出现次数非常少**。

---

### ✅ **具体体现：**

比如你用三元模型（trigram）：
- 模型希望根据前两个词预测第三个词，如：
  ```
  P(w3 | w1, w2)
  ```
- 但在真实的语料中，很多词对 `(w1, w2)` 是从未出现过的，这样的三元组 `(w1, w2, w3)` 就没有统计数据。

这就导致两个问题：
1. **无法估计概率**：因为没见过，MLE（最大似然估计）会给它概率0。
2. **长上下文更严重**：随着 n 增大，比如用五元模型，没见过的组合就更多，稀疏性更严重。

---

### ✅ **总结：**
你提到的“基于稀疏表示的 n 元语言模型”中，“稀疏”指的是：
> **随着 n 的增加，可能的 n 元组组合数量呈指数增长，但实际在训练语料中出现的只是极少数，因此数据和模型都呈现高度稀疏的特性，难以学习准确的概率分布。**


# Scaling Laws
## simply:**更大模型 + 更多数据 → 更低损失**

“**缩放准则（Scaling Laws）**”是指在深度学习模型（尤其是语言模型）中，**模型性能与计算资源之间存在可预测的规律**，当模型的规模（如参数数量）、训练数据量和计算量增加时，模型的性能（如损失或准确率）会按一定的趋势提升。

这个概念最早系统性地由 OpenAI 在 2020 年的论文 *“Scaling Laws for Neural Language Models”* 中提出，核心是揭示：**损失随着规模的增长呈幂律下降**。

---

## 🌟 一、核心思想：幂律缩放关系（Power Law）

对于语言模型，存在如下经验关系：

$$
L(N, D, C) \approx A \cdot N^{-\alpha} + B \cdot D^{-\beta} + C^{-\gamma}
$$

其中：

* $L$：训练或验证损失（如交叉熵）
* $N$：模型参数量（Model size）
* $D$：训练语料数据量（Data size）
* $C$：训练计算量（Compute，如 FLOPs）
* $\alpha, \beta, \gamma$：缩放指数，经验上都约为 0.07–0.1
* 损失随 N、D、C 的增加而**幂律下降**

---

## 📈 二、主要结论

1. **更大模型 + 更多数据 → 更低损失**（但不是线性，而是幂律下降）
2. **存在最优计算分配比例**：

   * 若计算量固定，有个最优的模型大小与数据量组合，使得效果最优（“compute-optimal”）。
3. **超大模型也有收益**：

   * 即使模型非常大，只要有足够的数据与计算，性能仍能持续提升。

---
## 🧠 四、后续扩展方向

* **Chinchilla Scaling Laws（2022, DeepMind）**：指出很多大模型（如 GPT-3）其实训练数据不足，浪费了参数。

  * 给出建议：**与其增大参数量，不如增加数据量训练轮数**。
* **Alignment Scaling Laws**：研究对齐（RLHF）阶段的缩放规律。

---

## 📌 总结一句话：

> **Scaling Laws 表明：只要有足够的数据和计算资源，模型规模继续增加是有意义的，性能提升是可预测的，且有幂律规律可以拟合和指导设计。**


# 自注意力机制使模型能够识别不同输入部分的重要性,而不受距离的影响
## simply: 注意力计算公式
这句话 **“自注意力机制使模型能够识别不同输入部分的重要性，而不受距离的影响”** 是对自注意力机制（self-attention）核心优势的精炼总结，其背后包含了对比于传统 RNN/CNN 的结构特点与计算方式。下面是它为什么可以这么说的详细解释：

---

## ✅ 1. 自注意力是**全局对齐机制**（global matching）

* 在自注意力中，每个位置 $i$ 的表示 $z_i$ 是通过对整个序列中所有位置 $j$ 的加权求和得到的：

  $$
  z_i = \sum_{j=1}^L \text{softmax}(e_{ij}) \cdot v_j
  $$

* 其中 $e_{ij} = q_i^\top k_j$ 表示位置 $i$ 与 $j$ 的**相关性**。

📌 这意味着：**每个词都可以“看到”其他所有词**，无论它们在序列中的位置相隔多远。

---

## ✅ 2. 相比 RNN / CNN：不再依赖于“距离近”的局部建模

| 模型                 | 感受野 / 可达路径 | 距离影响         |
| ------------------ | ---------- | ------------ |
| RNN                | 线性传播，步步传递  | 长距离依赖难（梯度消失） |
| CNN                | 局部卷积，层层堆叠  | 多层叠加才能感知全局   |
| **Self-Attention** | 单层可达所有位置   | **无距离偏差**    |

📌 自注意力不需要一步步传递，**位置远近不影响信息通达性**。

---

## ✅ 3. 注意力权重体现了“重要性”

* 自注意力的核心就是：根据每个词对其他词的注意力分数（如 $\text{softmax}(q_i^\top k_j)$）来决定其贡献大小。

  所以：

  * **谁重要 → 给谁更高的权重**
  * **不重要 → 权重接近 0**

📌 所以“识别重要性”是注意力的本质功能。

---

## ✅ 4. 举个例子理解

句子：

> “虽然下雨了，但是他**坚持**去跑步。”

* 想理解“坚持”，模型应该重点关注“下雨了”；
* 它与“坚持”之间距离较远，但自注意力可以直接对它分配较高的权重。

而在 RNN 中，可能需要一步步传递到达“坚持”位置，效果更差。

---

## ✅ 5. 位置编码补充位置信息

虽然注意力本身与距离无关，但为了让模型理解词序，Transformer 加入了 **位置编码（Positional Encoding）**。
👉 所以最终模型兼具：

* **对距离的“不敏感性”**（能处理长距离依赖）
* **对顺序的可感知能力**

---

## 🔚 总结：

> 自注意力机制之所以“**不受距离影响地识别输入的重要性**”，是因为它用的是全连接计算、无偏的匹配机制，每个词都能访问整个序列，并通过注意力权重直接表达“谁更重要”。


# 层归一化
### ✅ 什么是层归一化（Layer Normalization, LayerNorm）？

**层归一化**是一种用于深度神经网络中的**归一化技术**，其目标是：

> 在每个样本内部，对特征维度进行归一化，从而缓解训练不稳定、加速收敛，并提升泛化性能。

它由 **Jimmy Lei Ba 等人于 2016 年提出**，在 Transformer 中被广泛使用，是其核心构建块之一。

---

## 🔧 层归一化的计算过程

给定一个样本 $\mathbf{x} = [x_1, x_2, \dots, x_H]$（假设是隐藏层维度为 $H$ 的向量），LayerNorm 的计算为：

$$
\mu = \frac{1}{H} \sum_{i=1}^H x_i,\quad
\sigma = \sqrt{ \frac{1}{H} \sum_{i=1}^H (x_i - \mu)^2 + \epsilon }
$$

$$
\text{LayerNorm}(x_i) = \frac{x_i - \mu}{\sigma} \cdot \gamma + \beta
$$

* $\gamma, \beta$ 是可学习的缩放和平移参数，维度同 $H$
* $\epsilon$ 是一个小常数，防止除零
* 所有归一化是在每个样本内部进行的（区别于 BatchNorm）

---

## ✅ 在 Transformer 中怎么用？

Transformer 使用 LayerNorm 两种方式：

1. **Pre-Norm（前归一化）**：

   * LayerNorm 在子层（注意力/FFN）前：

     $$
     x \leftarrow x + \text{SubLayer}(\text{LayerNorm}(x))
     $$
2. **Post-Norm（后归一化）**：

   * LayerNorm 在残差后：

     $$
     x \leftarrow \text{LayerNorm}(x + \text{SubLayer}(x))
     $$

📌 当前主流如 GPT-2/3 等使用 **Pre-LN**，更稳定。

---

## ✅ 总结一句话：

> **层归一化是对每个样本内部进行的标准化处理，提升训练稳定性、加速收敛，并适用于序列模型中的长距离依赖问题，是 Transformer 成功的关键技术之一。**
