---
创建时间: 2025-七月-24日  星期四, 4:02:21 下午
---
[[Intro-LLM]]


## 2.3.1 LLaMA的模型结构
首先是主要采用类似于ChatGPT的架构，即利用transformer块的解码器结构，但不同的模型可能在位置编码，层归一化，激活函数方面有所区别，所以说大致的结构是一样的？？？
LLaMA为例，介绍在Transfomer结构上的改进，主要是三点：
1. 预归一化，把归一化的步骤放在多头注意力层之前，放在前馈层之前；相应地，修改残差层的位置
2. 使用SwiGLU的激活函数，替换原来的ReLU激活函数，这里有Why
3. 使用旋转位置嵌入的位置编码替换原来的位置编码。





## 2.3.2 注意力机制优化

几种优化的思路：
- 近似注意力机制
- 优化硬件的计算

1. 稀疏注意力机制运算，这里包括五种基础的减少计算的方法，现有的一些论文大多是在这几个基础上进行组合，比如：全局注意力，分组注意力，邻居注意力（带状），膨胀注意力（步长更大的带状），随机注意力。很多篇论文的例子，结合了不同的计算方法
2. FlashAttention
3. 多查询注意力，是多头注意力的变体
4. 多头潜在注意力，在DeepSeek-V2中引入的注意力优化模型



---

## 整体PPT的思路：

1. 
	1. 首先讲解LLaMA的相关的改进技术（三个）：预归一化技术，SwiGLU激活函数，RoPE旋转位置嵌入（PPT的内容
	2. 无消融实验，直接展示最终的结果。
	3. 关于与最新论文的结合：LLaMA 4 maybe 一些内容，可能再讨论一下LLaMA历来的几个版本的变化
	4. 大语言模型的结构，主要介绍LLaMA，可能再介绍一些其他相关的结构?
	5. 关于思考的问题，或者自己需要总结汇报的内容:???
2. 
	1. 注意力机制的优化，两个主要的方向，一个是注意力本身的计算优化，减少计算量；另一个是GPU硬件方面的计算优化方法。
	2. 这部分比较偏理论，工程优化，与最新发展如何结合？
	3. 这部分数学公式比较多，如何比较好地去描述这件事?显得不那么枯燥或者是背书?
	4. 原书中的思路不是很好整理，可以考虑以时间为维度去说这件事？



---


