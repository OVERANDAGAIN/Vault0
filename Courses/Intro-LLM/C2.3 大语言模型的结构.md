---
创建时间: 2025-七月-24日  星期四, 4:02:21 下午
---
[[Intro-LLM]]


## 2.3.1 LLaMA的模型结构
首先是主要采用类似于ChatGPT的架构，即利用transformer块的解码器结构，但不同的模型可能在位置编码，层归一化，激活函数方面有所区别，所以说大致的结构是一样的？？？
LLaMA为例，介绍在Transfomer结构上的改进，主要是三点：
1. 预归一化，把归一化的步骤放在多头注意力层之前，放在前馈层之前；相应地，修改残差层的位置
2. 使用SwiGLU的激活函数，替换原来的ReLU激活函数，这里有Why
3. 使用旋转位置嵌入的位置编码替换原来的位置编码。





## 2.3.2 注意力机制优化

几种优化的思路：
- 近似注意力机制
- 优化硬件的计算

1. 稀疏注意力机制运算，这里包括五种基础的减少计算的方法，现有的一些论文大多是在这几个基础上进行组合，比如：全局注意力，分组注意力，邻居注意力（带状），膨胀注意力（步长更大的带状），随机注意力。很多篇论文的例子，结合了不同的计算方法
2. FlashAttention
3. 多查询注意力，是多头注意力的变体
4. 多头潜在注意力，在DeepSeek-V2中引入的注意力优化模型



---

## 整体PPT的思路：

1. 
	1. 首先讲解LLaMA的相关的改进技术（三个）：预归一化技术，SwiGLU激活函数，RoPE旋转位置嵌入（PPT的内容
	2. 无消融实验，直接展示最终的结果。
	3. 关于与最新论文的结合：LLaMA 4 maybe 一些内容，可能再讨论一下LLaMA历来的几个版本的变化
	4. 大语言模型的结构，主要介绍LLaMA，可能再介绍一些其他相关的结构?
	5. 关于思考的问题，或者自己需要总结汇报的内容:???
2. 
	1. 注意力机制的优化，两个主要的方向，一个是注意力本身的计算优化，减少计算量；另一个是GPU硬件方面的计算优化方法。
	2. 这部分比较偏理论，工程优化，与最新发展如何结合？
	3. 这部分数学公式比较多，如何比较好地去描述这件事?显得不那么枯燥或者是背书?
	4. 原书中的思路不是很好整理，可以考虑以时间为维度去说这件事？



---


### 大语言模型结构总览

#### 1. **整体流程**

* 输入文本 → Tokenizer 编码 → 神经网络结构 → Tokenizer 解码 → 输出文本
* 网络部分为主要的学习模块：嵌入层 → Transformer 堆叠 → 输出层

---

#### 2. **Tokenizer：文本编码模块**

* 非神经网络模块，负责将字符串映射为 token 序列
* 支持将常用词组（如“你好”）合并为单个 token 编码，**缩短序列长度、提升推理效率**

---

#### 3. **词表规模与模型结构的权衡**

* 更大的词表 → 更短序列、更快推理
* 但也会 **增加嵌入层与输出层的参数量**
* 实际设计需权衡编码效率与模型复杂度




---



# **Decoder-only 架构**


## 架构定义

* 主流大语言模型（如 LLaMA、Qwen、DeepSeek）采用 **Decoder-only 架构**
* 不再区分编码器与解码器，统一以解码形式处理所有输入输出

## 架构特性

* **单向处理（Causal Attention）**：第 *t* 个 token 仅能访问前 *t* 个 token
* **每次仅预测一个新 token**，逐步构造输出序列
* 可用于语言建模、问答、翻译、代码生成等任务

## 解码器的本质用途

* 无法穷举所有文本 → 转为“逐 token 生成”问题
* 模型以“已知上下文”逐步预测下一个 token



---

# **推理流程中的两阶段机制**

##  推理阶段划分

尽管结构统一，**推理行为**可分为两个阶段：

1. **预装填（Prefilling）**

   * 输入：完整问题 token 序列
   * 并行处理所有 token，**仅输出最后一个 token 的表示**
   * 用于预测回答的起始 token
2. **解码（Decoding）**

   * 输入：问题 token + 已生成 token
   * 每次预测下一个 token，迭代直至结束符 `<eos>`

## 数学形式一致，但计算模式不同

* 两阶段共享相同网络结构和参数
* 差异仅在于：**是否可一次性并行处理**



---



