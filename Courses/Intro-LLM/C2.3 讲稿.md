---
创建时间: 2025-七月-25日  星期五, 11:18:59 上午
---
[[C2.3 大语言模型的结构]]

以下是你提供内容的【台上汇报文稿版本】，适用于学术风格PPT讲解，覆盖细节、逻辑清晰、语言正式、便于口头陈述。分为两大部分：

---

# 一、大语言模型结构概览（对应图1）

大家好，下面我将从大语言模型的整体结构出发，为大家梳理其从输入文本到输出结果的完整推理流程。

如图1所示，当前主流的大语言模型架构（例如 Qwen2、LLaMA3、DeepSeek-V3 等）基本遵循同一个通用框架。其整体流程可以分为三部分：输入前的编码处理、神经网络主体结构、输出后的解码处理。

首先我们来看输入部分。这里使用的是名为 **Tokenizer** 的模块来进行文本编码。需要强调的是，Tokenizer 并非神经网络，而是一个基于规则的离散分词器。它的主要作用是将自然语言文本转化为模型可以识别的编号序列，也就是 token 序列。

比如，“你”可能被编码为 1，“好”是 2，“世”是 3，“界”是 4。为了提高效率，如果“你好”在训练数据中高频共现，我们就可能将其作为一个整体词组，赋予一个新的编号，比如 5，这样“你好世界”就可以被编码为 \[5,3,4]，而不是 \[1,2,3,4]。这样做的优点在于可以**减少 token 数量**，从而降低模型的推理成本，提高运行速度。

但与此同时，也带来了一个权衡问题。为了合并更多词组，我们需要扩大 Tokenizer 的词表，这会显著增加嵌入层（Embedding Layer）与输出层（Output Layer）的参数规模。因此，Tokenizer 的词表不是越大越好，需要在效率与模型大小之间取得平衡。

此外，Tokenizer 中还会添加一些**特殊 token**，用于标记句子结构或控制模型行为。例如：

* `<bos>`：句子起始符（Begin of Sentence）
* `<eos>`：句子终止符（End of Sentence）
  这些特殊标记可以帮助模型识别生成的边界，控制生成流程的启动和停止。

接下来我们进入模型主体部分，也就是我们熟知的神经网络结构。主要由以下三个模块组成：

---

## 1. 嵌入层（Embedding Layer）

Embedding 层的任务是将每个 token 编号映射为一个高维浮点向量，从而让模型能够学习其语义特征。该层的输入是整数编号的 token 序列，输出是一个形如 `[batch_size, sequence_length, embedding_dim]` 的张量。

权重矩阵的形状为 `(vocab_size, embedding_dim)`，其中 vocab\_size 是 Tokenizer 的词表大小，embedding\_dim 是每个 token 的表示维度。与传统的线性层不同，Embedding 层通过查表的方式获取向量，无需矩阵乘法，速度更快、结构更简单。

---

## 2. Transformer 模块（Transformer Blocks）

Transformer 是大语言模型的核心组成部分，负责建模 token 之间的上下文关系，存储和传播语言知识。一个模型通常串联多个结构相同的 Transformer Block，每个 block 内部包含：

* 自注意力机制（Self-Attention）
* 前馈网络（Feed-Forward Network，或 MLP）
* 层归一化（LayerNorm）
* 残差连接（Residual Connection）

每个 block 的输入输出张量保持形状不变，通过残差连接提高了深层网络的训练稳定性。不同架构可能在细节上存在变体，但主干设计基本一致。我们将在后续章节中对每个子模块逐一分析。

---

## 3. 输出层（Output Layer）

模型最后一层是一个线性映射层，用于将 Transformer 输出的特征向量转为对所有可能 token 的预测分布。输出张量的维度为 `[batch_size, sequence_length, vocab_size]`，每个位置上的向量表示对应位置 token 的分类概率。

在推理时，输出可以通过两种方式生成文本：

* **贪婪预测**：直接取概率最大的 token，即 argmax；
* **采样预测**：基于 softmax 归一化后的概率进行采样，常用策略包括 Top-k、Top-p、Temperature 等。

需要补充一点知识点是，部分小模型会**共享 Embedding 层和输出层的权重**，本质上是通过参数共享来节省模型体积。这在参数量较小（如 1B 以下）的模型中比较常见，但需要注意它**不会减少计算量**，仅仅是减小了可训练参数的数量。

---

# 二、大语言模型的推理阶段（对应图2）

在了解完结构之后，我们进入第二部分内容，解释大语言模型的推理是如何执行的。

目前主流的大语言模型采用的是所谓的 **Decoder-only 架构**，也就是整个模型中只包含解码器部分，既没有 encoder，也没有交叉注意力结构。

那么问题来了：既然模型是 Decoder-only，为什么我们还要将推理过程划分为两个阶段——**预装填（Prefilling）** 和 **解码（Decoding）** 呢？

---

## 1. 什么是 Decoder-only 架构？

Decoder 本质上是用来**逐步生成文本**的。由于 token 的空间非常大（词表常在十万以上），不可能一次性枚举所有句子的组合，所以我们只能通过逐步生成 token 的方式来建构文本。

每次生成一个新 token，其输入是前面所有 token 的集合。这就引出了 **因果注意力（Causal Attention）** 的概念。具体来说，当前 token 只能访问它前面的 token，而不能看到未来的 token。比如：

* 第 3 个 token 的生成依赖第 1 到第 3 个 token；
* 它不能访问第 4 个及之后的 token，因为它们尚未生成。

这样的因果结构就构成了一个 **标准的 Transformer Decoder 网络**。

---

## 2. 为什么还要区分两个推理阶段？

虽然结构上统一，但 **模型推理的行为模式是不同的**：

### 🔹 预装填阶段（Prefilling）

* 输入是完整的问题文本（N 个 token）
* 模型并行计算所有 token 的表示，但只保留最后一个 token 的输出，因为它会作为回答的起点
* 中间的 token 输出会被丢弃，因为问题文本是已知的，不需要模型去预测

**优点是：可以并行计算所有问题 token，一次性输入，计算效率高**

---

### 🔹 解码阶段（Decoding）

* 每一步生成一个新的 token，直到遇到 `<eos>` 标志结束
* 每次输入是：问题 token + 当前已生成的 token

理论上，这要求每一步都重新计算前 N+1、N+2、... 个 token。但实际上我们使用了一种非常关键的加速机制——**KV Cache**。

---

## 3. KV Cache 的作用

KV Cache 是 Transformer 注意力机制中 **K（键）与 V（值）张量的缓存**，它的作用是：

* **避免重复计算历史 token 的注意力表示**
* 只需每一步输入一个新 token，结合之前缓存的 K/V 值，就能预测下一个 token

这一机制大幅降低了解码阶段的计算开销，使得解码变为**常数时间的增量计算**，而不是线性增长的重复运算。

---

## 4. Prefill vs Decode 的本质差异

虽然预装填和解码在数学上是等价的，都使用同一套网络和参数，但由于计算效率差异，我们采用了不同的实现路径：

* **Prefill 用并行方式计算**
* **Decode 借助 KV Cache 增量生成**

这构成了现代大语言模型推理系统中的核心优化路径。

---

## 5. 拓展：与 Encoder-Decoder 架构的对比

接下来我来讲一下一个容易混淆但非常关键的扩展点，也就是 **Encoder-Decoder 架构与 Prefill-Decode 机制的区别**。

我们知道，当前大语言模型采用的是 decoder-only 架构，推理过程中分为两个阶段，分别是预装填（Prefilling）和解码（Decoding）。这在工程实现中确实像是“前一段先处理输入，后一段逐步生成输出”，因此有人会自然联想到经典的 Encoder-Decoder 架构，认为它们是类似的两个结构分段。但实际上，两者在本质上是完全不同的。

---

首先来看 Encoder-Decoder 架构，这是传统机器翻译、摘要生成等任务中常见的框架。它包含两套神经网络结构：

* Encoder 负责处理输入，通常采用**双向注意力机制**来获取全局语义；
* Decoder 负责生成输出，采用**因果注意力 + 编码器交叉注意力**。

Encoder-Decoder 中，**输入和输出走的是不同的计算路径，结构与参数通常不共享**，这是一种数学上的非等价性。

---

而反观 Prefill-Decode，虽然它也表现为“先处理输入，再逐步生成输出”，但实际上，无论是处理输入还是生成答案，它们走的是**完全相同的神经网络结构，使用同一套参数**，且注意力机制全程保持因果型（causal attention）。从数学角度来看，**Decode 阶段的增量生成与 Prefill 阶段的并行推理是等价的**，只是在工程上为了提升效率，才使用 KV Cache 做优化处理。

---

因此，**Prefill-Decode 是一种工程分段，而非结构分离**。它不是 Encoder-Decoder 架构的简化版，而是完全不同的设计理念。我们在理解大模型结构时，务必要把这两者清晰地区分开来。

---

如需，我可以继续为此页补充：

* 流程对比图（如左：Encoder-Decoder；右：Decoder-only + Prefill-Decode）
* 图表样式 `.pptx` 设计稿
  是否继续生成结构图或转为 PPT 文件？
