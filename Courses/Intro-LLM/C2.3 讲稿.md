---
创建时间: 2025-七月-25日  星期五, 11:18:59 上午
---
[[C2.3 大语言模型的结构]]

以下是你提供内容的【台上汇报文稿版本】，适用于学术风格PPT讲解，覆盖细节、逻辑清晰、语言正式、便于口头陈述。分为两大部分：

---

# 一、大语言模型结构概览（对应图1）

大家好，下面我将从大语言模型的整体结构出发，为大家梳理其从输入文本到输出结果的完整推理流程。

如图1所示，当前主流的大语言模型架构（例如 Qwen2、LLaMA3、DeepSeek-V3 等）基本遵循同一个通用框架。其整体流程可以分为三部分：输入前的编码处理、神经网络主体结构、输出后的解码处理。

首先我们来看输入部分。这里使用的是名为 **Tokenizer** 的模块来进行文本编码。需要强调的是，Tokenizer 并非神经网络，而是一个基于规则的离散分词器。它的主要作用是将自然语言文本转化为模型可以识别的编号序列，也就是 token 序列。

比如，“你”可能被编码为 1，“好”是 2，“世”是 3，“界”是 4。为了提高效率，如果“你好”在训练数据中高频共现，我们就可能将其作为一个整体词组，赋予一个新的编号，比如 5，这样“你好世界”就可以被编码为 \[5,3,4]，而不是 \[1,2,3,4]。这样做的优点在于可以**减少 token 数量**，从而降低模型的推理成本，提高运行速度。

但与此同时，也带来了一个权衡问题。为了合并更多词组，我们需要扩大 Tokenizer 的词表，这会显著增加嵌入层（Embedding Layer）与输出层（Output Layer）的参数规模。因此，Tokenizer 的词表不是越大越好，需要在效率与模型大小之间取得平衡。

此外，Tokenizer 中还会添加一些**特殊 token**，用于标记句子结构或控制模型行为。例如：

* `<bos>`：句子起始符（Begin of Sentence）
* `<eos>`：句子终止符（End of Sentence）
  这些特殊标记可以帮助模型识别生成的边界，控制生成流程的启动和停止。

接下来我们进入模型主体部分，也就是我们熟知的神经网络结构。主要由以下三个模块组成：

---

## 1. 嵌入层（Embedding Layer）

Embedding 层的任务是将每个 token 编号映射为一个高维浮点向量，从而让模型能够学习其语义特征。该层的输入是整数编号的 token 序列，输出是一个形如 `[batch_size, sequence_length, embedding_dim]` 的张量。

权重矩阵的形状为 `(vocab_size, embedding_dim)`，其中 vocab\_size 是 Tokenizer 的词表大小，embedding\_dim 是每个 token 的表示维度。与传统的线性层不同，Embedding 层通过查表的方式获取向量，无需矩阵乘法，速度更快、结构更简单。

---

## 2. Transformer 模块（Transformer Blocks）

Transformer 是大语言模型的核心组成部分，负责建模 token 之间的上下文关系，存储和传播语言知识。一个模型通常串联多个结构相同的 Transformer Block，每个 block 内部包含：

* 自注意力机制（Self-Attention）
* 前馈网络（Feed-Forward Network，或 MLP）
* 层归一化（LayerNorm）
* 残差连接（Residual Connection）

每个 block 的输入输出张量保持形状不变，通过残差连接提高了深层网络的训练稳定性。不同架构可能在细节上存在变体，但主干设计基本一致。我们将在后续章节中对每个子模块逐一分析。

---

## 3. 输出层（Output Layer）

模型最后一层是一个线性映射层，用于将 Transformer 输出的特征向量转为对所有可能 token 的预测分布。输出张量的维度为 `[batch_size, sequence_length, vocab_size]`，每个位置上的向量表示对应位置 token 的分类概率。

在推理时，输出可以通过两种方式生成文本：

* **贪婪预测**：直接取概率最大的 token，即 argmax；
* **采样预测**：基于 softmax 归一化后的概率进行采样，常用策略包括 Top-k、Top-p、Temperature 等。

需要补充一点知识点是，部分小模型会**共享 Embedding 层和输出层的权重**，本质上是通过参数共享来节省模型体积。这在参数量较小（如 1B 以下）的模型中比较常见，但需要注意它**不会减少计算量**，仅仅是减小了可训练参数的数量。

---

# 二、大语言模型的推理阶段（对应图2）

在了解完结构之后，我们进入第二部分内容，解释大语言模型的推理是如何执行的。

目前主流的大语言模型采用的是所谓的 **Decoder-only 架构**，也就是整个模型中只包含解码器部分，既没有 encoder，也没有交叉注意力结构。

那么问题来了：既然模型是 Decoder-only，为什么我们还要将推理过程划分为两个阶段——**预装填（Prefilling）** 和 **解码（Decoding）** 呢？

---

## 1. 什么是 Decoder-only 架构？

Decoder 本质上是用来**逐步生成文本**的。由于 token 的空间非常大（词表常在十万以上），不可能一次性枚举所有句子的组合，所以我们只能通过逐步生成 token 的方式来建构文本。

每次生成一个新 token，其输入是前面所有 token 的集合。这就引出了 **因果注意力（Causal Attention）** 的概念。具体来说，当前 token 只能访问它前面的 token，而不能看到未来的 token。比如：

* 第 3 个 token 的生成依赖第 1 到第 3 个 token；
* 它不能访问第 4 个及之后的 token，因为它们尚未生成。

这样的因果结构就构成了一个 **标准的 Transformer Decoder 网络**。

---

## 2. 为什么还要区分两个推理阶段？

虽然结构上统一，但 **模型推理的行为模式是不同的**：

### 🔹 预装填阶段（Prefilling）

* 输入是完整的问题文本（N 个 token）
* 模型并行计算所有 token 的表示，但只保留最后一个 token 的输出，因为它会作为回答的起点
* 中间的 token 输出会被丢弃，因为问题文本是已知的，不需要模型去预测

**优点是：可以并行计算所有问题 token，一次性输入，计算效率高**

---

### 🔹 解码阶段（Decoding）

* 每一步生成一个新的 token，直到遇到 `<eos>` 标志结束
* 每次输入是：问题 token + 当前已生成的 token

理论上，这要求每一步都重新计算前 N+1、N+2、... 个 token。但实际上我们使用了一种非常关键的加速机制——**KV Cache**。

---

## 3. KV Cache 的作用

KV Cache 是 Transformer 注意力机制中 **K（键）与 V（值）张量的缓存**，它的作用是：

* **避免重复计算历史 token 的注意力表示**
* 只需每一步输入一个新 token，结合之前缓存的 K/V 值，就能预测下一个 token

这一机制大幅降低了解码阶段的计算开销，使得解码变为**常数时间的增量计算**，而不是线性增长的重复运算。

---

## 4. Prefill vs Decode 的本质差异

虽然预装填和解码在数学上是等价的，都使用同一套网络和参数，但由于计算效率差异，我们采用了不同的实现路径：

* **Prefill 用并行方式计算**
* **Decode 借助 KV Cache 增量生成**

这构成了现代大语言模型推理系统中的核心优化路径。

---

## 5. 拓展：与 Encoder-Decoder 架构的对比

接下来我来讲一下一个容易混淆但非常关键的扩展点，也就是 **Encoder-Decoder 架构与 Prefill-Decode 机制的区别**。

我们知道，当前大语言模型采用的是 decoder-only 架构，推理过程中分为两个阶段，分别是预装填（Prefilling）和解码（Decoding）。这在工程实现中确实像是“前一段先处理输入，后一段逐步生成输出”，因此有人会自然联想到经典的 Encoder-Decoder 架构，认为它们是类似的两个结构分段。但实际上，两者在本质上是完全不同的。

---

首先来看 Encoder-Decoder 架构，这是传统机器翻译、摘要生成等任务中常见的框架。它包含两套神经网络结构：

* Encoder 负责处理输入，通常采用**双向注意力机制**来获取全局语义；
* Decoder 负责生成输出，采用**因果注意力 + 编码器交叉注意力**。

Encoder-Decoder 中，**输入和输出走的是不同的计算路径，结构与参数通常不共享**，这是一种数学上的非等价性。

---

而反观 Prefill-Decode，虽然它也表现为“先处理输入，再逐步生成输出”，但实际上，无论是处理输入还是生成答案，它们走的是**完全相同的神经网络结构，使用同一套参数**，且注意力机制全程保持因果型（causal attention）。从数学角度来看，**Decode 阶段的增量生成与 Prefill 阶段的并行推理是等价的**，只是在工程上为了提升效率，才使用 KV Cache 做优化处理。

---

因此，**Prefill-Decode 是一种工程分段，而非结构分离**。它不是 Encoder-Decoder 架构的简化版，而是完全不同的设计理念。我们在理解大模型结构时，务必要把这两者清晰地区分开来。

---

如需，我可以继续为此页补充：

* 流程对比图（如左：Encoder-Decoder；右：Decoder-only + Prefill-Decode）
* 图表样式 `.pptx` 设计稿
  是否继续生成结构图或转为 PPT 文件？




## 🗣️ PPT讲解配套文稿

接下来我们来看注意力机制中一个非常核心的优化点：**多查询注意力（Multi-Query Attention, MQA）**。

我们知道，在标准的多头注意力机制中，每个 token 的 Query、Key、Value 都会分别通过线性层投影到 `num_heads × head_dim` 维空间。以 Key 和 Value 为例，它们的线性层权重形状是 `[num_heads × head_dim, input_dim]`，而输出后也要在推理时缓存下来，变成大小为 `[batch_size, seq_len, num_heads, head_dim]` 的 KV Cache。这在大模型中开销非常大。

**MQA 的出发点很简单**：我们是不是可以只对 Query 保持多头，而让 Key 和 Value 使用**单头表示**？

这样一来：

* Q 还是多头，用于捕捉多种上下文视角；
* K 和 V 统一为单头，线性层的参数量从原来的 `num_heads × head_dim` 缩减为仅 `head_dim`；
* 相应的 KV Cache 大小也缩小 `num_heads` 倍。

这种优化方式带来的直接好处就是：

* **线性层计算量显著下降**；
* **KV 缓存带宽和存储压力大幅减轻**；
* 这对于推理速度、内存占用、延迟控制都有很明显的好处。

当然，它也不是没有代价。由于 Key 和 Value 的表示被“压缩”为单头，它的表达能力会有所下降，尤其在需要细粒度注意力时表现不如标准多头。

但实际中，这种折中策略效果仍然不错。比如 GPT-3.5、Mistral 系列模型中，就大量采用了 MQA 来兼顾性能与效率。

---
以下是将“**Grouped-Query Attention (GQA)**”作为一页学术型PPT进行展示的内容整理，包括：

## 🗣️ 二、汇报讲解文稿

接下来我们介绍另一种注意力机制的优化方式，叫做 **Grouped-Query Attention，简称 GQA**。

我们前面提到，虽然 Multi-Query Attention 能大大减少计算和缓存开销，但由于 Key 和 Value 只使用一个头，它的表达能力会明显下降。这在一些需要高精度理解任务中会成为瓶颈。

因此 GQA 被提出，作为 MHA 和 MQA 的一种**中间折中形式**。

GQA 的基本思想是：Query 保持原有的 `num_heads` 多头结构，而 Key 和 Value 则使用较少的头，数量为 `num_groups`。每组 Query 头共享一组 K/V 表达。

举个例子：

* 如果 num\_heads = 16，num\_groups = 4，则每 4 个 Query head 共用一个 KV 头；
* 如果 num\_groups = 1，则是 MQA；
* 如果 num\_groups = 16，则退化为原始的 MHA；
* 介于其间时，就是标准的 GQA 架构。

也正因为这种结构是泛化的，它引入了一个可调参数 `num_key_value_heads`（也称为 num\_groups），可以在性能和效率之间灵活调节。

在实践中，GQA 通常可以在牺牲极少精度的前提下，显著减少：

* 线性层的参数量
* KV Cache 的占用
  因此被许多主流大语言模型广泛使用，例如 **Qwen2、LLaMA3** 中都默认支持 GQA 配置。

你可以把 GQA 理解为在“单一输出头”和“全部多头”的两个极端之间做的精细调节，是一种很务实、效果也不错的工程折中方案。

---
## 🗣️ 二、配套讲解文稿（口头汇报）

接下来我要介绍的是目前 DeepSeek 系列模型中使用的一种非常激进的注意力压缩机制：**Multi-head Latent Attention（多头潜在注意力）**，简称 MLA。

这个结构最早在 DeepSeek-V2 中引入，在 DeepSeek-V3 和 R1 中被广泛使用，是对 GQA 和 MQA 的进一步优化。

它背后的灵感来自一个大语言模型中常用的参数高效微调技术：**LoRA，Low-Rank Adaptation**。LoRA 的核心思想是，对于一个大的线性权重矩阵，我们不直接去训练它，而是训练两个小的低秩矩阵 A 和 B，让它们乘起来近似原来的权重。这样可以大幅减少显存消耗。

而 MLA 则更进一步：它不是辅助训练两个小矩阵，而是**直接用两个小矩阵替代原始的线性层本身**，将原本一次大的线性变换拆解为两次低秩变换，并配以归一化处理。

结构上，它将原始的 Q/K/V 的生成流程变得更复杂了，但带来的好处也非常可观。

---

特别是在 **KV Cache 的压缩方面**，效果尤为显著。

以 DeepSeek-V3 为例，每个 token 只需缓存：

* 一个 64 维的 k\_pe（带有位置编码）
* 一个 512 维的共享特征向量（可重构出 k\_nope 和 V）

总共只需 576 个 float 值。而相比之下，像 Qwen2.5 使用了组查询注意力后，每个 token 的 KV Cache 依然要占用 2048 个维度。MLA 这套结构实现了约 **3.5 倍的缓存压缩**。

当然，这也并不是没有代价的。由于我们不再直接保存完整的 Key 和 Value，而是保存一个潜在表示，**每次解码还需通过额外线性层进行还原**，这会引入额外的计算与带宽访问压力。也就是说，虽然缓存少了，但每一步的运算复杂度增加了，这是一个典型的“存储-计算权衡”。

但实际效果证明，在大模型规模下，这种方式可以带来**显著的推理效率与显存利用提升**。也因此成为 DeepSeek 系列 LLM 进一步压榨性能瓶颈的关键设计之一。

---
