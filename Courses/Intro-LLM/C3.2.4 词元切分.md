好的，那我直接给你第 1 页的最终文案内容，可以直接放到 PPT 上：

---

# 词元切分在大模型中的地位

在进入模型之前，所有文本必须先被切分为 **token** 并映射为 ID 与向量。
这一步是大模型的 **第零层**，决定了模型如何“看世界”。

在数据经过质量过滤、冗余去除和隐私剔除之后，仍然只是原始的文本流。
**Tokenization** 将其转化为 token 序列，进入嵌入层，才能被 Transformer 处理。

**思考**：

* 为什么不直接用单词作为基本单元？
* 为什么 GPT、BERT、T5 的分词器各不相同？

**一句话定义**：
Tokenization = 文本 → 词元序列 → ID → 向量。

---

要不要我继续帮你整理 **第2页（分词器目标与类别）** 的完整文案？
