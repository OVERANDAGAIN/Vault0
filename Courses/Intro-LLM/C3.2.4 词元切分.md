好的，那我直接给你第 1 页的最终文案内容，可以直接放到 PPT 上：

---

# 词元切分在大模型中的地位

在进入模型之前，所有文本必须先被切分为 **token** 并映射为 ID 与向量。
这一步是大模型的 **第零层**，决定了模型如何“看世界”。

在数据经过质量过滤、冗余去除和隐私剔除之后，仍然只是原始的文本流。
**Tokenization** 将其转化为 token 序列，进入嵌入层，才能被 Transformer 处理。

**思考**：

* 为什么不直接用单词作为基本单元？
* 为什么 GPT、BERT、T5 的分词器各不相同？

**一句话定义**：
Tokenization = 文本 → 词元序列 → ID → 向量。

---

要不要我继续帮你整理 **第2页（分词器目标与类别）** 的完整文案？


好的👌，这是第 2 页的完整 **PPT 文案**和**逐字讲稿**，你可以直接使用。

---

# 幻灯片内容

## 分词器的目标与类别

**目标**
把文本切分成若干子串，每个子串都有独立或相对完整的语义，便于模型学习 embedding 并进行后续计算。

**三种粒度**

1. **Word-level（词级）**

   * 优点：语义完整，直观易懂
   * 缺点：词表巨大（如 Transformer XL：267,735 词），embedding 矩阵庞大

2. **Char-level（字符级）**

   * 优点：词表小（英文字母仅 26 个）
   * 缺点：单个字符语义稀薄，模型学习困难

3. **Subword-level（子词级，主流）**

   * 优点：高频词保留完整，低频词拆为子词
   * 示例：“Transformers” → “Transform” + “ers”
   * 平衡：词表大小、OOV率、语义表达能力

---

# 讲稿（逐字可读）

这一页我们来看看 **分词器的目标和三种常见的粒度**。

首先，分词的目标非常简单，就是把一段文本切分成更小的片段，每个片段都有相对完整的语义。这样，模型就可以为这些片段学习 embedding，并在后续层里组合出更复杂的表示。

那么切分的时候，应该按照什么单位来分呢？主要有三种思路。

第一种是 **Word-level，也就是按单词来分**。它的好处是非常直观，符合我们人类的理解习惯，一个词就代表一个语义单元。但缺点也很明显，词表会变得非常大。比如在 Transformer XL 中，词汇表就有二十六万多个单词。这么大的词表意味着 embedding 矩阵非常庞大，对存储和计算都是巨大的负担。

第二种是 **Char-level，也就是按字符来分**。词表一下子变得很小，比如英文就只有 26 个字母。这样参数量大幅减少，但问题是，一个单独的字符几乎没有什么语义信息，比如字母 “t”，必须放在上下文里才能知道是什么意思。这就给模型学习带来了很大困难。

第三种是目前主流的方案，叫做 **Subword-level，也就是子词级切分**。它介于词和字符之间。常见的高频词会被保留为完整的 token，而低频词则会被拆分成子词。这样就能兼顾两方面的优势。比如单词 “Transformers” 就可以切成 “Transform” 和 “ers”。这种方式在词表大小、未登录词问题和语义表达之间取得了一个平衡，因此被大模型普遍采用。

总结一下：Word-level 直观但代价高，Char-level 精简但语义弱，Subword-level 在效率和表达力之间找到了折中，是今天我们会重点展开的方向。

---

要不要我继续帮你写 **第 3 页（为什么要子词切分，OOV 与词表大小问题）** 的 PPT 内容和讲稿？
