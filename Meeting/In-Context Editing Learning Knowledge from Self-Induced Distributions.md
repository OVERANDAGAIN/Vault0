---
created: 2025-01-10T14
updated: ...
创建时间: 2025-三月-11日  星期二, 7:30:46 晚上
---
#meeting 

**Reporter:**  
报告人：杨邦诚
主题：In-Context Editing: Learning Knowledge from Self-Induced Distributions
时间：2025年3月11日（周二）19：30~20：30
参考论文：https://arxiv.org/pdf/2406.11194
腾讯会议号：981-8006-9100
In scenarios where language models must incorporate new information efficiently without extensive retraining, traditional fine-tuning methods are prone to overfitting, degraded generalization, and unnatural language generation. To address these limitations, we introduce Consistent In-Context Editing (ICE), a novel approach leveraging the model's in-context learning capability to optimize towards a contextual distribution rather than a one-hot target. ICE introduces a simple yet effective optimization framework for the model to internalize new knowledge by aligning its output distributions with and without additional context. This method enhances the robustness and effectiveness of gradient-based tuning methods, preventing overfitting and preserving the model's integrity. We analyze ICE across four critical aspects of knowledge editing: accuracy, locality, generalization, and linguistic quality, demonstrating its advantages. Experimental results confirm the effectiveness of ICE and demonstrate its potential for continual editing, ensuring that the integrity of the model is preserved while updating information.
# Inspiration
# Probelms or Thinkings 
Knowledge Editing
# Context
1. content overload 
2.  parameter update


## Vanilla FT
![[Pasted image 20250311194123.png]]
# Innovation
# Background
# Related Work
# Theroy
# Methodology
# Evaluation
# Results
# Limitations
# FootNotes
