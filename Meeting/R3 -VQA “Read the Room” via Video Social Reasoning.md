---
created: 2025-01-10T14
updated: ...
创建时间: 2025-二月-25日  星期二, 7:27:01 晚上
---
#meeting 

**Reporter:**  牛力兴
报告人：牛力兴
主题：R^3-VQA:“Read the Room” via Video Social Reasoning
时间：2024年2月25日（周二）19：30~20：30
腾讯会议号：981-8006-9100
摘要：“Read the room” is a significant social reasoning capability in human daily life. Humans can infer others’ mental states, i.e., intent, emotion, etc., from subtle social cues such as eye gaze, expression, voice intonation, etc. Previous work often features overly simple scenes, very basic interactions between humans, few mental state variables, or single-step reasoning. In short, the social reasoning tasks and datasets they propose lack complexity and fall far short of the chal-lenges present in real-life social interactions. In this pa-per, we contribute a valuable, high-quality, and compre-hensive video dataset named Read-the-Room Reasoning for Video Question Answering (R3-VQA) with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social reasoning chains in complex social scenarios. More-over, we include human-annotated and model-generated QAs based on reasoning chains. Our task R3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate these social reasoning capa-bilities and reasoning consistency of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level social reasoning intelligence and there is still a great gap for improvement. (ii) Theory of Mind (ToM) can help LVLMs perform better on social reasoning tasks.
# Inspiration
# Probelms or Thinkings 

BID E
# Context
# Innovation
# Background
# Related Work
# Theroy
# Methodology
# Evaluation
# Results
# Limitations
# FootNotes
