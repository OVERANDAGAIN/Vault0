---
mindmap-plugin: basic
创建时间: 2025-一月-23日  星期四, 5:05:09 下午
created: 2025-02-12T14:58
updated: 2025-02-13-13.
---

# HOP

## 运行
- main(): train.py
    - ray.init()
    - register_env("StagHunt", ...): train.py
        - class StagHunt() : env.py
    - ModelCatalog.register_custom_model(): train.py
        - MyModel: mcts_model
        - RLmodel: gpu_deeper_mode
    - trainer = AlphaZeroTrainer(config): train.py
        - policy_mapping/ToM
            - 'ToM1':PolicySpec(AlphaZeroPolicyWrapperClass,...)
                - class AlphaZeroPolicyWrapperClass(AlphaZeroPolicy):
                    - class AlphaZeroPolicy(TorchPolicy): ToM_Alpha_MOA.py
        - __init__(): Alpha_Zero_MOA.py
            - 创建WorkerSet
            - 注册多策略配置
    - trainer.train(): Alpha_Zero_MOA.py
        - train(): trainable.py
            - step(): trainer.py
                - step_attempt(): trainer.py
        - Alpha_Zero_MOA.py  # 调用训练逻辑
            - execution_plan()  # 定义训练流程
                - rollouts.combine()  # 组合回合数据
                - ConcatBatches()  # 合并批次数据
                - TrainOneStep()  # 执行单步训练
            - 执行训练步骤  # 调用 TrainOneStep 更新策略和模型
        - WorkerSet.sample(): RLlib内部
            - RolloutWorker.sample(): RLlib内部
                - self.input_dict = obs_batch
                - compute_actions_from_input_dict(): **ToM_Alpha_MOA**.py
                    - self.env.set_state(): env.py
                    - self.mcts.compute_action(): mcts_moa.py
                        - Node.select() → expand() → backup(): mcts_moa.py
                            - self.model.compute_priors_and_value(): mcts_model.py
                            - self.env.step(): env.py
        - init():AlphaZeroPolicy(TorchPolicy): **ToM_Alpha_MOA**.py
            - 初始化mcts : **planning\mcts_moa()**
            - 创建model_list : MOAModel: model\moa_model.py
            - env_creator=config['ToM_config']['env_creator']
            - self.env.reset()
        -
            - **compute_actions_from_input_dict()**:
                - **初始化数据**
                    - 提取 `obs_batch`, `state_batches`, `episodes`
                    - `time == 0`
                        - 调用 `get_initial_state()`
                        - 更新 `new_state`, `stag_prob`, `hare_prob`
                    - **构建 `new_state`**
                        - 更新玩家位置和猎物位置
                        - 更新 `stag_prob`, `hare_prob`
                    - `self.state_dict=self.set_env_states(obs,new_state,time)`
                    - **计算最优动作**
                        - 无猎物
                            - 随机选择动作
                        - 有猎物
                            - 调用 `self.env.set_state(self.state_dict)`
                            - 初始化 MCTS 搜索树
                            - 并行 MCTS 计算: `self.mcts.compute_action()`
                            - 计算 `Q_value_total`，从中采样动作
                    - **存储数据**
                        - `new_state_batch.append(new_state)`
                        - `(mcts_policy)`
                        - `(new_state)`
                        - `action_batch.append(action)`
                    - **返回**
                        - `action_batch, np.array(new_state_batch).T, {}`
		- **Start processing the trajectory**
		    - **Loop through players** (excluding `self.my_id`)
		        - **Get the model ID** for each player (`model_id = self.model_id_list[i]`)
		        - **Determine the place ID** based on the model's relative position to `self.my_id`:
		            - `place_id = (model_id + 1 - self.my_id) % self.player_num`
		        - **Check if the last reward for the player is positive**:
		            - `if episode._agent_reward_history[f'player_{model_id+1}'][-1] > 0:`
		                - **Add observation, actions, and time indices** to the model buffer
		                    - `self.model_buffer[i][0].append(...)` (obs)
		                    - `self.model_buffer[i][1].append(...)` (actions)
		                    - `self.model_buffer[i][2].append(...)` (time indices)
		                - **Update the action type (stag or hare)** based on the player's last action:
		                    - **If last action was 'stag' (5)**: Update `self.discounted_stag_times`
		                    - **If last action was 'hare' (6)**: Update `self.discounted_hare_times`
		                - **Reset model buffer if it exceeds capacity**:
		                    - If buffer exceeds `moa_buffer_capacity`, call `self.moa_update(i)`
		            - **Add MCTS policies to the sample batch** (`sample_batch["mcts_policies"]`)
		            - **Calculate final reward** and adjust using reward normalization (`r2_buffer.add_reward(final_reward)`)
		            - **Assign value labels** for each transition in the trajectory (`value_label = final_reward * np.ones_like(sample_batch["t"])`)
		        - **End of trajectory processing**
		- **Start learning from the batch**
		    - **Increment learning count** (`self.learn_count`)
		    - **Skip learning if `my_id > 1` and `learn_count > 75`**:
		        - If conditions met, return early: `return {LEARNER_STATS_KEY: {}}`
		    - **Create a tensor dict for the batch**:
		        - `train_batch = self._lazy_tensor_dict(postprocessed_batch)`
		    - **Compute the loss**:
		        - `loss_out, policy_loss, value_loss = self._loss(...)`
		    - **Zero out gradients** before backpropagation:
		        - `self._optimizers[0].zero_grad()`
		    - **Backpropagate the loss**:
		        - `loss_out.backward()`
		    - **Process gradients** and update the optimizer:
		        - `grad_process_info = self.extra_grad_process(self._optimizers[0], loss_out)`
		        - `self._optimizers[0].step()`
		    - **Track and return gradient information**:
		        - `grad_info = self.extra_grad_info(train_batch)`
		        - Update `grad_info` with additional stats (`grad_process_info`, total loss, etc.)
		    - **End of learning batch processing**
        - learn_on_batch(): **ToM_Alpha_MOA**.py
            - self._loss(): ToM_Alpha_MOA.py
                - model.forward(): mcts_model.py
                - value_function(): mcts_model.py
        - postprocess_trajectory(): **ToM_Alpha_MOA**.py
            - moa_update(): ToM_Alpha_MOA.py
                - model.forward(): moa_model.py
                - optimizer.step(): ToM_Alpha_MOA.py
    - trainer.save(): Alpha_Zero_MOA.py