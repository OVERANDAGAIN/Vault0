---
created: 2025-01-15T12
updated: ...
åˆ›å»ºæ—¶é—´: 2025-ä¸‰æœˆ-27æ—¥  æ˜ŸæœŸå››, 4:37:45 ä¸‹åˆ
---
[[HOP+]]

# æ€»ç»“
å½“ç„¶å¯ä»¥ï¼ä¸‹é¢æˆ‘å°†ä»**å­—æ®µï¼ˆkeysï¼‰è§†è§’**å‡ºå‘ï¼Œç³»ç»Ÿæ€»ç»“ `sample_batch`ã€`other_agent_batches` å’Œ `episode` ä¸‰è€…çš„ç»“æ„å’Œä¸»è¦ç”¨é€”ï¼Œå¸®åŠ©ä½ å½»åº•ç†æ¸…å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«ä¸è”ç³»ã€‚

---

## âœ… æ€»è§ˆè¡¨ï¼šä¸‰è€…ç»“æ„å¯¹æ¯”ï¼ˆå­—æ®µä¸ºä¸»ï¼‰

| **ç»´åº¦** | **sample_batchï¼ˆæœ¬ agentï¼‰** | **other_agent_batchesï¼ˆä»–äººï¼‰** | **episodeï¼ˆå…¨å±€å¯¹è±¡ï¼‰** |
|----------|-----------------------------|-----------------------------|----------------------------|
| ç±»å‹ | `dict` æˆ– `SampleBatch` | `Dict[str, Tuple[Policy, SampleBatch]]` | `Episode` å¯¹è±¡ |
| æ•°æ®ç²’åº¦ | å½“å‰ agent çš„æ—¶é—´åºåˆ— | æ¯ä¸ªä»–äºº agent çš„æ—¶é—´åºåˆ— | å½“å‰ episode å…¨éƒ¨äº¤äº’è¿‡ç¨‹ |
| æ¥æº | æœ¬ agent rollout | ä»–äºº agent rollout | ç¯å¢ƒäº¤äº’å®Œæ•´ä¿¡æ¯ |
| å¸¸ç”¨å­—æ®µï¼ˆæ ¸å¿ƒï¼‰ | âœ… `obs`<br>âœ… `actions`<br>âœ… `rewards`<br>âœ… `dones`<br>âœ… `new_obs`<br>âœ… `prev_actions`<br>âœ… `subgoal` | âœ… `obs`<br>âœ… `actions`<br>âœ… `rewards`<br>ï¼ˆç»“æ„ä¸ sample_batch ç›¸åŒï¼‰ | âœ… `user_data`ï¼ˆè‡ªå®šä¹‰å­—æ®µï¼‰<br>âœ… `_agent_reward_history`<br>âœ… `_agent_to_last_action`<br>âœ… `.length()`<br>âœ… `.last_observation_for()` |
| å­ç›®æ ‡ä½ç½® | `sample_batch["subgoal"]` | `batch["subgoal"]`ï¼ˆæ¯ä¸ª agentï¼‰ | `episode.user_data[f"subgoal{self.my_id}"]`ï¼ˆlistï¼‰ |
| è®¿é—®æ–¹å¼ | ç›´æ¥å­—å…¸è®¿é—® | `other_agent_batches[f'player_{id}'][1]['obs']` | é€šè¿‡å±æ€§ï¼Œå¦‚ `episode.user_data[...]` |
| æ˜¯å¦å«å¤š agent | âŒï¼ˆå• agentï¼‰ | âœ…ï¼ˆæŒ‰ agent åˆ†å¼€ï¼‰ | âœ…ï¼ˆå¤š agent å…¨å±€ï¼‰ |

---

## ğŸ§© å„è‡ªå­—æ®µè¯¦ç»†è¯´æ˜

---

### ğŸ“¦ 1. `sample_batch`ï¼ˆå½“å‰ agent çš„ rollout æ•°æ®ï¼‰

```python
sample_batch.keys()
```

å…¸å‹å­—æ®µï¼ˆå• agentï¼‰ï¼š

| å­—æ®µ | å«ä¹‰ | ç¤ºä¾‹ shape |
|------|------|------------|
| `obs` | å½“å‰è§‚å¯Ÿ | `(T, obs_dim)` |
| `new_obs` | ä¸‹ä¸€ä¸ªè§‚å¯Ÿ | `(T, obs_dim)` |
| `actions` | æ‰§è¡ŒåŠ¨ä½œï¼ˆç¦»æ•£/è¿ç»­ï¼‰ | `(T,)` |
| `prev_actions` | ä¸Šä¸€æ­¥åŠ¨ä½œ | `(T,)` |
| `rewards` | æ¯æ­¥å¥–åŠ± | `(T,)` |
| `dones` | æ¯æ­¥æ˜¯å¦ episode ç»ˆæ­¢ | `(T,)` |
| `subgoal` âœ… | å­ç›®æ ‡ï¼ˆè‡ªå®šä¹‰æ·»åŠ ï¼‰ | `(T, subgoal_dim)` |
| `agent_index`, `t`, `eps_id`, `unroll_id` | å…ƒä¿¡æ¯ | `(T,)` |

---

### ğŸ“¦ 2. `other_agent_batches`ï¼ˆä»–äºº agent çš„ rollout æ•°æ®ï¼‰

ç»“æ„å¦‚ä¸‹ï¼š

```python
{
  "player_1": (PolicyObj, SampleBatch),
  "player_2": (PolicyObj, SampleBatch),
  ...
}
```

å­—æ®µä¸ `sample_batch` ç›¸åŒï¼Œå†…å®¹è¡¨ç¤ºä»–äººçš„è½¨è¿¹æ•°æ®ã€‚

ä½ å¯è¿™æ ·è·å–ï¼š

```python
other_agent_batches[f"player_{id}"][1]["obs"]
```

å­—æ®µä¸ä¸Šè¡¨ä¸€è‡´ï¼Œå¦‚ `obs`ã€`actions`ã€`subgoal` ç­‰ã€‚

---

### ğŸ§  3. `episode`ï¼ˆEpisode å¯¹è±¡ï¼Œå…¨å±€çŠ¶æ€è®°å½•ï¼‰

è¿™æ˜¯æœ€å¤æ‚ã€ä¹Ÿæœ€çµæ´»çš„ç»“æ„ã€‚

```python
type(episode) == <class 'ray.rllib.evaluation.episode.Episode'>
```

---

#### ğŸ”‘ é‡è¦å­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `user_data` âœ… | `dict` | è‡ªå®šä¹‰è®°å½•ç»“æ„ï¼Œæ”¯æŒå­˜ subgoalã€obsã€ç­–ç•¥ç­‰åºåˆ—ï¼ˆå¦‚ `subgoal1`ï¼‰ |
| `_agent_reward_history` âœ… | `dict[str -> List[float]]` | æ¯ä¸ª agent çš„å¥–åŠ±è½¨è¿¹ |
| `_agent_to_last_action` âœ… | `dict[str -> int]` | æ¯ä¸ª agent æœ€åä¸€æ¬¡åŠ¨ä½œ |
| `.length` | `int` | å½“å‰ episode é•¿åº¦ |
| `.last_observation_for(agent_id)` | æ–¹æ³• | è·å–æŸ agent çš„æœ€åè§‚å¯Ÿ |
| `.get_agents()` | æ–¹æ³• | è·å–æ‰€æœ‰ agent id |
| `.add_extra_batch()` | æ–¹æ³• | æ·»åŠ é¢å¤– batch ä¿¡æ¯ |

---


# HOP batch åˆ†æ
## Sample_batch å’Œ other_agent_batches æ—¶é—´é•¿çŸ­ä¸ä¸€
### ä»£ç :
```python
  @override(Policy)
    def postprocess_trajectory(
            self, sample_batch, other_agent_batches=None, episode=None
    ):

        print("Sample batch keys:", sample_batch.keys())
        # for k in sample_batch.keys():
        print(f"{'obs'} shape:", sample_batch['obs'].shape)

        print("other_agent_batches keys:", other_agent_batches.keys())
        print(f"{'obs'} shape:", other_agent_batches[f'player_{1 + 1}'][1]['obs'].shape)
```

### ç»“æœï¼š 
```bash
pid=23440) obs shape: (35, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 5/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (7, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 6/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (35, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 7/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (6, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 8/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (5, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (5, 89)
 pid=23440) OMG_AM buffer not ready to sample
```

### åŸå› ï¼š æ•åˆ°çŒä¹‹åç§»å‡ºåœ°å›¾ï¼Œä¸æ‰§è¡Œ `compute_action()`
````ad-tip

---

## ğŸ” **èƒŒæ™¯è®¾å®šï¼šStag Hunt åšå¼ˆç¯å¢ƒ**
- å¤šä¸ªæ™ºèƒ½ä½“å¯ä»¥é€‰æ‹©å»â€œå•ç‹¬æŠ“å…”å­â€æˆ–â€œå¤šäººåˆä½œæŠ“é¹¿â€ã€‚
- é¹¿ï¼ˆstagï¼‰åªèƒ½è¢«å¤šäººä¸€èµ·å›´ä½æ—¶æ•è·ã€‚
- æŠ“åˆ°é¹¿å¥–åŠ±é«˜ï¼Œä½†éœ€è¦å¤šäººåä½œã€‚
- æŠ“å…”å­æ˜¯ä¸ªä½“è¡Œä¸ºï¼Œå¥–åŠ±ä½ä½†ç¨³å®šã€‚

---

## âœ… è¿™æ®µä»£ç åšçš„äº‹ï¼šå¤„ç† **é¹¿è¢«æˆåŠŸå›´æ•åçš„ç¯å¢ƒå˜åŒ–**

æˆ‘ä»¬åˆ†æ®µè§£è¯»ï¼š

---

### ğŸ§©1. è¯†åˆ«è°æ•é¹¿äº†ï¼Œå¹¶æ‰¾åˆ°åŒä¸€ä¸ªä½ç½®çš„æ‰€æœ‰çŒäºº

```python
while len(hunt_stag_players) != 0:
    same_stag_with_zero = [hunt_stag_players[0]]
    zero_stag_pos = tuple(self.player_pos[hunt_stag_players[0]])
```

- `hunt_stag_players`ï¼šå½“å‰ tick ä¸­æ‰§è¡Œâ€œæ•é¹¿åŠ¨ä½œâ€çš„æ™ºèƒ½ä½“åˆ—è¡¨ã€‚
- `same_stag_with_zero`ï¼šä¸´æ—¶åˆ—è¡¨ï¼Œç”¨äºè®°å½•å›´æ•åŒä¸€å¤´é¹¿çš„ç©å®¶ã€‚
- `zero_stag_pos`ï¼šè¿™ä¸ªé¹¿æ‰€åœ¨çš„ä½ç½®ã€‚

---

### ğŸ§©2. æ‰¾å‡ºåœ¨åŒä¸€é¹¿ä½ç½®çš„å…¶ä»–ç©å®¶ï¼ˆåˆå›´è€…ï¼‰

```python
for i in range(len(hunt_stag_players)-1, 0, -1):
    if tuple(self.player_pos[hunt_stag_players[i]]) == zero_stag_pos:
        same_stag_with_zero.append(hunt_stag_players[i])
        hunt_stag_players.pop(i)
hunt_stag_players.pop(0)
```

- æŠŠå…¶ä»–ç«™åœ¨ `zero_stag_pos` çš„ç©å®¶åŠ å…¥å›´æ•ç»„ã€‚
- è¿™äº›äººæ˜¯â€œåŒæ—¶æ•é¹¿ï¼Œç«™åœ¨åŒä¸€æ ¼â€çš„åˆä½œè€…ã€‚

---

### ğŸ§©3. å¥–åŠ±ä¸ç»ˆæ­¢å¤„ç†

```python
total_strength = np.sum(self.player_strength[same_stag_with_zero])
if len(same_stag_with_zero) != 1:
    for id in same_stag_with_zero:
        self.state[zero_stag_pos[0], zero_stag_pos[1], id] = 0
        self.terminal[id] = 1
```

- å¦‚æœåªæœ‰ä¸€ä¸ªäººå»æ•é¹¿ï¼Œä¸ç®—æˆåŠŸï¼ˆå¯èƒ½è¢«å¿½ç•¥åœ¨å…¶ä»–é€»è¾‘é‡Œï¼‰ã€‚
- å¤šäººåˆä½œæ—¶ï¼š
  - æ¯ä¸ªå‚ä¸è€…ä» `state` ä¸­ç§»é™¤ï¼›
  - æ ‡è®°è¯¥ agent å·²ç»ˆæ­¢ï¼ˆ`self.terminal[id] = 1`ï¼‰ï¼›
  - å‘æ”¾å¥–åŠ±ï¼š

```python
if total_strength == 0:
    # æ‰€æœ‰äººstrengthä¸º0ï¼Œå‡åˆ†å¥–åŠ±
    rewards[self.players[id]] += self.stag_reward * (1.0 / len(same_stag_with_zero))
else:
    # æŒ‰ç…§æ¯äººstrengthå æ¯”å‘å¥–åŠ±
    rewards[self.players[id]] += self.stag_reward * self.player_strength[id] / total_strength
```

- å¼ºåº¦æœºåˆ¶å…è®¸è®¾å®šä¸åŒ agent çš„èƒ½åŠ›å·®å¼‚ï¼ˆæ¯”å¦‚æ¨¡æ‹Ÿç­‰çº§ã€åˆä½œèƒ½åŠ›ç­‰ï¼‰ã€‚

---

### ğŸ§©4. ç¯å¢ƒçŠ¶æ€æ›´æ–°

```python
if self.first_hunt_time == 0:
    self.first_hunt_time = self.time
self.state[zero_stag_pos[0], zero_stag_pos[1], -2] = 0
```

- è®°å½•â€œé¦–æ¬¡åˆä½œæ•é¹¿â€çš„æ—¶é—´ï¼ˆå¯èƒ½ç”¨äºå­¦ä¹ æŒ‡æ ‡ã€è¯„ä¼°æ¨¡å‹ç¤¾ä¼šæ€§ç­‰ï¼‰ï¼›
- æŠŠé¹¿ä»åœ°å›¾ä¸Šç§»é™¤ï¼ˆ`state[..., -2] = 0`ï¼‰ï¼Œè¯´æ˜å®ƒè¢«æ•äº†ã€‚

---

## âœ… æ€»ç»“ï¼šæ•é¹¿ä¹‹åç¯å¢ƒçŠ¶æ€çš„å˜åŒ–

| ç¯å¢ƒå…ƒç´  | å˜åŒ–æƒ…å†µ |
|----------|----------|
| `state` | - é¹¿ä½ç½®çš„ `-2` é€šé“è¢«æ¸…é™¤ï¼ˆé¹¿æ¶ˆå¤±ï¼‰<br>- ç©å®¶æ‰€åœ¨é€šé“ï¼ˆidé€šé“ï¼‰è¢«æ¸…é™¤ |
| `rewards` | æ‰€æœ‰æˆåŠŸæ•é¹¿çš„ç©å®¶è·å¾—åˆ†é…å¥–åŠ±ï¼ˆå‡åˆ†æˆ–æŒ‰ strength åˆ†ï¼‰ |
| `terminal` | è¿™äº›ç©å®¶è¢«æ ‡è®°ä¸ºç»ˆæ­¢ï¼ˆå¯èƒ½åœ¨ç¯å¢ƒä¸­æ¶ˆå¤±ï¼‰ |
| `done` | è¿™äº›ç©å®¶çš„ `done=True` |
| `first_hunt_time` | å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ•é¹¿ï¼Œè®°å½•æ—¶é—´ |
| `hunt_stag_players` | æ¸…é™¤å·²å¤„ç†ç©å®¶ï¼Œé˜²æ­¢é‡å¤å¤„ç† |

---

````




## `other_agent_batches` çš„ç»“æ„

### ç»“æœï¼šä¸¤ä¸ªå…ƒç¥–ï¼Œpolicy å’Œ sample_batch
```bash
 pid=30752) <class 'tuple'>
 pid=30752) 2
 pid=30752) Policy object type: <class 'policy.fixed_policy.NearestStagPolicy'>
 pid=30752) Sample batch1 type: <class 'ray.rllib.policy.sample_batch.SampleBatch'>
```


### ä»£ç ï¼š 
```python
        print(type(other_agent_batches['player_2']))  # åº”è¯¥æ˜¯ <class 'tuple'>
        print(len(other_agent_batches['player_2']))  # åº”è¯¥æ˜¯ 2
        policy_obj, sample_batch1 = other_agent_batches['player_2']
        print("Policy object type:", type(policy_obj))
        print("Sample batch1 type:", type(sample_batch1))
```




## episode çš„ç»“æ„

```bash
pid=24884) --- Episode Debug ---
 pid=24884) Type: <class 'ray.rllib.evaluation.episode.Episode'>
 pid=24884) Dir: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_agent_rewards', '_agent_index', '_agent_reward_history', '_agent_to_index', '_agent_to_last_action', '_agent_to_last_done', '_agent_to_last_extra_action_outs', '_agent_to_last_info', '_agent_to_last_obs', '_agent_to_last_raw_obs', '_agent_to_policy', '_agent_to_prev_action', '_agent_to_rnn_state', '_next_agent_index', '_policies', '_policy_mapping_fn', '_set_last_action', '_set_last_done', '_set_last_extra_action_outs', '_set_last_info', '_set_last_observation', '_set_last_raw_obs', '_set_rnn_state', 'add_extra_batch', 'agent_rewards', 'batch_builder', 'custom_metrics', 'env_id', 'episode_id', 'get_agents', 'hist_data', 'last_action_for', 'last_done_for', 'last_extra_action_outs_for', 'last_info_for', 'last_observation_for', 'last_pi_info_for', 'last_raw_obs_for', 'last_reward_for', 'length', 'media', 'new_batch_builder', 'policy_for', 'policy_map', 'policy_mapping_fn', 'prev_action_for', 'prev_reward_for', 'rnn_state_for', 'soft_reset', 'total_reward', 'user_data', 'worker']
 pid=24884) User data keys: ['initial_state', 'mcts_policies1', 'recurrent_state1', 'subgoal1', 'hist_obs1']
 pid=24884) Reward history keys: dict_keys(['player_1', 'player_2'])
 pid=24884) Last actions: {'player_1': 6, 'player_2': 4}
```






# OMG episode_batch åˆ†æ

## ç»“æœ: æ—¶é•¿éƒ½æ˜¯å›ºå®š36ï¼ˆ `max_t` +1 ) ï¼Œä¸ HOP çš„å¤šå˜ä¸åŒ

```python
 pid=5900) EpisodeBatch keys: dict_keys(['state', 'obs', 'actions', 'avail_actions', 'reward', 'time', 'terminated', 'actions_onehot', 'filled', 'infer_mu', 'infer_log_var'])
 pid=5900) state: shape = torch.Size([1, 36, 4, 4, 5]), type = <class 'torch.Tensor'>
 pid=5900) obs: shape = torch.Size([1, 36, 2, 4, 4, 5]), type = <class 'torch.Tensor'>
 pid=5900) actions: shape = torch.Size([1, 36, 2, 1]), type = <class 'torch.Tensor'>
 pid=5900) avail_actions: shape = torch.Size([1, 36, 2, 7]), type = <class 'torch.Tensor'>
 pid=5900) reward: shape = torch.Size([1, 36, 1]), type = <class 'torch.Tensor'>
 pid=5900) time: shape = torch.Size([1, 36, 1]), type = <class 'torch.Tensor'>
 pid=5900) terminated: shape = torch.Size([1, 36, 1]), type = <class 'torch.Tensor'>
 pid=5900) actions_onehot: shape = torch.Size([1, 36, 2, 7]), type = <class 'torch.Tensor'>
 pid=5900) filled: shape = torch.Size([1, 36, 1]), type = <class 'torch.Tensor'>
 pid=5900) infer_mu: shape = torch.Size([1, 36, 2, 16]), type = <class 'torch.Tensor'>
 pid=5900) infer_log_var: shape = torch.Size([1, 36, 2, 16]), type = <class 'torch.Tensor'>
```

## å…·ä½“æ•°æ® filled terminated infer_mu
```bash
 pid=23400) Filled: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]
 pid=23400) Terminated: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]
 pid=23400) Time steps where infer_mu is all zero: 34
```

```python
        filled = episode_batch.data.transition_data["filled"].squeeze().cpu().numpy()  # shape: (36,)
        terminated = episode_batch.data.transition_data["terminated"].squeeze().cpu().numpy()  # shape: (36,)

        print("Filled:", filled)
        print("Terminated:", terminated)

        infer_mu = episode_batch.data.transition_data["infer_mu"].squeeze()  # shape: (36, 2, 16)
        zero_mask = ~ (infer_mu == 0).all(dim=-1).all(dim=-1)  # shape: (36,), æ¯ä¸ªæ—¶é—´æ­¥æ˜¯å¦å…¨ä¸º0
        print("Time steps where infer_mu is all zero:", torch.nonzero(zero_mask).squeeze().tolist())
```







# FootNotes