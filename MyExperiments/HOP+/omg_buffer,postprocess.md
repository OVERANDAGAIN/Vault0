---
created: 2025-01-15T12
updated: ...
åˆ›å»ºæ—¶é—´: 2025-ä¸‰æœˆ-27æ—¥  æ˜ŸæœŸå››, 4:37:45 ä¸‹åˆ
---
[[HOP+]]



## Sample_batch å’Œ other_agent_batches æ—¶é—´é•¿çŸ­ä¸ä¸€
### ä»£ç :
```python
  @override(Policy)
    def postprocess_trajectory(
            self, sample_batch, other_agent_batches=None, episode=None
    ):

        print("Sample batch keys:", sample_batch.keys())
        # for k in sample_batch.keys():
        print(f"{'obs'} shape:", sample_batch['obs'].shape)

        print("other_agent_batches keys:", other_agent_batches.keys())
        print(f"{'obs'} shape:", other_agent_batches[f'player_{1 + 1}'][1]['obs'].shape)
```

### ç»“æœï¼š 
```bash
pid=23440) obs shape: (35, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 5/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (7, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 6/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (35, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 7/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (6, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (35, 89)
 pid=23440) OMG_AM buffer not ready to sample
 pid=23440) [Learn step 0] Attempting MOA update...
 pid=23440) Buffer size for model 0: 8/200
 pid=23440) Sample batch keys: dict_keys(['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't'])
 pid=23440) obs shape: (5, 89)
 pid=23440) Sample batch keys: dict_keys(['player_2'])
 pid=23440) obs shape: (5, 89)
 pid=23440) OMG_AM buffer not ready to sample
```

### åŸå› ï¼š æ•åˆ°çŒä¹‹åç§»å‡ºåœ°å›¾ï¼Œä¸æ‰§è¡Œ `compute_action()`
````ad-tip

---

## ğŸ” **èƒŒæ™¯è®¾å®šï¼šStag Hunt åšå¼ˆç¯å¢ƒ**
- å¤šä¸ªæ™ºèƒ½ä½“å¯ä»¥é€‰æ‹©å»â€œå•ç‹¬æŠ“å…”å­â€æˆ–â€œå¤šäººåˆä½œæŠ“é¹¿â€ã€‚
- é¹¿ï¼ˆstagï¼‰åªèƒ½è¢«å¤šäººä¸€èµ·å›´ä½æ—¶æ•è·ã€‚
- æŠ“åˆ°é¹¿å¥–åŠ±é«˜ï¼Œä½†éœ€è¦å¤šäººåä½œã€‚
- æŠ“å…”å­æ˜¯ä¸ªä½“è¡Œä¸ºï¼Œå¥–åŠ±ä½ä½†ç¨³å®šã€‚

---

## âœ… è¿™æ®µä»£ç åšçš„äº‹ï¼šå¤„ç† **é¹¿è¢«æˆåŠŸå›´æ•åçš„ç¯å¢ƒå˜åŒ–**

æˆ‘ä»¬åˆ†æ®µè§£è¯»ï¼š

---

### ğŸ§©1. è¯†åˆ«è°æ•é¹¿äº†ï¼Œå¹¶æ‰¾åˆ°åŒä¸€ä¸ªä½ç½®çš„æ‰€æœ‰çŒäºº

```python
while len(hunt_stag_players) != 0:
    same_stag_with_zero = [hunt_stag_players[0]]
    zero_stag_pos = tuple(self.player_pos[hunt_stag_players[0]])
```

- `hunt_stag_players`ï¼šå½“å‰ tick ä¸­æ‰§è¡Œâ€œæ•é¹¿åŠ¨ä½œâ€çš„æ™ºèƒ½ä½“åˆ—è¡¨ã€‚
- `same_stag_with_zero`ï¼šä¸´æ—¶åˆ—è¡¨ï¼Œç”¨äºè®°å½•å›´æ•åŒä¸€å¤´é¹¿çš„ç©å®¶ã€‚
- `zero_stag_pos`ï¼šè¿™ä¸ªé¹¿æ‰€åœ¨çš„ä½ç½®ã€‚

---

### ğŸ§©2. æ‰¾å‡ºåœ¨åŒä¸€é¹¿ä½ç½®çš„å…¶ä»–ç©å®¶ï¼ˆåˆå›´è€…ï¼‰

```python
for i in range(len(hunt_stag_players)-1, 0, -1):
    if tuple(self.player_pos[hunt_stag_players[i]]) == zero_stag_pos:
        same_stag_with_zero.append(hunt_stag_players[i])
        hunt_stag_players.pop(i)
hunt_stag_players.pop(0)
```

- æŠŠå…¶ä»–ç«™åœ¨ `zero_stag_pos` çš„ç©å®¶åŠ å…¥å›´æ•ç»„ã€‚
- è¿™äº›äººæ˜¯â€œåŒæ—¶æ•é¹¿ï¼Œç«™åœ¨åŒä¸€æ ¼â€çš„åˆä½œè€…ã€‚

---

### ğŸ§©3. å¥–åŠ±ä¸ç»ˆæ­¢å¤„ç†

```python
total_strength = np.sum(self.player_strength[same_stag_with_zero])
if len(same_stag_with_zero) != 1:
    for id in same_stag_with_zero:
        self.state[zero_stag_pos[0], zero_stag_pos[1], id] = 0
        self.terminal[id] = 1
```

- å¦‚æœåªæœ‰ä¸€ä¸ªäººå»æ•é¹¿ï¼Œä¸ç®—æˆåŠŸï¼ˆå¯èƒ½è¢«å¿½ç•¥åœ¨å…¶ä»–é€»è¾‘é‡Œï¼‰ã€‚
- å¤šäººåˆä½œæ—¶ï¼š
  - æ¯ä¸ªå‚ä¸è€…ä» `state` ä¸­ç§»é™¤ï¼›
  - æ ‡è®°è¯¥ agent å·²ç»ˆæ­¢ï¼ˆ`self.terminal[id] = 1`ï¼‰ï¼›
  - å‘æ”¾å¥–åŠ±ï¼š

```python
if total_strength == 0:
    # æ‰€æœ‰äººstrengthä¸º0ï¼Œå‡åˆ†å¥–åŠ±
    rewards[self.players[id]] += self.stag_reward * (1.0 / len(same_stag_with_zero))
else:
    # æŒ‰ç…§æ¯äººstrengthå æ¯”å‘å¥–åŠ±
    rewards[self.players[id]] += self.stag_reward * self.player_strength[id] / total_strength
```

- å¼ºåº¦æœºåˆ¶å…è®¸è®¾å®šä¸åŒ agent çš„èƒ½åŠ›å·®å¼‚ï¼ˆæ¯”å¦‚æ¨¡æ‹Ÿç­‰çº§ã€åˆä½œèƒ½åŠ›ç­‰ï¼‰ã€‚

---

### ğŸ§©4. ç¯å¢ƒçŠ¶æ€æ›´æ–°

```python
if self.first_hunt_time == 0:
    self.first_hunt_time = self.time
self.state[zero_stag_pos[0], zero_stag_pos[1], -2] = 0
```

- è®°å½•â€œé¦–æ¬¡åˆä½œæ•é¹¿â€çš„æ—¶é—´ï¼ˆå¯èƒ½ç”¨äºå­¦ä¹ æŒ‡æ ‡ã€è¯„ä¼°æ¨¡å‹ç¤¾ä¼šæ€§ç­‰ï¼‰ï¼›
- æŠŠé¹¿ä»åœ°å›¾ä¸Šç§»é™¤ï¼ˆ`state[..., -2] = 0`ï¼‰ï¼Œè¯´æ˜å®ƒè¢«æ•äº†ã€‚

---

## âœ… æ€»ç»“ï¼šæ•é¹¿ä¹‹åç¯å¢ƒçŠ¶æ€çš„å˜åŒ–

| ç¯å¢ƒå…ƒç´  | å˜åŒ–æƒ…å†µ |
|----------|----------|
| `state` | - é¹¿ä½ç½®çš„ `-2` é€šé“è¢«æ¸…é™¤ï¼ˆé¹¿æ¶ˆå¤±ï¼‰<br>- ç©å®¶æ‰€åœ¨é€šé“ï¼ˆidé€šé“ï¼‰è¢«æ¸…é™¤ |
| `rewards` | æ‰€æœ‰æˆåŠŸæ•é¹¿çš„ç©å®¶è·å¾—åˆ†é…å¥–åŠ±ï¼ˆå‡åˆ†æˆ–æŒ‰ strength åˆ†ï¼‰ |
| `terminal` | è¿™äº›ç©å®¶è¢«æ ‡è®°ä¸ºç»ˆæ­¢ï¼ˆå¯èƒ½åœ¨ç¯å¢ƒä¸­æ¶ˆå¤±ï¼‰ |
| `done` | è¿™äº›ç©å®¶çš„ `done=True` |
| `first_hunt_time` | å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ•é¹¿ï¼Œè®°å½•æ—¶é—´ |
| `hunt_stag_players` | æ¸…é™¤å·²å¤„ç†ç©å®¶ï¼Œé˜²æ­¢é‡å¤å¤„ç† |

---

````




## `other_agent_batches` çš„ç»“æ„

### ç»“æœï¼šä¸¤ä¸ªå…ƒç¥–ï¼Œpolicy å’Œ sample_batch
```bash
 pid=30752) <class 'tuple'>
 pid=30752) 2
 pid=30752) Policy object type: <class 'policy.fixed_policy.NearestStagPolicy'>
 pid=30752) Sample batch1 type: <class 'ray.rllib.policy.sample_batch.SampleBatch'>
```


### ä»£ç ï¼š 
```python
        print(type(other_agent_batches['player_2']))  # åº”è¯¥æ˜¯ <class 'tuple'>
        print(len(other_agent_batches['player_2']))  # åº”è¯¥æ˜¯ 2
        policy_obj, sample_batch1 = other_agent_batches['player_2']
        print("Policy object type:", type(policy_obj))
        print("Sample batch1 type:", type(sample_batch1))
```




## episode çš„ç»“æ„






## Problem1: 
- [?] 

### 1_Answers


### 2_Answers



## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes