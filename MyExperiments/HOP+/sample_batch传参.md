---
created: 2025-01-15T12
updated: ...
åˆ›å»ºæ—¶é—´: 2025-ä¸‰æœˆ-6æ—¥  æ˜ŸæœŸå››, 10:55:18 ä¸Šåˆ
---
[[HOP+]]


# Objectives
- åŸºäº `RLlib`  çš„é¡¹ç›®ï¼Œåœ¨ä¸€ç³»åˆ— `compute_action()` `compute_action_from_dict()` `postprocess_trajectory()`  ` learn_on_batch()` ä¸­ä¼ é€’ `Sample_batch` çš„è‡ªå®šä¹‰çš„å­—æ®µ
`
# Results

## `def postprocess_trajectory(self, sample_batch, other_agent_batches=None, episode=None):` ä¸­ **print** çš„ä¿¡æ¯ï¼š

### ä¸¤ç§ `batch` é‡Œé¢çš„æ ¼å¼ç±»ä¼¼
```python
        
    print(sample_batch['state_out_0'])
        
	print(other_agent_batches[name][1]['state_out_0'])

```


```bash
(RolloutWorker pid=1144881) ğŸ”¹ sample_batch before processing:
(RolloutWorker pid=1144881) SampleBatch(2: ['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'state_in_0', 'state_out_0'])
(RolloutWorker pid=1144881) ğŸ”¹ other_agent_batches:
(RolloutWorker pid=1144881) {'player_2': (LOLAPolicy, SampleBatch(4: ['obs', 'new_obs', 'actions', 'prev_actions', 'rewards', 'prev_rewards', 'dones', 'infos', 'eps_id', 'unroll_id', 'agent_index', 't', 'state_in_0', 'state_out_0']))}
```


### æ·»åŠ Sample_batché¢å¤–ä¿¡æ¯åï¼š
```python
 sample_batch[f'subgoal_self------------'] = sample_batch['state_out_0'].copy()
        if self.train:
            for name in self.player.opponent_model_name_list:
                sample_batch[f'obs_{name}'] = other_agent_batches[name][1]['obs'].copy()
                sample_batch[f'actions_{name}'] = other_agent_batches[name][1]['actions'].copy()
                # sample_batch[f'subgoal_{name}'] = other_agent_batches[name][1]['state_out_0'].copy()

        print("ğŸ”¹ sample_batch after processing:")
        print(sample_batch)
```

```bash
(RolloutWorker pid=1144881) ğŸ”¹ sample_batch after processing:
(RolloutWorker pid=1144881) SampleBatch(2: [ç•¥......ç•¥ï¼Œ 'subgoal_self------------', 'obs_player_2', 'actions_player_2'])
```


## `def learn_on_batch(self, samples):` ä¸­è·å¾—çš„ `subgoal` :

![[Pasted image 20250306110046.png]]


# Insights
# Setup
# Methodology

## 1. `compute_action`() ä¸­æ·»åŠ ä¿¡æ¯åˆ° `recurrent_state` é‡Œé¢

```python

    def compute_actions(self,obs_batch,*args,**kwargs):
		ç•¥ã€‚ã€‚ã€‚
        new_state_batches = []

        subgoal = torch.rand(64,)  # åˆ›å»ºä¸€ä¸ªåŒ…å«ä» 0 åˆ° 63 çš„ subgoal

        # å°† PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
        subgoal_numpy = subgoal.numpy()

        #ç¡®ä¿ new_state_batches ç»´åº¦åŒ¹é… obs_batch.shape[0]
        for _ in range(obs_batch.shape[0]):
            new_state_batches.append(subgoal_numpy)  # æ·»åŠ åˆ° new_state_batches
        new_state_batches = np.expand_dims(new_state_batches, axis=0)  # å˜æˆ (1,1, 64)

        return act_batch, new_state_batches,{}

```

````ad-warning
ç”±äº `new_state_batches` ä¸­åªä¼šä¿ç•™ç¬¬ä¸€åˆ—çš„ä¿¡æ¯ã€‚å› æ­¤ `new_state_batches` ï¼š
>1. (64,)
>2. (1,64)
>éƒ½ä¼šå¯¼è‡´ `state_out_0` åªåŒ…å« ä¸€ä¸ªæ•°æ® ï¼š
![[Pasted image 20250306112102.png]]
>- (1,1,64) å³å¯ï¼ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰
````



## 2. `def postprocess_trajectory()` æ·»åŠ åˆ° `Sample_batch`
```python
 @override(Policy)
    def postprocess_trajectory(
        self, sample_batch, other_agent_batches=None, episode=None
    ):
     

        sample_batch[f'subgoal_self------------'] = sample_batch['state_out_0'].copy()
       
        if self.train:
            for name in self.player.opponent_model_name_list:
                sample_batch[f'obs_{name}'] = other_agent_batches[name][1]['obs'].copy()
                sample_batch[f'actions_{name}'] = other_agent_batches[name][1]['actions'].copy()
                # sample_batch[f'subgoal_{name}'] = other_agent_batches[name][1]['state_out_0'].copy()

        return sample_batch
```

## 3.  `learn_on_batch()` ä¸­æ”¶åˆ°æ›´æ–°åçš„ `Sample_batch`

![[Pasted image 20250306112535.png]]


# Issues & Debugging

## Problem1: 
- [?] 

### 1_Answers


### 2_Answers



## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes