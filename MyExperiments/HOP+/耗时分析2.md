---
created: 2025-01-15T12
updated: ...
创建时间: 2025-六月-27日  星期五, 7:32:33 晚上
---
[[HOP+]]


# Objectives
在 [[耗时分析]] 的基础上，了解了耗时主要是花在 `MCTS` 的 `select` 几段，但是对于 `select` 几段中所包含的 `get_action()` `call_subgoal()` 各自的耗时差距并没有进一步分析。

```ad-note
`get_action()` 是 `MOA` model 的函数
`call_subgoal()` 是 `CVAE` Goal inference model 的函数

与 `direct-OM` 的区别就是 `HOP+` 在 MCTS 的 planning 阶段多了上面两个阶段
与 `HOP` 的区别就是 `HOP+` 在 MCTS 的 planning 阶段多了 `call_subgoal()` 这一个阶段
```

# Results
>经过初步分析， `call_subgoal()` 比 `get_action()` 花费了更多的时间。

# Insights
# Setup

```python
"""
Mcts implementation modified from
https://github.com/brilee/python_uct/blob/master/numpy_impl.py
"""
import collections
import math
import random
from secrets import choice
import numpy as np
import copy
from time import sleep, time
import torch


class Node:
    def __init__(self, action, obs, done, reward, state, model_list, model_id_list, model_goal, target_prey, mcts, id, count, r2_buffer, gamma, parent=None):
        self.env = parent.env
        self.action = action  # Action used to go to this state

        self.is_expanded = False
        self.parent = parent
        self.children = {}

        self.action_space_size = self.env.action_space.n
        self.child_total_value = np.zeros(
            [self.action_space_size], dtype=np.float32)  # Q
        self.child_priors = np.zeros(
            [self.action_space_size], dtype=np.float32)  # P
        self.child_number_visits = np.zeros(
            [self.action_space_size], dtype=np.float32)  # N
        self.valid_actions = obs["action_mask"].astype(bool)

        self.reward = reward
        self.done = done
        self.state = state
        self.obs = obs

        self.model_list=model_list
        self.model_id_list=model_id_list
        self.model_goal = model_goal
        self.target_prey=target_prey

        self.mcts = mcts

        self.id=id
        self.count=count

        self.gamma=gamma
        
        self.r2_buffer=r2_buffer



    @property
    def number_visits(self):
        return self.parent.child_number_visits[self.action]

    @number_visits.setter
    def number_visits(self, value):
        self.parent.child_number_visits[self.action] = value

    @property
    def total_value(self):
        return self.parent.child_total_value[self.action]

    @total_value.setter
    def total_value(self, value):
        self.parent.child_total_value[self.action] = value

    def child_Q(self):
        # TODO (weak todo) add "softmax" version of the Q-value
        return self.child_total_value / (1 + self.child_number_visits)

    def child_U(self):
        return math.sqrt(self.number_visits) * self.child_priors / (
            1 + self.child_number_visits)

    def best_action(self):
        """
        :return: action
        """
        child_score = self.child_Q() + self.mcts.c_puct * self.child_U()
        masked_child_score = child_score
        masked_child_score[~self.valid_actions] = -np.inf
        return np.argmax(masked_child_score)

    def select(self):
        current_node = self
        while current_node.is_expanded:
            best_action = current_node.best_action()
            current_node = current_node.get_child(best_action)
        return current_node

    def expand(self, child_priors):
        self.is_expanded = True
        self.child_priors = child_priors

    def get_child(self, action):
        if action not in self.children:
            self.env.set_state(self.state)
            alive_id=list(np.where(self.env.terminal==0)[0])
            alive_id.remove(self.id-1)
            all_obs=self.env.__obs__(alive_id)
            action_dict={}
            # ⏱️ 统计 get_action 总耗时
            t_get_action_start = time()
            for player in all_obs:
                player_id = int(player.split("_")[1])
                action_prob_dist=self.model_list[player].get_action(all_obs[player],self.state['time'],self.target_prey)
                # print(action_prob_dist)
                action_dict[player]=np.random.choice(7,p=action_prob_dist)
            action_dict[f'player_{self.id}']=action
            self.mcts.t_get_action += time() - t_get_action_start

            obs_all, reward_all, done_all, _ = self.env.step(action_dict)
            done=done_all[f'player_{self.id}']
            obs=obs_all[f'player_{self.id}']

            # ⏱️ 统计 cal_subgoal 耗时
            t_subgoal_start = time()
            target_prey_next = self.model_goal.cal_subgoal(obs['actions'], torch.from_numpy(self.state['state']).unsqueeze(0),
                                                                torch.from_numpy(obs['observation']).unsqueeze(0))
            self.mcts.t_cal_subgoal += time() - t_subgoal_start
            if self.r2_buffer==None:
                reward=reward_all[f'player_{self.id}']
            else:
                reward=self.r2_buffer.normalize(reward_all[f'player_{self.id}'])
            next_state = copy.deepcopy(self.env.get_state())
            self.children[action] = Node(
                state=next_state,
                action=action,
                parent=self,
                reward=reward,
                done=done,
                model_list=self.model_list,
                model_id_list=self.model_id_list,
                model_goal=self.model_goal,
                target_prey=target_prey_next[self.id - 1],
                obs=obs,
                mcts=self.mcts,
                id=self.id,
                count=self.count+1,
                r2_buffer=self.r2_buffer,
                gamma=self.gamma)
        return self.children[action]

    def backup(self, value):
        current = self
        curr_value=value
        while current.parent is not None:
            current.number_visits += 1
            current.total_value += curr_value
            current = current.parent
            curr_value *= self.gamma
    

class RootParentNode:
    def __init__(self, env):
        self.parent = None
        self.child_total_value = collections.defaultdict(float)
        self.child_number_visits = collections.defaultdict(float)
        self.env = env


class MCTS:
    def __init__(self, model, mcts_param, evaluation):
        self.model = model
        self.temperature = mcts_param["temperature"]
        self.dir_epsilon = mcts_param["dirichlet_epsilon"]
        self.dir_noise = mcts_param["dirichlet_noise"]
        self.num_sims = mcts_param["num_simulations"]
        self.exploit = mcts_param["argmax_tree_policy"]
        self.add_dirichlet_noise = mcts_param["add_dirichlet_noise"]
        self.c_puct = mcts_param["puct_coefficient"]
        self.evaluation=evaluation
        self.t_select = 0.0
        self.t_expand = 0.0
        self.t_backup = 0.0
        self.t_final = 0.0
        self.call_count = 0

        self.t_get_action = 0.0
        self.t_cal_subgoal = 0.0

    def compute_action(self, node):
        t0 = time()
        for _ in range(self.num_sims):
            # --- S1: select ---
            t1 = time()
            leaf = node.select()
            t2 = time()
            self.t_select += t2 - t1
            # --- S2: expand or terminal value ---
            t3 = time()

            if leaf.done:
                value = leaf.reward
            else:
                child_priors, value = self.model.compute_priors_and_value(leaf.obs, leaf.state['time'], leaf.target_prey)
                if self.add_dirichlet_noise:
                    child_priors = (1 - self.dir_epsilon) * child_priors
                    child_priors += self.dir_epsilon * np.random.dirichlet(
                        [self.dir_noise] * child_priors.size
                    )

                leaf.expand(child_priors)
            t4 = time()
            self.t_expand += t4 - t3

            # --- S3: backup ---
            t5 = time()
            leaf.backup(value)
            t6 = time()
            self.t_backup += t6 - t5

        # --- S4: Final action construction ---
        t7 = time()
        # Tree policy target (TPT)
        tree_policy = node.child_number_visits / node.number_visits
        Q_value=node.child_Q()
        Q_value[~node.valid_actions] = -np.inf
        tree_policy = tree_policy / np.max(
            tree_policy)  # to avoid overflows when computing softmax
        tree_policy = np.power(tree_policy, self.temperature)
        tree_policy = tree_policy / np.sum(tree_policy)
        if self.exploit:
            # if exploit then choose action that has the maximum
            # tree policy probability
            action = np.argmax(tree_policy)
        else:
            # otherwise sample an action according to tree policy probabilities
            action = np.random.choice(
                np.arange(node.action_space_size), p=tree_policy)
        t8 = time()
        self.t_final += t8 - t7
        self.call_count += 1
        print(
            f"[Timer] MCTS Total: {self.t_select + self.t_expand + self.t_backup + self.t_final:.4f}s | select: {self.t_select:.4f}s | expand: {self.t_expand:.4f}s | backup: {self.t_backup:.4f}s | final: {self.t_final:.4f}s "
        )
        print(
            f"[SubTimer] select阶段子耗时：get_action total: {self.t_get_action:.4f}s | cal_subgoal total: {self.t_cal_subgoal:.4f}s"
        )
        # '''
        # if node.state['time']>25:
        #     print(node.child_total_value)
        #     print(node.child_number_visits)
        #     print(node.child_Q())
        #     print(action)
        #     #print(node.state['state'].transpose((2,0,1)))
        #     #print(node.info)
        #     print(node.state['time'])
        #     #print(node.state['first_hunt_time'])
        #     print(tree_policy)
        #     p,v=self.model.compute_priors_and_value(node.obs,node.state['time'], node.target_prey)
        #     print(p)
        #     print(np.sum(np.log(p+1e-30)*tree_policy))
        #     print(v)
            # node.env.set_state(node.state)
            # node.env.render_show()


            # print(node.children[0].child_total_value)
            # print(node.children[0].child_number_visits)
            # print(node.children[0].child_Q())
        # '''
        return tree_policy, action, node.children[action],Q_value

```

# Methodology
# Issues & Debugging

## Problem1: 
- [?] 

### 1_Answers


### 2_Answers



## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes