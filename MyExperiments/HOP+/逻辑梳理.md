---
创建时间: 2025-一月-21日  星期二, 3:10:33 下午
---
[[HOP+]]

# Objectives
- 理解OMG中关于subgoal的实现机制
- 了解subgoal如何运用在pymarl框架里面的 $\Longrightarrow$ 抽离出来
- HOP中用到goal的地方
- 如何把subgoal结合到HOP

![[Pasted image 20250121145807.png|1050]]

![[Pasted image 20250121145733.png|1100]]


# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1 关于预训练相关问题
- [?] 

### 1_Answers `loss_func` 调用逻辑
[[how_OMG_codes]]

1. `args.am_model` 只会在 `am_leaner` 中使用
2. `alg_args.am_model` 在 `om_controller` 中使用

`loss_func` 
1. 只 `base_am` 和 `omg_am` 中被定义
2. 只在 `am_learner` 中被调用
3.  `omg_learner` 中使用的是 `omg_loss_func` 


- `omg_am` 中的 `loss_func` 才是关于 `VAE` 的逻辑
- `base_am` 中则不是


>关于预训练：
>设置 `am_model` 为 `omg_am` 。以使用它的 `loss_func` 
>使用 `am_leaner` 以调用 `loss_func`



### 2_Answers 关于 `forward`

##### **1. 两个 `forward` 的调用场景不同**：
- **`forward()`**：
  - 在环境交互阶段（如 `select_actions()`）调用。
  - 遍历所有 `agents`，返回主算法的输出。
  - 用于选择动作，不更新参数。

- **`main_alg_forward()`**：
  - 在训练阶段（如 `train()`）调用。
  - 只计算主算法对应的 agent 输出，用于损失计算和优化。
  - **只有主算法的参数会被更新**。

##### **2. 更新时只针对主算法**：
- 训练时，调用 `main_alg_forward()`，只更新 `train_alg` 对应的主算法参数。
- 其他算法的参数不会被更新，但在动作选择阶段依然会被调用，确保多算法环境下正常运行。

##### **3. 进一步验证**：
- 优化器只包含主算法的参数，训练时仅优化 `main_alg_idx` 对应的模型。


##### 代码上的区别：==`forward()` 支持处理 `agent_shift`操作== 


## Problem2
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes