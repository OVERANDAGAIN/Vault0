---
创建时间: 2025-一月-21日  星期二, 3:10:33 下午
---
[[HOP+]]

# Objectives
- 理解OMG中关于subgoal的实现机制
- 了解subgoal如何运用在pymarl框架里面的 $\Longrightarrow$ 抽离出来
- HOP中用到goal的地方
- 如何把subgoal结合到HOP

![[Pasted image 20250121145807.png|1050]]

![[Pasted image 20250121145733.png|1100]]


# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1 关于预训练相关问题
- [?] 

### 1_Answers `loss_func` 调用逻辑
[[how_OMG_codes]]

1. `args.am_model` 只会在 `am_leaner` 中使用
2. `alg_args.am_model` 在 `om_controller` 中使用

`loss_func` 
1. 只 `base_am` 和 `omg_am` 中被定义
2. 只在 `am_learner` 中被调用
3.  `omg_learner` 中使用的是 `omg_loss_func` 


- `omg_am` 中的 `loss_func` 才是关于 `VAE` 的逻辑
- `base_am` 中则不是


>关于预训练：
>设置 `am_model` 为 `omg_am` 。以使用它的 `loss_func` 
>使用 `am_leaner` 以调用 `loss_func`



### 2_Answers 关于 `om_controller` 中的几个 `forward` 函数

```ad-tip
- `forward` 在 `controller, am, agent` 中都被定义
- `main_alg_forward` 在 `omg_controller` 被定义，在 `omg_learner` 中被调用
```


##### **1. 两个 `forward` 的调用场景不同**：
- **`forward()`**：
  - 在环境交互阶段（如 `select_actions()`）调用。
  - 遍历所有 `agents`，返回主算法的输出。
  - 用于选择动作，不更新参数。

- **`main_alg_forward()`**：
  - 在训练阶段（如 `train()`）调用。
  - 只计算主算法对应的 agent 输出，用于损失计算和优化。
  - **只有主算法的参数会被更新**。

##### **2. 更新时只针对主算法**：
- 训练时，调用 `main_alg_forward()`，只更新 `train_alg` 对应的主算法参数。
- 其他算法的参数不会被更新，但在动作选择阶段依然会被调用，确保多算法环境下正常运行。

##### **3. 进一步验证**：
- 优化器只包含主算法的参数，训练时仅优化 `main_alg_idx` 对应的模型。


##### 代码上的一个区别：==`forward()` 支持处理 `agent_shift`操作== 
````ad-tip
### **是的，可以这样总结：**

**无论是 Basic Controller（单算法）还是 OM Controller（多算法），最终返回的结果都是一个算法的输出。**

---

### **1. 返回结果的本质一致性**

- **Basic Controller**：
  ```python
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 直接使用单一策略模型的输出。

- **OM Controller**：
  ```python
  agent_outs = th.cat(agent_outs, dim=-2).view(ep_batch.batch_size, -1, self.args.n_actions)
  agent_outs = agent_outs[:, self.agent_alg_idx, :]
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 遍历所有策略模型后，提取 `agent_alg_idx` 对应的主策略输出，最终返回格式与 Basic Controller 一致。

---

### **2. 逻辑上的差异：多策略管理**

- OM Controller 需要管理多个策略模型，因此会有策略遍历和输出选择的过程。
  - 多策略环境的 **主策略** 是由 `agent_alg_idx` 确定的。
  - 只有主策略的输出会被用作最终结果。

- Basic Controller 直接使用单一策略模型，不需要多策略管理逻辑。

---

### **3. 为什么都返回单个策略的输出？**

- **输出格式的统一性**：  
  无论是单策略还是多策略环境，调用控制器的上层逻辑（例如 `select_actions()`）通常需要统一的输出格式（批量大小 × 代理数量 × 动作数量）。  

- **主策略的确定性**：  
  在多策略环境中，尽管控制器管理多个策略模型，最终的训练或推理只依赖主策略的输出。

---

### **4. 功能实现上的意义**

- 在 Basic Controller 中，**默认使用一个策略模型**进行训练和推理。
- 在 OM Controller 中，**支持多策略管理**，可以在同一实验中切换不同算法，灵活性更高。

这种设计使得代码可以在不同策略环境下复用，既支持单策略的实验，也支持多策略的对比和研究。

---

### **5. 总结**

可以得出以下结论：

> **尽管 Basic Controller 和 OM Controller 的内部逻辑不同（单策略 vs. 多策略），最终返回的都是一个策略的输出，且返回格式一致（批量大小 × 代理数量 × 动作数量）。**  
>
> 这种设计在保证灵活性的同时，维持了上层逻辑的统一性和兼容性。
````

### 3_Answer 关于 `load_model()` 
1. 在 `learners` 中被调用: `mac.load_model()`
2. 在 `am` 中被定义：`load_state_dict` 
	1. `vae(omg_am)`
	2. `base_am`
3. 在 `controller` 中定义（即1中的调用）：
	1. `agent - agent.th`
	2. `am load...()`


  "checkpoint_path": "",
  "eval_model_path": "",
  "evaluate_path": "",
  "init_param_path": "",


## Problem2
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes