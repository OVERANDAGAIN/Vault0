---
创建时间: 2025-一月-21日  星期二, 3:10:33 下午
---
[[HOP+]]

# Objectives
- 理解OMG中关于subgoal的实现机制
- 了解subgoal如何运用在pymarl框架里面的 $\Longrightarrow$ 抽离出来
- HOP中用到goal的地方
- 如何把subgoal结合到HOP

![[Pasted image 20250121145807.png|1050]]

![[Pasted image 20250121145733.png|1100]]


# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1 关于预训练相关问题
- [?] 

### 1_Answers `loss_func` 调用逻辑
[[how_OMG_codes]]

1. `args.am_model` 只会在 `am_leaner` 中使用
2. `alg_args.am_model` 在 `om_controller` 中使用

`loss_func` 
1. 只 `base_am` 和 `omg_am` 中被定义
2. 只在 `am_learner` 中被调用
3.  `omg_learner` 中使用的是 `omg_loss_func` 


- `omg_am` 中的 `loss_func` 才是关于 `VAE` 的逻辑
- `base_am` 中则不是


>关于预训练：
>设置 `am_model` 为 `omg_am` 。以使用它的 `loss_func` 
>使用 `am_leaner` 以调用 `loss_func`



### 2_Answers 关于 `om_controller` 中的几个 `forward` 函数

```ad-tip
- `forward` 在 `controller, am, agent` 中都被定义
- `main_alg_forward` 在 `omg_controller` 被定义，在 `omg_learner` 中被调用
```


##### **1. 两个 `forward` 的调用场景不同**：
- **`forward()`**：
  - 在环境交互阶段（如 `select_actions()`）调用。
  - 遍历所有 `agents`，返回主算法的输出。
  - 用于选择动作，不更新参数。

- **`main_alg_forward()`**：
  - 在训练阶段（如 `train()`）调用。
  - 只计算主算法对应的 agent 输出，用于损失计算和优化。
  - **只有主算法的参数会被更新**。

##### **2. 更新时只针对主算法**：
- 训练时，调用 `main_alg_forward()`，只更新 `train_alg` 对应的主算法参数。
- 其他算法的参数不会被更新，但在动作选择阶段依然会被调用，确保多算法环境下正常运行。

##### **3. 进一步验证**：
- 优化器只包含主算法的参数，训练时仅优化 `main_alg_idx` 对应的模型。


##### 代码上的一个区别：==`forward()` 支持处理 `agent_shift`操作== 
````ad-tip
### **是的，可以这样总结：**

**无论是 Basic Controller（单算法）还是 OM Controller（多算法），最终返回的结果都是一个算法的输出。**

---

### **1. 返回结果的本质一致性**

- **Basic Controller**：
  ```python
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 直接使用单一策略模型的输出。

- **OM Controller**：
  ```python
  agent_outs = th.cat(agent_outs, dim=-2).view(ep_batch.batch_size, -1, self.args.n_actions)
  agent_outs = agent_outs[:, self.agent_alg_idx, :]
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 遍历所有策略模型后，提取 `agent_alg_idx` 对应的主策略输出，最终返回格式与 Basic Controller 一致。

---

### **2. 逻辑上的差异：多策略管理**

- OM Controller 需要管理多个策略模型，因此会有策略遍历和输出选择的过程。
  - 多策略环境的 **主策略** 是由 `agent_alg_idx` 确定的。
  - 只有主策略的输出会被用作最终结果。

- Basic Controller 直接使用单一策略模型，不需要多策略管理逻辑。

---

### **3. 为什么都返回单个策略的输出？**

- **输出格式的统一性**：  
  无论是单策略还是多策略环境，调用控制器的上层逻辑（例如 `select_actions()`）通常需要统一的输出格式（批量大小 × 代理数量 × 动作数量）。  

- **主策略的确定性**：  
  在多策略环境中，尽管控制器管理多个策略模型，最终的训练或推理只依赖主策略的输出。

---

### **4. 功能实现上的意义**

- 在 Basic Controller 中，**默认使用一个策略模型**进行训练和推理。
- 在 OM Controller 中，**支持多策略管理**，可以在同一实验中切换不同算法，灵活性更高。

这种设计使得代码可以在不同策略环境下复用，既支持单策略的实验，也支持多策略的对比和研究。

---

### **5. 总结**

可以得出以下结论：

> **尽管 Basic Controller 和 OM Controller 的内部逻辑不同（单策略 vs. 多策略），最终返回的都是一个策略的输出，且返回格式一致（批量大小 × 代理数量 × 动作数量）。**  
>
> 这种设计在保证灵活性的同时，维持了上层逻辑的统一性和兼容性。
````

### 3_Answer 关于 `load_model()` 
1. 在 `learners` 中被调用: `mac.load_model()`
2. 在 `am` 中被定义：`load_state_dict` 
	1. `vae(omg_am)`
	2. `base_am`
3. 在 `controller` 中定义（即1中的调用）：
	1. `agent - agent.th`
	2. `am load...()`


  "checkpoint_path": "",
  "eval_model_path": "",
  "evaluate_path": "",
  "init_param_path": "",



### 4_Answer 关于 `obs_is_state`
### **关于 `obs_is_state` 的完整总结**

#### **1. `om_controller` 类中的 `obs_is_state` 相关处理**

##### **1.1 `_build_inputs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:  # 没有 n_agents 维度
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)  # 扩展 n_agents 维度
    elif len(state.shape) != 4:
        raise IndexError

obs_inputs = []
if self.args.obs_is_state:
    obs_inputs.append(state[:, t])  # 取当前时间步的状态
else:
    obs_inputs.append(batch["obs"][:, t])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若 `state` 没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
    - 最终在时间步 `t` 时为：`[batch_size, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 在时间步 `t` 时为：`[batch_size, n_agents, obs_dim]`

---

#### **2. `omg_am` 类中的 `obs_is_state` 相关处理**

##### **2.1 `_build_inputs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:  # 没有 n_agents 维度
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)

if self.args.obs_is_state:
    fc_inputs.append(state[:, ts])
else:
    fc_inputs.append(batch["obs"][:, ts])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若 `state` 没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
    - 在时间步 `t` 取值后为：`[batch_size, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 在时间步 `t` 取值后为：`[batch_size, n_agents, obs_dim]`

---

##### **2.2 `_build_vae_inputs` 方法** 有维度差异

- **代码片段：**
```python
if self.args.obs_is_state:
    obs = batch["state"][:, :-1]
else:
    obs = batch["obs"][:, :-1]

reshaped_obs = obs.reshape(-1, obs.shape[-1])
reshaped_mask = mask.unsqueeze(2).repeat(1, 1, self.n_agents, 1).reshape(-1, 1)
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 取除最后时间步后的序列为：`[batch_size, seq_len - 1, state_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1), state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 取除最后时间步后的序列为：`[batch_size, seq_len - 1, n_agents, obs_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

##### **2.3 `_build_obs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)
    inputs.append(state)
else:
    inputs.append(batch["obs"])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`

---

##### **2.4 `_cal_subgoal` 方法** 有维度差异

- **代码片段：**
```python
if self.args.obs_is_state:
    obs_for_vae = batch["state"][:, 1:]
else:
    obs_for_vae = batch["obs"][:, 1:]
obs_for_vae = obs_for_vae.reshape(-1, obs_for_vae.shape[-1])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 取除第一个时间步后的序列为：`[batch_size, seq_len - 1, state_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1), state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 取除第一个时间步后的序列为：`[batch_size, seq_len - 1, n_agents, obs_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

### **总结**

- **涉及到的所有方法：**
  - **om_controller：**
    - `_build_inputs`
  - **omg_am：**
    - `_build_inputs`
    - `_build_vae_inputs`
    - `_build_obs`
    - `_cal_subgoal`

- **维度差异：**
  - 当 `obs_is_state=True` 时：
    - 数据形状为 `[batch_size, seq_len, state_dim]`，通常在输入中需要扩展 n_agents 维度，变为 `[batch_size, seq_len, n_agents, state_dim]`
    - 最终 reshape 结果为 `[batch_size * (seq_len - 1), state_dim]`

  - 当 `obs_is_state=False` 时：
    - 数据形状为 `[batch_size, seq_len, n_agents, obs_dim]`
    - 最终 reshape 结果为 `[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

### **潜在问题**

- `obs_is_state` 为 `True` 和 `False` 时的处理逻辑不一致，导致后续操作中可能出现维度不匹配。
- 统一扩展为 `[batch_size, seq_len, n_agents, dim]` 格式可以减少维度不一致问题。



### 关于其他模块如何处理 `build_inputs`

我们可以从代码中总结不同类型的 AM 和 Controller 对于 `build_inputs`（或类似方法）在处理 observation（`obs`）时的不同方式。下面逐一分析它们的处理逻辑和维度变化。

---

### **1. Base_AM**
```python
def _build_inputs(self, batch, t=None):
    bs = batch.batch_size
    max_t = batch.max_seq_length if t is None else 1
    ts = slice(None) if t is None else slice(t, t+1)
    inputs = []

    # observation
    inputs.append(batch["obs"][:, ts])

    inputs = th.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
    return inputs
```
- **输入内容**：只处理 `obs`，没有处理其他辅助信息。
- **处理方式**：  
  - 直接从 `batch["obs"]` 获取观测数据，提取时间片段 `ts`。
  - 将观测数据 **按 `batch_size`, `max_t`, `n_agents`, `feature_dim`** 维度进行 reshape 并拼接。
- **维度**：`[batch_size, max_t, n_agents, feature_dim]`
  
---

### **2. None_AM**
```python
def _build_inputs(self, batch, t=None):
    bs = batch.batch_size
    max_t = batch.max_seq_length if t is None else 1
    
    return th.zeros((bs, max_t, self.n_agents, 0), device=batch['obs'].device)
```
- **输入内容**：没有实际输入，仅返回一个全 0 张量。
- **处理方式**：  
  - 直接生成 **零张量**。
  - 张量维度与 `batch_size`, `max_t`, `n_agents` 对应，但没有特征维度。
  
- **维度**：`[batch_size, max_t, n_agents, 0]`

---

### **3. Basic_Controller**
```python
def _build_inputs(self, batch, t):
    bs = batch.batch_size
    inputs = []

    # Add observation
    inputs.append(batch["obs"][:, t])  # b1av

    # Add last action (if applicable)
    if self.args.obs_last_action:
        if t == 0:
            inputs.append(th.zeros_like(batch["actions_onehot"][:, t]))
        else:
            inputs.append(batch["actions_onehot"][:, t-1])

    # Add agent ID (if applicable)
    if self.args.obs_agent_id:
        inputs.append(th.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))

    inputs = th.cat([x.reshape(bs * self.n_agents, -1) for x in inputs], dim=1)
    return inputs
```
- **输入内容**：处理观测数据、上一步的动作、agent ID 信息。
- **处理方式**：  
  - 先获取时间步 `t` 的 `obs`。
  - 如果启用 `obs_last_action`，还会添加上一时间步的动作 `actions_onehot`。
  - 如果启用 `obs_agent_id`，还会加入 `n_agents` 的 one-hot 编码。
  - 所有输入拼接后，reshape 成 **[batch_size * n_agents, feature_dim]**。

- **维度**：`[batch_size * n_agents, feature_dim]`

---

### **4. OM_Controller**
```python
def _build_inputs(self, batch, t):
    bs = batch.batch_size

    # 如果启用 obs_is_state，则使用 state，否则使用 obs
    if self.args.obs_is_state:
        state = batch["state"]
        if len(state.shape) == 3:
            state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)

    obs_inputs = []
    if self.args.obs_is_state:
        obs_inputs.append(state[:, t])
    else:
        obs_inputs.append(batch["obs"][:, t])
        # 添加其他信息（如 last action 和 agent ID）
```
- **输入内容**：处理 `state` 或 `obs`，以及可选的 `actions_onehot` 和 `agent_id` 信息。
- **处理方式**：
  - 如果启用 `obs_is_state`，会使用全局状态 `state`，并扩展到 `n_agents` 维度。
  - 否则直接使用 `obs`。
  - 同时还会拼接 last action 和 agent ID。

- **维度**：  
  - 如果 `obs_is_state=True`：`[batch_size, n_agents, feature_dim]`
  - 否则：`[batch_size, n_agents, feature_dim]`

---

### **5. OMG_AM**
```python
def _build_inputs(self, batch, t):
    bs = batch.batch_size
    max_t = batch.max_seq_length if t is None else 1

    # 使用 state 或 obs
    if self.args.obs_is_state:
        state = batch["state"]
        if len(state.shape) == 3:
            state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)

    fc_inputs = []
    if self.args.obs_is_state:
        fc_inputs.append(state[:, t])
    else:
        fc_inputs.append(batch["obs"][:, t])

    # 构建条件输入 cond_inputs
    cond_inputs = []
    if self.args.obs_is_state:
        last_obs = th.cat([th.zeros_like(state[:, 0:1]), state[:, :-1]], dim=1)
        cond_inputs.append(last_obs[:, t])
    else:
        last_obs = th.cat([th.zeros_like(batch["obs"][:, 0:1]), batch["obs"][:, :-1]], dim=1)
        cond_inputs.append(last_obs[:, t])

    # 其他条件输入：动作、agent ID 等
```
- **输入内容**：处理 `obs` 或 `state`，以及条件输入（如上一时间步的 `obs`、动作等）。
- **处理方式**：
  - 根据 `obs_is_state` 决定使用 `state` 或 `obs`。
  - 同时构建条件输入（如 `last_obs`、上一时间步的动作等），用于 CVAE 推断。
  
- **维度**：
  - `fc_inputs`：`[batch_size, n_agents, feature_dim]`
  - `cond_inputs`：`[batch_size, n_agents, condition_dim]`

---

### **总结：**

1. **是否使用 `obs_is_state`**：
   - 仅在 `OM_Controller` 和 `OMG_AM` 中引入，目的是在某些场景下使用全局状态 `state` 作为输入，而不是局部观测 `obs`。

2. **不同模块的处理方式**：
   - `base_am`、`basic_controller` 直接使用 `obs` 作为输入。
   - `none_am` 返回空输入，不处理任何特征。
   - `OM_Controller` 和 `OMG_AM` 根据 `obs_is_state` 动态选择输入来源（`state` 或 `obs`）。

3. **维度处理**：
   - `base_am`、`none_am` 维度保持 **[batch_size, max_t, n_agents, feature_dim]**。
   - `basic_controller` 将输入 reshape 为 **[batch_size * n_agents, feature_dim]**。
   - `OM_Controller` 和 `OMG_AM` 在某些情况下扩展 `n_agents` 维度，但在 VAE 阶段的 `_build_vae_inputs` 方法中没有扩展 `n_agents`。

## Problem2
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes