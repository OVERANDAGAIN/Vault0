---
创建时间: 2025-一月-21日  星期二, 3:10:33 下午
---
[[HOP+]]

# Objectives
- 理解OMG中关于subgoal的实现机制
- 了解subgoal如何运用在pymarl框架里面的 $\Longrightarrow$ 抽离出来
- HOP中用到goal的地方
- 如何把subgoal结合到HOP

![[Pasted image 20250121145807.png|1050]]

![[Pasted image 20250121145733.png|1100]]


# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1 关于预训练相关问题
- [?] 

### 1_Answers `loss_func` 调用逻辑
[[how_OMG_codes]]

1. `args.am_model` 只会在 `am_leaner` 中使用
2. `alg_args.am_model` 在 `om_controller` 中使用

`loss_func` 
1. 只 `base_am` 和 `omg_am` 中被定义
2. 只在 `am_learner` 中被调用
3.  `omg_learner` 中使用的是 `omg_loss_func` 


- `omg_am` 中的 `loss_func` 才是关于 `VAE` 的逻辑
- `base_am` 中则不是


>关于预训练：
>设置 `am_model` 为 `omg_am` 。以使用它的 `loss_func` 
>使用 `am_leaner` 以调用 `loss_func`



### 2_Answers 关于 `om_controller` 中的几个 `forward` 函数

```ad-tip
- `forward` 在 `controller, am, agent` 中都被定义
- `main_alg_forward` 在 `omg_controller` 被定义，在 `omg_learner` 中被调用
```


##### **1. 两个 `forward` 的调用场景不同**：
- **`forward()`**：
  - 在环境交互阶段（如 `select_actions()`）调用。
  - 遍历所有 `agents`，返回主算法的输出。
  - 用于选择动作，不更新参数。

- **`main_alg_forward()`**：
  - 在训练阶段（如 `train()`）调用。
  - 只计算主算法对应的 agent 输出，用于损失计算和优化。
  - **只有主算法的参数会被更新**。

##### **2. 更新时只针对主算法**：
- 训练时，调用 `main_alg_forward()`，只更新 `train_alg` 对应的主算法参数。
- 其他算法的参数不会被更新，但在动作选择阶段依然会被调用，确保多算法环境下正常运行。

##### **3. 进一步验证**：
- 优化器只包含主算法的参数，训练时仅优化 `main_alg_idx` 对应的模型。


##### 代码上的一个区别：==`forward()` 支持处理 `agent_shift`操作== 
````ad-tip
### **是的，可以这样总结：**

**无论是 Basic Controller（单算法）还是 OM Controller（多算法），最终返回的结果都是一个算法的输出。**

---

### **1. 返回结果的本质一致性**

- **Basic Controller**：
  ```python
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 直接使用单一策略模型的输出。

- **OM Controller**：
  ```python
  agent_outs = th.cat(agent_outs, dim=-2).view(ep_batch.batch_size, -1, self.args.n_actions)
  agent_outs = agent_outs[:, self.agent_alg_idx, :]
  return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)
  ```
  - 遍历所有策略模型后，提取 `agent_alg_idx` 对应的主策略输出，最终返回格式与 Basic Controller 一致。

---

### **2. 逻辑上的差异：多策略管理**

- OM Controller 需要管理多个策略模型，因此会有策略遍历和输出选择的过程。
  - 多策略环境的 **主策略** 是由 `agent_alg_idx` 确定的。
  - 只有主策略的输出会被用作最终结果。

- Basic Controller 直接使用单一策略模型，不需要多策略管理逻辑。

---

### **3. 为什么都返回单个策略的输出？**

- **输出格式的统一性**：  
  无论是单策略还是多策略环境，调用控制器的上层逻辑（例如 `select_actions()`）通常需要统一的输出格式（批量大小 × 代理数量 × 动作数量）。  

- **主策略的确定性**：  
  在多策略环境中，尽管控制器管理多个策略模型，最终的训练或推理只依赖主策略的输出。

---

### **4. 功能实现上的意义**

- 在 Basic Controller 中，**默认使用一个策略模型**进行训练和推理。
- 在 OM Controller 中，**支持多策略管理**，可以在同一实验中切换不同算法，灵活性更高。

这种设计使得代码可以在不同策略环境下复用，既支持单策略的实验，也支持多策略的对比和研究。

---

### **5. 总结**

可以得出以下结论：

> **尽管 Basic Controller 和 OM Controller 的内部逻辑不同（单策略 vs. 多策略），最终返回的都是一个策略的输出，且返回格式一致（批量大小 × 代理数量 × 动作数量）。**  
>
> 这种设计在保证灵活性的同时，维持了上层逻辑的统一性和兼容性。
````

### 3_Answer 关于 `load_model()` 
1. 在 `learners` 中被调用: `mac.load_model()`
2. 在 `am` 中被定义：`load_state_dict` 
	1. `vae(omg_am)`
	2. `base_am`
3. 在 `controller` 中定义（即1中的调用）：
	1. `agent - agent.th`
	2. `am load...()`


  "checkpoint_path": "",
  "eval_model_path": "",
  "evaluate_path": "",
  "init_param_path": "",



### 4_Answer 关于 `obs_is_state`
### **关于 `obs_is_state` 的完整总结**

#### **1. `om_controller` 类中的 `obs_is_state` 相关处理**

##### **1.1 `_build_inputs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:  # 没有 n_agents 维度
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)  # 扩展 n_agents 维度
    elif len(state.shape) != 4:
        raise IndexError

obs_inputs = []
if self.args.obs_is_state:
    obs_inputs.append(state[:, t])  # 取当前时间步的状态
else:
    obs_inputs.append(batch["obs"][:, t])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若 `state` 没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
    - 最终在时间步 `t` 时为：`[batch_size, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 在时间步 `t` 时为：`[batch_size, n_agents, obs_dim]`

---

#### **2. `omg_am` 类中的 `obs_is_state` 相关处理**

##### **2.1 `_build_inputs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:  # 没有 n_agents 维度
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)

if self.args.obs_is_state:
    fc_inputs.append(state[:, ts])
else:
    fc_inputs.append(batch["obs"][:, ts])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若 `state` 没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
    - 在时间步 `t` 取值后为：`[batch_size, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 在时间步 `t` 取值后为：`[batch_size, n_agents, obs_dim]`

---

##### **2.2 `_build_vae_inputs` 方法** 有维度差异

- **代码片段：**
```python
if self.args.obs_is_state:
    obs = batch["state"][:, :-1]
else:
    obs = batch["obs"][:, :-1]

reshaped_obs = obs.reshape(-1, obs.shape[-1])
reshaped_mask = mask.unsqueeze(2).repeat(1, 1, self.n_agents, 1).reshape(-1, 1)
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 取除最后时间步后的序列为：`[batch_size, seq_len - 1, state_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1), state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 取除最后时间步后的序列为：`[batch_size, seq_len - 1, n_agents, obs_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

##### **2.3 `_build_obs` 方法**

- **代码片段：**
```python
if self.args.obs_is_state:
    state = batch["state"]
    if len(state.shape) == 3:
        state = state.unsqueeze(2).repeat(1, 1, self.n_agents, 1)
    inputs.append(state)
else:
    inputs.append(batch["obs"])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 若没有 n_agents 维度，扩展为：`[batch_size, seq_len, n_agents, state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`

---

##### **2.4 `_cal_subgoal` 方法** 有维度差异

- **代码片段：**
```python
if self.args.obs_is_state:
    obs_for_vae = batch["state"][:, 1:]
else:
    obs_for_vae = batch["obs"][:, 1:]
obs_for_vae = obs_for_vae.reshape(-1, obs_for_vae.shape[-1])
```

- **维度情况：**
  - `obs_is_state=True`：
    - `batch["state"]` 原始维度：`[batch_size, seq_len, state_dim]`
    - 取除第一个时间步后的序列为：`[batch_size, seq_len - 1, state_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1), state_dim]`
  
  - `obs_is_state=False`：
    - `batch["obs"]` 原始维度：`[batch_size, seq_len, n_agents, obs_dim]`
    - 取除第一个时间步后的序列为：`[batch_size, seq_len - 1, n_agents, obs_dim]`
    - reshape 后为：`[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

### **总结**

- **涉及到的所有方法：**
  - **om_controller：**
    - `_build_inputs`
  - **omg_am：**
    - `_build_inputs`
    - `_build_vae_inputs`
    - `_build_obs`
    - `_cal_subgoal`

- **维度差异：**
  - 当 `obs_is_state=True` 时：
    - 数据形状为 `[batch_size, seq_len, state_dim]`，通常在输入中需要扩展 n_agents 维度，变为 `[batch_size, seq_len, n_agents, state_dim]`
    - 最终 reshape 结果为 `[batch_size * (seq_len - 1), state_dim]`

  - 当 `obs_is_state=False` 时：
    - 数据形状为 `[batch_size, seq_len, n_agents, obs_dim]`
    - 最终 reshape 结果为 `[batch_size * (seq_len - 1) * n_agents, obs_dim]`

---

### **潜在问题**

- `obs_is_state` 为 `True` 和 `False` 时的处理逻辑不一致，导致后续操作中可能出现维度不匹配。
- 统一扩展为 `[batch_size, seq_len, n_agents, dim]` 格式可以减少维度不一致问题。



### 关于其他模块如何处理 `build_inputs`



## Problem2
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes