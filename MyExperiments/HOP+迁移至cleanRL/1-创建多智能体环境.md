---
创建时间: 2025-七月-17日  星期四, 3:59:33 下午
---
[[HOP+迁移至cleanrl]]

[[2025-07-14]]


# Objectives
# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1: 
- [?] 
## 多智能体训练的构建方式：
1. **当前采用的方式**：自定义 `StagHuntEnv` 环境（继承自 `MultiAgentEnv`）经过修改后，封装为 `MultiAgentVecEnv`，再手动管理每个 agent 在多个环境中的生存状态，从而支持并行 rollout 和异步退出。

2. **（PettingZoo + Supersuit）**：PettingZoo 标准化接口，配合 Supersuit 将环境向量化实现并行。问题：
	1. 原环境中存在较强的 RLlib 依赖（如 `MultiAgentEnv`、`reset()`/`step()` 返回结构不兼容）；
	2. PettingZoo支持；
	3. 自定义字段接口对齐。

3. **CleanRL 官方示例方式**：使用 `gym.vector.SyncVectorEnv`/`AsyncVectorEnv` 等提供的并行机制，支持单智能体。
![[Pasted image 20250714154226.png]]


### 目前采用第一种方法
完全自定义，向量环境中使用 `for` 循环遍历处理各个人的 `obs` `action` 等信息
>HYZ说：这里应该对性能不会造成太大的影响


### 2_Answers



## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes