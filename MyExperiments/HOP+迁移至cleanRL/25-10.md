---
创建时间: 2025-十月-20日  星期一, 7:43:01 晚上
---
[[HOP+迁移至cleanrl]]
[[HOP+迁移至cleanrl]]


# Objectives

* **定位问题根源**：针对 HOP+ 迁移至 CleanRL 框架后无法稳定训练出合作策略的现象，本周的主要目标是通过逐模块排查的方式，明确问题究竟出在 VAE、world model、MOA 模块，还是在整体策略训练逻辑。
* **验证 VAE 表现**：通过对比原始 VAE 与新版 VAE 的 loss 曲线，检验不同 loss 设计（重建项 vs. MOA loss）对模型训练稳定性和表征能力的影响，确认 VAE 是否在信息压缩和动作预测中存在缺陷。
* **评估 world_model 能力**：重点分析 world_model 在状态预测和奖励建模中的精度，观察其是否能为策略提供可靠的前瞻性信息，并判断异常预测（如 agent “跳动”）是否会干扰训练。
* **检查 MOA 模块训练效果**：通过混淆矩阵、动作分布和召回率指标，评估 MOA 在多类动作上的预测性能，分析是否由于离线数据分布偏差或 buffer 逻辑设计，导致模型在自博弈训练中失效。
* **建立 baseline 对照**：结合 PPO baseline 与 HOP+（含 MCTS/不含 MCTS）实验结果，形成性能对比，进一步缩小问题来源，为后续改进提供明确方向。





## Problem1: VAE
- [?] 

### 原来的VAE: loss图: 总体loss很低
4p5b8\*8 
```terminal
D:\BaiduNetdiskDownload\msh-records\4p5b8_8\vae\exp20250401_203634p\dim16
```
![[myplot_4p_16dim.png]]

### moa-VAE：loss图： loss比较大
4p5b8\*8
```
D:\cleanrl\cleanRL\vae_ckpts\vae__1758371809\dim16
```

![[vae_loss_curve.png]]


### moa情况
总体保持稳定，也就是moa预测0.6左右

#### 混淆矩阵：对角线越深越好
![[moa_eval_epoch9_confusion_matrix.png]]

#### 召回率：整体动作准确率

$$
\text{Recall}_i = \frac{\text{预测正确为 i 的样本数}}{\text{真实标签为 i 的总样本数}}
$$

也就是：在所有 **真实为某类** 的样本里，模型有多少被成功识别出来。

![[moa_eval_epoch9_per_class_recall.png]]



## Problem：World_model
- [?] 

![[Pasted image 20250921015931.png]]

在上面这张图的例子中，world_model预测的agent一下子走了两个，并且取得了0.042的奖励。
>不符合物理规律 —————— 联想到：对比学习促进强化学习的那篇文章。

world_model总体预测的还是大体上位置正确，但是做不到非常精准.




- [?] 看一下具体channel 中具体的值是多少，而不是直接四舍五入




---


### 一、通道分布对比

#### 图含义：

每个 “chX dist” 子图展示一个通道（例如 ch0-ch6，对应玩家/障碍/鹿/兔等通道）在真值（true）与预测（pred）下的像素值分布直方图。

蓝色：**真实** 观测值分布（true）
橙色：world model **预测** 分布（pred）

![[calibration_hist.png|1375]]

---

### 二、像素级散点图
####  图含义：
**横轴是真实**像素值（true），**纵轴是预测**像素值（pred）。
黑色虚线：**理想预测线**（y = x）。


![[calibration_scatter.png]]

#### 看到的现象：
* 两条主要的竖线分布：
  * 左侧在 true≈0 附近：绝大多数背景像素；
  * 右侧在 true≈1 附近：物体像素（例如玩家或猎物位置）。
* 在 true=0 时，大部分预测值在 0~0.1 之间，但存在少量 0.3~0.5 的“假阳性”；
* 在 true=1 时，预测值集中在 0.8~1.0 区间，少量点掉落到 0.5 以下。

#### 总结：
> 模型输出在统计意义上是**有校准性的（分布正确）**，
> 但个体像素存在一定的过度平滑。


---

### 三、可视化对比


![[triplet_soft_015.png]]

### **Sample #15**

> `self_a=0 | opp_as=[4,3,6] | true_rew=5.000 | pred_rew=5.012`

**现象：**

* 预测帧 `pred_obs_{t+1}` 中，蓝色五边形（其他玩家）位置基本准确；
* 鹿（黄色星形）被成功预测为消失；
* 误差热力图仅在左中部有轻微亮斑；
* 奖励预测几乎完美（5.012 ≈ 5.000）。


---


![[triplet_soft_026.png]]


### **Sample #26**

> `self_a=4 | opp_as=[4,4,4] | true_rew=0.000 | pred_rew=0.007`

**现象：**

* 模型预测奖励几乎为零（无捕猎）。


* 右图里那些“虚化/半透明”的形状，本质上是 **world_model 的连续输出（0~1 概率/强度）不够接近 1**——通常出现在**多模态/不确定**的地方（它拿不准下一个精确格子），或被 MSE 类损失“平均化”。
* 如果你 **用固定阈值 0.5 二值化**（<0.5→0），就会把这些“半可信”的像素直接抹掉——于是出现你在图三看到的现象：**有些 agent/物体“消失”**。

```ad-seealso
![[Pasted image 20251020153547.png]]

* **现象**：像 sample#4 那张，预测里某 agent 像是“走了两步”。
* **原因**：

  * 多个相邻格子都有中等强度（0.3~0.6），可视化时像“拖影”；
  * 如果后处理取 **argmax**，在噪声/模糊中可能选到了“更远的一个峰”，看起来像“跨格”；
```


---


![[triplet_soft_027.png]]


### **Sample #27**

> `self_a=2 | opp_as=[4,1,0] | true_rew=5.000 | pred_rew=4.968`

**现象：**

* 鹿在预测中正确消失；
* 但左下方一名蓝色五边形位置略有漂移；
* 误差热力图仅在猎物消失区域和玩家交互区亮起；
* 奖励预测依然准确（4.968 ≈ 5.000）。




## 3NS

### player_1 个体奖励

![[Pasted image 20251020153911.png]]

### 总体平均奖励
![[Pasted image 20251020153959.png]]

### moa预测情况

![[Pasted image 20251020154114.png]]



| 指标                 | 含义                      | 趋势解读                                            |
| ------------------ | ----------------------- | ----------------------------------------------- |
| `acc_top1`         | 模型准确预测对手下一步动作的比例（Top-1） | 约 0.52–0.62 → 轻微上升后回落，说明模型初期学习有效但出现**过拟合/噪声干扰** |
| `acc_top3`         | 预测Top-3准确率              | 约 0.84–0.88 → 稳定较高，说明模型掌握了**动作空间的大致分布**         |
| `ce_loss`          | Cross Entropy 损失        | 前期快速下降后反弹（U型）→ 模型早期收敛、后期不稳定                     |
| `per_class_acc[i]` | 各动作类别的分类准确率（动作 0–6）     | 不同动作波动显著，部分类明显偏低                                |

| 动作ID      | 曲线表现             | 可能含义                   |
| --------- | ---------------- | ---------------------- |
| 0–3（移动方向） | 稳定在 0.5–0.6，轻微下降 | 模型能部分预测方向，但在动态博弈时混淆度上升 |
| 4（原地不动）   | 约 0.45–0.55，波动较大 | 静态状态少，模型置信不足           |
| 5（猎鹿）     | 几乎完美（>0.99）      | 捕猎动作特征极强，易识别           |
| 6（猎兔）     | 仅 0.35–0.45，且波动大 | 样本少 + 难区分，导致模型混淆       |

### 结合 World Model 实验的推论

结果与之前你在 **world_model ** 实验中的：

| 观测现象          | 解释                            |
| ------------- | ----------------------------- |
| 玩家动作预测偏移 / 模糊 | MOA 无法稳定锁定个体动作（acc_top1≈0.55） |
| 动作空间不平衡       | CE loss 后期反弹反映了样本分布偏移         |
| 捕猎类事件预测准确     | 动作5 准确率极高（>0.99），说明语义层捕捉能力强   |
| 鹿/兔 相关错误集中    | 对应动作6（猎兔）准确率最低，说明该子事件建模最弱     |

 对应：

> MOA 模型的不确定性（特别是移动方向/猎兔） →
> World Model 预测的“虚化”或“agent 消失” →
> 最终影响 action mask 和策略行为稳定性。


---


## 1NS-2NH

### player_1 个体奖励

![[Pasted image 20251020154449.png]]
### 总体平均奖励

![[Pasted image 20251020154458.png]]

| 指标类别                                           | 1NS–2NH（混合对手）            |     |
| :--------------------------------------------- | :----------------------- | :-- |
| **平均个体奖励 (agent_player_1)**                    | 峰值约 **4.6**，稳定区间 4.2–4.5 |     |
| **全体平均奖励 (avg_reward_per_episode_all_agents)** | 稳定在 **2.9**，收敛平稳         |     |
|                                                |                          |     |



### moa预测情况
![[Pasted image 20251020154713.png]]

| 层面                    | 观察               | 含义                             |
| --------------------- | ---------------- | ------------------------------ |
| **CE Loss 显著降低**      | 混合场景下 Loss < 0.1 | 对手行为不再复杂，模型只需识别有限几类动作（主要是捕兔）即可 |
| **类别 6 (猎兔) 准确率 1.0** | 模型几乎“完美”预测       | NH 对手高度确定性，完全放弃合作行为            |
| **类别 5 (猎鹿) 准确率 0**   | 无捕鹿样本            | NS 对手稀少（仅1名），其合作行为在全局分布中被稀释    |
| **类别 4 (原地) 准确率 0**   | 无此动作样本           | 所有对手均保持持续运动或捕猎行为               |
| **高精度但低多样性**          | 各移动类动作准确率均>0.9   | 说明 MOA 学到识别“固定模式”              |




# Limitations
# Future Work


你说得对，不能拍脑袋。把你给的三段代码串起来看，**根因非常明确**：

* `next_obs_raw["actions"]` 的顺序是**全局顺序** `[player_1, player_2, player_3, player_4]`（且包含自己）。这是在 `__action__` 里按 `self.players` 顺序拼出来的。
* 但 `observation` 的玩家通道顺序**被旋转过**：`__obs_state__(id)` 会把**“自己”旋到通道 0**（ego-first），顺序变成 `[self, 下一个, …]`。

所以在 `test_collect` 里，你把 `next_action`（全局顺序）直接和 `next_hwc`（ego-first 顺序）做对照，就会**对不齐**。这既可能让你误以为“动作顺序错了”，也会让你看到“显示 4 但图里在移动”的错觉（因为你拿的是 `obs_action`=t−1 的动作去对照 `obs_t→obs_{t+1}` 的位移）。


---



## 这份采集代码到底是怎么存的（关键点）

采集时你存了：



```python
obs_flat       = [ obs_dict_raw["actions"], obs_dict_raw["action_mask"], obs_dict_raw["observation"].flatten() ]
next_obs_flat  = [ next_obs_raw["actions"], next_obs_raw["action_mask"], next_obs_raw["observation"].flatten() ]
action         = action_batch[i][aid]        # 这名 agent 在时刻 t 选的动作（自动作）
reward_value   = reward_batch[aid][i]        # 这名 agent 的即时 reward（自回报）
t, aid
```


# FootNotes