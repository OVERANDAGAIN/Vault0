---
创建时间: 2025-十二月-1日  星期一, 2:33:18 下午
---
[[HOP+迁移至cleanrl]]


# 总览
1. 解决value_loss过高的问题
2. 解决idx_bias 的问题
3. self_play实验，正在进行的实验，进一步优化



# 问题1:解决idx_bias 的问题

现象：

* 4 个独立学习的 MBRL_Policy，self-play、对称环境。
* 多次实验中，出现：
  **$R_1 > R_2 > R_3 > R_4$**（按 agent 序号有不一致现象）。

## 解决

[[debug-agent-index-bias]]



---


# 问题2:解决value_loss过高的问题
## 问题描述
[[25-11-1#HOP+ _w /o_subgoal 跑 4 *4 4h1s 面对三个NS的策略对手]]

[[25-11-1#HOP+ 跑 4 *4-4h1s 面对三个NS的策略对手]]

### 从训练角度：

>值网络平均会在每个状态上多估/少估大约 $6\sim7$ 分的长期奖励

### 从评价角度：

>如果训练已经进入后期、策略稳定了、reward 基本收敛，但 value_loss 仍然在 40 上下抖动 → 
	值函数没有很好地拟合 true return；
	但这未会直接拖累策略。


## 解决


# w/o Subgoal - MOA- World_Model: (update_from_transtions)
```python
 tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764501961"
```
![[Pasted image 20251130193217.png]]

# w/o Subgoal - MOA- World_Model :(update from episode_buffer) 
![[Pasted image 20251130114410.png]]

# update from episode_buffer(debug)

```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns____2__1764502637"
```
![[Pasted image 20251130200839.png]]
![[Pasted image 20251201011308.png]]







# 第三部分


## 1. self-play实验
[[debug-agent-index-bias#PPO self-play(4 *4 4h1s)]]

### PPO self-play (4\*4 4h1s)
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\PPO__2__1764077683"
```
#### reward(4个agent类似)
![[Pasted image 20251126093347.png]]


#### loss
![[Pasted image 20251126093434.png]]


### HOP+（running)



---


## 2. World_Model

doing:
1. 在world_model中添加预测`done`的能力，环境是否终止
   world_model 当前没有输出 done 或终止概率，terminated 在 rollout 中恒为 0；
2. 想象里 action_mask 默认全 1，没模拟环境中的非法动作；
3. 应用新的world_model去生成轨迹







# Limitations
# Future Work
# FootNotes