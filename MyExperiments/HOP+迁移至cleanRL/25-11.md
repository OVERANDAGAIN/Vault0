---
创建时间: 2025-十一月-11日  星期二, 11:21:29 晚上
---
[[HOP+迁移至cleanrl]]


# Objectives

# Results


## MBRL - wm - 4h2s
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns__1__1762852769" 
```
### reward (蓝色线)
![[Pasted image 20251111232718.png]]


----


## MBRL-run 8 \* 8 
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns__1__1762877095"
```

### reward (蓝色线)

![[Pasted image 20251117151857.png]]


## PPO 8\*8 4h1s baseline
```python
(cleanrl) PS D:\cleanrl> tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_1hop_3ns__1__1763368736"  
```
 
### reward (蓝色线)

![[Pasted image 20251120235344.png]]
### value_loss

![[Pasted image 20251121151108.png]]

---


## HOP+\_w\/o_subgoal  跑 4\*4 4h1s 面对三个NS的策略对手

```python
(cleanrl) PS D:\cleanrl> tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns__2__1763368708\agent_player_1_reward_per_episode_mean"
```
### reward

![[Pasted image 20251120235531.png]]

### value_loss



![[Pasted image 20251121144646.png]]


```ad-info


>PPO baseline 

平均 `reward` 2.6-2.7 
![[Pasted image 20250929132236.png]]


```




---



## HOP+ 跑 4\*4-4h1s 面对三个NS的策略对手 

>正在跑

![[Pasted image 20251121150946.png]]


![[Pasted image 20251121150953.png]]




---


>值网络平均会在每个状态上多估/少估大约 $6\sim7$ 分的长期奖励

从评价角度：



>如果训练已经进入后期、策略稳定了、reward 基本收敛，但 value_loss 仍然在 40 上下抖动 → 
	值函数没有很好地拟合 true return；
	但这未会直接拖累策略。



## Q1: subgoal 对价值判断的影响
```python
	logits, _ = self.model.forward(input_dict, None, [1], t_input, sub_input)
	values = self.model.value_function().reshape(BT)
```
>subgoal 如果 noisy / 不稳定，会直接污染 value 估计?
## Q2: reward与value_loss
## Q3： 如何验证 subgoal 对策略与价值的可解释影响？

# Insights
# Setup
# Methodology
world_model初期 reward 预测 可能 不准


# Issues & Debugging

## Problem1: 
- [?] 

### 1_Answers


### 2_Answers



## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes