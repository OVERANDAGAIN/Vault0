---
创建时间: 2025-九月-20日  星期六, 9:07:06 晚上
---
[[HOP+迁移至cleanrl]]


# Objectives

* **定位问题根源**：针对 HOP+ 迁移至 CleanRL 框架后无法稳定训练出合作策略的现象，本周的主要目标是通过逐模块排查的方式，明确问题究竟出在 VAE、world model、MOA 模块，还是在整体策略训练逻辑。
* **验证 VAE 表现**：通过对比原始 VAE 与新版 VAE 的 loss 曲线，检验不同 loss 设计（重建项 vs. MOA loss）对模型训练稳定性和表征能力的影响，确认 VAE 是否在信息压缩和动作预测中存在缺陷。
* **评估 world_model 能力**：重点分析 world_model 在状态预测和奖励建模中的精度，观察其是否能为策略提供可靠的前瞻性信息，并判断异常预测（如 agent “跳动”）是否会干扰训练。
* **检查 MOA 模块训练效果**：通过混淆矩阵、动作分布和召回率指标，评估 MOA 在多类动作上的预测性能，分析是否由于离线数据分布偏差或 buffer 逻辑设计，导致模型在自博弈训练中失效。
* **建立 baseline 对照**：结合 PPO baseline 与 HOP+（含 MCTS/不含 MCTS）实验结果，形成性能对比，进一步缩小问题来源，为后续改进提供明确方向。



# Results
# Insights
# Setup
# Methodology
# Issues & Debugging


## Problem1: VAE的loss有点大
- [?] 

### 原来的VAE: loss图: 总体loss很低
4p5b8\*8 
```terminal
D:\BaiduNetdiskDownload\msh-records\4p5b8_8\vae\exp20250401_203634p\dim16
```
![[myplot_4p_16dim.png]]

### moa-VAE：loss图： loss比较大
可以看到，`total_loss` 在1.6左右，远不如上面的接近0的loss表现

4p5b8\*8
```
D:\cleanrl\cleanRL\vae_ckpts\vae__1758371809\dim16
```

![[vae_loss_curve.png]]

### Answer：因为VAE-loss去掉了重建项，加上了moa-loss项
D:\cleanrl\cleanRL\pretrain_vae.py
```python

    def train(self, states, actions, times):
        recons_loss, kld_loss, loss, z = self.am_model.loss_func(states)
        predict_action_dist = self.policy_model(states, times, z)


        # 计算 policy_loss: MSE between predicted action and true action

        policy_loss = nn.CrossEntropyLoss()(predict_action_dist, actions.long())
        total_loss = policy_loss + self.args['omg_vae_alpha'] * kld_loss
        # total_loss = recons_loss + self.args['omg_vae_alpha'] * kld_loss
        self.optimiser.zero_grad()
        total_loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(self.params, self.args['omg_config']['grad_norm_clip'])
        self.optimiser.step()
        return policy_loss, kld_loss, total_loss
```



当改为：

```
        # total_loss = policy_loss + self.args['omg_vae_alpha'] * kld_loss
        total_loss = recons_loss + self.args['omg_vae_alpha'] * kld_loss
```

D:\cleanrl\cleanRL\vae_ckpts\vae__1758373486\dim16
![[vae_loss_curve 1.png]]
loss图就变得很小了

### 总结：不是数据或者代码的问题，是loss设计的不同导致的
moa的预测动作部分的loss 相比较下来就很大，所以说会呈现出这样的不同
>之前用moa_loss替换掉recons_loss的原因就是recons_loss随着训练过程就是会降到很低，所以想到不用这个，而是用moa_loss。

### DLC : moa情况
总体保持稳定，也就是moa预测0.6左右
>但是有个问题，预测的动作中，很大一部分是4，也就是静止，而对于捕猎的动作（5,6）在数据中的占比不大
>但是：也不能这么比，因为离线数据汇总就是混杂这NS和NH的不同策略


| action | count | ratio    | precision | recall   | f1       |
| ------ | ----- | -------- | --------- | -------- | -------- |
| 0      | 13848 | 0.129408 | 0.440787  | 0.608247 | 0.511151 |
| 1      | 13913 | 0.130016 | 0.47323   | 0.674693 | 0.556283 |
| 2      | 13701 | 0.128035 | 0.522613  | 0.582804 | 0.55107  |
| 3      | 13848 | 0.129408 | 0.489164  | 0.57864  | 0.530153 |
| 4      | 36988 | 0.34565  | 0.97463   | 0.581621 | 0.728501 |
| 5      | 5518  | 0.051565 | 0.896499  | 0.742479 | 0.812252 |
| 6      | 9194  | 0.085917 | 0.657345  | 0.697955 | 0.677042 |


```ad-help
2. **count**
   真实数据中该类别的样本数量。

   * 比如 `class=4, count=36988`，说明数据里真实标签为 4 的样本有 36988 个。

3. **ratio**
   该类在整个数据集中所占比例（count / 总样本数）。

   * 比如 `class=4, ratio=0.34565`，表示类别 4 占了 34.6%，是一个“大类”。
   * 类别 5 只有 5.15%，说明是“小样本类”。

4. **precision**（精确率）

   $$
   \text{Precision}_i = \frac{\text{预测为 i 且正确的数量}}{\text{预测为 i 的总数量}}
   $$

   → “预测成该类的有多少是真的”。

   * 类别 4 的 precision=0.97，说明只要模型预测成 4，几乎都是对的。
   * 但类别 0 的 precision=0.44，说明很多被预测成 0 的其实是别的类（误报率高）。

5. **recall**（召回率）

   $$
   \text{Recall}_i = \frac{\text{真实是 i 且预测正确的数量}}{\text{真实是 i 的总数量}}
   $$

   → “真实是该类的有多少被预测出来了”。

   * 类别 1 的 recall=0.67，表示真实=1 的样本有 67% 被正确识别出来。
   * 类别 4 的 recall=0.58，虽然 precision 很高，但漏掉了不少真实=4 的样本。

```


#### 数据集中动作分布：4远远多于其他
![[moa_eval_epoch9_class_distribution.png]]

#### 混淆矩阵：对角线越深越好
![[moa_eval_epoch9_confusion_matrix.png]]

#### 召回率：整体动作准确率

$$
\text{Recall}_i = \frac{\text{预测正确为 i 的样本数}}{\text{真实标签为 i 的总样本数}}
$$

也就是：在所有 **真实为某类** 的样本里，模型有多少被成功识别出来。

![[moa_eval_epoch9_per_class_recall.png]]

```
D:\anaconda\envs\hopplus\python.exe D:\cleanrl\cleanRL\pretrain_vae.py 
[VAE] Loaded 107010 samples.
[VAE] Begin offline training...

[Epoch 1/10]
[MOA] epoch 0: top1=0.5337, top3=0.7465, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch0.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch0.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 2/10]
[MOA] epoch 1: top1=0.5474, top3=0.7430, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch1.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch1.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 3/10]
[MOA] epoch 2: top1=0.5608, top3=0.7441, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch2.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch2.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 4/10]
[MOA] epoch 3: top1=0.5724, top3=0.7431, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch3.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch3.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 5/10]
[MOA] epoch 4: top1=0.5853, top3=0.7421, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch4.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch4.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 6/10]
[MOA] epoch 5: top1=0.5936, top3=0.7421, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch5.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch5.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 7/10]
[MOA] epoch 6: top1=0.5983, top3=0.7424, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch6.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch6.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 8/10]
[MOA] epoch 7: top1=0.6080, top3=0.7411, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch7.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch7.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 9/10]
[MOA] epoch 8: top1=0.6120, top3=0.7412, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch8.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch8.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16

[Epoch 10/10]
[MOA] epoch 9: top1=0.6152, top3=0.7405, saved:
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch9.csv
       ./vae_ckpts/vae__1758375079\dim16\moa_eval_epoch9.npz
[VAE] 模型保存至: ./vae_ckpts/vae__1758375079\dim16
```


数据在： D:\cleanrl\cleanRL\vae_ckpts\vae__1758375079\dim16



	## Problem：World_model
- [?] 

![[Pasted image 20250921015931.png]]

在上面这张图的例子中，world_model预测的agent一下子走了两个，并且取得了0.042的奖励。
>不符合物理规律 —————— 联想到：对比学习促进强化学习的那篇文章。

world_model总体预测的还是大体上位置正确，但是做不到非常精准

- [?] 看一下具体channel 中具体的值是多少，而不是直接四舍五入

```
D:\anaconda\envs\hopplus\python.exe D:\cleanrl\cleanRL\test_obs_reward_model.py 
[INFO] loaded model. obs_shape=(4, 4, 7), action_dim=7
[INFO] dataset size=50000, inferred n_agents=4

=== Metrics ===
Obs  MSE (mean over B,C,H,W): 0.007402
Obs  PSNR: 24.08 dB
Rew  MSE:  0.001189
Per-channel MSE:
  ch[0]: 6.775978e-03
  ch[1]: 1.623118e-02
  ch[2]: 1.559676e-02
  ch[3]: 8.610861e-03
  ch[4]: 4.749758e-08
  ch[5]: 7.902491e-04
  ch[6]: 3.805568e-03

进程已结束，退出代码为 0
```



### world_model加入训练权重
原本是 loss可以降到0.01左右

现在设置权重为【2,2,2,2,1,1,1】，后，loss在0.07左右

```
[Epoch 1760] Loss: 0.076094
[Epoch 1761] Loss: 0.076446
[Epoch 1762] Loss: 0.076996
[Epoch 1763] Loss: 0.077331
[Epoch 1764] Loss: 0.077460
[Epoch 1765] Loss: 0.077460
[Epoch 1766] Loss: 0.077518
[Epoch 1767] Loss: 0.077713
[Epoch 1768] Loss: 0.077528
[Epoch 1769] Loss: 0.077788
[Epoch 1770] Loss: 0.077189
[Epoch 1771] Loss: 0.077524
[Epoch 1772] Loss: 0.077544
[Epoch 1773] Loss: 0.077387
[Epoch 1774] Loss: 0.077324
[Epoch 1775] Loss: 0.077374
```



### 2_Answers

## Problem： moa 的训练效果不好

### 效果图：
>下面是moa模块的训练情况分析：在最后的收敛中，在7个动作中，除了动作4准确度25%左右，其他的动作预测准确度都降到了0。
>推测出现这一情况的原因是（结合之前训练vae出现的情况），得到的训练数据中，由于这个环境中对手捕猎后就停止了，所以动作会被默认设置为0.当对手补完猎物后，会和猎物一起消失，但是此时agent可能仍然需要预测对手的动作，这当然都是4了。
>但是奇怪的是，在长期的训练过程中，top-1大概是15%（这当中还可能大部分都是静止的动作）而ce_loss也是在10左右不下降。所以说moa这个模块的训练效果就很差。


![[Pasted image 20250921233945.png|1175]]


### 原本moa逻辑

一句话总结：**MOA buffer 是以“对手在局中实际经历过的轨迹”为存储对象，前提是这段轨迹获得过奖励；对手退出后就不会再有数据进入 buffer，因此不会用到退出后的信息。**


由代码可知的：
```python
 @override(Policy)
    def postprocess_trajectory(
        self, sample_batch, other_agent_batches=None, episode=None
    ):
        start_time = tm()

        for i in range(self.player_num-1):
            model_id=self.model_id_list[i]
            place_id=(model_id+1-self.my_id)%self.player_num
            if episode._agent_reward_history[f'player_{model_id+1}'][-1]>0:
                total_time=other_agent_batches[f'player_{model_id+1}'][1]['actions'].shape[0]
                self.model_buffer[i][0].append(other_agent_batches[f'player_{model_id+1}'][1]['obs'].copy())
                self.model_buffer[i][1].append(other_agent_batches[f'player_{model_id+1}'][1]['actions'].copy())
                self.model_buffer[i][2].append(np.array(range(total_time)))

                if episode._agent_to_last_action[f'player_{model_id+1}']==5:
                    #self.old_model_buffer[i].extend(episode.user_data[f"s_a_pair{model_id+1}"])
                    self.model_buffer[i][3].extend([0]*total_time) # 0 for stag

                    self.discounted_stag_times[place_id]=self.discount_factor*self.discounted_stag_times[place_id]+1
                    self.discounted_hare_times[place_id]=self.discount_factor*self.discounted_hare_times[place_id]

                elif episode._agent_to_last_action[f'player_{model_id+1}']==6:
                    '''
                    for pair in episode.user_data[f"s_a_pair{model_id+1}"]:
                        pair[-1]=1
                    self.old_model_buffer[i].extend(episode.user_data[f"s_a_pair{model_id+1}"])
                    '''
                    self.model_buffer[i][3].extend([1]*total_time) # 1 for hare

                    self.discounted_stag_times[place_id]=self.discount_factor*self.discounted_stag_times[place_id]
                    self.discounted_hare_times[place_id]=self.discount_factor*self.discounted_hare_times[place_id]+1

                else:
                    pass
                    #raise ValueError('No way!!!')

                # if len(self.model_buffer[i])>self.moa_buffer_capacity:
                if len(self.model_buffer[i][3])>self.moa_buffer_capacity:
                    if self.my_id == 1:
                        self.moa_update(i)###moa_update
                    else:
                        self.model_buffer[i]=[[],[],[],[]]
        # add mcts policies to sample batch
        sample_batch["mcts_policies"] = np.array(episode.user_data[f"mcts_policies{self.my_id}"])[
            sample_batch["t"]
        ]
        # final episode reward corresponds to the value (if not discounted)
        # for all transitions in episode
        final_reward = sample_batch["rewards"][-1]

        # if r2 is enabled, then add the reward to the buffer and normalize it
        if self.r2_enable:
            self.r2_buffer.add_reward(final_reward)
            final_reward = self.r2_buffer.normalize(final_reward)

        # sample_batch["value_label"] = final_reward * np.ones_like(sample_batch["t"])
        sample_batch["value_label"] = final_reward * (self.gamma**np.array(sample_batch["t"][::-1]))
        train_duration = tm() - start_time  # ⏱️ 计算耗时
        print(f"[Train] progress time  耗时: {train_duration:.4f} 秒")
        return sample_batch
```


```ad-info
## 1. sample\_batch 的长度

* 在 RLlib 中，每个 agent 的 `sample_batch` **长度不一定相同**。
* 具体长度由该 agent 在本次采样中经历的步数决定：

  
## 2. 关于退出后的数据

* RLlib 在 agent done 后不再生成 `obs/actions`，所以 `other_agent_batches` 不包含退出后的轨迹。
* 你的代码又完全依赖 `total_time = actions.shape[0]` 来存储，因此不会人工补齐退出后的部分。
* 结论：**moa\_buffer 只记录对手在场时的轨迹，不会包含任何退出后的数据。**

```




### 思考
>离线训练时的moa效果还不错是吧？但是为什么hopplus真正训练时效果却非常差呢？一开始效果差可以理解，因为直接加载的moa是使用的离线的rule-based 的离线轨迹数据去拟合的，而且基本上是训练成了过拟合的样子，所以说在hopplus的self-play中一开始预测准确率不高也是正常的。但是随着训练的进程，moa似乎没有很好地利用到离线数据去训练（我们刚才认为可能是因为训练数据中动作4的占比太大，但是在离线数据中4的占比也很大，但是训练的效果并不差，所以需要分析一下moa的训练过程是否有问题）




# 实验结果



平均 `reward` 2.6-2.7 
![[Pasted image 20250929132236.png]]


## HOP+-MCTS
![[Pasted image 20250929132441.png]]
![[Pasted image 20250929140820.png]]


## HOP+
![[Pasted image 20250929132813.png]]


moa_buffer改进前：

![[Pasted image 20250921233945.png|925]]

moa_buffer改进后： 
![[Pasted image 20250929132909.png]]







---



## ToDo


* **单智能体 + 3 个 rule-based 对照实验**
  >如果对手也是学习策略，MOA 的预测可能受震荡或训练逻辑影响，难以判断真实效果；而对 rule-based，对应动作模式更稳定（尤其捕鹿与捕兔差异明显），更容易验证 MOA 的区分能力。

* **PPO Baseline 多随机种子实验**
  >之前出现 player1 学得更好而其他较差的现象，可能是种子固定导致的偶然性。需更换 `random.seed / np.random.seed / torch.manual_seed` 并多次运行，确认结果是否稳健。

* **world_model 预测可视化改进**
  >当前可视化用四舍五入可能掩盖了细节误差（如 0.4 被当成 0，导致 agent“消失”），需直接检查各通道连续值，避免误判模型精度。


本阶段的实验主要是针对 **reward 长期维持较低** 的问题进行系统排查。我们按照模块（VAE、world_model、MOA、buffer 机制等）逐步验证其效果，虽然已经发现并修复了部分问题（例如 **moa_buffer 的逻辑缺陷**），但在完整的 **HOP+ 版本** 中，最终的 reward 依然没有显著提升。
同时，**HOP-MCTS 版本**虽然一度表现较好，但出现了 reward 先升后降的现象。
因此，下一步的重点将放在对 MCTS 版本进行深入分析，优先寻找其`训练`或者`数据搜集`原因




# Limitations
# Future Work
# FootNotes