---
创建时间: 2025-九月-20日  星期六, 9:07:06 晚上
---
[[HOP+迁移至cleanrl]]


# Objectives
# Results
# Insights
# Setup
# Methodology
# Issues & Debugging

## Problem1: VAE的loss有点大
- [?] 

### 原来的VAE: loss图: 总体loss很低
4p5b8\*8 
```terminal
D:\BaiduNetdiskDownload\msh-records\4p5b8_8\vae\exp20250401_203634p\dim16
```
![[myplot_4p_16dim.png]]

### 新版VAE：loss图： loss比较大
可以看到，`total_loss` 在1.6左右，远不如上面的接近0的loss表现

4p5b8\*8
```
D:\cleanrl\cleanRL\vae_ckpts\vae__1758371809\dim16
```

![[vae_loss_curve.png]]

### Answer：因为VAE-loss去掉了重建项，加上了moa-loss项
D:\cleanrl\cleanRL\pretrain_vae.py
```python

    def train(self, states, actions, times):
        recons_loss, kld_loss, loss, z = self.am_model.loss_func(states)
        predict_action_dist = self.policy_model(states, times, z)


        # 计算 policy_loss: MSE between predicted action and true action

        policy_loss = nn.CrossEntropyLoss()(predict_action_dist, actions.long())
        total_loss = policy_loss + self.args['omg_vae_alpha'] * kld_loss
        # total_loss = recons_loss + self.args['omg_vae_alpha'] * kld_loss
        self.optimiser.zero_grad()
        total_loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(self.params, self.args['omg_config']['grad_norm_clip'])
        self.optimiser.step()
        return policy_loss, kld_loss, total_loss
```



当改为：

```
        # total_loss = policy_loss + self.args['omg_vae_alpha'] * kld_loss
        total_loss = recons_loss + self.args['omg_vae_alpha'] * kld_loss
```

D:\cleanrl\cleanRL\vae_ckpts\vae__1758373486\dim16
![[vae_loss_curve 1.png]]
loss图就变得很小了

### 总结：不是数据或者代码的问题，是loss设计的不同导致的
moa的预测动作部分的loss 相比较下来就很大，所以说会呈现出这样的不同
>之前用moa_loss替换掉recons_loss的原因就是recons_loss随着训练过程就是会降到很低，所以想到不用这个，而是用moa_loss。





## Problem2: 
- [?] 

### 1_Answers


### 2_Answers



# Limitations
# Future Work
# FootNotes