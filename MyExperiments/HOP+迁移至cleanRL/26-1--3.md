---
创建时间: 2026-一月-24日  星期六, 5:35:10 下午
---
[[HOP+迁移至cleanrl]]

```bash
python MBRL_OMG_run_noGoal.py ^
  --msd-world-h 4 ^
  --msd-world-w 4 ^
  --msd-obs-h 4 ^
  --msd-obs-w 4 ^
  --msd-final-time 15 ^
  ^
  --vae-load-dir "./vae_ckpts/msd_vae_1769360196/dim16" ^
  --moa-load-dir "./vae_ckpts/msd_vae_1769360196/dim16" ^
  --cvae-load-dir None ^
  --eval-policy-roots './params/exp20251209_221042_policy_4p,./params/exp20251209_221033_policy_4p,./params/exp20251209_221033_policy_4p,./params/exp20251209_221033_policy_4p' ^
  --eval-policy-tags  'upd18000,upd18000,upd18000,upd18000' ^
  --omg-buffer-size 500 ^
  --moa-buffer-capacity 1000
```


# Objectives
[[26-1--2]]中：
cleanup和Coingame环境存在固定优策略，当对手行为变化时，己方不需要改变策略就能达到高奖励，所以测试few-shot adaptation不合适。
一下实验在MSD环境下测试：

上轮实验测试了4PPO self-play的效果（全观测）：
>MSD 4 * 4 ，final_time =50 下 $\Longrightarrow$  Average_reward = 29.5
>MSD 6 * 6 ，final_time =50 下 $\Longrightarrow$  Average_reward = 29.5
>MSD 8 * 8 ，final_time =50 下 $\Longrightarrow$  Average_reward = 25.3

可能是由于final_time设置的太高（50）的原因，所以在这个环境里面随机走都能达到较好的奖励

下面针对每个环境大小设置 合适的 final_time 重新测试：



# Results
##  4\*4

```bash
python MSG/MBRL_OMG_run_Goal_noWM.py \
  --env-name msd \
  --msd-world-h 4 --msd-world-w 4 \
  --msd-final-time 15 \
  --msd-player-num 4 \
  --msd-drift-num 6 --msd-reward 6 --msd-cost 4 \
  --num-envs 32  --seed 3 \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal" \
```


|       算法       | Policy |            Average Reward            | Remark |
| :------------: | :----: | :----------------------------------: | ------ |
|   rule-based   |  4FIX  | ![[Pasted image 20260124215023.png]] |        |
|       ^        |  4ND   | ![[Pasted image 20260124215220.png]] |        |
|       ^        | 4RAND  | ![[Pasted image 20260124215940.png]] |        |
| learning-based |  4PPO  | ![[Pasted image 20260124214215.png]] |        |
|       ^        |        |                 <br>                 |        |
|       ^        |        |                                      |        |

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__FIX-FIX-FIX-FIX__3__20260124_214828"
```

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__ND-ND-ND-ND__3__20260124_215014"
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__RAND-RAND-RAND-RAND__3__20260124_215304"
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal__3__20260124_183148"
```

### HOP+ self -play

4\*4 final_time=15 :
>vae:msd_vae_1769365105
>world_model_4:


```bash
python MBRL_OMG_run_noGoal.py \
  --msd-world-h 4 \
  --msd-world-w 4 \
  --msd-obs-h 4 \
  --msd-obs-w 4 \
  --msd-final-time 15 \
  --vae-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --moa-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --cvae-load-dir None \
  --eval-policy-roots None \
  --eval-policy-tags None \
  --omg-buffer-size 500 \
  --moa-buffer-capacity 1000\
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/MSG/runs/MSD__HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM__3__20260126_025834"
```

![[Pasted image 20260126090903.png]]

### ppo重新跑“（没保存上）


```python
python MBRL_OMG_run_noGoal.py \
  --msd-world-h 4 \
  --msd-world-w 4 \
  --msd-obs-h 4 \
  --msd-obs-w 4 \
  --msd-final-time 15 \
  --vae-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --moa-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --cvae-load-dir None \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal"\
  --eval-policy-roots None \
  --eval-policy-tags None \
  --omg-buffer-size 500 \
  --moa-buffer-capacity 1000\
```











---
##  6\*6


```bash
python MSG/MBRL_OMG_run_Goal_noWM.py \
  --env-name msd \
  --msd-world-h 6 --msd-world-w 6 \
  --msd-final-time 25 \
  --msd-player-num 4 \
  --msd-drift-num 6 --msd-reward 6 --msd-cost 4 \
  --num-envs 32  --seed 3 \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal" \
```

|       算法       | Policy |            Average Reward            | Remark |
| :------------: | :----: | :----------------------------------: | ------ |
|   rule-based   | 4RAND  | ![[Pasted image 20260124220214.png]] |        |
| learning-based |  4PPO  | ![[Pasted image 20260124214200.png]] |        |
|       ^        |        |                                      |        |
|       ^        |        |                                      |        |
|       ^        |        |                                      |        |
|       ^        |        |                                      |        |




```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__RAND-RAND-RAND-RAND__3__20260124_220024"
```

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal__3__20260124_183213"
```

### HOP+ self -play


```
python MBRL_OMG_run_noGoal.py \
  --msd-world-h 6 \
  --msd-world-w 6 \
  --msd-obs-h 6 \
  --msd-obs-w 6 \
  --msd-final-time 25 \
  --vae-load-dir "./vae_ckpts/msd_vae_1769390417/dim16" \
  --moa-load-dir "./vae_ckpts/msd_vae_1769390417/dim16" \
  --cvae-load-dir None \
  --eval-policy-roots None \
  --eval-policy-tags None \
  --omg-buffer-size 800 \
  --moa-buffer-capacity 1600\
```





```python
python MBRL_OMG_run_noGoal.py \
  --msd-world-h 6 \
  --msd-world-w 6 \
  --msd-obs-h 6 \
  --msd-obs-w 6 \
  --msd-final-time 25 \
  --vae-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --moa-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --cvae-load-dir None \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal"\
  --eval-policy-roots None \
  --eval-policy-tags None \
  --omg-buffer-size 800 \
  --moa-buffer-capacity 1600\
```



![[Pasted image 20260126145301.png]]


![[Pasted image 20260126151907.png]]

![[Pasted image 20260126153012.png]]


![[Pasted image 20260126153048.png]]
![[Pasted image 20260126154119.png]]

---



# FootNotes

```
tensorboard --logdir="/home/fengxue/projects/hopplus/MSG/runs/MSD__HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal__3__20260126_095249"
```
![[Pasted image 20260126155220.png]]
```
tensorboard --logdir="/home/fengxue/projects/hopplus/MSG/runs/MSD__HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM__3__20260126_095224"
```
![[Pasted image 20260126155251.png]]
```
tensorboard --logdir="/home/fengxue/projects/hopplus/MSG/runs/MSD__HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM__3__20260126_025834"
```

![[Pasted image 20260126155549.png]]






## Overcooked


“cramped_room 布局下，单智能体 PPO + 固定 GreedyHumanModel 搭档（self-play 的简化版）”

```bash

D:\anaconda\envs\cleanrl\python.exe D:\cleanrl\MSG\test_overcooked.py 
===== 1) Official (control group) =====
[Official] greedy-vs-greedy ep_returns: [80 80 80]
[Official] mean=80.00 std=0.00

===== 2) Wrapper (greedy + greedy) =====
[Wrapper] EP 0: return=80.0, deliveries=0
[Wrapper] EP 1: return=80.0, deliveries=0
[Wrapper] EP 2: return=80.0, deliveries=0
==== [Wrapper] Summary ====
[Wrapper] mean return: 80.00 +/- 0.00
[Wrapper] mean deliveries: 0.00

===== 3) Verdict =====
Official mean=80.00, Wrapper mean=80.00, |diff|=0.00
结论：wrapper 的 reward/step/joint_action 基本与官方完全一致。

进程已结束，退出代码为 0

```




```
 tensorboard --logdir "D:\cleanrl\MSG\runs\overcooked_ppo"
```


![[Pasted image 20260126142131.png]]
