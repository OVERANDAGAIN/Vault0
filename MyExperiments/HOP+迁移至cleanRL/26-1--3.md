---
创建时间: 2026-一月-24日  星期六, 5:35:10 下午
---
[[HOP+迁移至cleanrl]]

```bash
python MBRL_OMG_run_noGoal.py ^
  --msd-world-h 4 ^
  --msd-world-w 4 ^
  --msd-obs-h 4 ^
  --msd-obs-w 4 ^
  --msd-final-time 15 ^
  ^
  --vae-load-dir "./vae_ckpts/msd_vae_1769360196/dim16" ^
  --moa-load-dir "./vae_ckpts/msd_vae_1769360196/dim16" ^
  --cvae-load-dir None ^
  --eval-policy-roots './params/exp20251209_221042_policy_4p,./params/exp20251209_221033_policy_4p,./params/exp20251209_221033_policy_4p,./params/exp20251209_221033_policy_4p' ^
  --eval-policy-tags  'upd18000,upd18000,upd18000,upd18000' ^
  --omg-buffer-size 500 ^
  --moa-buffer-capacity 1000
```
# Objectives
[[26-1--2]]中：
cleanup和Coingame环境存在固定优策略，当对手行为变化时，己方不需要改变策略就能达到高奖励，所以测试few-shot adaptation不合适。
一下实验在MSD环境下测试：

上轮实验测试了4PPO self-play的效果（全观测）：
>MSD 4 * 4 ，final_time =50 下 $\Longrightarrow$  Average_reward = 29.5
>MSD 6 * 6 ，final_time =50 下 $\Longrightarrow$  Average_reward = 29.5
>MSD 8 * 8 ，final_time =50 下 $\Longrightarrow$  Average_reward = 25.3

可能是由于final_time设置的太高（50）的原因，所以在这个环境里面随机走都能达到较好的奖励

下面针对每个环境大小设置 合适的 final_time 重新测试：



# Results
##  4\*4

```bash
python MSG/MBRL_OMG_run_Goal_noWM.py \
  --env-name msd \
  --msd-world-h 4 --msd-world-w 4 \
  --msd-final-time 15 \
  --msd-player-num 4 \
  --msd-drift-num 6 --msd-reward 6 --msd-cost 4 \
  --num-envs 32  --seed 3 \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal" \
```


|       算法       | Policy |            Average Reward            | Remark |
| :------------: | :----: | :----------------------------------: | ------ |
|   rule-based   |  4FIX  | ![[Pasted image 20260124215023.png]] |        |
|       ^        |  4ND   | ![[Pasted image 20260124215220.png]] |        |
|       ^        | 4RAND  | ![[Pasted image 20260124215940.png]] |        |
| learning-based |  4PPO  | ![[Pasted image 20260124214215.png]] |        |
|       ^        |        |                 <br>                 |        |
|       ^        |        |                                      |        |

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__FIX-FIX-FIX-FIX__3__20260124_214828"
```

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__ND-ND-ND-ND__3__20260124_215014"
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__RAND-RAND-RAND-RAND__3__20260124_215304"
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal__3__20260124_183148"
```



4\*4 final_time=15 :
>vae:msd_vae_1769365105
>world_model_4:


```bash
python MBRL_OMG_run_noGoal.py \
  --msd-world-h 4 \
  --msd-world-w 4 \
  --msd-obs-h 4 \
  --msd-obs-w 4 \
  --msd-final-time 15 \
  --vae-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --moa-load-dir "./vae_ckpts/msd_vae_1769365105/dim16" \
  --cvae-load-dir None \
  --eval-policy-roots None \
  --eval-policy-tags None \
  --omg-buffer-size 500 \
  --moa-buffer-capacity 1000\
```


```
tensorboard --logdir="/home/fengxue/projects/hopplus/MSG/runs/MSD__HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM-HOPp_GOAL_NOWM__3__20260126_025834"
```

![[Pasted image 20260126090903.png]]


---
##  6\*6


```bash
python MSG/MBRL_OMG_run_Goal_noWM.py \
  --env-name msd \
  --msd-world-h 6 --msd-world-w 6 \
  --msd-final-time 25 \
  --msd-player-num 4 \
  --msd-drift-num 6 --msd-reward 6 --msd-cost 4 \
  --num-envs 32  --seed 3 \
  --agent-roles "HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal,HOP+_NOGoal" \
```

|       算法       |     Policy     |            Average Reward            | Remark |
| :------------: | :------------: | :----------------------------------: | ------ |
|   rule-based   |     4RAND      | ![[Pasted image 20260124220214.png]] |        |
| learning-based |      4PPO      | ![[Pasted image 20260124214200.png]] |        |
|       ^        | 1GOAL_noWM 3ND |                                      |        |
|       ^        |                |                                      |        |
|       ^        |                |                                      |        |
|       ^        |                |                                      |        |




```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__RAND-RAND-RAND-RAND__3__20260124_220024"
```

```
tensorboard --logdir="/home/fengxue/projects/hopplus/runs/MSD__HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal-HOPp_NOGoal__3__20260124_183213"
```






# MSD
* **RandomPolicy（MSD）**：在当前可行动作里均匀随机选一个动作。

* **FixPolicy（MSD）**：只在前 5 个动作（通常不包含特殊动作）里随机选一个可行动作。

* **NearestDriftPolicy（MSD）**：若脚下有雪则清雪，否则朝视野内最近的雪堆移动；看不到雪就随机走。



---



---



`class SnowDriftEnv:`
>Episode 长度固定 `self.final_time=50`

>场景：8\*8 ; 4Agent; 6Drift ; Reward=6 ; Cost = 4; final_time = 50
>观测大小：完全观测


|       算法       |     Policy     | Average Reward | Remark                          |
| :------------: | :------------: | :------------: | ------------------------------- |
|   rule-based   |      4FIX      |                | 都是0，合理 $\Longrightarrow$ 没有人去打扫 |
|       ^        |      4ND       |                |                                 |
|       ^        |     4RAND      |                | 可能是由于地图太小                       |
| learning-based |    PPO+3ND     |                |                                 |
|       ^        |    2PPO+2ND    |                |                                 |
|       ^        |    3PPO+ND     |                |                                 |
|       ^        |      4PPO      |                |                                 |
|       ^        | 1GOAL_noWM 3ND |                |                                 |





# FootNotes