---
创建时间: 2026-二月-13日  星期五, 12:43:08 下午
---
[[HOP+迁移至cleanrl]]



# 可视化world_model

```bash
python test_obs_reward_model.py  --ckpt ./world_model.pth  --data ./collect_states/msd_o1o2_a_r_t_seed1__world4x4__obs4x4__T15__drift6__R6.0_C4.0__1769349184/all_collect_samples.npy
```


![[triplet_013.png]]




```
python MSD_train_nextobsrewarddone_run.py `
  --data-path ./collect_states/msd_o1o2_a_r_t_seed1__world4x4__obs4x4__T15__drift6__R6.0_C4.0__1769349184/all_collect_samples.npy `
  --save-path ./world_model_legacy.pth `
  --obs-h 4 --obs-w 4 `
```




```bash
python test_obs_reward_model.py  --ckpt ./world_model_legacy.pth  --data ./collect_states/msd_o1o2_a_r_t_seed1__world4x4__obs4x4__T15__drift6__R6.0_C4.0__1769349184/all_collect_samples.npy
```

## 可能的问题所在：
```
3.1 对 world-model（无论 learned 还是 rule-based）

最合适的接口是：

输入：obs_rotated（ego=0 的 rotated-obs）

输入：joint_action_rotated（列对齐 obs 的玩家通道：0=ego，1=ego+1...）

输出：next_obs_rotated + reward_bn + done

理由很直接：
rule-based step 本质是在“棋盘上推进玩家通道”，如果动作列和玩家通道一一对应，就不需要额外的 player_id/反旋转逻辑
```

也就是说可能world-model训练的时候的agent有点错位？
但是也不一定。。。




# Objectives




# Results

# FootNotes