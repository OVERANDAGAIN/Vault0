---
创建时间: 2025-十一月-23日  星期日, 2:03:31 凌晨
---
[[HOP+迁移至cleanrl]]
# self-play / adaptation结果
## 首先是MBRL下的一些结果
[[25-11--1]]


## MBRL_OMG_run.py

### w/o generate_model_rollouts
```python
tensorboard --logdir="D:\cleanrl\cleanRL\runs\msh_MBRL_1hop_3ns__2__1764125894" 
```
#### 基本情况: HOPP+3NS (4\*4 4h1s)
- reward上升到2.8（合作）
- value_loss没有特别高的值出现（0.x)

>在完整的基础山，不通过world_model生成轨迹
```python
    def train(self, agent_trajectories: dict, agent_ids: list):
        # --- 2. Train World Model ---
        self._maybe_train_wm()
        
        # --- 3. Train MOA ---
        self._maybe_train_moa()
        
        # --- 4. Generate Imagined Data ---
        # We run this *every* train step to keep the buffer full of fresh model data
        # self._generate_model_rollouts()    #to debug , comment here for now!!!
        
        # --- 5. Train PPO Policy ---
        # (Trains on combined real + imagined data)
        self._maybe_train_policy()
        
        # --- 6. Train OMG (Subgoal) Model ---
        # (Trains on combined real + imagined data)
        self._maybe_train_omg()
```

>或者说调整 `radio` 为1 $\Longrightarrow$ 全为真实的轨迹
```python
    parser.add_argument("--ppo-real-ratio", type=float, default=1.0) # <-- NEW ARGUMENT
```


#### reward
![[Pasted image 20251127010103.png]]

#### value_loss
![[Pasted image 20251127010125.png]]









# Debug结果

## 关于日志计时是否花费过多时间？
>偶然观察到训练rule-based policy在self-play时，update信息跳的比较慢
![[d58ff4be-ecff-4b53-b40b-0592fa6a0862.png|475]]
![[dab22249-ed46-4a0d-9727-a2850dd8c65e.png|450]]

![[ec307b76-8f18-4f17-b69e-e3088f02da93.png|450]]



### 三者性能差异总结表

| 版本         | logging 时间（秒/step） | 相对性能             | 主要原因                                 |
| ---------- | ------------------ | ---------------- | ------------------------------------ |
| **原始版**    | ~0.0004            | baseline         | 少量 scalar logging                    |
| **我优化的版本** | ~0.00015           | **最快**（≈3× 原始版本） | 合并 log，减少 write 次数                   |
| **你的复杂版本** | ~0.009             | **最慢（25×~60×）**  | 多重统计、histogram、多次 add_scalars、写入规模激增 |
### 总结：十万步多花费十几分钟，还能接受

```ad-tip
100k updates

版本 1：
$0.0004 \times 100000 = 40$ 秒
版本 2：
$0.00015 \times 100000 = 15$ 秒
版本 3：
$0.009 \times 100000 = 900$ 秒 ≈ 15 分钟

1M updates（你现在脚本里的 total-timesteps 级别）

```




# Limitations
# Future Work
# FootNotes