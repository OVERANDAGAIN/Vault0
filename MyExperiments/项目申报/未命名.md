---
创建时间: 2025-十月-13日  星期一, 3:11:24 下午
---

## 多对多博弈均衡策略求解研究进展

多对多博弈的均衡求解问题，长期以来是多智能体系统中策略设计与决策优化的重要研究方向。该领域的发展大致经历了精确求解—启发式近似—强化学习逼近—分布式优化与广义纳什—无模型在线学习—时间约束收敛等阶段，方法体系逐步从小规模静态博弈扩展至大规模动态系统。

### （1）精确求解与路径延拓方法

早期研究集中于小规模静态博弈的精确均衡计算。Lemke–Howson 算法（Lemke & Howson, 1964）是二人双矩阵博弈的经典方法，通过补集路径实现精确纳什均衡计算，被后续求解器广泛采用[1]。为应对多均衡与非线性问题，Govindan 与 Wilson 提出了全局 Newton 法（Govindan & Wilson, 2003），Herings（2004）则系统总结了同伦与路径延拓方法在纳什求解中的应用[2][3]。
这些方法具有严格的理论完备性与数值精度，但在参与方众多或策略维数高时计算开销急剧增加，难以在复杂系统中推广。

### （2）启发式与演化近似方法

为突破规模瓶颈，研究者将纳什求解转化为优化问题，通过群体智能算法实现近似搜索。典型代表包括：基于粒子群优化（PSO）的多无人系统追逃与任务分配算法（PLOS ONE, 2021）[4]、基于遗憾值排序的遗传算法（Regret-based NSGA）（Evolutionary Computation, 2022）[5]，以及融合差分进化与粒子群的混合算法（AIMS Mathematics, 2021）[6]。
这类方法实现简单、适应性强，可用于大规模任务中的快速近似或策略初始化，但收敛性依赖参数调节，缺乏严格的全局均衡保证。

### （3）强化学习与深度学习逼近方法

进入动态多智能体阶段，强化学习成为求解纳什均衡的重要工具。Littman（1994）提出的 Minimax-Q 将 Q-learning 扩展到零和随机博弈；Hu 与 Wellman（2003）提出 Nash-Q，实现了一般和博弈中的混合策略学习；Bowling 与 Veloso（2002）的 WoLF-PHC 通过可变学习率改善了并发训练下的稳定性[7–9]。
在深度强化学习框架中，Lowe 等（2017）提出的 MADDPG 构建集中训练与分布执行（CTDE）机制，Rashid 等（2020）的 QMIX 通过值函数分解处理多人联合动作问题，Foerster 等（2018）的 LOLA（Learning with Opponent-Learning Awareness） 引入对手学习建模以提升博弈稳定性[10–12]。这些方法为多对多场景下的策略优化提供了可扩展的学习体系。

#### （3.1）对手建模与策略塑形（Opponent Modeling / Shaping）

在多主体博弈中，智能体的最优策略不仅依赖环境状态，也受到对手学习行为的影响。为此，研究从“对手建模”发展出“策略塑形”范式，通过在更新过程中显式考虑对手的学习规则或反应机制，从而在动态交互中实现可控的均衡收敛。
Foerster 等（2018）提出的 LOLA 在梯度更新中引入“对对手学习步的可微估计”，即在自身策略梯度中嵌入对对手一次或多次学习更新的期望回报导数，从而使智能体能够显式影响对手未来的学习轨迹[12]；在此基础上，DiCE 给出通用高阶梯度估计器，用于稳定实现“对手学习可微化”[13]。随后，SOS（Stable Opponent Shaping） 引入稳定性与一致性约束以避免进入鞍点并保持学习一致[14]；COLA（Consistent Opponent-Learning Awareness） 在双方同时塑形时建立一致的更新规则[15]；POLA（Proximal Opponent-Learning Awareness） 以近端约束替代一阶近似，提高对参数化的鲁棒性[16]。
另一方面，LIO（Learning with Incentivized Opponents） 将“激励函数”与策略解耦，由施加者学习奖励并直接进入受体回报，以实现显式激励[17]。面向长时序与跨局面塑形，M-FOS（Meta-Foresight Opponent Shaping） 采用元学习视角在多局游戏上学习塑形策略；Shaper 扩展到高维长时序；COALA-PG 将“会学习的对手”形式化为元 POMDP 并给出高效策略梯度；MeVa（Meta-Value Learning） 用元价值函数显式建模更新的长期效应[18–21]。此外，Best Response Shaping 以近似/摊销的最优响应替代高阶梯度[22]；LOQA 直接在对手的 Q 值或学习信号上进行干预[23]；Latent Representation Influence 学习低维对手行为表征并通过自身策略可控地影响其演化[24]。

### （4）分布式与广义纳什均衡求解

在网络化与资源受限系统中，博弈模型通常涉及局部约束与共享资源，对应广义纳什均衡（GNE）问题。Ratliff 等（2016）从连续博弈出发刻画了局部纳什条件；Ye 与 Hu（2017）提出基于一致性估计的分布式梯度算法；Salehisadaghiani 与 Pavel（2016）提出基于 Gossip 通信的异步更新机制；Pang 与 Hu（2021）设计了无梯度分布式寻优方法[25–28]。
在受约束场景中，Zou 等（2021）与 Deng 等（2019）基于投影算子与原始–对偶分裂构建了连续时间与聚合博弈的分布式算法；Yi 与 Pavel（2017）在算子单调性假设下给出 Operator Splitting 框架[35][36][38]；分布式优化中的投影/神经动态与非均匀约束工具为实现与复杂度控制提供支撑[31–34]。此外，Yu 等（2017）提出的随机 GNE 学习算法扩展了方法在不确定环境下的适用性[37]。
这一阶段的方法在收敛性与分布式可实现性上取得突破，适用于通信网络、能源管理与协同调度等典型多对多场景。

### （5）动力学约束与物理系统博弈

为应对实际系统中的动力学约束，研究者将博弈模型推广到动态多智能体。Stankovic 等（2012）在移动传感器网络中实现了分布式 NE 寻优；Ye（2021）考虑控制输入有界问题；Romano 与 Pavel（2020）结合干扰观测器处理外扰影响[39–41]。
在非线性与 Euler–Lagrange（EL）系统中，Vamvoudakis 与 Lewis（2011）提出策略迭代强化学习求解非零和微分博弈；Huang 等（2021）设计了基于高增益观测的非线性 NE 控制策略；Deng 与 Liang（2019）提出了异构 EL 系统下的分布式聚合博弈方法[42–45]。
后续研究还引入障碍 Lyapunov 函数（Garg & Roy, 2020）和自适应机制应对不确定动力学与混合阶系统，并在二阶非线性玩家、外部扰动与混合/异构场景中提出相应策略与收敛结论[46–52]，实现了策略求解与物理控制的一体化。

### （6）无模型博弈学习：虚拟行动与极值搜索

在缺乏精确代价模型的场景下，无模型博弈学习得到关注。
虚拟行动（Fictitious Play, FP）方法通过统计对手历史行为实现经验响应更新，自 Brown（1951）起衍生出多种变体，如 Marden 等（2009）的 Joint-Strategy FP、Fudenberg 与 Takahashi（2011）的异质信念 FP、Eksin 与 Ribeiro（2018）的分布式 FP，以及在巡逻等任务中的平滑/选择性实现[53–57]。
极值搜索（Extremum Seeking, ES）通过实时代价测量进行零阶优化。Krstić 与 Wang（2000）建立非线性系统下的 ES 稳定性理论，Adetola 与 Guay（2007）给出参数收敛；随后形成滑模、随机、Lie-bracket 与时变 ES 等多种形式（Tanelli, 2006；Guay, 2015；等）[58–71]；在网络化系统中，还提出了分布式 ES与基于 ES 的 GNE 学习，用于动态耦合与不稳定网络情形[72][73]。



---

## 参考文献

### 精确/同伦与近似
[1] Lemke C.E., Howson J.T. Equilibrium Points of Bimatrix Games. 1964.
[2] Govindan S., Wilson R. A Global Newton Method to Compute Nash Equilibria. 2003.
[3] Herings P.J.-J. Computing Nash Equilibria: Homotopy Methods. 2004.
[4] An improved predator–prey particle swarm optimization for multi-agent pursuit–evasion. PLOS ONE, 2021.
[5] Regret-Based Nash Equilibrium Sorting Genetic Algorithm. Evolutionary Computation, 2022.
[6] Differential-evolution–particle-swarm hybrid for N-player finite games. AIMS Mathematics, 2021.

### RL / 深度 MARL 与对手建模
[7] Littman M. Markov Games as a Framework for Multi-Agent Reinforcement Learning. 1994.
[8] Hu J., Wellman M. Nash Q-Learning for General-Sum Stochastic Games. JMLR, 2003.
[9] Bowling M., Veloso M. WoLF-PHC: Convergence in Self-Play. 2002.
[10] Lowe R. et al. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (MADDPG). 2017.
[11] Rashid T. et al. QMIX: Monotonic Value Function Factorisation. JMLR, 2020.
[12] Foerster J. et al. Learning with Opponent-Learning Awareness (LOLA). 2018.
[13] Foerster J. et al. DiCE: The Infinitely Differentiable Monte Carlo Estimator. ICML, 2018.
[14] Letcher A. et al. Stable Opponent Shaping in Differentiable Games. NeurIPS, 2019.
[15] Willi T. et al. Consistent Opponent Learning Awareness (COLA). ICLR, 2022.
[16] Zhao W. et al. Proximal Opponent Learning Awareness (POLA). ICLR, 2022.
[17] Yang Y. et al. Learning with Incentivized Opponents (LIO). ICLR, 2020.
[18] Lu C. et al. Meta-Foresight Opponent Shaping (M-FOS). ICLR, 2022.
[19] Khan Z. et al. Shaper: Meta-Learning to Influence Multi-Agent Interaction. arXiv:2402.01068, 2024.
[20] Bertrand Q. et al. COALA_PG: Meta-POMDP Policy Gradient for Learning-to-Learn in Games. arXiv:2406.04378, 2024.
[21] Cooijmans T. et al. Meta-Value Learning for Multi-Agent Shaping. ICLR, 2023.
[22] Aghajohari M. et al. Best Response Shaping in Multi-Agent Systems. arXiv:2404.06519, 2024.
[23] Aghajohari M. et al. LOQA: Learning with Opponent Q-Learning Awareness. arXiv:2406.02920, 2024.
[24] Xie E. et al. Learning Latent Representations to Influence Multi-Agent Interaction. PMLR, 2021.

### 分布式 / GNE / 分布式优化支撑
[25] Ratliff L.J., Burden S.A., Sastry S.S. On the Characterization of Local Nash Equilibria in Continuous Games. IEEE TAC, 2016.
[26] Ye M., Hu G. Distributed Nash Equilibrium Seeking by a Consensus-Based Approach. IEEE TAC, 2017.
[27] Salehisadaghiani F., Pavel L. Distributed Nash Equilibrium Seeking: A Gossip-Based Algorithm. Automatica, 2016.
[28] Pang Y., Hu G. Distributed Nash Equilibrium Seeking with Limited Cost Function Knowledge via a Gradient-Free Method. IEEE TAC, 2021.
[29] Yin H., Shanbhag U.V., Mehta P.G. Nash Equilibrium with Scaled Congestion Costs and Shared Constraints. IEEE TAC, 2011.
[30] Maojiao Y., Guoqiang H. Game Design and Analysis for Price-Based Demand Response: An Aggregate Game Approach. IEEE Trans. Cybernetics, 2017.
[31] Wang D., Chen M., Wang W. Distributed Extremum Seeking for Optimal Resource Allocation…. IEEE TNNLS, 2019.
[32] Lin P., Ren W., Yang C., et al. Distributed Optimization with Nonuniform Unbounded Convex Constraints and Stepsizes. IEEE TAC, 2019.
[33] Zhang Y., Lou Y., Hong Y., et al. Distributed Projection-Based Algorithms for Source Localization. IEEE TWC, 2015.
[34] Liu Q., Yang S., Wang J. A Collective Neurodynamic Approach to Distributed Constrained Optimization. IEEE TNNLS, 2017.
[35] Zou Y., Huang B., Meng Z., et al. Continuous-Time Distributed NE for Constrained Games. Automatica, 2021.
[36] Deng Z., Nian X. Distributed GNE for Aggregative Games over Weight-Balanced Digraphs. IEEE TNNLS, 2019.
[37] Yu C.K., van der Schaar M., Sayed A.H. Distributed Learning for Stochastic GNE. IEEE TSP, 2017.
[38] Yi P., Pavel L. Operator-Splitting-Based Distributed GNE. CDC, 2017.

### 动力学系统与控制实现
[39] Stankovic M.S., Johansson K.H., Stipanovic D.M. Distributed NE Seeking with Applications to Mobile Sensor Networks. IEEE TAC, 2012.
[40] Ye M. Distributed NE Seeking for Games in Systems with Bounded Control Inputs. IEEE TAC, 2021.
[41] Romano A.R., Pavel L. Dynamic NE Seeking for Multi-Integrator Agents with Disturbance Rejection. IEEE TCNS, 2020.
[42] Vamvoudakis K.G., Lewis F.L. Online Adaptive Learning Solution of Coupled HJ Equations for Multi-Player Non-Zero-Sum Games. Automatica, 2011.
[43] Huang B., Zou Y., Meng Z. Distributed-Observer-Based NE for Quadratic Games with Nonlinear Dynamics. IEEE TSMC-Systems, 2021.
[44] Spong M.W., Hutchinson S., Vidyasagar M. Robot Modeling and Control. 2006.
[45] Deng Z., Liang S. Aggregative Games of Heterogeneous Euler–Lagrange Systems. Automatica, 2019.
[46] Garg T., Roy S.B. Barrier Lyapunov Controller for Euler–Lagrange Systems with Reduced Effort. IFAC-PapersOnLine, 2020.
[47] Deng Z. Distributed NE Seeking for Aggregative Games with Second-Order Nonlinear Players. Automatica, 2022.
[48] Zhang Y., Liang S., Wang X., et al. Distributed NE for Aggregative Games with Nonlinear Dynamics under External Disturbances. IEEE TCYB, 2020.
[49] Tang Y., Yi P. NE Seeking for High-order MAS with Unknown Dynamics. arXiv:2101.02883, 2021.
[50] Arbanas B., Ivanovic A., Car M., et al. Decentralized Planning and Control for UAV–UGV Cooperative Teams. Autonomous Robots, 2018.
[51] Yin J., Ye M. Adaptive NE for Games with Second-Order and Mixed-Order Players. ICCA, 2020.
[52] Ye M., Yin L., Wen G., et al. On Distributed NE Computation: Hybrid Games and a Consensus-Tracking Perspective. IEEE TCYB, 2021.

### 无模型：虚拟行动与极值搜索
[53] Brown G. Iterative Solution of Games by Fictitious Play. 1951.
[54] Marden J.R., Arslan G., Shamma J.S. Joint-Strategy Fictitious Play with Inertia. IEEE TAC, 2009.
[55] Fudenberg D., Takahashi S. Heterogeneous Beliefs and Local Information in Stochastic FP. GEB, 2011.
[56] Eksin C., Ribeiro A. Distributed FP in Uncertain Environments. IEEE TAC, 2018.
[57] Hernández E., Barrientos A., Cerro J.D. Selective Smooth FP for Multi-Robot Patrolling. Expert Systems with Applications, 2014.
[58] Krstić M., Wang H.-H. Stability of Extremum Seeking Feedback for Nonlinear Systems. Automatica, 2000.
[59] Adetola V., Guay M. Parameter Convergence in Adaptive ES Control. Automatica, 2007.
[60] Cheng L., Yuan Y., Liu H. Dynamic ES for Three-Player Attack-Defense with Unknown Gradient. Journal of the Franklin Institute, 2022.
[61] Paz P., Oliveira T.R., Pino A.V., et al. Model-Free Neuromuscular Electrical Stimulation by Stochastic ES. IEEE TCST, 2020.
[62] Abdelgalil M., Taha H. Lie-Bracket Approximation-Based ES with Vanishing Oscillations. Automatica, 2021.
[63] Shen D., Khayyer P., Izadian A. Sliding-Mode ES for MPPT in Wind System. PECI, 2016.
[64] 张思齐, 徐德民. 湍流环境中多弱感知机器人气味源搜索算法. 控制与决策, 2015.
[65] 陈万考, 郭玉英. 基于极值搜索的四旋翼无人机自抗扰控制. 飞行力学, 2021.
[66] Tanelli M., Astolfi A., Savaresi S.M. Non-local ES for Active Braking. 2006.
[67] Salamah Y.B., Ozguner U. Sliding-Mode Multivariable ES for Wind Farm Power Optimization. 2018.
[68] Pan Y., Özgüner Ü., Acarman T. Stability and Performance Improvement of ES with Sliding Mode. IJC, 2003.
[69] Pan Y., Ozguner U. Sliding-Mode ES for Linear-Quadratic Dynamic Game. ACC, 2004.
[70] Guay M., Dochain D. A Time-Varying ES Approach. Automatica, 2015.
[71] Adetola V., Guay M. Finite-Time Parameter Estimation in Adaptive Control. IEEE TAC, 2008.
[72] Guay M., Vandermeulen I., Dougherty S., et al. Distributed ES over Dynamically Coupled Unstable Agents. Automatica, 2018.
[73] Krilašević S., Grammatico S. Learning GNE in Multi-Agent Dynamical Systems via ES. Automatica, 2021.









## 多对多博弈均衡策略求解研究进展

多对多博弈的均衡求解问题，长期以来是多智能体系统中策略设计与决策优化的重要研究方向。该领域的发展大致包含 精确求解—启发式近似—强化学习逼近—分布式优化与广义纳什—无模型在线学习 等阶段，方法体系逐步从小规模静态博弈扩展至大规模动态系统。

### （1）精确求解与路径延拓方法

早期研究集中于小规模静态博弈的精确均衡计算。Lemke–Howson 算法（Lemke & Howson, 1964）是二人双矩阵博弈的经典方法，通过互补主元（complementary pivoting）沿最佳响应多面体上的补集路径寻找均衡，数值上可精确收敛到一个纳什点，但存在指数级最坏复杂度与多均衡的选择性问题，常用作离线“真值”标注/小规模基准[1]（L–H）。为应对多均衡与非线性问题，Govindan 与 Wilson 的全局 Newton/同伦路线通过连续变形原问题—跟踪解路径实现均衡选择与分支定位，对初值与步长有要求，路径转角与退化点处需特殊数值处理[2]（Global Newton）；Herings（2004）系统总结了同伦与路径延拓在纳什与精炼均衡计算中的实现细节（参数化、正则性、弧长控制等）[3]（Homotopy）。

### （2）启发式与演化近似方法

为突破规模瓶颈，研究者将纳什求解转化为优化问题，通过群体智能算法实现近似搜索。典型代表包括：基于粒子群优化（PSO）的多无人系统追逃与任务分配（以联合策略编码个体动作，群体更新=候选均衡搜索，以最大后悔/互为最佳响应准则做停机）[4]；基于遗憾值排序的遗传算法（Regret-based NSGA），在多目标进化中以后悔最小化作为排序指标，一次运行可得到多个候选纳什，便于后续筛选与仿真验证[5]；以及差分进化 + 粒子群的混合算法，在N 人有限博弈场景下给出经验收敛与复杂度评估[6]。

### （3）强化学习与深度学习逼近方法

进入动态多智能体阶段，强化学习成为求解纳什均衡的重要工具。Littman（1994）提出的 Minimax-Q 将 Q-learning 扩展到零和随机博弈，按Bellman–Shapley 极小极大更新价值；Hu 与 Wellman（2003）的 Nash-Q 在每个状态上求局部阶段博弈的混合纳什并回代到 Q 更新（涉及状态依赖的纳什求解器）；Bowling 与 Veloso（2002）的 WoLF-PHC 通过赢/输不同学习率缓解并发学习不稳定[7–9]。
在深度强化学习框架中，Lowe 等（2017）提出 MADDPG（CTDE），集中式评论家接收全体行动/状态，分布式执行策略仅依赖本地观测；Rashid 等（2020）的 QMIX 以单调性约束实现值函数可分解，训练时经混合网络保证团队值与个体 Q 的一致性[10–11]；Foerster 等（2018）的 LOLA 在策略更新中显式加入对手一次/多次学习步的影响项，通过对期望回报对对手参数的高阶可微实现对手学习感知[12]。这些方法为多对多场景下的策略优化提供了可扩展的学习体系。

#### （3.1）对手建模与策略塑形（Opponent Modeling / Shaping）

在多主体博弈中，智能体的最优策略不仅依赖环境状态，也受到对手学习行为的影响。为此，研究从“对手建模”发展出“策略塑形”范式，通过在更新过程中显式考虑对手的学习规则或反应机制，从而在动态交互中实现可控的均衡收敛。
LOLA：在自身梯度中加入对手一次/多次更新的影响项（(\nabla_\theta \mathbb{E}[R(\phi')]), (\phi'=\phi+\alpha \nabla_\phi \mathbb{E}[R])），从而影响对手的下一步[12]；DiCE 给出可微蒙特卡洛高阶梯度估计器，避免 REINFORCE 权重爆炸，便于在长时序上稳定估计[13]；SOS 在可微博弈中引入稳定性/一致性约束，避免进入鞍点并保证双方假设一致[14]；COLA 在双方同时塑形时给出一致的更新规则[15]；POLA 用近端约束替代一阶近似，使更新对参数化不敏感[16]。
LIO：学习外加激励函数并直接进入对手的回报（奖励塑形），不需假设对手白盒可微[17]。
元层塑形：M-FOS 将“与会学习对手交互”上升为元博弈，跨多局学习塑形策略；Shaper 面向高维长时序；COALA-PG 将会学习对手形式化为元 POMDP并给出高效 PG；MeVa 用元价值函数显式表征当前更新的长远效应[18–21]。
最优响应/值函数层塑形：Best Response Shaping 用近似/摊销最优响应替代高阶梯度[22]；LOQA 直接干预对手的 Q/学习信号[23]；Latent Representation Influence 学习低维对手表征并通过自身策略可控影响其演化，绕开深层递归推理[24]。

### （4）分布式与广义纳什均衡求解

在网络化与资源受限系统中，博弈模型通常涉及局部约束与共享资源，对应广义纳什均衡（GNE）问题。
无约束 NE（连续时间/估计–共识）：Ratliff 等给出连续博弈的局部 NE 一阶/二阶刻画（单调/伪梯度条件）[25]（Local NE）；Ye 与 Hu 基于一致性 + 梯度下降，在无向连通图上用Leader–Following/邻居交换估计非邻居行动，实现全分布式 NE 寻优，收敛到(\epsilon)-NE/NE取决于网络与步长[26]（Consensus-based）；Salehisadaghiani–Pavel 的Gossip 异步更新允许随机通信，在期望意义下收敛[27]（Gossip-based）；Pang–Hu 的无梯度寻优采用零阶/扰动–滤波估计梯度，覆盖步长递减/常值两种收敛分析[28]（Gradient-free）。
受约束/耦合 GNE：Zou 等的连续时间投影在集合约束（局部可行集）与等/不等式耦合下采用投影–伪梯度设计，收敛到变分 GNE[35]；Deng–Nian 面向聚合博弈 + 线性耦合 + 集合约束，在权重平衡有向图上给出分布式投影–微分包含算法[36]；Yi–Pavel 用算子分裂（原始–对偶/不动点）构造离散时间算法（靠单调/利普希茨条件与邻居交换确保收敛）[38]。与此配套，分布式优化中的不均匀约束/步长[32]、投影定位/源定位[33]、神经动态法[34]可作为子模块用于可行域处理与复杂度控制。随机 GNE 学习在不确定/随机环境下给出均值收敛与方差界[37]。

### （5）动力学约束与物理系统博弈

为应对实际系统中的动力学约束，研究者将博弈模型推广到动态多智能体。
线性积分器/有界控制/外扰：Stankovic 等在移动传感网络中用离散时间随机极值/一致性实现NE 寻优[39]；Ye 在输入饱和/有界控制下结合饱和控制 + 一致性共享保持可行与收敛[40]；Romano–Pavel 融合干扰观测器（DOB）在多积分器上实现扰动抑制 + NE 收敛[41]。
非线性与 Euler–Lagrange（EL）系统：Vamvoudakis–Lewis 以策略迭代强化学习在线求解耦合 HJ 方程的非零和微分博弈[42]；Huang 等在非线性二次博弈中引入分布式观测器/高增益反馈[43]；Deng–Liang 针对异构 EL与聚合项，在参数已知/未知两种情形给出分布式收敛[45]；障碍 Lyapunov用于状态/输入约束与碰撞避免[46]；进一步有二阶非线性参与者[47]、外部扰动[48]、高阶/混合阶玩家与自适应协议[49–52]。

### （6）无模型博弈学习：虚拟行动与极值搜索

在缺乏精确代价模型的场景下，无模型博弈学习得到关注。
虚拟行动（Fictitious Play, FP）：Brown 的原始 FP 假设可观察对手行动并对经验频率做最佳响应，在二人零和、势博弈等类收敛；JSFP with Inertia 通过惰性/惯性避免振荡并扩展可计算博弈类别[53–54]；异质信念/局部信息 FP处理随机与部分信息[55]；分布式 FP在不确定环境/局部通信下实现更新[56]；平滑/选择性 FP用于多机器人巡逻等任务[57]。这些方法通信与计算成本随参与者数上升（与学位论文对 FP 的“直方图/通信代价”描述一致）。
极值搜索（Extremum Seeking, ES）：以正弦摄动 + 滤波零阶估计代价梯度，时标分离与平均法给出稳定性理论与参数整定规则[58]；自适应 ES 参数收敛[59]；扩展到多玩家攻防（动态 ES）[60]、随机 ES[61]、Lie-bracket ES（衰减扰动）[62]；滑模 ES避免连续抖动、可设定收敛速度，但多输入多输出实现与状态切换需谨慎[63],[66–69]；时变 ES把问题改写为时变估计降低对抖动幅频的依赖[70–71]；在网络上有分布式 ES与基于 ES 的 GNE 学习以应对动态耦合/不稳定代理[72–73]。

## 参考文献

### 精确/同伦与近似
[1] Lemke C.E., Howson J.T. Equilibrium Points of Bimatrix Games. 1964.
[2] Govindan S., Wilson R. A Global Newton Method to Compute Nash Equilibria. 2003.
[3] Herings P.J.-J. Computing Nash Equilibria: Homotopy Methods. 2004.
[4] An improved predator–prey particle swarm optimization for multi-agent pursuit–evasion. PLOS ONE, 2021.
[5] Regret-Based Nash Equilibrium Sorting Genetic Algorithm. Evolutionary Computation, 2022.
[6] Differential-evolution–particle-swarm hybrid for N-player finite games. AIMS Mathematics, 2021.

### RL / 深度 MARL 与对手建模
[7] Littman M. Markov Games as a Framework for Multi-Agent Reinforcement Learning. 1994.
[8] Hu J., Wellman M. Nash Q-Learning for General-Sum Stochastic Games. JMLR, 2003.
[9] Bowling M., Veloso M. WoLF-PHC: Convergence in Self-Play. 2002.
[10] Lowe R. et al. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (MADDPG). 2017.
[11] Rashid T. et al. QMIX: Monotonic Value Function Factorisation. JMLR, 2020.
[12] Foerster J. et al. Learning with Opponent-Learning Awareness (LOLA). 2018.
[13] Foerster J. et al. DiCE: The Infinitely Differentiable Monte Carlo Estimator. ICML, 2018.
[14] Letcher A. et al. Stable Opponent Shaping in Differentiable Games. NeurIPS, 2019.
[15] Willi T. et al. Consistent Opponent Learning Awareness (COLA). ICLR, 2022.
[16] Zhao W. et al. Proximal Opponent Learning Awareness (POLA). ICLR, 2022.
[17] Yang Y. et al. Learning with Incentivized Opponents (LIO). ICLR, 2020.
[18] Lu C. et al. Meta-Foresight Opponent Shaping (M-FOS). ICLR, 2022.
[19] Khan Z. et al. Shaper: Meta-Learning to Influence Multi-Agent Interaction. arXiv:2402.01068, 2024.
[20] Bertrand Q. et al. COALA_PG: Meta-POMDP Policy Gradient for Learning-to-Learn in Games. arXiv:2406.04378, 2024.
[21] Cooijmans T. et al. Meta-Value Learning for Multi-Agent Shaping. ICLR, 2023.
[22] Aghajohari M. et al. Best Response Shaping in Multi-Agent Systems. arXiv:2404.06519, 2024.
[23] Aghajohari M. et al. LOQA: Learning with Opponent Q-Learning Awareness. arXiv:2406.02920, 2024.
[24] Xie E. et al. Learning Latent Representations to Influence Multi-Agent Interaction. PMLR, 2021.

### 分布式 / GNE / 分布式优化支撑
[25] Ratliff L.J., Burden S.A., Sastry S.S. On the Characterization of Local Nash Equilibria in Continuous Games. IEEE TAC, 2016.
[26] Ye M., Hu G. Distributed Nash Equilibrium Seeking by a Consensus-Based Approach. IEEE TAC, 2017.
[27] Salehisadaghiani F., Pavel L. Distributed Nash Equilibrium Seeking: A Gossip-Based Algorithm. Automatica, 2016.
[28] Pang Y., Hu G. Distributed Nash Equilibrium Seeking with Limited Cost Function Knowledge via a Gradient-Free Method. IEEE TAC, 2021.
[29] Yin H., Shanbhag U.V., Mehta P.G. Nash Equilibrium with Scaled Congestion Costs and Shared Constraints. IEEE TAC, 2011.
[30] Maojiao Y., Guoqiang H. Game Design and Analysis for Price-Based Demand Response: An Aggregate Game Approach. IEEE Trans. Cybernetics, 2017.
[31] Wang D., Chen M., Wang W. Distributed Extremum Seeking for Optimal Resource Allocation…. IEEE TNNLS, 2019.
[32] Lin P., Ren W., Yang C., et al. Distributed Optimization with Nonuniform Unbounded Convex Constraints and Stepsizes. IEEE TAC, 2019.
[33] Zhang Y., Lou Y., Hong Y., et al. Distributed Projection-Based Algorithms for Source Localization. IEEE TWC, 2015.
[34] Liu Q., Yang S., Wang J. A Collective Neurodynamic Approach to Distributed Constrained Optimization. IEEE TNNLS, 2017.
[35] Zou Y., Huang B., Meng Z., et al. Continuous-Time Distributed NE for Constrained Games. Automatica, 2021.
[36] Deng Z., Nian X. Distributed GNE for Aggregative Games over Weight-Balanced Digraphs. IEEE TNNLS, 2019.
[37] Yu C.K., van der Schaar M., Sayed A.H. Distributed Learning for Stochastic GNE. IEEE TSP, 2017.
[38] Yi P., Pavel L. Operator-Splitting-Based Distributed GNE. CDC, 2017.

### 动力学系统与控制实现
[39] Stankovic M.S., Johansson K.H., Stipanovic D.M. Distributed NE Seeking with Applications to Mobile Sensor Networks. IEEE TAC, 2012.
[40] Ye M. Distributed NE Seeking for Games in Systems with Bounded Control Inputs. IEEE TAC, 2021.
[41] Romano A.R., Pavel L. Dynamic NE Seeking for Multi-Integrator Agents with Disturbance Rejection. IEEE TCNS, 2020.
[42] Vamvoudakis K.G., Lewis F.L. Online Adaptive Learning Solution of Coupled HJ Equations for Multi-Player Non-Zero-Sum Games. Automatica, 2011.
[43] Huang B., Zou Y., Meng Z. Distributed-Observer-Based NE for Quadratic Games with Nonlinear Dynamics. IEEE TSMC-Systems, 2021.
[44] Spong M.W., Hutchinson S., Vidyasagar M. Robot Modeling and Control. 2006.
[45] Deng Z., Liang S. Aggregative Games of Heterogeneous Euler–Lagrange Systems. Automatica, 2019.
[46] Garg T., Roy S.B. Barrier Lyapunov Controller for Euler–Lagrange Systems with Reduced Effort. IFAC-PapersOnLine, 2020.
[47] Deng Z. Distributed NE Seeking for Aggregative Games with Second-Order Nonlinear Players. Automatica, 2022.
[48] Zhang Y., Liang S., Wang X., et al. Distributed NE for Aggregative Games with Nonlinear Dynamics under External Disturbances. IEEE TCYB, 2020.
[49] Tang Y., Yi P. NE Seeking for High-order MAS with Unknown Dynamics. arXiv:2101.02883, 2021.
[50] Arbanas B., Ivanovic A., Car M., et al. Decentralized Planning and Control for UAV–UGV Cooperative Teams. Autonomous Robots, 2018.
[51] Yin J., Ye M. Adaptive NE for Games with Second-Order and Mixed-Order Players. ICCA, 2020.
[52] Ye M., Yin L., Wen G., et al. On Distributed NE Computation: Hybrid Games and a Consensus-Tracking Perspective. IEEE TCYB, 2021.

### 无模型：虚拟行动与极值搜索
[53] Brown G. Iterative Solution of Games by Fictitious Play. 1951.
[54] Marden J.R., Arslan G., Shamma J.S. Joint-Strategy Fictitious Play with Inertia. IEEE TAC, 2009.
[55] Fudenberg D., Takahashi S. Heterogeneous Beliefs and Local Information in Stochastic FP. GEB, 2011.
[56] Eksin C., Ribeiro A. Distributed FP in Uncertain Environments. IEEE TAC, 2018.
[57] Hernández E., Barrientos A., Cerro J.D. Selective Smooth FP for Multi-Robot Patrolling. Expert Systems with Applications, 2014.
[58] Krstić M., Wang H.-H. Stability of Extremum Seeking Feedback for Nonlinear Systems. Automatica, 2000.
[59] Adetola V., Guay M. Parameter Convergence in Adaptive ES Control. Automatica, 2007.
[60] Cheng L., Yuan Y., Liu H. Dynamic ES for Three-Player Attack-Defense with Unknown Gradient. Journal of the Franklin Institute, 2022.
[61] Paz P., Oliveira T.R., Pino A.V., et al. Model-Free Neuromuscular Electrical Stimulation by Stochastic ES. IEEE TCST, 2020.
[62] Abdelgalil M., Taha H. Lie-Bracket Approximation-Based ES with Vanishing Oscillations. Automatica, 2021.
[63] Shen D., Khayyer P., Izadian A. Sliding-Mode ES for MPPT in Wind System. PECI, 2016.
[64] 张思齐, 徐德民. 湍流环境中多弱感知机器人气味源搜索算法. 控制与决策, 2015.
[65] 陈万考, 郭玉英. 基于极值搜索的四旋翼无人机自抗扰控制. 飞行力学, 2021.
[66] Tanelli M., Astolfi A., Savaresi S.M. Non-local ES for Active Braking. 2006.
[67] Salamah Y.B., Ozguner U. Sliding-Mode Multivariable ES for Wind Farm Power Optimization. 2018.
[68] Pan Y., Özgüner Ü., Acarman T. Stability and Performance Improvement of ES with Sliding Mode. IJC, 2003.
[69] Pan Y., Ozguner U. Sliding-Mode ES for Linear-Quadratic Dynamic Game. ACC, 2004.
[70] Guay M., Dochain D. A Time-Varying ES Approach. Automatica, 2015.
[71] Adetola V., Guay M. Finite-Time Parameter Estimation in Adaptive Control. IEEE TAC, 2008.
[72] Guay M., Vandermeulen I., Dougherty S., et al. Distributed ES over Dynamically Coupled Unstable Agents. Automatica, 2018.
[73] Krilašević S., Grammatico S. Learning GNE in Multi-Agent Dynamical Systems via ES. Automatica, 2021.