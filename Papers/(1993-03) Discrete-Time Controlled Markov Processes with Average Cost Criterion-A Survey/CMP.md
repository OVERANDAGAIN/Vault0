---
创建时间: 2025-十月-25日  星期六, 6:37:47 晚上
---

[[(1993-03) Discrete-Time Controlled Markov Processes with Average Cost Criterion-A Survey]]
# CMP == MDP?

这段关于 **Controlled Markov Process（CMP）** 的描述，其实就是在**严格数学化地定义马尔可夫决策过程（MDP）**。我来为你逐层解释其含义，帮助你理解每一个符号与定义的意义。

---

## 一、总体定义：五元组 $(S, A, U, P, c)$

一个**离散时间的、平稳的受控马尔可夫过程（Controlled Markov Process, CMP）**，也称作**马尔可夫决策过程（Markov Decision Process, MDP）**，由以下五个要素组成：

| 记号            | 含义                                                                                                                   |                                           |
| ------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------------- |
| $S$           | 状态空间（state space），系统可能处于的所有状态的集合。是一个 **Borel空间**，即带有σ-代数的可测空间。                                                       |                                           |
| $A$           | 动作空间（action/control space），控制者可以采取的动作集合。也是一个Borel空间。                                                                 |                                           |
| $U:S\to B(A)$ | **可行动作映射（admissible action mapping）**。对每个状态 $x\in S$，$U(x)$ 给出了在该状态下可以选择的动作集合，是一个**严格可测、值为紧集（compact-valued）**的多值函数。 |                                           |
| $P$           | **转移核（transition kernel）**。这是一个条件概率分布 $P(\cdot                                                                       | x,a)$，表示在状态 $x$ 采取动作 $a$ 后，系统转移到下一个状态的概率。 |
| $c$           | **单阶段代价函数（one-stage cost function）**，$c(x,a)$ 表示在状态 $x$ 下采取动作 $a$ 所付出的代价（或负的奖励）。                                     |                                           |

---

## 二、系统的演化过程

在时刻 $t$：

1. 系统当前状态为 $X_t = x$。
2. 控制者选择一个动作 $A_t = a \in U(x)$。
3. 立即产生代价 $c(x, a)$。
4. 系统根据概率分布 $P(\cdot|x, a)$ 转移到下一个状态 $X_{t+1}$。
5. 重复这一过程，形成一个马尔可夫链控制系统。

---

## 三、规划区间（Planning Horizon）

系统运行的总时间段称为**决策区间（horizon）**：

* **有限时域**：$T = {0, 1, ..., N-1}$。
* **无限时域**：$T = \mathbb{N}_0$（即所有非负整数）。

---

## 四、历史空间与样本空间

为了描述随机过程的全貌，定义：

* 历史空间（history space）：
  $$
  H_0 := S, \quad H_t := H_{t-1} \times K, \quad K := {(x,a) : x\in S, a\in U(x)}.
  $$
  即 $H_t$ 记录从初始状态到当前时刻的全部状态–动作历史。

* 样本空间（sample space）：
  $$
  \Omega := (S \times A)^{\infty}
  $$
  即所有可能的状态–动作序列 $(x_0, a_0, x_1, a_1, \dots)$。

每个 $\omega \in \Omega$ 表示一次系统完整的随机演化过程。

---

## 五、随机过程定义

定义三个核心随机过程：

| 名称   | 定义                                         | 含义                          |
| ---- | ------------------------------------------ | --------------------------- |
| 状态过程 | $X_t(\omega) = x_t$                        | 表示在样本 $\omega$ 下时刻 $t$ 的状态。 |
| 动作过程 | $A_t(\omega) = a_t$                        | 表示在样本 $\omega$ 下时刻 $t$ 的动作。 |
| 信息过程 | $H_t(\omega) = (x_0, \dots, a_{t-1}, x_t)$ | 表示截至时刻 $t$ 的全部历史信息。         |

这些随机过程都定义在可测空间 $(\Omega, \mathcal{B}(\Omega))$ 上。

---

## 六、两个例子

### ✅ **例 2.1：非线性随机系统**

* 系统状态转移方程：
  $$
  X_{t+1} = F(X_t, A_t, W_t)
  $$
  其中 $W_t$ 是独立同分布的噪声变量（随机扰动）。
* 对应的转移核定义为：
  $$
  P(D|x,a) = \int_W \mathbf{1}_{{F(x,a,w)\in D}} P_W(dw)
  $$
  表示在 $(x,a)$ 下，经过噪声 $w$ 后进入集合 $D$ 的概率。
* 给定代价函数 $c(x,a)$ 后，$(S, A, U, P, c)$ 完全刻画了此CMP。

---

### ✅ **例 2.2：离散状态空间**

* 状态集 $S$ 是可数的（如 $\mathbb{N}_0$）。
* $U(x) = A$，即所有动作都可行。
* 转移核 $P$ 可写为：
  $$
  P(a) = [p_{ij}(a)]
  $$
  其中 $p_{ij}(a)$ 表示在动作 $a$ 下从状态 $i$ 转移到 $j$ 的概率。
* 这时 CMP 就简化为“动作依赖的马尔可夫链”，附带一个代价函数 $c(i,a)$。

---

## 七、总结直观理解

一句话总结：

> **Controlled Markov Process (CMP)** 就是一个在每个时间步根据当前状态选择一个控制动作，从而以概率方式演化状态的随机系统，其演化和代价完全由五元组 $(S, A, U, P, c)$ 决定。

它是强化学习（RL）和最优控制理论的数学基础。

* 在强化学习中，$P$ 通常未知；
* 在最优控制中，$P$ 通常已知；
* 而 $U(x)$ 体现了可行动作的约束。

---

是否希望我接着为你画一张结构图（展示 $S,A,U,P,c$ 的关系和状态转移流）？这会让整个 CMP 框架更直观。
