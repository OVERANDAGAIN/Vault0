---
åˆ›å»ºæ—¶é—´: 2025-åæœˆ-30æ—¥  æ˜ŸæœŸå››, 6:41:52 æ™šä¸Š
---
[[25-10 Between MDPs and semi-MDPs-A framework for temporal abstraction in reinforcement learning]]



# ðŸ”¹åŽŸæ–‡

> Eqs. (16) and (17) were applied **whenever an option terminated**,
> whereas, in the **intra-option model-learning method**,
> Eqs. (18) and (19) were applied **on every step** to **all options that were consistent with the action taken on that step.**

---

## ä¸€ã€(16)-(17)ï¼š**â€œoption-levelâ€ æ›´æ–°ï¼Œåªåœ¨ç»ˆæ­¢æ—¶è¿›è¡Œ**

è¿™ä¸¤å¼æ˜¯æœ€æ—©çš„ *SMDP model learning* å½¢å¼ï¼š

$$
\hat r^o_s \leftarrow \hat r^o_s + \alpha [r - \hat r^o_s], \
\hat p^o_{sx} \leftarrow \hat p^o_{sx} + \alpha [\gamma^k \delta_{s'x} - \hat p^o_{sx}],
$$

å®ƒä»¬çš„å«ä¹‰æ˜¯ï¼š

* ä½ æŠŠæ•´ä¸ª option çœ‹æˆä¸€ä¸ª**å®åŠ¨ä½œï¼ˆmacro-actionï¼‰**ï¼›
* åªæœ‰å½“ option **æ‰§è¡Œç»“æŸï¼ˆterminatedï¼‰** æ—¶ï¼Œæ‰çŸ¥é“ï¼š

  * è¿™æ¬¡ option ä»Žå“ªå¼€å§‹ï¼ˆ$s$ï¼‰ï¼›
  * æ‰§è¡Œäº†å¤šä¹…ï¼ˆ$k$ï¼‰ï¼›
  * æœ€ç»ˆè½åˆ°å“ªä¸ªçŠ¶æ€ï¼ˆ$s'$ï¼‰ï¼›
  * å¾—åˆ°äº†ä»€ä¹ˆæ€»å¥–åŠ±ï¼ˆ$r$ï¼‰ã€‚
* ç„¶åŽä¸€æ¬¡æ€§æ›´æ–°å¯¹ option æ¨¡åž‹çš„ä¼°è®¡ã€‚

ðŸ‘‰ å› æ­¤ï¼Œå®ƒæ˜¯ **episodic / option-level æ›´æ–°**ï¼š
åªæœ‰åœ¨ option æ‰§è¡Œå®Œä¹‹åŽï¼ˆ$\beta=1$ï¼‰æ‰æ›´æ–°ã€‚

---

## äºŒã€(18)-(19)ï¼š**â€œintra-optionâ€ æ›´æ–°ï¼Œæ¯æ­¥éƒ½æ›´æ–°**

æ–°çš„å…¬å¼ï¼š

$$
\hat r^o_{s_t} \leftarrow \hat r^o_{s_t}

+ \alpha [r_{t+1} + \gamma(1-\beta(s_{t+1}))\hat r^o_{s_{t+1}} - \hat r^o_{s_t}],
  $$

$$
\hat p^o_{s_t x} \leftarrow \hat p^o_{s_t x}

+ \alpha [\gamma(1-\beta(s_{t+1}))\hat p^o_{s_{t+1} x}
+ \gamma\beta(s_{t+1})\delta_{s_{t+1},x} - \hat p^o_{s_t x}].
  $$

ç‰¹ç‚¹æ˜¯ï¼š

* **æ¯ä¸€æ­¥**ï¼ˆä¸è®ºæ˜¯å¦ç»ˆæ­¢ï¼‰éƒ½è¿›è¡Œæ›´æ–°ï¼›
* æ›´æ–°ç›®æ ‡æ˜¯ *option å†…éƒ¨çš„Bellmanä¸€è‡´æ€§*ï¼›
* æ¯æ­¥éƒ½åœ¨é€¼è¿‘â€œå¦‚æžœæˆ‘ç»§ç»­æ‰§è¡Œè¯¥ optionï¼Œå…¶å¥–åŠ±ä¸Žç»ˆæ­¢æ¦‚çŽ‡å¦‚ä½•é€’æŽ¨â€ã€‚

è¿™å°±åƒæ˜¯ TD(0) å¯¹å€¼å‡½æ•°çš„å­¦ä¹ ï¼Œåªä¸è¿‡ç›®æ ‡å˜æˆäº† *option çš„æ¨¡åž‹*ã€‚

---

## ä¸‰ã€â€œapplied â€¦ on every step to all options consistent with the action takenâ€

è¿™ä¸€å¥éžå¸¸é‡è¦ï¼Œæ„æ€æ˜¯ï¼š

> æ¯å½“æ‰§è¡ŒçŽ¯å¢ƒåŠ¨ä½œ $a_t$ æ—¶ï¼Œæ‰€æœ‰**å…¶å†…éƒ¨ç­–ç•¥ $\pi_o$ åœ¨è¯¥çŠ¶æ€ $s_t$ ä¸‹ä¹Ÿå¯èƒ½é€‰æ‹© $a_t$** çš„ optionï¼Œ
> éƒ½å¯ä»¥ç”¨è¿™ä¸€æ­¥çš„ç»éªŒ $(s_t,a_t,r_{t+1},s_{t+1})$ æ¥æ›´æ–°è‡ªå·±çš„æ¨¡åž‹ã€‚

æ¢å¥è¯è¯´ï¼š

* å‡å¦‚å½“å‰æ‰§è¡Œçš„ action $a_t$ ç¬¦åˆæŸä¸ª option çš„å†…éƒ¨ç­–ç•¥ $\pi_o(a_t|s_t)>0$ï¼Œ
  é‚£ä¹ˆè¿™æ¡ç»éªŒå¯¹è¯¥ option ä¹Ÿæ˜¯â€œæœ‰æ•ˆçš„â€ï¼Œå¯ä»¥ç”¨äºŽæ›´æ–°è¯¥ option çš„æ¨¡åž‹ä¼°è®¡ã€‚
* å› æ­¤ï¼Œä¸€ä¸ªç»éªŒæ ·æœ¬å¯ä»¥**åŒæ—¶æ›´æ–°å¤šä¸ª options** çš„æ¨¡åž‹ã€‚

è¿™å°±æ˜¯ â€œconsistent with the action takenâ€ çš„å«ä¹‰ã€‚

å®ƒåæ˜ å‡º option å­¦ä¹ çš„ **off-policy / parallel updating** ç‰¹å¾ï¼š
å³ä¾¿å½“å‰æ‰§è¡Œçš„ option ä¸æ˜¯ $o$ï¼Œ
åªè¦å½“å‰åŠ¨ä½œå¯¹ $o$ æ¥è¯´æ˜¯å¯èƒ½çš„ï¼Œ
ä¹Ÿå¯ä»¥è®© $o$ çš„æ¨¡åž‹èŽ·å¾—å­¦ä¹ ä¿¡å·ã€‚

---

## å››ã€æ€»ç»“å¯¹æ¯”è¡¨

| é¡¹ç›®     | æ—§æ–¹æ³• (16â€“17)      | æ–°æ–¹æ³• (18â€“19)                 |
| ------ | ---------------- | --------------------------- |
| æ›´æ–°æ—¶æœº   | ä»…å½“ option ç»ˆæ­¢æ—¶    | æ¯ä¸€æ­¥éƒ½æ›´æ–°                      |
| å­¦ä¹ ç²’åº¦   | æ•´ä¸ª option æ‰§è¡Œä¸€æ¬¡æ›´æ–° | è¿žç»­æ—¶é—´æ­¥ TD æ›´æ–°                 |
| å¯æ›´æ–°çš„å¯¹è±¡ | åªæœ‰å½“å‰æ‰§è¡Œçš„ option   | æ‰€æœ‰å†…éƒ¨ç­–ç•¥åŒ…å«å½“å‰åŠ¨ä½œçš„ options       |
| æ›´æ–°ä¿¡æ¯   | æ€»å¥–åŠ± + æœ€ç»ˆç»ˆæ­¢çŠ¶æ€     | å±€éƒ¨å¥–åŠ± + ä¸‹ä¸€æ­¥çŠ¶æ€                |
| ç±»æ¯”     | SMDP-level å­¦ä¹     | TD(0)-style online learning |

---

## äº”ã€ç›´è§‚ç†è§£

* **(16)-(17)** å°±åƒï¼šâ€œç­‰ä½ æ•´ä¸ªä»»åŠ¡åšå®ŒåŽå†æ€»ç»“ä¸€æ¬¡ç»éªŒã€‚â€
* **(18)-(19)** åˆ™æ˜¯ï¼šâ€œæ¯èµ°ä¸€æ­¥å°±åæ€ä¸€ä¸‹åˆšæ‰çš„ç»“æžœâ€ï¼Œ
  è€Œä¸”ä¸æ­¢è‡ªå·±ï¼ˆå½“å‰optionï¼‰åæ€ï¼Œ
  ä»»ä½•â€œå¯èƒ½ä¹Ÿä¼šèµ°è¿™æ­¥â€çš„optionéƒ½èƒ½åŒæ—¶å­¦åˆ°ã€‚

---
---

å³ä¾¿å½“å‰æ‰§è¡Œçš„ option ä¸æ˜¯ $o$ï¼Œåªè¦å½“å‰åŠ¨ä½œå¯¹ $o$ æ¥è¯´æ˜¯å¯èƒ½çš„ï¼Œä¹Ÿå¯ä»¥è®© $o$ çš„æ¨¡åž‹èŽ·å¾—å­¦ä¹ ä¿¡å·ã€‚

# ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆè¯´


éžå¸¸å¥½çš„è¿½é—®ã€‚
ä¸ºä»€ä¹ˆæˆ‘ä»¬èƒ½è¯´ï¼š

> å³ä¾¿å½“å‰æ‰§è¡Œçš„ option ä¸æ˜¯ $o$ï¼Œåªè¦å½“å‰åŠ¨ä½œå¯¹ $o$ æ¥è¯´æ˜¯å¯èƒ½çš„ï¼Œä¹Ÿå¯ä»¥è®© $o$ çš„æ¨¡åž‹èŽ·å¾—å­¦ä¹ ä¿¡å·ï¼Ÿ

è¿™å¥è¯çš„æ ¸å¿ƒä¾æ®æ¥è‡ª **intra-option learning çš„ off-policy ç‰¹æ€§**ã€‚
ä¸‹é¢åˆ†ä¸‰æ­¥è§£é‡Šå®ƒçš„é€»è¾‘ã€‚

---

## ä¸€ã€option å†…éƒ¨æ¨¡åž‹ä¾èµ–çš„æ˜¯çŠ¶æ€â€“åŠ¨ä½œâ€“ä¸‹ä¸€ä¸ªçŠ¶æ€ä¸‰å…ƒç»„

è§‚å¯Ÿ Bellman å¼ (18) å’Œ (19)ï¼š

$$
\hat r^o_{s_t} \leftarrow \hat r^o_{s_t} +
\alpha,[r_{t+1}+\gamma(1-\beta(s_{t+1}))\hat r^o_{s_{t+1}}-\hat r^o_{s_t}],
$$
$$
\hat p^o_{s_t x} \leftarrow \hat p^o_{s_t x} +
\alpha,[\gamma(1-\beta(s_{t+1}))\hat p^o_{s_{t+1}x}
+\gamma\beta(s_{t+1})\delta_{s_{t+1},x}-\hat p^o_{s_t x}].
$$

å¯ä»¥çœ‹åˆ°ï¼š
æ›´æ–°æ‰€éœ€çš„ä¿¡æ¯åªæœ‰å½“å‰ $(s_t,a_t,r_{t+1},s_{t+1})$ ä»¥åŠè¯¥ option çš„ç»ˆæ­¢æ¦‚çŽ‡ $\beta(s_{t+1})$ã€‚

**å¹¶æ²¡æœ‰è¦æ±‚å½“å‰æ•´ä¸ª episode ç¡®å®žåœ¨æ‰§è¡Œè¯¥ optionã€‚**
åªè¦æˆ‘ä»¬çŸ¥é“ option $o$ çš„å†…éƒ¨ç­–ç•¥ $\pi_o$ åœ¨ $s_t$ ä¸‹ä¼šå¦‚ä½•é€‰åŠ¨ä½œï¼Œå°±èƒ½è¯„ä¼°è¿™æ¡ç»éªŒå¯¹ $o$ çš„ä¸€è‡´æ€§ã€‚

---

## äºŒã€â€œconsistent with the action takenâ€ çš„å«ä¹‰

å¯¹äºŽæŸä¸ª option $o$ï¼Œå¦‚æžœå®ƒçš„å†…éƒ¨ç­–ç•¥æ»¡è¶³
$$
\pi_o(a_t|s_t) > 0,
$$
å°±è¡¨ç¤ºè¯¥åŠ¨ä½œæ˜¯ $o$ åœ¨è¯¥çŠ¶æ€ä¸‹**å¯èƒ½é€‰æ‹©çš„åŠ¨ä½œ**ã€‚

æ¢å¥è¯è¯´â€”â€”å¦‚æžœçŽ°åœ¨çŽ¯å¢ƒä¸­æ‰§è¡Œçš„åŠ¨ä½œ $a_t$ æ˜¯ $o$ æœ‰å¯èƒ½æ‰§è¡Œçš„ï¼Œé‚£ä¹ˆä»Ž $o$ çš„è§’åº¦çœ‹ï¼Œè¿™ä¸ª transition $(s_t,a_t,s_{t+1},r_{t+1})$ æ˜¯å®ƒçš„ä¸€ä¸ªâ€œåˆæ³•æ ·æœ¬â€ã€‚

å› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªæ ·æœ¬è§†ä¸º $o$ çš„ä¸€æ¬¡ off-policy ç»éªŒï¼Œç”¨äºŽæ”¹è¿› $o$ çš„æ¨¡åž‹ä¼°è®¡ï¼ˆå³ $\hat r^o$ ä¸Ž $\hat p^o$ï¼‰ã€‚

---

## ä¸‰ã€ä¸ºä»€ä¹ˆè¿™æ ·åšæ˜¯åˆç†çš„ï¼ˆç†è®ºä¾æ®ï¼‰

1. **ç›®æ ‡æ˜¯ä¸€è‡´æ€§å­¦ä¹ ï¼Œè€Œéžç­–ç•¥è¡Œä¸ºè¿˜åŽŸã€‚**
   æˆ‘ä»¬å­¦ä¹ çš„æ˜¯ option çš„ *model*ï¼Œå³å®ƒæ»¡è¶³çš„ Bellman ä¸€è‡´æ€§æ–¹ç¨‹ã€‚
   è¿™ä¸ªæ–¹ç¨‹åªä¾èµ–äºŽçŽ¯å¢ƒåŠ¨åŠ›å­¦ $p(s'|s,a)$ ä¸Ž $\pi_o(a|s)$ï¼Œè€Œä¸ä¾èµ–å½“å‰è¡Œä¸ºç­–ç•¥æ˜¯å¦ç­‰äºŽ $\pi_o$ã€‚

2. **off-policy å­¦ä¹ åŽŸç†ï¼š**
   åœ¨ off-policy åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»»æ„â€œè¡Œä¸ºç­–ç•¥â€æ”¶é›†çš„æ•°æ®ï¼Œåªè¦å¯¹ç›®æ ‡ç­–ç•¥å¯è¾¾çš„ $(s,a)$ é‡‡æ ·è¦†ç›–åˆ°ã€‚
   è¿™é‡Œè¡Œä¸ºç­–ç•¥æ˜¯â€œå®žé™…æ‰§è¡Œçš„ optionâ€ï¼Œç›®æ ‡ç­–ç•¥æ˜¯â€œè¢«å­¦ä¹ çš„ option $o$â€ã€‚
   åªè¦ $\pi_o(a_t|s_t)>0$ï¼Œå°±æ„å‘³ç€å½“å‰æ ·æœ¬å¯¹ $o$ æœ‰ç»Ÿè®¡æ„ä¹‰ã€‚

3. **å¤š option å¹¶è¡Œæ›´æ–°çš„ä¼˜åŠ¿ï¼š**
   é€šè¿‡å…è®¸ä¸åŒ option åŒæ—¶æ›´æ–°ï¼ˆåªè¦åŠ¨ä½œä¸€è‡´ï¼‰ï¼Œæˆ‘ä»¬èƒ½åœ¨ä¸€æ¬¡äº¤äº’ä¸­å­¦ä¹ æ‰€æœ‰ä¸Žå½“å‰è¡Œä¸ºç›¸å…¼å®¹çš„ optionsã€‚
   è¿™æžå¤§åœ°æé«˜äº†æ ·æœ¬åˆ©ç”¨çŽ‡ï¼Œä½¿ option çš„æ¨¡åž‹èƒ½æ›´å¿«æ”¶æ•›ã€‚

---

## å››ã€ç›´è§‚ç±»æ¯”

å¯ä»¥æŠŠä¸€ä¸ª option çœ‹ä½œä¸€ä¸ªâ€œå‡æƒ³çš„ä¸“å®¶â€ï¼Œå®ƒå¯¹æ¯ä¸ªçŠ¶æ€éƒ½æœ‰è‡ªå·±çš„è¡ŒåŠ¨åˆ†å¸ƒ $\pi_o(a|s)$ã€‚

* å¦‚æžœå½“å‰çœŸå®žæ‰§è¡Œçš„åŠ¨ä½œ $a_t$ ä¹Ÿæ˜¯è¿™ä¸ªä¸“å®¶å¯èƒ½é‡‡å–çš„åŠ¨ä½œï¼Œ
  é‚£ä¹ˆè¯¥ä¸“å®¶å¯ä»¥è§‚å¯Ÿè¿™æ¬¡æ‰§è¡Œç»“æžœ $(r_{t+1}, s_{t+1})$ æ¥ä¿®æ­£è‡ªå·±å¯¹ä¸–ç•Œçš„ç†è§£ï¼ˆmodelï¼‰ã€‚
* å³ä½¿å½“ä¸‹æ‰§è¡Œçš„ä¸æ˜¯å®ƒè‡ªå·±ä¸»å¯¼çš„å†³ç­–ï¼Œå®ƒä»ç„¶èƒ½ä»Žè§‚å¯Ÿåˆ°çš„ç»éªŒä¸­å­¦ä¹ ã€‚

è¿™å°±åƒå¤šä¸ªå¸æœºè§‚å¯ŸåŒä¸€ä¸ªäº¤é€šåœºæ™¯ï¼š
åªè¦ä»–ä»¬éƒ½è®¤ä¸ºâ€œåœ¨è¿™ä¸ªè·¯å£å¯ä»¥å·¦è½¬â€ï¼Œé‚£å½“å…¶ä¸­ä¸€ä¸ªå¸æœºçœŸçš„å·¦è½¬å¹¶è§‚å¯Ÿåˆ°ç»“æžœï¼ˆå µè½¦æˆ–é¡ºç•…ï¼‰ï¼Œ
å…¶ä»–å¸æœºä¹Ÿèƒ½æ›´æ–°è‡ªå·±å¯¹â€œå·¦è½¬åŽè·¯å†µæ¨¡åž‹â€çš„ä¼°è®¡ã€‚

---

âœ… **æ€»ç»“ä¸€å¥è¯ï¼š**

> åœ¨ intra-option model learning ä¸­ï¼Œ
> option çš„æ¨¡åž‹å­¦ä¹ ä¾èµ–çš„æ˜¯çŽ¯å¢ƒåŠ¨åŠ›å­¦å’Œå…¶å†…éƒ¨ç­–ç•¥çš„åŠ¨ä½œåˆ†å¸ƒï¼›
> å› æ­¤ï¼Œåªè¦å½“å‰æ‰§è¡Œçš„åŠ¨ä½œæ˜¯è¯¥ option å†…ç­–ç•¥å¯èƒ½é‡‡å–çš„ï¼Œ
> å°±èƒ½è§†ä¸ºè¯¥ option çš„ä¸€æ¬¡æœ‰æ•ˆæ ·æœ¬ï¼Œä»Žè€Œå¯¹å®ƒçš„æ¨¡åž‹è¿›è¡Œ off-policy æ›´æ–°ã€‚
