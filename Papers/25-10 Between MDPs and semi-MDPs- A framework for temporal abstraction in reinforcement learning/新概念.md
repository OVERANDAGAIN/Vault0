[[25-10 Between MDPs and semi-MDPs-A framework for temporal abstraction in reinforcement learning]]


# 一种近似的value function
![[Pasted image 20251027223956.png]]


```ad-tip

### 🧩 原文关键句

> The plans (policies) found using **temporally abstract options** are **approximate** in the sense that they achieve only $V^*_{\mathcal{O}}$, which may be less than the maximum possible, $V^*$.

---

### ✅ 含义分解

1. **“temporally abstract options”**
   指的是选项（options）这一类“时间抽象动作”，也就是比单步 action 更长的复合策略。它们可以跨多个时间步执行，如“从厨房走到客厅”这种高层动作。

2. **“approximate in the sense that they achieve only $V^*_{\mathcal{O}}$”**
   意思是：
   当我们使用“选项”而不是原始动作集时，我们是在一个**受限的策略空间**里寻找最优解。
   因此，我们找到的最优值函数 $V^*_{\mathcal{O}}$ 是**在选项集合 $\mathcal{O}$ 下**的最优值，而不是全局最优 $V^*$。

   换句话说：
   $$
   V^**{\mathcal{O}}(s)=\max*{\pi\in\Pi_{\mathcal{O}}} V^{\pi}(s)
   $$
   但因为 $\Pi_{\mathcal{O}}$（基于选项的策略空间）比完整策略空间 $\Pi$ 更小，所以一般有
   $$
   V^*_{\mathcal{O}}(s)\leq V^*(s)
   $$
   这就是为什么作者说“approximate”。

3. **“if the models used to find them are correct, they are guaranteed to achieve $V^*_{\mathcal{O}}$”**
   这一句的意思是：
   即使选项导致了次优解，只要你的**模型（world model 或 transition model）是正确的**，你至少能保证达到选项空间下的最优值 $V^*_{\mathcal{O}}$。

   也就是说，虽然你没法超过选项所定义的上限，但至少能达到它的上限，不会因为模型误差而更糟。

---

### 💡 “value achievement property” 的含义

他们将这种性质称为 **value achievement property**：

> 只要模型正确，使用选项规划就能“实现”在选项空间下的最优值 $V^*_{\mathcal{O}}$。

这和一些其他规划方法（例如直接在状态空间做抽象）形成对比——后者即便模型正确，也**不一定**能真正达到抽象规划时所期望的值，因为状态抽象可能会丢失信息、引入误差。

---

### 📘 小结与直觉比喻

| 概念                           | 含义            | 类比理解                                  |
| ---------------------------- | ------------- | ------------------------------------- |
| $V^*$                        | 原始动作空间下的真正最优值 | 你走每一步都可以自由选择，能得到最高分                   |
| $V^*_{\mathcal{O}}$          | 受限于选项集合下的最优值  | 只能使用“去客厅”“去卧室”这样的宏动作，可能不如精确控制每一步      |
| “approximate”                | 近似最优          | 因为动作空间受限                              |
| “value achievement property” | 保证能实现选项空间内的最优 | 模型正确时至少不会低于理论上该有的 $V^*_{\mathcal{O}}$ |

---

### 🧠 总结一句话

> 使用选项（options）进行规划是一种“安全的近似”：虽然它限制了策略空间，可能达不到全局最优 $V^*$，但只要模型正确，就一定能达到在选项空间下的最优 $V^*_{\mathcal{O}}$，这一点比很多其他抽象方法更有保证。

```










# FootNotes
