---
created: 2024-12-18T20
updated: ...
---
[[A Review of Cooperation in Multi-agent Learning]]

# Questions

- [?] 


# Answers
### 3.1 团队与混合动机场景下的共同挑战

#### 非平稳性与智能体数量扩展性

多智能体系统高度非平稳。由于任一智能体策略的改进，会改变其他智能体所观察到的环境分布【Foerster et al., 2017；Lowe et al., 2017a；Leibo et al., 2019a】。从单个智能体的角度看，环境本身会不断变化，因为环境包括了其他智能体的行为与反馈。

同时，随着智能体数量增加，联合动作空间呈指数级增长。若直接建模联合动作空间，训练代价将急剧上升。一个常见但粗糙的解决方式是为每个智能体设计独立去中心化的学习器【Claus & Boutilier, 1998】，但由于忽略了非平稳性，性能往往不佳【Foerster et al., 2017】。

此外，**探索问题**也变得更加复杂：单智能体可以通过在动作中添加随机噪声来进行探索，而多智能体情境下则需要在联合动作空间中协调探索策略，这大大增加了探索成本【Bard et al., 2020】。

另一方面，Leibo等人【2019b】认为，**非平稳性有时反而能促进探索**：在一定条件下，增大智能体数量可以激发一种称为“利用中探索（exploration by exploitation）”的机制，从而跳出局部最优解与训练平台期【Sunehag et al., 2023】。



#### 集体利益的定义与实现（Collective Good）

假设用户希望训练一组智能体以最大化“集体福利”，这一目标的定义本身就不唯一。例如：

* **功利主义视角**：总奖励之和；
* **罗尔斯主义视角**【Rawls, 1971】：最弱者的回报最大；
* **公平性视角**：如使用Gini系数衡量奖励分布的不均【Ceriani & Verme, 2012】。

即便目标已定义清晰，由于个体激励不一致，单纯优化自身奖励往往不能实现良好全局结果【Leibo et al., 2017；McKee et al., 2020】。通常需要**额外机制设计**来促进合作行为。


# Codes

```python

```


# FootNotes
