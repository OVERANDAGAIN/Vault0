---
created: 2024-12-18T20
updated: ...
---
[[A Review of Cooperation in Multi-agent Learning]]

# Questions

- [?] 


# Answers
### 3.1 团队与混合动机场景下的共同挑战

#### 非平稳性与智能体数量扩展性

多智能体系统高度非平稳。由于任一智能体策略的改进，会改变其他智能体所观察到的环境分布【Foerster et al., 2017；Lowe et al., 2017a；Leibo et al., 2019a】。从单个智能体的角度看，环境本身会不断变化，因为环境包括了其他智能体的行为与反馈。

同时，随着智能体数量增加，联合动作空间呈指数级增长。若直接建模联合动作空间，训练代价将急剧上升。一个常见但粗糙的解决方式是为每个智能体设计独立去中心化的学习器【Claus & Boutilier, 1998】，但由于忽略了非平稳性，性能往往不佳【Foerster et al., 2017】。

此外，**探索问题**也变得更加复杂：单智能体可以通过在动作中添加随机噪声来进行探索，而多智能体情境下则需要在联合动作空间中协调探索策略，这大大增加了探索成本【Bard et al., 2020】。

另一方面，Leibo等人【2019b】认为，**非平稳性有时反而能促进探索**：在一定条件下，增大智能体数量可以激发一种称为“利用中探索（exploration by exploitation）”的机制，从而跳出局部最优解与训练平台期【Sunehag et al., 2023】。



#### 集体利益的定义与实现（Collective Good）

假设用户希望训练一组智能体以最大化“集体福利”，这一目标的定义本身就不唯一。例如：

* **功利主义视角**：总奖励之和；
* **罗尔斯主义视角**【Rawls, 1971】：最弱者的回报最大；
* **公平性视角**：如使用Gini系数衡量奖励分布的不均【Ceriani & Verme, 2012】。

即便目标已定义清晰，由于个体激励不一致，单纯优化自身奖励往往不能实现良好全局结果【Leibo et al., 2017；McKee et al., 2020】。通常需要**额外机制设计**来促进合作行为。



以下是第4章中第4.1节与第4.2节的完整中文翻译：

---

## 4 在团队博弈中的合作

在**完全合作**的情形下，所有智能体共享统一的奖励函数，形式为：

$$
R_1 = R_2 = \dots = R_N = R
$$

由于这与单智能体任务高度相关，该设定是多智能体学习中研究最多的方向之一。本章将首先对相关算法进行分类说明（基于学习范式与策略类型），并在后续小节中介绍团队博弈中的代表性算法。**表1**总结了近年来处理共同收益博弈的代表性算法。

---

### 4.1 学习范式

相比单智能体学习，\*\*多智能体强化学习（Multi-agent Reinforcement Learning, MARL）\*\*引入了更为复杂的信息结构，这决定了在训练和执行阶段，智能体能获得哪些信息。例如，在马尔可夫博弈中，每个智能体可以只观测当前状态 $s$，其局部策略 $\pi$ 即从状态映射到动作。

不同的信息结构导致了不同的学习方式，从而形成了多种算法变体。下面介绍几种典型的学习范式：

#### 🟦 独立学习（Independent Learning）

这是最简单的方式，智能体只观测**本地的动作与奖励**，彼此独立学习。尽管实现简单，但存在严重的**非平稳性问题**，难以收敛【Tan, 1993】。
最早提出的独立学习方法是“**独立Q学习（IQL）**”，它将传统的Q-learning直接应用于多智能体环境。其核心思想是将其他智能体视为“环境的一部分”，从而忽略它们策略变化所带来的干扰。

#### 🟦 集中式控制（Centralised Training）

为应对非平稳性与部分可观测性问题，一种流行方法是假设存在一个**中央控制器**，它可在训练时访问所有智能体的动作、奖励、观测等信息，并统一设计各智能体策略。
虽然该假设在现实中难以满足，但在模拟环境（如游戏、机器人）中常常成立【Peng et al., 2017；Han et al., 2019】。

#### 🟦 中心化训练-去中心化执行（CTDE）

最流行的方案是：**中心化训练 + 去中心化执行**（CTDE），该思想源于\*\*部分可观测博弈（Dec-POMDP）\*\*研究【Oliehoek et al., 2016】。其机制如下：

* **Critic（评论者）**：在训练阶段接收所有智能体的观测与动作，用于估计值函数；
* **Actor（行为者）**：在执行阶段每个智能体只使用**自己的观测**选择动作。

若奖励为全局共享，一个critic即可；若奖励为个体私有，则每个智能体需单独训练critic模型。

该方法在训练时可充分利用全局信息，有效缓解非平稳性；而执行阶段则保持智能体的去中心化，便于实际部署。

---

### 4.2 求解方法（Solution Approaches）

多智能体合作学习的算法主要分为以下两类：

---

#### 🟩 策略梯度类方法（Policy-Based Methods）

这类方法使用\*\*策略迭代（Policy Iteration）\*\*范式，与单智能体情形类似。策略梯度定理在单智能体中形式如下：

$$
g = \mathbb{E}_{\pi}[Q(s, a) \nabla_{\theta} \log \pi_\theta(a|\tau)]
$$

* 若策略为联合策略 $\pi_\theta(a|\tau)$，需要**集中式控制器**管理所有策略；
* 若策略为多个智能体各自独立策略 $\pi_i(a_i|\tau_i)$，则为**去中心化行为者**：

$$
g_i = \mathbb{E}_{\pi} [Q(s,a) \nabla_{\theta} \log \pi_i(a_i|\tau_i)]
$$

为了减小方差，可引入**基线函数** $b(s,a_{-i})$，用于估算当前动作相对于其他动作的优势值：

$$
g_i = \mathbb{E}_{\pi} [(Q(s,a) - b(s,a_{-i})) \nabla_{\theta} \log \pi_i(a_i|\tau_i)]
$$

##### ✅ 代表性算法：

* **MAPPO**【Yu et al., 2022】：PPO的多智能体扩展，基于CTDE，参数共享；
* **HAPPO / HATRPO**【Kuba et al., 2021】：不需要参数共享，支持异质性；
* **COMA**【Foerster et al., 2018a】：基于反事实基线的多智能体策略梯度；
* **LIIR**【Du et al., 2019】：引入个体内在奖励，提升多样性和归因能力。

##### ✅ 连续动作情形：

若动作空间为连续，则使用**确定性策略梯度（DDPG）**，多智能体扩展如下【Lowe et al., 2017b】：

$$
\nabla_{\theta_i} J(\mu_i) = \mathbb{E}_{x,a} [\nabla_{\theta_i} \mu_i(a_i|s_i) \nabla_{a_i} Q_i^\mu(x, a_1, ..., a_N)]
$$

每个智能体拥有自己独立的critic，适用于协作、竞争或混合环境。若不能直接观测其他智能体动作，MADDPG中可引入**推理机制**进行估计。

---

#### 🟦 值函数类方法（Value-Based Methods）

该类方法关注于critic的训练，通常基于CTDE架构。

* **集中式Critic**：接收所有状态和动作作为输入，但**可扩展性差**；
* **去中心化Critic**：每个智能体仅输入本地信息，但无法建模非平稳性；
* **值函数因式分解（Value Factorisation）**：在两个极端之间折中，先训练局部critic，再通过\*\*混合网络（Mixing Network）\*\*组合出集中式的Q函数。

核心约束条件为：

$$
\arg\max_a Q_{\text{tot}}(\tau, a) = (\arg\max_{a_1} Q_1(\tau_1,a_1), ..., \arg\max_{a_n} Q_n(\tau_n,a_n))
$$

即全局Q的最优解应由局部Q的最优解组合而成（IGM约束）。

##### ✅ 代表性算法：

* **VDN**【Sunehag et al., 2018】：将各个智能体的局部Q值简单求和；
* **QMIX**【Rashid et al., 2018】：使用具有**单调性约束**的混合网络；
* **QTRAN**【Son et al., 2019】：弱化约束条件，提升表达能力；
* **Qatten**【Yang et al., 2020a】：引入多头注意力机制建模权重；
* **QPLEX**【Wang et al., 2020b】：引入优势函数与状态值函数分离；
* **FMA-FQI**【Wang et al., 2021a】：形式化拟合型多智能体Q学习过程。

CTDE方法虽有效，但也存在以下局限：

1. Q函数仅为**全局回报的估计值**，在大规模任务上扩展性不足；
2. 无法捕捉各智能体的个体差异，难以提供细致的奖励归因。

---

如需继续翻译**4.3 通信机制（Communication）**，请继续说明。

# Codes

```python

```


# FootNotes
