---
created: 2024-12-18T20
updated: ...
---
[[A Review of Cooperation in Multi-agent Learning]]

# Questions

- [?] 


# Answers
### 3.1 团队与混合动机场景下的共同挑战

#### 非平稳性与智能体数量扩展性

多智能体系统高度非平稳。由于任一智能体策略的改进，会改变其他智能体所观察到的环境分布【Foerster et al., 2017；Lowe et al., 2017a；Leibo et al., 2019a】。从单个智能体的角度看，环境本身会不断变化，因为环境包括了其他智能体的行为与反馈。

同时，随着智能体数量增加，联合动作空间呈指数级增长。若直接建模联合动作空间，训练代价将急剧上升。一个常见但粗糙的解决方式是为每个智能体设计独立去中心化的学习器【Claus & Boutilier, 1998】，但由于忽略了非平稳性，性能往往不佳【Foerster et al., 2017】。

此外，**探索问题**也变得更加复杂：单智能体可以通过在动作中添加随机噪声来进行探索，而多智能体情境下则需要在联合动作空间中协调探索策略，这大大增加了探索成本【Bard et al., 2020】。

另一方面，Leibo等人【2019b】认为，**非平稳性有时反而能促进探索**：在一定条件下，增大智能体数量可以激发一种称为“利用中探索（exploration by exploitation）”的机制，从而跳出局部最优解与训练平台期【Sunehag et al., 2023】。



#### 集体利益的定义与实现（Collective Good）

假设用户希望训练一组智能体以最大化“集体福利”，这一目标的定义本身就不唯一。例如：

* **功利主义视角**：总奖励之和；
* **罗尔斯主义视角**【Rawls, 1971】：最弱者的回报最大；
* **公平性视角**：如使用Gini系数衡量奖励分布的不均【Ceriani & Verme, 2012】。

即便目标已定义清晰，由于个体激励不一致，单纯优化自身奖励往往不能实现良好全局结果【Leibo et al., 2017；McKee et al., 2020】。通常需要**额外机制设计**来促进合作行为。



以下是第4章中第4.1节与第4.2节的完整中文翻译：

---

## 4 在团队博弈中的合作

在**完全合作**的情形下，所有智能体共享统一的奖励函数，形式为：

$$
R_1 = R_2 = \dots = R_N = R
$$

由于这与单智能体任务高度相关，该设定是多智能体学习中研究最多的方向之一。本章将首先对相关算法进行分类说明（基于学习范式与策略类型），并在后续小节中介绍团队博弈中的代表性算法。**表1**总结了近年来处理共同收益博弈的代表性算法。

---

### 4.1 学习范式

相比单智能体学习，\*\*多智能体强化学习（Multi-agent Reinforcement Learning, MARL）\*\*引入了更为复杂的信息结构，这决定了在训练和执行阶段，智能体能获得哪些信息。例如，在马尔可夫博弈中，每个智能体可以只观测当前状态 $s$，其局部策略 $\pi$ 即从状态映射到动作。

不同的信息结构导致了不同的学习方式，从而形成了多种算法变体。下面介绍几种典型的学习范式：

#### 🟦 独立学习（Independent Learning）

这是最简单的方式，智能体只观测**本地的动作与奖励**，彼此独立学习。尽管实现简单，但存在严重的**非平稳性问题**，难以收敛【Tan, 1993】。
最早提出的独立学习方法是“**独立Q学习（IQL）**”，它将传统的Q-learning直接应用于多智能体环境。其核心思想是将其他智能体视为“环境的一部分”，从而忽略它们策略变化所带来的干扰。

#### 🟦 集中式控制（Centralised Training）

为应对非平稳性与部分可观测性问题，一种流行方法是假设存在一个**中央控制器**，它可在训练时访问所有智能体的动作、奖励、观测等信息，并统一设计各智能体策略。
虽然该假设在现实中难以满足，但在模拟环境（如游戏、机器人）中常常成立【Peng et al., 2017；Han et al., 2019】。

#### 🟦 中心化训练-去中心化执行（CTDE）

最流行的方案是：**中心化训练 + 去中心化执行**（CTDE），该思想源于\*\*部分可观测博弈（Dec-POMDP）\*\*研究【Oliehoek et al., 2016】。其机制如下：

* **Critic（评论者）**：在训练阶段接收所有智能体的观测与动作，用于估计值函数；
* **Actor（行为者）**：在执行阶段每个智能体只使用**自己的观测**选择动作。

若奖励为全局共享，一个critic即可；若奖励为个体私有，则每个智能体需单独训练critic模型。

该方法在训练时可充分利用全局信息，有效缓解非平稳性；而执行阶段则保持智能体的去中心化，便于实际部署。

---

### 4.2 求解方法（Solution Approaches）

多智能体合作学习的算法主要分为以下两类：

---

#### 🟩 策略梯度类方法（Policy-Based Methods）

这类方法使用\*\*策略迭代（Policy Iteration）\*\*范式，与单智能体情形类似。策略梯度定理在单智能体中形式如下：

$$
g = \mathbb{E}_{\pi}[Q(s, a) \nabla_{\theta} \log \pi_\theta(a|\tau)]
$$

* 若策略为联合策略 $\pi_\theta(a|\tau)$，需要**集中式控制器**管理所有策略；
* 若策略为多个智能体各自独立策略 $\pi_i(a_i|\tau_i)$，则为**去中心化行为者**：

$$
g_i = \mathbb{E}_{\pi} [Q(s,a) \nabla_{\theta} \log \pi_i(a_i|\tau_i)]
$$

为了减小方差，可引入**基线函数** $b(s,a_{-i})$，用于估算当前动作相对于其他动作的优势值：

$$
g_i = \mathbb{E}_{\pi} [(Q(s,a) - b(s,a_{-i})) \nabla_{\theta} \log \pi_i(a_i|\tau_i)]
$$

##### ✅ 代表性算法：

* **MAPPO**【Yu et al., 2022】：PPO的多智能体扩展，基于CTDE，参数共享；
* **HAPPO / HATRPO**【Kuba et al., 2021】：不需要参数共享，支持异质性；
* **COMA**【Foerster et al., 2018a】：基于反事实基线的多智能体策略梯度；
* **LIIR**【Du et al., 2019】：引入个体内在奖励，提升多样性和归因能力。

##### ✅ 连续动作情形：

若动作空间为连续，则使用**确定性策略梯度（DDPG）**，多智能体扩展如下【Lowe et al., 2017b】：

$$
\nabla_{\theta_i} J(\mu_i) = \mathbb{E}_{x,a} [\nabla_{\theta_i} \mu_i(a_i|s_i) \nabla_{a_i} Q_i^\mu(x, a_1, ..., a_N)]
$$

每个智能体拥有自己独立的critic，适用于协作、竞争或混合环境。若不能直接观测其他智能体动作，MADDPG中可引入**推理机制**进行估计。

---

#### 🟦 值函数类方法（Value-Based Methods）

该类方法关注于critic的训练，通常基于CTDE架构。

* **集中式Critic**：接收所有状态和动作作为输入，但**可扩展性差**；
* **去中心化Critic**：每个智能体仅输入本地信息，但无法建模非平稳性；
* **值函数因式分解（Value Factorisation）**：在两个极端之间折中，先训练局部critic，再通过\*\*混合网络（Mixing Network）\*\*组合出集中式的Q函数。

核心约束条件为：

$$
\arg\max_a Q_{\text{tot}}(\tau, a) = (\arg\max_{a_1} Q_1(\tau_1,a_1), ..., \arg\max_{a_n} Q_n(\tau_n,a_n))
$$

即全局Q的最优解应由局部Q的最优解组合而成（IGM约束）。

##### ✅ 代表性算法：

* **VDN**【Sunehag et al., 2018】：将各个智能体的局部Q值简单求和；
* **QMIX**【Rashid et al., 2018】：使用具有**单调性约束**的混合网络；
* **QTRAN**【Son et al., 2019】：弱化约束条件，提升表达能力；
* **Qatten**【Yang et al., 2020a】：引入多头注意力机制建模权重；
* **QPLEX**【Wang et al., 2020b】：引入优势函数与状态值函数分离；
* **FMA-FQI**【Wang et al., 2021a】：形式化拟合型多智能体Q学习过程。

CTDE方法虽有效，但也存在以下局限：

1. Q函数仅为**全局回报的估计值**，在大规模任务上扩展性不足；
2. 无法捕捉各智能体的个体差异，难以提供细致的奖励归因。

---

如需继续翻译**4.3 通信机制（Communication）**，请继续说明。



以下是第5章中第5.1节和第5.2节的完整中文翻译：

---

## 5 混合动机下的合作（Cooperation with Mixed Motivation）

本章重点讨论**混合动机情景中的合作问题**，如图1所示。与纯粹的共同利益设置（如传感器网络、协同搬运机器人等）不同，混合动机场景中，智能体之间既有合作需求，又存在个体目标冲突，因此其研究重点也包含了**个体表现优化**。

---

### 5.1 社会困境（Social Dilemmas）

混合动机合作研究的核心概念是**社会困境（social dilemma）**，即**个体理性**与**集体最优**之间存在冲突的情境【Rapoport, 1974】。在此类困境中，智能体在“为集体利益合作”与“为个体最大化收益而背叛”之间产生矛盾。

#### 📊 经典博弈模型

以两个玩家、两个动作（合作C、背叛D）的矩阵博弈为例，文献中常用以下三个典型博弈来描述社会困境【Macy and Flache, 2002；Leibo et al., 2017】：

* 囚徒困境（Prisoner’s Dilemma）
* 鸡游戏（Chicken）
* 鹿猎游戏（Stag Hunt）

其支付矩阵如下：

|       | C    | D    |
| ----- | ---- | ---- |
| **C** | R, R | S, T |
| **D** | T, S | P, P |

其中的四个关键值为：

* **R**：互相合作的奖励（Reward）
* **P**：互相背叛的惩罚（Punishment）
* **T**：我背叛你合作时的诱惑收益（Temptation）
* **S**：我合作你背叛时的受害者收益（Sucker）

**社会困境成立的充要条件**为表3中的不等式（部分示意）：

1. $R > P$：合作优于互相背叛
2. $R > S$：合作优于被背叛
3. $2R > T + S$：互惠优于一半概率被背叛
4. $T > R$（贪婪） 或 $P > S$（恐惧）

#### 🧠 现实困境的复杂性

尽管矩阵博弈广泛应用于社会科学、经济学与生物学，但它们**无法准确建模真实世界的社会困境**【Leibo et al., 2017】：

* 真实世界中困境具有**状态性**与**时间延展性**，而非一次博弈；
* 合作行为不是二元决策，而是**连续谱系**；
* 玩家间可观察到行为演化轨迹，并进行**动态响应**；
* 存在**部分可观测性**；
* 更重要的是：现实合作涉及**整套策略**的学习，而非仅选一个动作。

因此，需要将建模从静态博弈转向**马尔可夫博弈（Markov games）**，从动作空间扩展到**策略空间**。

---

#### 🟦 顺序社会困境（Sequential Social Dilemmas, SSD）

为解决上述问题，Leibo 等人提出了\*\*顺序社会困境（SSD）\*\*建模方法。

设定如下：

* 使用马尔可夫博弈 $M$；
* 定义合作策略集 $\Pi_C$，背叛策略集 $\Pi_D$；
* 对任意状态 $s$，定义对应策略对下的长期收益：

$$
\begin{align*}
R(s) &:= V_1^{\pi_C, \pi_C}(s) = V_2^{\pi_C, \pi_C}(s) \\
P(s) &:= V_1^{\pi_D, \pi_D}(s) = V_2^{\pi_D, \pi_D}(s) \\
T(s) &:= V_1^{\pi_D, \pi_C}(s) = V_2^{\pi_C, \pi_D}(s) \\
S(s) &:= V_1^{\pi_C, \pi_D}(s) = V_2^{\pi_D, \pi_C}(s)
\end{align*}
$$

若上述四个值满足社会困境不等式，则称 $(M, \Pi_C, \Pi_D)$ 构成一个 SSD。

实际中，这些值通过**经验博弈分析**（Empirical Game-Theoretic Analysis, EGTA）在模拟中估计得出【Walsh et al., 2002；Tuyls et al., 2020】。

---

#### 🟩 多智能体SSD的推广

在N个玩家的情形下，顺序社会困境定义如下：

* 合作策略集 $\Pi_C$，背叛策略集 $\Pi_D$，总人数 $N = \ell + m$
* 分别计算 $\ell$ 个合作者和 $m$ 个背叛者下的平均回报 $R_c(\ell)$ 和 $R_d(m)$

若满足以下条件，则构成N人SSD：

1. 全体合作优于全体背叛：$R_c(N) > R_d(0)$
2. 全体合作优于被利用：$R_c(N) > R_c(0)$
3. 存在**恐惧**或**贪婪**：

   * 恐惧：$R_d(i) > R_c(i)$（少数合作时不如背叛）
   * 贪婪：$R_d(i) > R_c(i)$（多数合作时背叛仍收益更高）

此外，还可定义**跨期SSD**：即短期最优策略是背叛，长期才是合作。

---

#### 📈 Schelling图（Schelling Diagrams）

Schelling图用于直观展示**他人选择如何影响个体激励**【Schelling, 1973】。其假设：

* 每个个体面临“合作/背叛”二选一；
* 其收益由自己选择及**有多少人合作**决定。

横轴为他人合作人数 $\ell$，纵轴为在第$\ell+1$个合作者时的个体收益：

* $R_c(\ell+1)$：自己选择合作时的回报；
* $R_d(\ell)$：自己选择背叛时的回报。

**最小可行合作联盟**：使得个体合作收益大于背叛收益所需的最小合作者数量。

---

#### 🌐 非对称困境（Asymmetrical Dilemmas）

现实中智能体通常具有**个体差异**（奖励结构、行为能力、感知能力等），导致非对称博弈出现：

* 在生态学中表现为不同物种；
* 在经济学中表现为不同社会角色【Sunehag et al., 2019；Zheng et al., 2021b】。

此类异质性促使**互补性合作结构**的出现，并可实现更高的社会总福利。
Willis等【2023】尝试将社会困境的定义扩展至**非对称多智能体系统**。

---

### 5.2 顺序社会困境中的合作策略（Cooperation in Sequential Social Dilemmas）

为了促进智能体在SSD中的合作行为，已有多种方法被提出，目标是提升**群体总回报（社会福利）**，即：

$$
SW(R) = \frac{1}{n} \sum_{i=1}^n R_i
$$

---

#### ✅ 常见方法：奖励塑形（Reward Shaping）

一种常见手段是引入**内在奖励（intrinsic reward）**，与外部环境奖励（extrinsic reward）共同构成即时奖励：

$$
r_i = r_{\text{ex},i} + r_{\text{in},i}
$$

此处，若内在奖励来源于其他智能体的外部奖励，即可视为“**社会动机**”。

---

#### 🤝 利他主义（Altruism）

利他主义是最常见的内在奖励形式之一。在本文中，指智能体关心他人的奖励（不一定以牺牲自我为代价）。

例如，Chen & Kempe【2008】提出以下模型：

$$
r_i(\beta) = (1 - \beta) r_{\text{ex},i} + \frac{\beta}{n} \sum_{j} r_{\text{ex},j}
$$

其中，$\beta \in [0,1]$ 表示个体收益与集体福利的权重分配。

这种方式可直接抵消“损人利己”的激励，使个体在损害他人时也需承担回报损失。

---

如需继续翻译第5.3节 “解决策略（Solution Approaches）”，请继续说明。



# Codes

```python

```


# FootNotes
