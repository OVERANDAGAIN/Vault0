---
创建时间: 2025-七月-29日  星期二, 2:49:34 下午
---

###  Slide：预备知识 – 目标导向强化学习（Goal-Conditioned RL）


### 定义（任务建模）

在目标导向强化学习（Goal-Conditioned RL, GCRL）中，每个任务由一个目标状态 $s_g$ 表示，其问题定义如下：

* **状态空间**：$s_t \in \mathcal{S}$
* **动作空间**：$a_t \in \mathcal{A}$
* **初始状态分布**：$p_0(s)$
* **环境动力学（转移概率）**：$p(s_{t+1} \mid s_t, a_t)$
* **目标状态分布**：$p_g(s_g)$
* **每个目标对应的奖励函数**：$r_g(s, a)$

---

是否需要我将这部分格式化为适用于 PowerPoint 的文字或图示版本？我也可以继续为下一页方法部分设计。


---

###  Slide：GCRL 中的目标奖励定义与优化目标

####  奖励函数定义（避免手动设计距离函数）：

$$
r_g(s_t, a_t) = (1 - \gamma) \cdot p(s_{t+1} = s_g \mid s_t, a_t)
$$

> 即一步之内达到目标状态的概率密度，替代手动设定距离度量（如 L2）

---

####  强化学习目标函数

* 最大化采样目标后轨迹回报：

$$
\max_\pi \mathbb{E}_{s_g \sim p_g, \tau \sim \pi(\cdot|s_g)} \left[ \sum_{t=0}^\infty \gamma^t r_g(s_t, a_t) \right]
$$

* Q函数定义为带条件目标的期望累计回报：

$$
Q^\pi_{s_g}(s,a) = \mathbb{E} \left[ \sum_{t'=t}^\infty \gamma^{t'-t} r_g(s_{t'}, a_{t'}) \mid s_t = s, a_t = a \right]
$$

---

---


###  Slide：折扣状态占用分布 & 多策略经验融合

#### 折扣状态占用分布（Discounted State Occupancy Measure）：

$$
p^\pi(st^+ = s \mid s, a, s_g) = (1 - \gamma) \sum_{t=0}^\infty \gamma^t p^\pi_t(s_t = s)
$$

* 从几何分布  $t \sim \text{Geom}(1 - \gamma)$ 中采样时间步，取该时刻访问的状态

#### 平均策略下的目标加权状态分布（用于经验融合）：

$$
p^\pi(st^+ = s \mid s, a) = \int p^\pi(st^+ = s \mid s, a, s_g) \cdot p^\pi(s_g \mid s, a) \, ds_g
$$




---
---

---

###  预备知识（2/2）：对比表示学习

#### 基本概念

* **对比学习（Contrastive Learning）**：从**正样本对**与**负样本对**中学习表示，使得：

  * 正样本对表示相似（靠近）
  * 负样本对表示不相似（远离）

---

#### 示例形式

* 设输入为一对样本 $(u, v)$，例如：

  * $u$：一张原始图像；
  * $v$：该图像的某种增强版本。

* 样本对的采样方式：

  * **正样本对**：从联合分布 $p(u, v)$ 中采样；
  * **负样本对**：从边缘分布 $p(u)p(v)$ 中独立采样（无语义关系）。

---

#### 目标函数定义：基于二分类的 NCE 损失

我们使用 **NCE-binary 对比损失**，也被称为 **InfoNCE/InfoMax 目标**：

$$
\max_{f(u, v)} \; \mathbb{E}_{(u, v^+) \sim p(u, v)} \; \mathbb{E}_{v^- \sim p(v)} \left[
\log \sigma(f(u, v^+)) + \log (1 - \sigma(f(u, v^-)))
\right]
$$

其中：

* $f(u, v) = \phi(u)^\top \psi(v)$：称为**critic函数**，即两种编码器之间的相似度；

  * $\phi(u)$：表示 u 的编码器；
  * $\psi(v)$：表示 v 的编码器；
* $\sigma(\cdot)$：Sigmoid 函数，用于二分类；
* $v^+$：与 u 同源的正样本；
* $v^-$：与 u 无关的负样本。

---



下面是将论文中第 4.1 节 **“Relating the Q-function to probabilities”** 内容组织为演讲用的 PPT 页面，适用于你的30分钟报告“方法部分”第一节。

---

## 方法 (1/4)：对比学习 ≈ Q函数估计

---

###  核心目标

* 使用**对比学习**机制，\*\*直接完成目标导向强化学习（goal-conditioned RL）\*\*任务。
*  **关键想法（引理 4.1）**：

  > 对比学习可以估计某个策略下的 Q 函数。

---

###  核心数学联系（一）：奖励与概率的等价

我们从一个**概率视角**出发，重新表达强化学习目标函数：

####  命题 1（Proposition 1）

> 对于目标导向的奖励函数 $r_g$，其对应的 Q 函数可以表示为：
> **从当前状态-动作 $(s, a)$ 出发，最终达到目标状态 $s_g$ 的概率密度**

$$
Q^{\pi}_{s_g}(s, a) = p^{\pi}(\cdot|\cdot,s_g)(s_t^+ = s_g \mid s, a)
$$

* 右边是**折扣后的状态占用概率** $p^{\pi}(s_t^+ = s_g \mid s, a)$
* 也就是说：

  > Q值其实就是“执行动作 $a$ 后最终能到达 $s_g$”的概率

---



非常好，以下是你论文演讲中\*\*“方法部分（2/4）”\*\*对应于 Section 4.2 的 PPT 页面内容，重点说明对比学习如何估计 Q 函数：

---

## 方法 (2/4)：对比学习估计 Q 函数

---

###  关键构造：如何将 Contrastive Learning 用于 RL？

我们希望用对比学习**直接学习 Q 函数**。为此，我们需要合理设计输入对：

| 输入               | 内容        | 来源                                       |
| ---------------- | --------- | ---------------------------------------- |
| $u = (s_t, a_t)$ | 当前状态 + 动作 | 从 replay buffer 采样                       |
| $v = s_f$        | 未来状态      | “正例”：从策略下的折扣状态分布中采样 <br> “负例”：从总体状态分布中采样 |

---

###  对比目标函数（复用 NCE-binary）

我们希望正例之间相似，负例之间相异：

$$
\mathcal{L}(s, a, s_f^+, s_f^-) =
\log \sigma(\phi(s, a)^T \psi(s_f^+)) + \log(1 - \sigma(\phi(s, a)^T \psi(s_f^-)))
$$

* 其中 $f(s, a, s_f) = \phi(s, a)^T \psi(s_f)$ 是“critic函数”
* 类比 RL 中 critic 表示回报，这里 critic 表示“到达目标 $s_f$”的相关性

---

###  关键引理（Lemma 4.1）

> 优化上述对比损失函数的最优 critic 函数 $f^*$ 满足：

$$
\exp(f^*(s, a, s_f)) = \frac{1}{p(s_f)} \cdot Q^{\pi}_{s_f}(s, a)
$$

解释：

* critic 实际是一个 **未归一化的 Q 函数估计器**
* 忽略常数项 $\frac{1}{p(s_f)}$ 后，它**等价于 Q 值函数**

---

### 从对比学习走向策略学习

* 由于 Q 函数 ≈ critic，可用来指导 policy：

  > “最大化 Q 就等价于找到最可能达到目标 $s_f$ 的动作”
* 这就建立了对比学习与 actor-critic RL 的连接

---

### 关键优势：统一密度建模 & 策略评估

| 对比学习观点                | RL 观点           |
| --------------------- | --------------- |
| 估计 $p(s_f \mid s, a)$ | 估计 $Q(s, a)$    |
| critic = 相似度打分        | critic = 期望未来回报 |
| 无需 partition function | 不影响选动作，只关心相对值   |

---


以下是论文 Section 4.3 “Learning the Goal-Conditioned Policy” 对应的 PPT 页面内容，聚焦于策略学习部分，是方法部分的第 3 页。

---

## 方法 (3/4)：从 Critic 到策略学习

---

###  基本思路：用 critic 引导策略

我们已经得到了一个近似的 Q 函数（通过对比学习的 critic）：

> 如何选动作？选择使得目标状态 $s_g$ 最可能出现在未来的动作！

---

###  策略学习目标（Actor Loss）

目标函数形式如下：

$$
\max_{\pi(a|s, s_g)} \mathbb{E}_{\pi(a|s, s_g), p(s), p(s_g)}\left[ f(s, a, s_f = s_g) \right]
$$

近似等价于：

$$
\mathbb{E} \left[ \log Q^\pi_{s_g}(s, a) - \log p(s_g) \right]
$$

* 其中 $f(s, a, s_g) \approx \log Q^\pi_{s_g}(s, a) - \log p(s_g)$
* 目标就是最大化动作使得达成目标 $s_g$ 的可能性（或 Q 值）

---

### 中文解释

* 想象你知道某个目标状态在未来的“可能性”，你当然希望选择那些最能“提高”这个可能性的动作；
* 这就相当于最大化一个对目标的 **Q 值打分**。

---

###  实际训练做法

* 用一个神经网络表示策略 $\pi(a|s, s_g)$
* **输入**：当前状态 $s$、目标状态 $s_g$
* **输出**：动作分布
* 使用 reparametrization trick 从策略中采样动作并求梯度
* 图像任务中会**加入熵正则项**，鼓励策略探索

---

###  对比传统方法的优势

| 方法              | 是否使用 reward | 是否估计模型 | 是否估计 Q 函数           | 是否学习表示 |
| --------------- | ----------- | ------ | ------------------- | ------ |
| SAC / TD3 + HER |           | ❌      |                   | ❌      |
| C-learning      | ❌           | ❌      | 隐式                  | ❌      |
| 本文方法            | ❌           | ❌      | (via contrastive) |      |

---

### 中文小结

> 本节展示了如何将学习到的 critic（作为近似 Q 函数）用来优化目标导向策略。该策略被设计为使得达到目标状态的可能性最大，从而无需额外的奖励工程或模型学习。

---

---

## 方法 (5/5)：收敛性保证与策略改进理论

---

### 为什么 RL 方法难以收敛？

* 多数 relabeling 方法缺乏理论保障：

  * 要么**无收敛保证** \[6, 21, 22]
  * 要么**假设条件苛刻** \[40, 119]
* Contrastive RL 提出一个简单但有效的 **筛选机制（filtering）** 来保证收敛

---

###  筛选机制（Filtering for Convergence）

**直觉**：如果轨迹是在 “错误目标” 下生成的，就不要用于训练！

$$
\text{EXCLUDETRAJ}(\tau_{i:j}) = \delta\left( \left| \frac{\pi(\tau_{i:j} \mid s_g)}{\pi(\tau_{i:j} \mid s_j)} - 1 \right| > \epsilon \right)
$$

* 若轨迹在“目标 \$s\_g\$”和“实际达成 \$s\_j\$”下的概率差异过大，排除掉
* 实验发现：**虽然有理论意义，实际反而会影响效果**（见 Fig.13）

---

###  引理 4.2：近似策略改进（Approximate Policy Improvement）

假设：

* 状态-动作是\*\*有限（tabular）\*\*的
* Critic 是**贝叶斯最优**（Bayes-optimal）

结论：

* 执行一次 Contrastive RL 迭代后，策略性能有下界保证：

$$
\mathbb{E}_{\pi'}\left[ \sum_{t=0}^{\infty} \gamma^t r_{s_g}(s_t, a_t) \right]
\ge
\mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t r_{s_g}(s_t, a_t) \right]
- \frac{2\gamma}{1 - \gamma}
$$

* 表明 Contrastive RL 每次执行都能带来“近似策略改进”

---

###  总结：Contrastive RL 的理论魅力

* 本质上是一个**完整的 RL 算法**
* 具备以下**优雅属性**：

| ❌ 不需要     | 具备能力                    |
| --------- | ----------------------- |
| 多 Q 网络    | 学习 Q 函数近似               |
| 目标 Q 网络   | 实现策略改进保证                |
| 数据增强（DA）  | 自然具备泛化能力                |
| 辅助任务（aux） | 单一 contrastive 目标即可驱动学习 |

---



下面是根据你提供的内容和格式要求，**重新整理后的 PPT 内容（含数学公式书写规范）**，用于介绍论文的实验设置与环境：

---

##  实验部分（1/6）：实验设置与任务环境

---

###  实验实现细节（基于 ACME + JAX）

Contrastive RL 的实现基于 SAC 算法，在 [ACME](https://github.com/deepmind/acme) + JAX 框架中作如下关键修改：

1. **目标拼接观测：**

   * 环境返回 $\text{obs} = \text{concat}(o, s_{g})$，将目标状态 $s_{g}$ 拼接入原始观测。
   * 利于 Q 函数和策略网络统一处理，无需额外输入目标。

2. **Replay Buffer 改为轨迹存储：**

   * 存储完整轨迹，便于构建三元组 $(s_{t}, a_{t}, s_{f})$ 用于对比学习。

3. **Critic 表示方式重构：**

   * 使用内积形式 $f(s, a, s_{g}) = \phi(s, a)^{\top} \psi(s_{g})$，共享目标表示加速计算。

4. **损失函数替换：**

   * Critic 损失使用对比学习目标（参见 Alg. 1），
   * Actor 损失仅略作调整（移除状态任务中的 entropy 项）。

---

###  网络结构与训练设置

* **表示网络：**

  * $\phi(s, a), \psi(s_{g})$：均为两层全连接网络，宽度为 256，激活函数为 ReLU。
  * 不使用表示归一化、可学习温度或最终激活函数（实验证明影响性能）。

* **图像任务处理：**

  * 输入为尺寸 $(64, 64, 3)$ 图像，采用现有 CNN（参考 DrQ \[88]）编码后再输入表示网络。

* **策略网络结构：**

  * 图像任务中使用图像编码器 + 两层 MLP（大小为 256-ReLU）。

---

###  任务环境简介（共 6 个）

| 任务名称               | 类型      | 说明                          | 成功判定标准     |
| ------------------ | ------- | --------------------------- | ---------- |
| Fetch Reach        | 图像 / 状态 | 控制机械臂在空中移动至目标点              | 离目标小于 5cm  |
| Fetch Push         | 图像 / 状态 | 推动物体至目标位置                   | 离目标小于 5cm  |
| Sawyer Push        | 图像 / 状态 | 推动物体到桌面目标位置                 | 离目标小于 5cm  |
| Sawyer Bin         | 图像 / 状态 | 从一个容器抓取方块并放入另一个容器           | 离目标小于 5cm  |
| Ant U-Maze         | 状态      | 控制蚂蚁机器人在 “U” 型迷宫内导航至目标位置    | 离目标小于 0.5m |
| Point Spiral 11×11 | 图像      | 控制小球 XY 坐标通过螺旋迷宫导航到达目标（纯图像） | 离目标小于 2m   |

---

 接下来将展示在这些任务中的实验结果，重点分析 Contrastive RL 相较于其他方法（如 Goal-BC、TD3+HER）的性能优势。

是否继续进入 **5.1 低维状态任务实验结果** PPT 内容？



以下是针对第 5.1 节实验的 **PPT 内容组织（约 2–3 张幻灯片）**，用于清晰介绍 contrastive RL 在不同任务下对比表现的**方法选择、任务设置、结果对比与分析结论**：

---

##  实验部分（2/6）：与主流方法对比

---

###  实验目标与对比方法

**目标：**
验证 Contrastive RL 是否优于现有主流 goal-conditioned RL 方法。

**对比方法汇总：**

| 方法              | 类型         | 说明                                                 |                                        |
| --------------- | ---------- | -------------------------------------------------- | -------------------------------------- |
| **TD3 + HER**   | Model-free | 基于 hindsight relabeling 的主流方法，无需奖励函数，强化学习 + 回放重标   |                                        |
| **GCBC**        | Imitation  | Goal-Conditioned Behavioral Cloning，基于到达目标的轨迹做监督学习 |                                        |
| **Model-based** | Density 模型 | 拟合 ( p\_{\pi}(\cdot                                | s, a) )，再训练 policy 使目标概率最大。维度敏感，适合低维任务 |

 注：Contrastive RL 与 model-based 方法都建模 $p_{\pi}(\cdot | s, a)$，但 Contrastive RL 通过对比损失学习 **隐式 Q 函数**，且更易扩展至高维图像任务。

---

###  评测任务设置（任务复现一致性）

**Manipulation 任务：**

* **Fetch Reach / Push**（Plappert et al.）
* **Sawyer Push / Bin**（Yu et al.）

**Navigation 任务：**

* **Ant U-Maze**（Fu et al.）：高维状态空间（111维），目标仅为 XY 位置。
* **Point Spiral 11×11**（Eysenbach et al.）：图像观察，目标为迷宫终点。

 **评估策略统一**：

* 同样初始状态分布、目标分布与成功定义。
* 不依赖 reward 函数（reward-free setting）。
* 每个实验使用 5 个随机种子，报告平均值 ± 标准差。

---

##  实验部分（3/6）：结果分析（5.1节）

---

###  结果总结（一）：State-based 任务

* **Fetch Reach**：所有方法均表现良好。
* **Sawyer Bin**：**仅 Contrastive RL 成功完成任务**。
* **Push 类任务**：Contrastive RL 明显优于其他方法。
* **Ant U-Maze**：model-based 方法效果最好（目标维度低，建模容易）。

结论：**Contrastive RL 在挑战性更高的状态任务中表现优越**，并能处理稀疏反馈与复杂目标条件。

---

###  结果总结（二）：Image-based 任务

* **Fetch Reach、Point Spiral**：大部分方法都有一定进展。
* **Sawyer Push / Bin**：**仅 Contrastive RL 成功学习有效策略**，尽管成功率仍 <50%，但其他方法基本失败。
* **Augmented HER 基线（DrQ, AE, CURL）**：

  * 图像增强与重建 loss 提升了 HER 性能；
  * 但替换算法本体（使用 Contrastive RL）带来更大提升。

结论：**Contrastive RL 具备出色的图像表示学习能力**，对复杂视觉任务适应性更强。

---

下一节将介绍该方法在表示学习方面的优势，是否继续进入 **5.2 Representation Analysis**？




以下是 **5.2节：与表征学习方法对比** 的 PPT 内容整理（建议占用 1–2 张幻灯片），用于突出 contrastive RL 在无需额外表征损失或图像增强的情况下，仍优于多种 SOTA 表征学习方法的关键结果与结论。

---

##  实验部分（4/6）：与表征学习方法对比

---

###  对比动机与实验设置

**核心假设：**

> Contrastive RL（NCE）**可以在无需辅助损失和图像增强的情况下自动学习良好表征**。

**对比方法（全部结合 TD3 + HER 使用）：**

| 方法       | 类型   | 说明                                  |
| -------- | ---- | ----------------------------------- |
| **DrQ**  | 图像增强 | 使用数据增强、Q值平均提升性能（\[72, 135, 136]）    |
| **AE**   | 重建目标 | 加入自编码器辅助损失以强化表征（\[31, 91, 93, 137]） |
| **CURL** | 对比学习 | 图像增强生成正样本，结合对比损失（\[74, 89, 117]）    |

**Contrastive RL 特点：**

* 不使用数据增强；
* 不使用重建或 InfoNCE 目标；
* 直接通过任务信号构造 contrastive loss；
* **从结构上接近 representation learning 方法，而非表征“外挂”。**

---

###  对比结果与分析

**结果结论：**

* DrQ 与 AE 在部分任务（如 fetch reach、sawyer push）有一定提升；
* **Contrastive RL（NCE）在所有任务中表现更优**；
* 在 **无需引入额外损失函数或视觉先验的前提下**，能取得更强的表征能力。

 **关键观点：**

* 加法式表征技巧 ≠ 结构性提升；
* 更有效的方式是设计结构上具备表征能力的 RL 方法；
* **Contrastive RL 体现了表征学习机制与 RL 目标的一致性建模**（参见 4.2 节中的 Q-function 表达式）。

---

###  结论摘要

* 表征学习技巧（如 DrQ、AE、CURL）虽有提升，但受限于外部模块依赖；
* Contrastive RL 在结构上融合对比学习与 RL，**无需外部表征技巧也能实现泛化与收敛性**；
* 为 RL 算法引入内生的结构表征机制，比堆叠表征 loss 更具长期可扩展性。

---

是否继续整理 **5.3：结构设计消融分析（设计决策）** 的 PPT 内容？




