---
创建时间: 2025-七月-29日  星期二, 7:48:25 早上
---
好的，以下是你提供的这部分英文内容的**整理修正+完整中文翻译**：

---

### 原文整理
明白了，以下是整理后的中文逐字翻译，内容保持忠实，同时进行自然分段，便于阅读：

---

好的，呃，那我们开始吧。

过去十年对机器学习来说非常激动人心，因为我们已经获得了真正能工作的系统，比如在许多领域中可以相当可靠、相当准确地做出预测的系统。我认为人工智能的下一个前沿，将是那些能够与现实世界互动的系统——它们可以优化一系列动作，而这正是强化学习的核心。强化学习关注的是如何通过经验改进，包括执行一系列会导致某种结果的动作。

我整个博士期间的工作，是将强化学习看作是一种概率型的机器学习问题。今天我要讲讲为什么这种看法是有用的。它之所以有用，是因为它让用户更容易使用强化学习工具来指定他们想要的结果，比如只需提供一些结果示例即可。不仅如此，这个想法也推动了更好的算法的产生：这些算法不仅优于以往的方法，还能在决策问题的解中发现结构和模式。这正是我今天要分享的内容——我们在博士论文中所做的工作，以及其中的一些工作如何可能推动强化学习的潜在应用。

强化学习在我们生活的几乎每一个方面都有应用潜力。很多早期的强化学习工作集中在围棋、Atari 或扑克等游戏中，但它的应用前景远不止如此。例如机器人领域：可以帮助病人、服务医院的机器人，能够安全驾驶汽车的机器人，能让仓库更安全的机器人。这些问题其实非常复杂。比如说，这个机器人拥有20个马达，每个关节一个马达。你如何控制这些马达？应该给出怎样的动作序列？你要如何演示，才能让它最终解出一个魔方？

在我们建造的世界中也存在很多潜在应用，比如我们要如何优化交通灯的时序，使得我们在十字路口等待的时间更短？我们又该如何利用美国大城市中心收集到的大规模数据流实现这一目标？在医疗领域，人工智能系统已经被用来预测某些病人的治疗结果。但未来的AI工具可能进一步发展，协助医生做出决策。像机器人和医疗这样的领域，都需要基于历史数据进行学习，而不能依赖试错学习。

教育领域同样存在类似的潜力。随着计算机科学课程注册人数激增，我们如何利用这一趋势？如何确定应该给学生展示哪一类问题序列？这些问题又该如何根据学生的过往经验个性化设计？

在科学研究中，传统的机器学习方法也许能够预测实验结果，而强化学习方法或许能够动态控制实验过程。

当然，所有这些潜在应用不可能仅靠强化学习研究者来实现，而需要与领域专家合作，也需要融合其他机器学习方法。但我认为强化学习是解决这些问题拼图中至关重要的一块，因为它能够帮助我们优化长期结果。

目前为止，大多数强化学习的成功应用出现在游戏领域。我认为原因在于：我们知道游戏的规则，可以模拟游戏，从而生成大量数据。而在其他应用中，进展则要缓慢得多。因为在这些应用中，数据难以获取，反馈机制也更难构建。

因此，如果我们希望在这些潜在应用上取得进展，就必须开发出能够在更少反馈下学习的强化学习算法。今天我将介绍我们在博士论文中为实现这一目标所做的两方面工作：首先，我们让用户可以通过仅提供好的结果示例来反馈目标；其次，我们提出了能够在决策问题解中推理结构的算法，从而把复杂问题分解成更容易学习的小问题，降低对反馈的依赖。

在开始前，为了确保我们对问题有一致的理解，我想简单回顾一下强化学习是如何工作的。

左边这个是一个机器人任务，我称之为“环境”。我们要控制这个机器人，比如闭合它的手臂、移动它的手臂向左或向右，并从中观察它的行为。在这种设置中，观测就是一张图像，表示当前环境中正在发生什么。最重要的部分是这个“策略”，它告诉我们在当前观测（即当前状态 $s$）下应该采取什么动作。最终，我们的目标就是学习这个策略。我们通过奖励来学习这个策略。奖励告诉我们某个状态是好是坏，我们希望找到一个策略，它不仅能让我们现在进入好状态，也能让我们在未来持续进入好状态。

强化学习非常令人兴奋，因为它非常通用。之前提到的所有应用场景，都是这种问题的具体例子。但正是这种通用性，也让问题变得格外困难。我认为强化学习最大的挑战是缺乏反馈。

让我详细解释一下这个问题。假设我们有一个机器人要去拾取一个方块，它可能执行了一步、两步、三步、四步，也许到了第五步才最终收到一个反馈，比如说“你做得不错”，得到一个 +5 的奖励。但在这之前，它可能完全没有任何反馈。这与监督学习非常不同，在监督学习中，每一个输入都会得到一个标签（反馈）。

这个问题还被高维环境进一步放大。那些观测图像，在计算机看来就是一大块数字。而我们之所以关注高维问题，是因为我们相信在这些高维问题上，学习驱动的方法能优于传统方法。

还有第三个挑战是反馈的构建非常繁琐。比如这个状态为什么要给 +5 的奖励？实际上，是因为一些研究生花了很多小时写了一个巨大的代码块，当状态满足某些条件时，就给出 +5 的奖励。但这个大代码块成为了强化学习应用的门槛。很多用户想使用强化学习，却不知道该怎么写这样的规则代码。即便是专家，也会质疑这些代码是否正确，是否能带来期望的行为。

而且，当我们进入真实世界问题时，甚至运行这段代码都很难。比如说这段代码可能需要知道场景中每个物体的位置，而即便是最先进的目标检测器，有时也会出错。所以，如果我们希望推进强化学习的发展，我们就必须研究如何从更少的反馈中进行学习。

其实，从高维数据中以有限反馈学习，并不是强化学习独有的问题。我们在计算机视觉和自然语言处理等其他机器学习领域中也遇到过类似挑战，而在这些领域，自监督学习方法已经取得了非常出色的成果。

我们来看看这些方法是怎么做的。在计算机视觉中，一种方法是遮盖住图像的一些像素或图块，然后训练模型去预测被遮盖的部分。这类技术就是很多生成式AI模型背后的关键。自然语言处理领域也一样，把句子中某些词遮住，然后预测这些缺失词，或是预测下一个词。这些技术也是过去一年中你可能见过的大型语言模型背后的基础。

还有一类自监督学习方法叫作对比学习，它通过对输入的表示进行建模，让相似的输入拥有相似的向量表示。我们今天的内容主要就是基于这类方法展开的。

这些自监督方法已经在其他领域中成功地解决了“高维+有限反馈”的学习问题。那么，我们是否可以在强化学习中也实现类似的效果？换句话说，强化学习中的“自监督”到底意味着什么？

如果要用五个词来总结我的博士论文，那就是：“我们刚刚开始奠基自监督强化学习”。今天我希望带大家了解这个方向意味着什么：我们开发了可以在现实中运行的实用算法，也建立了相关理论保证，能帮助我们回答这些方法在什么条件下有效。此外，我们还揭示了这些强化学习方法与计算机科学其他领域，乃至机器学习其他子方向之间的一些联系。

（对线上参会者说）如果你在Zoom上听讲，麻烦把麦静音，谢谢。

那么，接下来我将讲讲几个具体内容。

首先是我们如何学习“目标导向技能”（goal-reaching skills）。这些技能将带来几个能力：它们可以让用户仅通过提供一张“期望场景的图片”来指定任务，而无需再编写复杂的奖励代码。我会介绍我们如何将这些方法部署在现实中，与相关人员合作使用，并进一步讲讲我们证明的一些理论性质——这些性质把目标导向技能与奖励最大化联系起来，并使我们能够将方法扩展到一般性强化学习问题中。

这些“目标导向技能”本身就已经很有用，它们还提供了良好的抽象结构，适用于规划（planning）。通过将这些技能整合进规划算法，我们能够将其能力提升一个数量级以上。这也让我们可以把这些方法应用到更复杂的任务中，比如控制这个户外机器人。


以下是上述英文内容的自然分段+忠实逐字翻译，保持演讲口吻与逻辑清晰，同时避免AI化表述：

---

当然，当我们希望将强化学习应用到真实世界的问题中时，确保我们的方法具备鲁棒性就变得非常、非常重要。我们从多个角度研究了鲁棒性，比如我们研究了正则化（regularization）如何带来一定程度的鲁棒性，也研究了如何利用近似模型进行学习，或者识别这些模型的误差。

但我今天关注的是**鲁棒的系统泛化能力（robust system generalization）**。

我们从第一个部分开始——学习“目标达成技能”（goal reaching skills）。我将从问题定义开始介绍，也就是所谓的“目标条件强化学习”（goal-conditioned reinforcement learning）问题。

假设我们有一个机器人，就像我在演讲开头展示的那个，我们希望它把绿色的物体捡起来，并放进蓝色的箱子里。传统做法是，我们可以写出那个之前看到的庞大且复杂的奖励函数。但这一次，我们不这么做，而是让用户直接提供一张图像，来表示他们希望机器人完成的结果，也就是“期望结果”。

例如，在这个例子中，我们希望机器人往下移动、抓起绿色物体，然后移动到蓝色箱子上方，把它放进去。

这种“目标条件强化学习”其实是一个非常老的问题，早期的一些AI研究，包括NSH（可能指代某位研究者或团队）的工作，就曾探讨过如何学习目标导向的行为。

而我们研究的是一个特定场景：我们拥有一个数据集，数据来自一些视频，每帧带有动作标签。要注意的是，这些视频中的动作**并不一定是最优的**，动作也不一定是“好”的。但我们要从这些数据中学习，尽可能快速地达成目标的技能。

这个问题为何有用？在实际中，它让用户更容易指定任务；而在数学层面，它让我们可以将方法应用到更一般的环境中，包括**连续状态空间、连续动作空间、随机动态系统**，同时还能保留某些理论性质，因为这个目标是一个良定义的（well-defined）优化目标。

那么我们该怎么做呢？我们将通过两个步骤来学习这些“目标达成技能”：

第一步是**学习数据的表示**，因为我们处理的是高维数据，表示学习就变得很关键；
第二步是**从这些表示中学习目标导向技能**。

这就是我们的总体计划。下面我会先讲讲我们是怎么学习这些表示的。

---

在高维数据的表示学习中，有很多已有方法。例如，有些方法试图预测视频的下一帧；还有一些方法，会对视频中每一帧单独提取特征，学习能表达画面内容的表示。但这些方法通常**没有建模“动作”**——也就是说，它们并不能告诉我们：如果你想从状态A到达状态B，该采取什么动作。

因此我们需要另一种表示学习方法。

我们的方法构建在计算机视觉和自然语言处理中已有的表示学习方法之上，并将其扩展到了强化学习环境中。我们使用的是**对比学习（contrastive learning）**，下面我来解释我们是如何将其应用于强化学习的。

我们要学习两类表示：
一类是**当前状态和动作的联合表示**；
另一类是**未来状态的表示**。

数据来源就是我们拥有的视频和动作标签。我们首先会从一段视频中取出两张图像，这两张图像之间在时间上相隔若干步，并认为它们的表示应该相似。

这两帧图像的间隔是通过一个\*\*几何分布（geometric distribution）\*\*进行采样的，通常间隔为1或2步，但也可能是3、4甚至更多步。这样的分布有助于提升方法的某些理论性质。

接着我们从另一段视频中取两帧图像，它们则应该具有不同的表示。

直觉上来说：
这些表示学出来之后，应该能够告诉我们从一个状态到另一个状态的“到达难度”。如果两个状态的表示很相似，那么说明从一个状态到另一个状态的“到达”是很容易的。关键在于，我们的表示**依赖于动作**，这是很多早期方法所缺失的重要特性。

---

下面我来展示我们所学习到的表示是怎样的。

我们在一个模拟的机器人操控任务中测试了这个方法。这个任务中有一个机器人手臂，有一个红色物体可以移动，还有一个抽屉可以被打开。我们把这些表示看作是在球面上归一化后的点（这是一种常见做法），学习方式是最大化这些表示之间的点积（dot product），用以度量它们之间的相似性。

这些表示可以看作是某种“世界模型”，不过它并不直接预测未来的观测图像长什么样，而是预测未来状态的“表示”应该长什么样。

这种方法和强化学习中一个经典思想——\*\*继承表示（successor representation）\*\*有非常强的联系。如果大家感兴趣我可以在最后讲更多。

如果我们把一段“从初始状态到最终状态”的路径可视化，我们会得到一些有趣的发现。这些图像是通过**最近邻检索**的方式生成的。

观察这些图像我们会发现两点：

1. 它们和初始观测、最终观测图像中的物体是一样的，比如都有红色和绿色物体；
2. 更重要的是，它们还表现出一定的“物理规则”。例如：红色物体只有在被触碰时才移动；抽屉也只有在机器人手臂向下移动时才会打开。

这和其他方法完全不同。

比如我们用常见的深度学习方法，比如**变分自编码器（VAE）**，来可视化其表示时，我们会发现这些表示可以捕捉图像中的物体信息，但却无法体现“物理交互”。你会看到手臂在动、物体也在动，但手臂从未真正触碰到物体。

这正说明，我们所学习的表示具有物理意义，这也为我们提供了线索：或许这些表示正是我们构建“目标达成技能”的合适基础。

因为说到底，这些表示本来就是为了学习目标导向技能而设计的。

---

接下来我们来看看如何从这些表示中学习目标导向技能。

举个例子，还是之前那个机器人任务。假设当前有两个可选动作：一个是向右移动方块，另一个是向左移动它。

记住，我们的表示是依赖于动作的，也就是说，两个动作各自都有自己的表示。

那么我们就可以比较这两个动作的表示，看哪个更接近目标状态的表示。显然，如果“动作二”的表示离目标更近，那么它就是应该选择的动作。

这正是我们所做的：我们定义每一个技能（skill）为——它对应的动作的表示最接近目标表示。

总结一下这部分内容：

我们从视频和动作数据出发，
学习了状态-动作与未来状态的表示，
再从这些表示中提取出了“目标达成技能”。

---

到这里我们就可以使用这些技能去完成一些任务了，也可以用这些技能回过头去收集更多的数据。我们发现这两种方式都有效。

刚才我一开始讲强化学习之所以难，是因为反馈很稀少。而我们通常认为强化学习需要大量奖励信号才能学得好。而\*\*目标条件强化学习（goal-conditioned RL）\*\*通常被认为更难——因为你不是学一个目标，而是要学一堆目标，每一个目标可能都需要一个奖励函数。

但我们这里展示的是：我们可以将“目标条件强化学习”完全转化为一个**自监督学习问题**，我们不再需要手写奖励函数，不需要用户写出之前那段冗长复杂的奖励代码。这也解释了这些方法为何有效。

---

为了验证这些方法是否能从模拟走向现实，我们和合作伙伴一起，将这些方法应用到了一个真实世界的机器人操控任务中。

我们使用的机器人是一个 WidowX——它体积小、成本低，便于实验，但使用起来也非常挑剔。我们要完成的目标是让机器人从起始状态移动到一个新状态（例如红色汤勺移动到左边）。

从机器人研究的角度来看，这是一个很有挑战的任务：

* 数据是**次优的**，不全是演示；
* 我们**不知道物体的位置**，完全依赖图像输入；
* 整个过程**不进行任何交互训练**，完全从已有数据中学习。

为了对比，我们还看了一个基线方法，它可以将手臂移动到大致位置，但不会去抓红色汤勺。而我们的方法则能明确地伸手去抓那个汤勺，把它放到左边，然后抬起手臂。

这些技能有一个重要特性：**它们在训练时并没有“目标”**，也就是说，它们是从数据中泛化得到的。
所以，如果我们现在想让机器人执行另一个任务，比如推易拉罐过桌子，我们只需要提供一张新的“目标图像”，就能完成任务。

无需重新训练、无需做梯度更新、无需额外提供数据。只需用**相同的策略**，换一个目标，就能完成新的任务。


