---
创建时间: 2025-七月-29日  星期二, 7:48:25 早上
---
好的，以下是你提供的这部分英文内容的**整理修正+完整中文翻译**：

---

### 原文整理

这段演讲介绍了作者博士研究的核心理念：将强化学习（RL）视为一种**概率机器学习**问题，并通过**自监督学习**的方式缓解强化学习中**反馈稀缺**的挑战。作者逐步引出了目标导向技能的学习，提出通过图像而非奖励函数来定义任务目标，并强调这些技能可用于计划（planning），进而在机器人、交通、医疗等多个领域应用。

---

### 中文翻译（完整表达，语言流畅）

好了，我们开始吧。

过去十年是机器学习领域极其令人振奋的时期，因为我们已经拥有了能够真正运作的系统——它们在很多任务中都能较为稳定、准确地进行预测。

我认为，人工智能的下一个前沿，将是那些能够**主动与世界交互**的系统。这些系统将不只是被动地输出，而是能够优化**一系列动作**，不断通过经验改进自身决策——这正是强化学习的核心所在。

强化学习关注的是：给定一个起始状态，该采取怎样的一连串行动，最终实现理想结果。我的博士研究，正是将强化学习重新框定为一个**概率机器学习问题**。今天我要讲的，正是这种视角为何有用——它不仅让用户能更轻松地使用强化学习工具（比如仅需给出一些理想结果示例即可），同时也催生出了新的强化学习算法。这些算法不但在性能上超越了之前的方法，更能在复杂的决策问题中挖掘出**结构与模式**。

我将分享我们在这个方向上的研究进展，以及它们如何推动强化学习的广泛应用。

强化学习的潜力几乎可以扩展到我们生活的方方面面。虽然当前很多基础性研究仍集中在游戏领域（例如围棋、Atari 游戏、扑克等），但强化学习的实际应用远不止于此。

例如，在机器人领域，我们可以设计协助患者的机器人、自动驾驶汽车、提升仓储效率的机械臂等。但这些任务往往非常复杂，比如下图中的机器人，它的每个指节都由单独的马达控制——我们该如何为它编排一系列动作，以完成比如“解魔方”这类任务？

强化学习同样可以用于优化城市中的交通灯控制序列，让我们在十字路口等待的时间变得更少。又如医疗场景，现在的AI已经在帮助预测病患结果，未来它们也许能直接辅助医生做出决策。而在这些真实领域中，我们往往只能依赖**历史数据**，无法像在游戏里那样反复试错。

教育领域也是如此。随着计算机科学课程的迅猛增长，我们是否可以为学生动态推荐问题序列？是否能根据他们的过往经验定制问题难度？

在科学实验中，传统机器学习可以预测某次实验的结果，但强化学习则可以**动态控制实验的执行方式**，从而提升效率。

当然，实现这些应用并不只靠强化学习专家。我们需要和领域专家深度合作，并融合其他机器学习方法。但强化学习将会是解决这些任务的重要一环——因为它擅长优化长期回报。

目前强化学习的大多数成功都集中在游戏环境中，这并不令人意外。游戏世界的规则清晰、环境可模拟、数据丰富；而现实世界中，反馈稀缺、数据采集成本高，导致进展相对缓慢。

为了让强化学习在更多实际场景中发挥作用，我们就需要开发出**能在较少反馈下学习的算法**。

今天的分享中，我将介绍我们论文中的两项关键工作：

1. **我们让用户可以仅通过给出“理想结果示例”来提供反馈**，不再需要编写复杂的奖励函数；
2. **我们设计了能够在决策问题中自动挖掘结构的算法**，也就是将复杂问题拆解为更容易学的问题，从而以更少的反馈学到更好的策略。

---

我们先回顾一下强化学习的基本流程。

左边是一个机器人操作任务，它所处的环境由视觉观测表示（即图像中的像素信息）。我们的目标是控制它的手，做出如张开、移动等动作。

核心在于**策略（policy）**——它根据当前观测（状态 $s$）来决定执行什么动作。而我们要学习的，正是这个策略。学习策略的依据是**奖励信号（reward）**，告诉我们某个状态是好是坏。最终，我们希望学到一个策略，能够不仅获得即时奖励，更能通向未来的好状态。

强化学习之所以令人兴奋，是因为它具有极强的**通用性**。我们前面提到的那些场景，都可以用这个统一的问题框架来表述。但通用性带来的是高难度——其最大挑战就是：**反馈稀缺**。

比如，我们想让一个机器人把积木捡起来。它可能经历了五六步，才最终得到“+5”奖励。而在前几步中，它什么反馈都没有。这与监督学习不同：后者每条输入数据都有明确标签。

此外，强化学习通常处理的是**高维观测**，比如图像。在计算机眼中，图像就是一大堆数字矩阵。这正是我们更希望用基于学习的方法去替代传统手工规则的原因。

另一个实际挑战在于：为什么某个状态要设定为 +5 奖励？往往是因为某位研究生辛辛苦苦写了一大段代码，显式定义这个奖励条件。但这不仅门槛高、难以维护，而且即使是专家也常常不确定这段规则逻辑是否会导致期望行为。

而在真实世界中，哪怕这段规则代码编好了，运行它本身也很难——比如你需要精准获取场景中每个物体的位置，而即使最先进的目标检测器也会出错。

因此，如果我们希望强化学习真正走向现实，我们就必须探索一种**从更少反馈中学习的方式**。

幸运的是，这并不是强化学习独有的问题。我们在计算机视觉和自然语言处理中，也遇到了类似情况。而近年来的突破，正是依赖于**自监督学习**方法。

举例来说，在图像任务中，我们可以遮住图像某些区域，训练模型预测这些区域的内容；语言任务中，我们遮住句子中的某些词，让模型预测缺失的词或下一个词。这些机制也是你们熟悉的生成式AI背后的基础。

还有一类叫“对比学习（contrastive learning）”的方法——它让模型学会将相似输入映射到相近的表示空间中。

今天的讲座，我们就是要基于这些最后一类方法构建强化学习算法。

这些自监督方法在其它领域中已经取得显著成果——**它们能在缺乏标签的高维数据中学到有用的表示**。那我们能否在强化学习中实现类似的成就？什么才是“自监督的强化学习”？

如果用五个词来总结我的博士论文，那就是：

> **为自监督强化学习奠定基础**

今天我将分享我们提出的**可运行的实际算法**，它们有理论保障，并可应用于真实世界任务。同时我还会展示，这些方法如何与计算机科学的其它分支产生交集。

---

那么今天的讲座会包括哪些内容？

首先，我会介绍如何学习**目标导向技能（goal-reaching skills）**。这些技能允许用户通过一张图像指定任务目标——再也无需手写奖励函数。

我们会介绍这些技能如何应用于现实机器人，以及它们与最大化奖励之间的理论联系。这也将使我们能将它们推广到**通用强化学习问题**。

不仅如此，这些目标导向技能还能为“规划”提供正确的抽象表示。通过将它们整合进规划算法中，我们将能将能力扩展一个数量级以上，最终使其支持如室外机器人导航等复杂任务。

当然，要在现实世界部署强化学习，还必须保证方法的**鲁棒性（robustness）**。我们也研究了正则化策略、近似模型学习、误差识别等方法来提升泛化能力。

---

接下来我将从第一部分讲起：**如何学习目标导向技能**。

我们仍以开始展示的机器人为例。假设我们希望它把绿色物体放进蓝色箱子。我们可以写一个复杂的奖励函数，但这很困难。

于是，我们选择一种更简单的方式：让用户提供一张目标图像，展示任务完成后的状态即可。

这个“目标条件强化学习”（goal-conditioned RL）其实是一个很早的研究问题。我们今天研究的是一种新的设定：给定一个数据集，里面是带有动作标签的视频片段（注意，这些动作未必是最优的），我们希望从中学习出**能够尽快达成目标的技能**。

这不仅简化了任务指定方式，也使得我们能将算法推广到连续状态空间、连续动作空间、随机动态系统等更复杂的环境中。

我们的总体策略分为两步：

1. 从数据中学习状态表示（representation）；
2. 基于这些表示，学习目标导向技能。

---

我接下来将重点介绍第一步：如何学习这种表示。

以往有很多方法可以从视频中学习状态表示，例如预测下一帧画面，或者提取单帧的图像内容。但它们普遍**忽略了动作的影响**——也就是说，它们无法告诉我们“如果你想从状态 A 到状态 B，应当采取什么动作”。

因此，我们提出了基于**对比学习**的新方法，专门为强化学习场景设计。我们将学习两个表示函数：

* 一个对当前状态和动作进行编码；
* 一个对未来状态进行编码。

我们使用标注了动作的视频数据来训练这个表示。具体地说，我们会从一个视频中抽取两帧图像，这两帧之间的时间间隔服从几何分布（通常是一两步，也可能更远），然后训练模型使它们的表示尽量相似。相反地，来自不同视频的帧表示应尽量不同。

这种设计背后的直觉是：如果两个状态的表示很相近，就意味着我们很可能能从一个状态顺利地到达另一个状态。更重要的是：我们的表示**依赖于动作**，这是许多传统方法中所缺乏的关键性质。

---

我们将这一方法应用于一个模拟操作任务：一个机器人手臂可以移动红色物体，打开抽屉等。

我们将这些状态表示投影到单位球面上（这是一种常见做法），然后通过点积衡量状态之间的距离。你可以把这些表示看作是一种“世界模型”，它并不预测图像像素，而是预测**未来状态的表示**。

这些表示与强化学习中的\*\*继承表示（successor representations）\*\*有很强的联系。

我们还使用“最近邻检索”来可视化这些表示的效果。你会发现，所有相似的状态不仅物体位置一致，而且体现出一定的物理规律——比如：红色物体必须被碰触才能移动，抽屉只有在手臂下压时才会打开。

而这，正是我们希望这些表示能捕捉到的核心信息。

---

后续部分我会继续整理下一段内容，包括如何基于这些表示学习策略、部署到现实机器人上、理论分析、规划结合等内容。你可以继续贴第二部分内容，我将无缝接上。
以下是你提供的第二部分内容的**完整整理与中文翻译**，接续前文，保持统一风格与学术表达：

---

### （续）目标导向技能的提取与强化学习问题的重构

更进一步地，这些表示似乎还捕捉到了某种**物理规律**的概念。

比如你可以注意到，那个红色的物体，只有在被机器人手臂碰触后才会移动；抽屉也是如此，必须手臂下压触碰才能打开。而这是其他方法所不具备的。

我们用传统的变分自编码器（Variational Autoencoder）做了对比实验：这些方法学到的表示也能表达场景内容，但**完全缺乏物理因果结构**。例如，机器人手臂在移动时并未接触物体，但物体却在移动，这显然是不合理的。

因此，这些新表示以及它们的可视化结果，提示我们：它们或许正是构建\*\*目标导向技能（goal-reaching skills）\*\*的理想基础。

### 如何从表示中提取技能？

接下来我来说明我们是如何从这些表示中学到技能的。

我们以最开始提到的机器人操作任务为例。假设现在有两个动作：一个将物体向右移动，另一个向左。记得我们最关键的一点是：我们学到的表示是**依赖于动作**的。

因此，每一个动作都对应一个不同的表示，我们可以计算每个动作表示与目标表示之间的距离。哪个动作更接近目标？我们就选择哪个。

这正是我们实际采用的方法：**定义技能为“使得表示最靠近目标表示的动作”**。

总结一下：我们从带动作标签的视频数据中学习表示，然后从这些表示中提取目标导向技能。这些技能既可以用于完成任务，也可以用于采集新的数据，支持自我改进。

---

### 自监督学习如何解决“奖励函数”难题？

我们一开始讲到强化学习的难点在于**缺乏反馈**，而目标导向的强化学习甚至更难——因为每一个目标都可能需要一个奖励函数。

但我们展示了一个关键突破：**无需任何奖励函数**，我们就可以以**完全自监督的方式学习这些目标达成技能**。不需要写任何那种复杂的奖励代码。

我们还将这一方法应用到了现实机器人任务中，所使用的是一个低成本的 WidowX 机械臂。我们要让机器人完成一个目标状态：把红色勺子移动到左边。

这个任务很有挑战性：我们只能从**非最优演示数据**中学习，不能使用对象位置等结构信息，也不能进行任何交互——一切都依赖于已有数据。

我们与传统方法进行了对比：传统方法能将手臂移动到正确位置，但不会执行“抓取”动作。而我们的模型则会主动抓起红勺子、移动、放置并收回手臂。

更令人兴奋的是：这个技能是在**没有指定具体目标**的情况下学到的，它对任务是通用的。也就是说，只需提供**一个新的目标图像**，我们就可以用同一个策略去完成完全不同的任务，比如推动易拉罐。

这说明：我们确实可以通过表示 → 技能的流程，仅从历史数据中学习出强大的通用策略——而用户不需要提供奖励函数或进行额外训练。

---

### 理论视角：这些技能是否最大化了奖励？

当然，从理论角度来看，我们仍需回答一个关键问题：这些技能是否真的在最大化某种**奖励**？

强化学习的基础目标是最大化累计奖励。但这些通过自监督方式学到的技能，是否与某种奖励函数等价？

我们的主要理论结果是：

> 学到的表示之间的点积，相当于预测了未来将获得的回报。

更具体地说，点积越大，表示之间越接近，说明该动作更可能在下一个时间步实现目标。我们甚至无需实际计算这个奖励函数，它自然地隐含在表示中。

这个理论目标本质上就是最大化“以最大概率达成目标”的行为策略，与最短路径规划等目标也高度一致。

这说明，我们在选择“哪个动作的表示更接近目标”的行为方式，并不是启发式的，而是**有理论依据的最优选择**。

我们还进一步推广了这个结论：即使数据集来自不同策略，依然可以从中推理出与回报相关的表示，从而提取出目标导向技能。

因此，这些技能实际上是在解决某类**带有目标奖励的强化学习问题**。这个联系也使我们能够将其推广到更一般的强化学习问题。

例如，如果我们给定多个目标状态的示例，并让它们的概率反映我们希望获得的奖励大小，我们就能构建一种**分布式目标偏好**，进而适配更多任务。

---

### 小结：目标导向技能的实用性与推广性

回顾本节内容：

* 我们展示了：不再需要奖励函数，用户只需提供期望结果图像；
* 这些技能可以在交互式环境中学习，也可以仅从离线数据中提取；
* 它们与奖励最大化之间存在理论连接，具有高度可推广性。

但这些技能不是“万能魔法”。我们发现，它们在部分任务中会失败，例如在3D模拟房屋中，机器人只有在目标和起始状态位于**同一个房间**时才成功导航；若目标在另一个房间，则通常失败。

原因在于：搜索路径空间的组合爆炸。比如动作序列长度为20，可能的组合就是天文数字。

不过，好消息是：我们已有的组件——表示与技能——正好可以作为**构建规划算法的抽象**。只要我们将这些技能整合进一个规划器，就能显著扩展它们的能力。

---

### 将技能与规划结合：高维数据下的图搜索

传统上，解决决策问题的方法有两类：

1. 基于学习的方法：直接从数据中学策略；
2. 基于规划的方法：依赖**符号抽象**与图结构进行搜索。

规划方法非常强大，是早期 AI 的核心，也支撑了互联网等现代系统。但它的一个问题是：**难以处理高维原始数据**。

而我们所展示的自监督强化学习方法，恰好可以为其**提供中间抽象层**。

我们从前面失败的 3D 房屋环境出发，利用视频数据训练表示与技能，然后构建一张“状态图”。

具体方法：

* 每一帧图像是一个节点（从视频中随机采样）；
* 使用表示函数定义图中边的连通性（两状态是否能通过技能到达）；
* 忽略距离过远的边以提升可视化清晰度。

接下来：

* 给定起点与终点图像；
* 在图中找到最短路径；
* 使用技能逐跳导航至中间节点，最终到达目标。

我们可以将该过程等价为一个**概率推理任务**：推断“若从A到B，可能经过哪些中间状态”。

我们展示了机器人在房屋中从走廊导航到厨房的路径，推断出的中间图像几乎与最短路径一致。

虽然机器人没有鸟瞰视角，但实验显示：原始技能无法完成任务时，**结合规划器后成功率超过 95%**。

---

### 实际部署与未来展望

我们还将此方法部署到了现实中的户外机器人上。在疫情期间，我们构建了一个送披萨机器人系统。给机器人一张“商店门口”的照片，它就能自主规划路径、顺利将披萨送达。这是一个真实的“无接触配送”应用。

该方法的价值不仅在于实验成功，还在于为**融合学习与规划**提供了一种新范式。

长期以来，AI 社区一直在争论应优先发展“符号规划”还是“端到端学习”。我们的工作表明：两者并不冲突，反而可以互为补充。

这就是本讲座第二部分的核心结论：

> **自监督学到的技能提供了适合规划的抽象表示，使得高维强化学习问题可转化为图搜索问题。**

---

### 最后展望：跨模态、分层与生成融合

我们从“强化学习的难点是反馈稀缺”讲起，最终展示了无需奖励即可学习技能，并将其组合解决复杂任务的可能性。

接下来，我将加入普林斯顿，继续推动以下研究方向：

1. **强化学习与自然语言的结合**
   当前的大模型（如 GPT）虽能处理图像与语言，但缺乏**对真实世界的理解与交互能力**；而我们的方法虽然能做决策与感知，但缺乏语言理解。二者的理论基础——如对比学习——其实是统一的。我们希望融合两者优势：

   * 从“图像 → 动作”推广到“语言 → 动作”；
   * 或者推断一组语言指令来达成某个复杂目标。

2. **分层技能与结构建模**
   本文第二部分展示了如何推断图像路径。下一步，我们希望：

   * 学会“技能之间的交互”；
   * 构建“技能的技能”，形成层次化结构；
   * 若每层可扩展一个数量级，我们就有可能攻克当前模型无法解决的更复杂任务。
     我们也在探索借鉴**层次贝叶斯建模**理论，这可能是强化学习中处理动态数据分布的更优策略。

3. **与生成建模的深度融合**
   生成模型也从原始数据中学习，只要给出“山间城堡”这样的描述，就能生成图像。而从 RL 视角看，这就像是在构建通往目标的行动序列。
   这些问题非常难，可能需要巨量数据与更强抽象，但我们已经取得初步进展。
   在科学应用中，这种融合或许能用于分子设计、实验控制等——直接从期望结果示例出发，反推出合理的实验路径。

---

### 结语

今天的分享，希望向大家展示了以下几点：

* 如何在无奖励、无交互的情况下，从数据中学习目标导向技能；
* 如何将这些技能抽象为支持规划的模块；
* 如何将自监督、表示学习与强化学习方法深度融合；
* 以及如何进一步跨越模态、构建结构、与生成模型结合，面向更广阔的应用。

这些研究不仅服务于现实世界任务，也将持续推动我们对智能系统的根本理解。

非常感谢大家！

---

如果你需要我将这段内容转换为摘要、PPT、讲稿或思维导图，也可以继续告诉我。
