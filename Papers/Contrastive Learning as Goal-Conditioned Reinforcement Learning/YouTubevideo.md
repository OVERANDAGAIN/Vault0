---
创建时间: 2025-七月-29日  星期二, 7:48:25 早上
---
好的，以下是你提供的这部分英文内容的**整理修正+完整中文翻译**：

---

### 原文整理
明白了，以下是整理后的中文逐字翻译，内容保持忠实，同时进行自然分段，便于阅读：

---

好的，呃，那我们开始吧。

过去十年对机器学习来说非常激动人心，因为我们已经获得了真正能工作的系统，比如在许多领域中可以相当可靠、相当准确地做出预测的系统。我认为人工智能的下一个前沿，将是那些能够与现实世界互动的系统——它们可以优化一系列动作，而这正是强化学习的核心。强化学习关注的是如何通过经验改进，包括执行一系列会导致某种结果的动作。

我整个博士期间的工作，是将强化学习看作是一种概率型的机器学习问题。今天我要讲讲为什么这种看法是有用的。它之所以有用，是因为它让用户更容易使用强化学习工具来指定他们想要的结果，比如只需提供一些结果示例即可。不仅如此，这个想法也推动了更好的算法的产生：这些算法不仅优于以往的方法，还能在决策问题的解中发现结构和模式。这正是我今天要分享的内容——我们在博士论文中所做的工作，以及其中的一些工作如何可能推动强化学习的潜在应用。

强化学习在我们生活的几乎每一个方面都有应用潜力。很多早期的强化学习工作集中在围棋、Atari 或扑克等游戏中，但它的应用前景远不止如此。例如机器人领域：可以帮助病人、服务医院的机器人，能够安全驾驶汽车的机器人，能让仓库更安全的机器人。这些问题其实非常复杂。比如说，这个机器人拥有20个马达，每个关节一个马达。你如何控制这些马达？应该给出怎样的动作序列？你要如何演示，才能让它最终解出一个魔方？

在我们建造的世界中也存在很多潜在应用，比如我们要如何优化交通灯的时序，使得我们在十字路口等待的时间更短？我们又该如何利用美国大城市中心收集到的大规模数据流实现这一目标？在医疗领域，人工智能系统已经被用来预测某些病人的治疗结果。但未来的AI工具可能进一步发展，协助医生做出决策。像机器人和医疗这样的领域，都需要基于历史数据进行学习，而不能依赖试错学习。

教育领域同样存在类似的潜力。随着计算机科学课程注册人数激增，我们如何利用这一趋势？如何确定应该给学生展示哪一类问题序列？这些问题又该如何根据学生的过往经验个性化设计？

在科学研究中，传统的机器学习方法也许能够预测实验结果，而强化学习方法或许能够动态控制实验过程。

当然，所有这些潜在应用不可能仅靠强化学习研究者来实现，而需要与领域专家合作，也需要融合其他机器学习方法。但我认为强化学习是解决这些问题拼图中至关重要的一块，因为它能够帮助我们优化长期结果。




---
---
---



目前为止，大多数强化学习的成功应用出现在游戏领域。我认为原因在于：我们知道游戏的规则，可以模拟游戏，从而生成大量数据。而在其他应用中，进展则要缓慢得多。因为在这些应用中，数据难以获取，反馈机制也更难构建。

因此，如果我们希望在这些潜在应用上取得进展，就必须开发出能够在更少反馈下学习的强化学习算法。今天我将介绍我们在博士论文中为实现这一目标所做的两方面工作：首先，我们让用户可以通过仅提供好的结果示例来反馈目标；其次，我们提出了能够在决策问题解中推理结构的算法，从而把复杂问题分解成更容易学习的小问题，降低对反馈的依赖。

在开始前，为了确保我们对问题有一致的理解，我想简单回顾一下强化学习是如何工作的。

左边这个是一个机器人任务，我称之为“环境”。我们要控制这个机器人，比如闭合它的手臂、移动它的手臂向左或向右，并从中观察它的行为。在这种设置中，观测就是一张图像，表示当前环境中正在发生什么。最重要的部分是这个“策略”，它告诉我们在当前观测（即当前状态 $s$）下应该采取什么动作。最终，我们的目标就是学习这个策略。我们通过奖励来学习这个策略。奖励告诉我们某个状态是好是坏，我们希望找到一个策略，它不仅能让我们现在进入好状态，也能让我们在未来持续进入好状态。

强化学习非常令人兴奋，因为它非常通用。之前提到的所有应用场景，都是这种问题的具体例子。但正是这种通用性，也让问题变得格外困难。我认为强化学习最大的挑战是缺乏反馈。

让我详细解释一下这个问题。假设我们有一个机器人要去拾取一个方块，它可能执行了一步、两步、三步、四步，也许到了第五步才最终收到一个反馈，比如说“你做得不错”，得到一个 +5 的奖励。但在这之前，它可能完全没有任何反馈。这与监督学习非常不同，在监督学习中，每一个输入都会得到一个标签（反馈）。

这个问题还被高维环境进一步放大。那些观测图像，在计算机看来就是一大块数字。而我们之所以关注高维问题，是因为我们相信在这些高维问题上，学习驱动的方法能优于传统方法。

还有第三个挑战是反馈的构建非常繁琐。比如这个状态为什么要给 +5 的奖励？实际上，是因为一些研究生花了很多小时写了一个巨大的代码块，当状态满足某些条件时，就给出 +5 的奖励。但这个大代码块成为了强化学习应用的门槛。很多用户想使用强化学习，却不知道该怎么写这样的规则代码。即便是专家，也会质疑这些代码是否正确，是否能带来期望的行为。

而且，当我们进入真实世界问题时，甚至运行这段代码都很难。比如说这段代码可能需要知道场景中每个物体的位置，而即便是最先进的目标检测器，有时也会出错。所以，如果我们希望推进强化学习的发展，我们就必须研究如何从更少的反馈中进行学习。

---
---


其实，从高维数据中以有限反馈学习，并不是强化学习独有的问题。我们在计算机视觉和自然语言处理等其他机器学习领域中也遇到过类似挑战，而在这些领域，自监督学习方法已经取得了非常出色的成果。

我们来看看这些方法是怎么做的。在计算机视觉中，一种方法是遮盖住图像的一些像素或图块，然后训练模型去预测被遮盖的部分。这类技术就是很多生成式AI模型背后的关键。自然语言处理领域也一样，把句子中某些词遮住，然后预测这些缺失词，或是预测下一个词。这些技术也是过去一年中你可能见过的大型语言模型背后的基础。

还有一类自监督学习方法叫作对比学习，它通过对输入的表示进行建模，让相似的输入拥有相似的向量表示。我们今天的内容主要就是基于这类方法展开的。

这些自监督方法已经在其他领域中成功地解决了“高维+有限反馈”的学习问题。那么，我们是否可以在强化学习中也实现类似的效果？换句话说，强化学习中的“自监督”到底意味着什么？


假设我们有三张图片 I_1、I_2 和 I_3。 前两幅图像描绘了一只狗，第三幅图像描绘了一只猫，我们想要为每幅图像（x_1、x_2 和 x_3）学习低维表示：

在对比学习中，我们希望最小化相似样本之间的距离，最大化不同样本之间的距离。 在我们的示例中，我们希望最小化距离 d(x_1, x_2) 并最大化距离 d(x_1, x_3) 和 d(x_2, x_3)，其中 d() 是像欧几里得这样的度量函数。

与锚样本（I_1）相似的样本定义为正样本（I_2），与锚样本（I_3）不相似的样本定义为负样本。

---

---

如果要用五个词来总结我的博士论文，那就是：“我们刚刚开始奠基自监督强化学习”。今天我希望带大家了解这个方向意味着什么：我们开发了可以在现实中运行的实用算法，也建立了相关理论保证，能帮助我们回答这些方法在什么条件下有效。此外，我们还揭示了这些强化学习方法与计算机科学其他领域，乃至机器学习其他子方向之间的一些联系。

（对线上参会者说）如果你在Zoom上听讲，麻烦把麦静音，谢谢。

那么，接下来我将讲讲几个具体内容。


---
---


首先是我们如何学习“目标导向技能”（goal-reaching skills）。这些技能将带来几个能力：它们可以让用户仅通过提供一张“期望场景的图片”来指定任务，而无需再编写复杂的奖励代码。我会介绍我们如何将这些方法部署在现实中，与相关人员合作使用，并进一步讲讲我们证明的一些理论性质——这些性质把目标导向技能与奖励最大化联系起来，并使我们能够将方法扩展到一般性强化学习问题中。


这些“目标导向技能”本身就已经很有用，它们还提供了良好的抽象结构，适用于规划（planning）。通过将这些技能整合进规划算法，我们能够将其能力提升一个数量级以上。这也让我们可以把这些方法应用到更复杂的任务中，比如控制这个户外机器人。


当然，当我们希望将强化学习应用到真实世界的问题中时，确保我们的方法具备鲁棒性就变得非常、非常重要。我们从多个角度研究了鲁棒性，比如我们研究了正则化（regularization）如何带来一定程度的鲁棒性，也研究了如何利用近似模型进行学习，或者识别这些模型的误差。

但我今天关注的是**鲁棒的系统泛化能力（robust system generalization）**。

---
---


---


我们从第一个部分开始——学习“目标达成技能”（goal reaching skills）。我将从问题定义开始介绍，也就是所谓的“目标条件强化学习”（goal-conditioned reinforcement learning）问题。

假设我们有一个机器人，就像我在演讲开头展示的那个，我们希望它把绿色的物体捡起来，并放进蓝色的箱子里。传统做法是，我们可以写出那个之前看到的庞大且复杂的奖励函数。

但这一次，我们不这么做，而是让用户直接提供一张图像，来表示他们希望机器人完成的结果，也就是“期望结果”。

例如，在这个例子中，我们希望机器人往下移动、抓起绿色物体，然后移动到蓝色箱子上方，把它放进去。

这种“目标条件强化学习”其实是一个非常老的问题，早期的一些AI研究，包括NSH（可能指代某位研究者或团队）的工作，就曾探讨过如何学习目标导向的行为。

而我们研究的是一个特定场景：我们拥有一个数据集，数据来自一些视频，每帧带有动作标签。要注意的是，这些视频中的动作**并不一定是最优的**，动作也不一定是“好”的。但我们要从这些数据中学习，尽可能快速地达成目标的技能。

这个问题为何有用？在实际中，它让用户更容易指定任务；而在数学层面，它让我们可以将方法应用到更一般的环境中，包括**连续状态空间、连续动作空间、随机动态系统**，同时还能保留某些理论性质，因为这个目标是一个良定义的（well-defined）优化目标。


---
---


那么我们该怎么做呢？我们将通过两个步骤来学习这些“目标达成技能”：

第一步是**学习数据的表示**，因为我们处理的是高维数据，表示学习就变得很关键；
第二步是**从这些表示中学习目标导向技能**。

这就是我们的总体计划。下面我会先讲讲我们是怎么学习这些表示的。
在高维数据的表示学习中，有很多已有方法。例如，有些方法试图预测视频的下一帧；还有一些方法，会对视频中每一帧单独提取特征，学习能表达画面内容的表示。但这些方法通常**没有建模“动作”**——也就是说，它们并不能告诉我们：如果你想从状态A到达状态B，该采取什么动作。

因此我们需要另一种表示学习方法。

我们的方法构建在计算机视觉和自然语言处理中已有的表示学习方法之上，并将其扩展到了强化学习环境中。我们使用的是**对比学习（contrastive learning）**，下面我来解释我们是如何将其应用于强化学习的。

![[Pasted image 20250729112619.png]]


---


我们要学习两类表示：
一类是**当前状态和动作的联合表示**；
另一类是**未来状态的表示**。

数据来源就是我们拥有的视频和动作标签。我们首先会从一段视频中取出两张图像，这两张图像之间在时间上相隔若干步，并认为它们的表示应该相似。

这两帧图像的间隔是通过一个\*\*几何分布（geometric distribution）\*\*进行采样的，通常间隔为1或2步，但也可能是3、4甚至更多步。这样的分布有助于提升方法的某些理论性质。

接着我们从另一段视频中取两帧图像，它们则应该具有不同的表示。

直觉上来说：
这些表示学出来之后，应该能够告诉我们从一个状态到另一个状态的“到达难度”。如果两个状态的表示很相似，那么说明从一个状态到另一个状态的“到达”是很容易的。关键在于，我们的表示**依赖于动作**，这是很多早期方法所缺失的重要特性。

---

下面我来展示我们所学习到的表示是怎样的。

我们在一个模拟的机器人操控任务中测试了这个方法。这个任务中有一个机器人手臂，有一个红色物体可以移动，还有一个抽屉可以被打开。我们把这些表示看作是在球面上归一化后的点（这是一种常见做法），学习方式是最大化这些表示之间的点积（dot product），用以度量它们之间的相似性。

这些表示可以看作是某种“世界模型”，不过它并不直接预测未来的观测图像长什么样，而是预测未来状态的“表示”应该长什么样。

这种方法和强化学习中一个经典思想——\*\*继承表示（successor representation）\*\*有非常强的联系。如果大家感兴趣我可以在最后讲更多。

如果我们把一段“从初始状态到最终状态”的路径可视化，我们会得到一些有趣的发现。这些图像是通过**最近邻检索**的方式生成的。

观察这些图像我们会发现两点：

1. 它们和初始观测、最终观测图像中的物体是一样的，比如都有红色和绿色物体；
2. 更重要的是，它们还表现出一定的“物理规则”。例如：红色物体只有在被触碰时才移动；抽屉也只有在机器人手臂向下移动时才会打开。

这和其他方法完全不同。


---
比如我们用常见的深度学习方法，比如**变分自编码器（VAE）**，来可视化其表示时，我们会发现这些表示可以捕捉图像中的物体信息，但却无法体现“物理交互”。你会看到手臂在动、物体也在动，但手臂从未真正触碰到物体。

这正说明，我们所学习的表示具有物理意义，这也为我们提供了线索：或许这些表示正是我们构建“目标达成技能”的合适基础。

因为说到底，这些表示本来就是为了学习目标导向技能而设计的。

---
---


接下来我们来看看如何从这些表示中学习目标导向技能。

举个例子，还是之前那个机器人任务。假设当前有两个可选动作：一个是向右移动方块，另一个是向左移动它。

记住，我们的表示是依赖于动作的，也就是说，两个动作各自都有自己的表示。

那么我们就可以比较这两个动作的表示，看哪个更接近目标状态的表示。显然，如果“动作二”的表示离目标更近，那么它就是应该选择的动作。

这正是我们所做的：我们定义每一个技能（skill）为——它对应的动作的表示最接近目标表示。

总结一下这部分内容：

我们从视频和动作数据出发，
学习了状态-动作与未来状态的表示，
再从这些表示中提取出了“目标达成技能”。

---

到这里我们就可以使用这些技能去完成一些任务了，也可以用这些技能回过头去收集更多的数据。我们发现这两种方式都有效。

---
---
---

刚才我一开始讲强化学习之所以难，是因为反馈很稀少。而我们通常认为强化学习需要大量奖励信号才能学得好。而\*\*目标条件强化学习（goal-conditioned RL）\*\*通常被认为更难——因为你不是学一个目标，而是要学一堆目标，每一个目标可能都需要一个奖励函数。

但我们这里展示的是：我们可以将“目标条件强化学习”完全转化为一个**自监督学习问题**，我们不再需要手写奖励函数，不需要用户写出之前那段冗长复杂的奖励代码。这也解释了这些方法为何有效。

---

为了验证这些方法是否能从模拟走向现实，我们和合作伙伴一起，将这些方法应用到了一个真实世界的机器人操控任务中。

我们使用的机器人是一个 WidowX——它体积小、成本低，便于实验，但使用起来也非常挑剔。我们要完成的目标是让机器人从起始状态移动到一个新状态（例如红色汤勺移动到左边）。

从机器人研究的角度来看，这是一个很有挑战的任务：

* 数据是**次优的**，不全是演示；
* 我们**不知道物体的位置**，完全依赖图像输入；
* 整个过程**不进行任何交互训练**，完全从已有数据中学习。

为了对比，我们还看了一个基线方法，它可以将手臂移动到大致位置，但不会去抓红色汤勺。而我们的方法则能明确地伸手去抓那个汤勺，把它放到左边，然后抬起手臂。


这些技能有一个重要特性：**它们在训练时并没有“目标”**，也就是说，它们是从数据中泛化得到的。
所以，如果我们现在想让机器人执行另一个任务，比如推易拉罐过桌子，我们只需要提供一张新的“目标图像”，就能完成任务。

无需重新训练、无需做梯度更新、无需额外提供数据。只需用**相同的策略**，换一个目标，就能完成新的任务。


以下是接续前文的自然段落整理与忠实翻译（逐字、口语风格保留，同时保持学术清晰）：

---

我希望这些实验结果能够进一步强调这种方法在实际中的实用价值：我们可以完全基于历史数据进行学习，无需用户去手工指定奖励函数。而我们所做的方法是：首先学习表示，再从这些表示中提取技能。



---


---

---


从理论角度来看，这里仍然有一个非常重要的问题：这些“技能”（goal-reaching skills）与强化学习中核心的“奖励最大化”目标之间，到底有没有联系？毕竟，强化学习的基础是最大化累积奖励。

那么，这些技能是否真的在最大化某种形式的奖励呢？

我们将看到，回答这个问题、证明这些技能确实是在解决某类强化学习问题，这样的工作使我们能够将这些方法推广，应用到更加通用的强化学习任务中。

稍微补充一下背景：我们所借鉴的自监督方法，在其他领域（如视觉或语言）中常常存在一个断层——即你所学的表示未必真正有利于下游任务。而在强化学习中，我们发现恰恰相反：学到的表示和解决决策问题之间，存在非常紧密的联系。

这就是我们的核心理论结果：我们证明这些表示确实能够揭示“未来回报”信息。具体来说，如果你去看两个状态表示之间的点积（或相似度），它其实就暗含着你将获得的未来奖励的多少。

这里的“奖励”是非常自然的一类：例如“你是否在下一步达到了目标状态”。重要的是，我们在算法中**从未明确计算过这些奖励**。

顺便提一下，右边的这个强化学习目标，其实就是在**最大化到达目标状态的概率**。它和另一个目标非常接近，那就是**最小化从当前状态到目标状态所需的步数**。
![[Pasted image 20250729113539.png]]


---
---

---

用图像来解释这个理论结果更直观：当我们在定义这些技能时，我们其实在看表示之间的距离。而这个理论结果的意思是——这种比较方式并不是一个启发式规则，而是有理论根据的“合理做法”。

同样地，当我们根据哪个动作的表示更靠近目标表示来选择动作时，这种行为也是有原则支持的。

我们还扩展了这个理论，使得它也适用于**不同策略收集的数据集**，也就是说，我们可以分析如果你用不同的行为策略去采样数据，这些方法会学到什么样的表示。

总结一下：我们在前面展示的那些技能，其实就是在解决一类强化学习问题——具有“目标达成”奖励的强化学习问题。而正是因为这个连接关系，我们才能将这些方法进一步扩展，应用于完全通用的强化学习任务。

其中一种扩展方式是：提供多个结果示例，每个示例的概率大小可以用来表示“你有多想要这个结果”——这其实相当于定义了一个奖励函数的软版本。

---

总结这一部分的核心内容：

我们展示了用户无需再指定奖励函数，他们只需给出期望结果的图像，无论是单个目标状态，还是多个目标状态；我们展示了这些技能可以在**交互式学习环境中**或**完全离线数据集**中被学习出来；我们还展示了这些技能与“奖励最大化”的强化学习目标之间存在紧密的理论联系。

当然，我必须指出的是：我们所学到的这些“目标导向技能”并非万能。

例如，我们尝试将这些方法应用到这个任务上——一个三维模拟房屋中的机器人导航任务。这个机器人可以在房间内行走，配备有摄像头。我们给它一张图像，告诉它“我们想去这个地方”，然后看看它是否能到达那个地方。

实验发现：如果目标图像在**当前房间内**，机器人通常能成功抵达；但如果目标图像在**另一个房间**，机器人基本就失败了。

这其实也不太令人意外。因为如果你考虑20步的动作序列，其组合数量是天文数字级别的。

但关键是：我们所学到的表示与技能，**可以解决这些遥远目标的问题——只要我们将它们整合进“规划算法”中**。



---
---

v这就是接下来要讲的部分：这些目标导向技能，为什么提供了构建“规划抽象层”的基础。
从历史上来看，解决决策问题的方法大致分为两类：

* 第一类是我们刚才讲的基于学习的方法，它们从原始数据中直接学习；
* 另一类是**规划方法（planning methods）**，它们非常强大，是早期人工智能研究的主要内容，也支撑了很多现代技术，比如互联网数据的路径路由。

规划方法通常需要你具备一个**符号化的世界表示（symbolic representation）**，也就是说，需要将高维原始输入转换成结构化表示（比如图）。

而之前我们讲的**自监督强化学习方法**，其实正好可以提供这种抽象能力，帮助我们将高维数据转化为适合规划的方法所用的图结构。

---

我现在来展示这个想法如何运作。

回到之前失败的模拟房屋导航任务：我们用相同的方法，用它的视频数据集（视频+动作）训练出目标技能和状态表示。

然后，我们使用这些组件来**构建一张图（graph）**。这是规划算法所需要的结构。

具体方法如下：

* 图中的**节点**来自视频中的图像帧，随机采样；
* 图中的**边**则基于我们学习到的表示构建而成；
* 为了方便可视化，我们忽略一些边太长的连接。

现在我们有了图，就可以执行规划了。比如我们希望从房子的一个角落导航到另一个角落：我们将起点和目标图像嵌入图中，搜索最短路径，然后使用目标技能一个一个地执行中间路径点（waypoint）。

每到一个中间目标点，我们就调用目标技能将机器人从当前位置移动过去，然后继续这个过程，直到抵达最终目标。

这个过程也可以用概率模型解释：即我们是在**推断一条路径上可能访问的状态序列**，这个我可以在结束时再展开讲讲。

---
---


来看一个具体例子。

我们在多个目标上评估这个方法。下面这个例子中，机器人起点在这里，目标是厨房。右边这组图像是方法自动“推断”出来的路径点图像：先靠近橱柜，然后进入厨房，最后靠近炉灶。

从俯视图来看，这条路径非常合理，几乎就是这栋房子的最短路径。（俯视图只是为了展示，机器人并看不到。）

在量化指标上我们也看到：虽然单独的目标技能在长距离导航任务上失败率很高，但一旦将其集成到规划框架中，任务完成率可以超过95%。

---

在模拟成功之后，我们与合作伙伴一起将方法应用到了这个**户外机器人**身上。

我们给机器人一张目标图像，我的合作者 Drew 把一块披萨放在机器人上。目标图像是顾客家的门口，机器人就能**自主地、不接触地**完成送餐。这正好发生在新冠疫情期间，“无接触送披萨”确实是个研究生关注的问题。

这些将学习与规划相结合的方法也催生了很多后续工作——更鲁棒、更具泛化性。

---

---
---

在更广义的人工智能领域，长期存在一个讨论：到底是“规划方法”更优，还是“从原始数据中直接学习的方法”更好？而我对这项工作最感兴趣的一点就是：它提供了一种思路，可以将这两者结合起来。

所以这部分的结论是：我们用自监督方法学到的这些技能，**提供了支持规划的抽象基础**。

---

现在我们往回拉远一点，回到最初的问题：强化学习为什么难？它难在反馈稀少。

但我们今天看到的是，我们可以**在没有奖励、没有反馈**的条件下学到目标技能，还能组合这些技能去完成复杂任务。

我希望今天的分享让你对我博士论文的一部分工作有了初步的了解。如果你感兴趣，我鼓励你去读一下原论文，了解其中开发的实际算法、理论保证，以及强化学习方法与机器学习和计算机科学其他领域之间的联系。

---

最后讲讲我未来的研究计划。

我即将在秋季加入**普林斯顿大学**任教，我非常期待在那里与学生一起探索一些新的研究问题。




首先，有一些非常令人兴奋的问题出现在**强化学习与自然语言处理（NLP）结合的交叉点**。过去几年甚至几个月中，我们看到很多大模型（multimodal models）在图像和语言推理方面取得了惊人的进展。

但这些方法通常**缺乏对世界的“实际理解”**，而我开发的方法可以处理动作、交互和图像，但还不能理解语言或语义。

我们能否将这两类方法结合起来，集两者之所长？

其实，它们在理论上也有共同点——**都是基于对比学习（contrastive learning）方法**。我相信存在一些优雅的方式可以将它们融合。

正如我们已经展示的，我们可以学习到“图像目标”的导航技能；那么，也许我们可以扩展到**语言目标技能**，让系统能根据语言指令推断要采取的动作序列。


---

第二，我今天演讲的后一部分主要讲的是“结构发现”：我们如何把复杂任务拆解成更简单的任务。比如，我们可以根据两张图像推断出中间可能经历的状态图像序列。

但我们还能否走得更远？比如说，学习能够操作“表示”的技能？学习可以操作“其他技能”的技能？

我们的一些初步实验表明，这种**层级结构的扩展能力是指数级增长的**。如果每一层都能放大一阶能力，也许我们就可以开始尝试解决那些现阶段完全做不到的任务。

我还认为，我们可以借助经典的**分层贝叶斯建模（hierarchical Bayesian modeling）**，它有一些理论优势，特别是在强化学习中你可以动态改变数据分布。

---

最后，这项工作还与\*\*生成建模（generative modeling）\*\*有着紧密联系。

生成模型也是从数据中学习，给它一句话比如“山中的城堡”，它可以生成一个图像来展示这个城堡。

而当我们将强化学习视为生成问题时，我们不只是要建这个“城堡”，我们还要**找出建造它的具体步骤和策略**。这无疑是非常困难的问题，可能需要非常大量的数据、合适的表示和抽象机制。

但我们在这方面已经取得了一些进展。

这类研究或许会启发新的**游戏、创意应用**，甚至是物理科学实验的设计。在科学研究中，生成模型已经被用来设计药物分子。而如果我们把这个过程转化成强化学习问题，也许我们可以真正推导出实验方案的执行路径。

这当然也是个非常难的问题，但我相信我们所做的一些工作能为这类应用打下基础。我们还没有讲的那部分关于**鲁棒性与泛化**的研究，探讨了如何利用近似模型，而这在科学场景中非常常见；而我们今天讲的“从结果学习”的工作也可以用于科学实验设计中——我们只需要给系统一些期望实验结果的示例。

---

好，那我今天的未来工作展望部分就讲到这里了。
