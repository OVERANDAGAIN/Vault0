---
创建时间: 2025-七月-29日  星期二, 7:48:25 早上
---
好的，以下是你提供的这部分英文内容的**整理修正+完整中文翻译**：

---

### 原文整理
明白了，以下是整理后的中文逐字翻译，内容保持忠实，同时进行自然分段，便于阅读：

---

好的，呃，那我们开始吧。

过去十年对机器学习来说非常激动人心，因为我们已经获得了真正能工作的系统，比如在许多领域中可以相当可靠、相当准确地做出预测的系统。我认为人工智能的下一个前沿，将是那些能够与现实世界互动的系统——它们可以优化一系列动作，而这正是强化学习的核心。强化学习关注的是如何通过经验改进，包括执行一系列会导致某种结果的动作。

我整个博士期间的工作，是将强化学习看作是一种概率型的机器学习问题。今天我要讲讲为什么这种看法是有用的。它之所以有用，是因为它让用户更容易使用强化学习工具来指定他们想要的结果，比如只需提供一些结果示例即可。不仅如此，这个想法也推动了更好的算法的产生：这些算法不仅优于以往的方法，还能在决策问题的解中发现结构和模式。这正是我今天要分享的内容——我们在博士论文中所做的工作，以及其中的一些工作如何可能推动强化学习的潜在应用。

强化学习在我们生活的几乎每一个方面都有应用潜力。很多早期的强化学习工作集中在围棋、Atari 或扑克等游戏中，但它的应用前景远不止如此。例如机器人领域：可以帮助病人、服务医院的机器人，能够安全驾驶汽车的机器人，能让仓库更安全的机器人。这些问题其实非常复杂。比如说，这个机器人拥有20个马达，每个关节一个马达。你如何控制这些马达？应该给出怎样的动作序列？你要如何演示，才能让它最终解出一个魔方？

在我们建造的世界中也存在很多潜在应用，比如我们要如何优化交通灯的时序，使得我们在十字路口等待的时间更短？我们又该如何利用美国大城市中心收集到的大规模数据流实现这一目标？在医疗领域，人工智能系统已经被用来预测某些病人的治疗结果。但未来的AI工具可能进一步发展，协助医生做出决策。像机器人和医疗这样的领域，都需要基于历史数据进行学习，而不能依赖试错学习。

教育领域同样存在类似的潜力。随着计算机科学课程注册人数激增，我们如何利用这一趋势？如何确定应该给学生展示哪一类问题序列？这些问题又该如何根据学生的过往经验个性化设计？

在科学研究中，传统的机器学习方法也许能够预测实验结果，而强化学习方法或许能够动态控制实验过程。

当然，所有这些潜在应用不可能仅靠强化学习研究者来实现，而需要与领域专家合作，也需要融合其他机器学习方法。但我认为强化学习是解决这些问题拼图中至关重要的一块，因为它能够帮助我们优化长期结果。

目前为止，大多数强化学习的成功应用出现在游戏领域。我认为原因在于：我们知道游戏的规则，可以模拟游戏，从而生成大量数据。而在其他应用中，进展则要缓慢得多。因为在这些应用中，数据难以获取，反馈机制也更难构建。

因此，如果我们希望在这些潜在应用上取得进展，就必须开发出能够在更少反馈下学习的强化学习算法。今天我将介绍我们在博士论文中为实现这一目标所做的两方面工作：首先，我们让用户可以通过仅提供好的结果示例来反馈目标；其次，我们提出了能够在决策问题解中推理结构的算法，从而把复杂问题分解成更容易学习的小问题，降低对反馈的依赖。

在开始前，为了确保我们对问题有一致的理解，我想简单回顾一下强化学习是如何工作的。

左边这个是一个机器人任务，我称之为“环境”。我们要控制这个机器人，比如闭合它的手臂、移动它的手臂向左或向右，并从中观察它的行为。在这种设置中，观测就是一张图像，表示当前环境中正在发生什么。最重要的部分是这个“策略”，它告诉我们在当前观测（即当前状态 $s$）下应该采取什么动作。最终，我们的目标就是学习这个策略。我们通过奖励来学习这个策略。奖励告诉我们某个状态是好是坏，我们希望找到一个策略，它不仅能让我们现在进入好状态，也能让我们在未来持续进入好状态。

强化学习非常令人兴奋，因为它非常通用。之前提到的所有应用场景，都是这种问题的具体例子。但正是这种通用性，也让问题变得格外困难。我认为强化学习最大的挑战是缺乏反馈。

让我详细解释一下这个问题。假设我们有一个机器人要去拾取一个方块，它可能执行了一步、两步、三步、四步，也许到了第五步才最终收到一个反馈，比如说“你做得不错”，得到一个 +5 的奖励。但在这之前，它可能完全没有任何反馈。这与监督学习非常不同，在监督学习中，每一个输入都会得到一个标签（反馈）。

这个问题还被高维环境进一步放大。那些观测图像，在计算机看来就是一大块数字。而我们之所以关注高维问题，是因为我们相信在这些高维问题上，学习驱动的方法能优于传统方法。

还有第三个挑战是反馈的构建非常繁琐。比如这个状态为什么要给 +5 的奖励？实际上，是因为一些研究生花了很多小时写了一个巨大的代码块，当状态满足某些条件时，就给出 +5 的奖励。但这个大代码块成为了强化学习应用的门槛。很多用户想使用强化学习，却不知道该怎么写这样的规则代码。即便是专家，也会质疑这些代码是否正确，是否能带来期望的行为。

而且，当我们进入真实世界问题时，甚至运行这段代码都很难。比如说这段代码可能需要知道场景中每个物体的位置，而即便是最先进的目标检测器，有时也会出错。所以，如果我们希望推进强化学习的发展，我们就必须研究如何从更少的反馈中进行学习。

其实，从高维数据中以有限反馈学习，并不是强化学习独有的问题。我们在计算机视觉和自然语言处理等其他机器学习领域中也遇到过类似挑战，而在这些领域，自监督学习方法已经取得了非常出色的成果。

我们来看看这些方法是怎么做的。在计算机视觉中，一种方法是遮盖住图像的一些像素或图块，然后训练模型去预测被遮盖的部分。这类技术就是很多生成式AI模型背后的关键。自然语言处理领域也一样，把句子中某些词遮住，然后预测这些缺失词，或是预测下一个词。这些技术也是过去一年中你可能见过的大型语言模型背后的基础。

还有一类自监督学习方法叫作对比学习，它通过对输入的表示进行建模，让相似的输入拥有相似的向量表示。我们今天的内容主要就是基于这类方法展开的。

这些自监督方法已经在其他领域中成功地解决了“高维+有限反馈”的学习问题。那么，我们是否可以在强化学习中也实现类似的效果？换句话说，强化学习中的“自监督”到底意味着什么？

如果要用五个词来总结我的博士论文，那就是：“我们刚刚开始奠基自监督强化学习”。今天我希望带大家了解这个方向意味着什么：我们开发了可以在现实中运行的实用算法，也建立了相关理论保证，能帮助我们回答这些方法在什么条件下有效。此外，我们还揭示了这些强化学习方法与计算机科学其他领域，乃至机器学习其他子方向之间的一些联系。

（对线上参会者说）如果你在Zoom上听讲，麻烦把麦静音，谢谢。

那么，接下来我将讲讲几个具体内容。

首先是我们如何学习“目标导向技能”（goal-reaching skills）。这些技能将带来几个能力：它们可以让用户仅通过提供一张“期望场景的图片”来指定任务，而无需再编写复杂的奖励代码。我会介绍我们如何将这些方法部署在现实中，与相关人员合作使用，并进一步讲讲我们证明的一些理论性质——这些性质把目标导向技能与奖励最大化联系起来，并使我们能够将方法扩展到一般性强化学习问题中。

这些“目标导向技能”本身就已经很有用，它们还提供了良好的抽象结构，适用于规划（planning）。通过将这些技能整合进规划算法，我们能够将其能力提升一个数量级以上。这也让我们可以把这些方法应用到更复杂的任务中，比如控制这个户外机器人。


