---
创建时间: 2025-七月-29日  星期二, 10:15:15 上午
---
以下是你提供的讲稿“小版本”的忠实逐字翻译，并保持自然分段、逻辑清晰、学术表达通顺，适用于展示或汇报用途：

---

我很高兴今天能介绍我们的工作，标题是：**将对比学习视为目标条件强化学习**（Contrastive Learning as Goal-Conditioned Reinforcement Learning）。



在过去几年中，深度强化学习算法取得了非常出色的成果，但与此同时，它们也变得相当复杂。如今的强化学习就像一台拥有许多运转部件的引擎——可能有一个模块用于预测未来奖励，另一个模块学习环境模型，还有一个模块用于从高维数据中学习紧凑的表示。

而在本项工作中，我们提出了一种**更加简洁的方案**：我们仅使用**一个目标函数**，就同时完成了未来奖励的预测、环境建模和表示学习。

---
---
我们关注的任务是**目标条件强化学习（goal-conditioned RL）**。具体来说，我们希望学习一个策略，它以当前图像和目标图像为输入，并采取一系列动作，最终使系统到达目标图像所对应的状态。

我们使用一个大型视频数据集来训练这个策略，数据中标注了每一帧的动作。但与标准强化学习中的轨迹不同，这些数据**并没有奖励标注**。

我们定义的奖励函数是：**如果在下一个时间步到达目标状态，则获得奖励**。这种定义完全基于物理过程，不需要任何奖励工程。

我们优化的目标是这些奖励的总和，Q函数就是在当前状态和动作条件下的这个目标函数。

我们方法的核心思想是：**使用对比学习，构建整个强化学习算法**。这个算法不需要额外的表示学习、奖励学习或模型学习。

---
---
接下来我会先解释我们如何使用对比学习，之后会展示为什么它本身就是一个完整的强化学习算法。

我们的方法在形式上类似于已有的视频、音频或文本的表示学习方法，但关键不同点在于：我们的方法**可以直接解决强化学习问题**。

我们将学习两种表示：

* 一种是**状态-动作的联合表示**；
* 一种是**未来状态的表示**。

我们从轨迹中采样数据：

* 正样本来自**同一条轨迹中的状态、动作与未来状态**，其中时间间隔服从**几何分布**，这样可以将我们的目标与 Q 值联系起来；
* 负样本来自**其他轨迹中的随机状态**，我们希望将它们的表示拉远。

直观地看，这些表示让我们能够推理未来可能到达的状态，更重要的是，它们能帮助我们判断**哪些动作更可能将我们带到目标状态**。

---
---

完整算法如下：

我们从一个数据集开始，这个数据可能来自另一项实验或一个随机策略。我们使用对比学习来训练表示网络，然后再训练策略网络，让其最大化到达目标状态的概率。

当然，我们也可以选择性地回到环境中收集新数据。但我们方法的一个优势是：**即使不额外收集数据，它也可以表现良好**。

我们的算法没有采用许多以往方法中常见的“技巧”或“复杂机制”，而且我们开源的代码**非常高效**。

---

关于对比学习的效用，我们可以从多个角度来看：

* 它提供了一种**隐式的世界模型**，能够预测“将会发生什么”，而不需要直接生成高维的图像；
* 它也可以被看作是一种**未来回报的估计器**（Q 函数）；
* 它还能学习出**状态的低维表示**。

训练 actor（策略）网络，也有一个很直观的几何解释：

设想你有一个目标状态的表示，还有当前状态（可能因动作不同而不同）的表示。如果我们简单地比较这些表示之间的相似度，策略训练就相当于选择那个能使状态-动作表示最接近目标表示的动作。

这可以被看作是在**表示空间中的规划过程**。

而通常规划是很难的。比如说，如果你让机器人手臂逐步移动到目标点，它可能根本不会去捡起那个方块。

但通过我们所学习的表示，我们把这个规划问题变得简单了——在表示空间中进行贪心式规划，**就是最优的**。

---

我们在多个任务上评估了该方法，其中包括一个“放块入桶”的任务：智能体需要拾起一个方块并放入蓝色桶中。这是一个非常困难的任务：

* 没有奖励塑形；
* 没有演示数据；
* 完全基于图像进行学习。

我们设置的第一个 baseline 是“目标条件行为克隆（goal-conditioned behavioral cloning）”，这是一个在早期工作中表现良好的简单基线方法。

第二个 baseline 是 TD3 + HER（Hindsight Experience Replay 与时序差分方法结合）。

结果表明，**只有我们的方法成功解决了这个任务**。

我们还在一系列其他操控任务上与基线方法做了对比，发现除了最简单的任务外，我们的方法在所有任务中都取得了更好的表现。

---

欢迎大家来我们的海报区域了解更多实验细节和未来研究方向。谢谢！
