---
创建时间: 2025-七月-26日  星期六, 2:33:12 下午
---
[[Contrastive Learning as Goal-Conditioned Reinforcement Learning]]

# Questions

- [?] 

# 翻译
## Abstract

- 本文中，我们并非是在已有强化学习算法中添加表示学习模块，而是展示了对比学习（contrastive representation learning）方法本身就可以被看作是一类强化学习算法。

- 为此，我们在已有工作的基础上，将对比表示学习方法应用于带有动作标签的轨迹中，并设计方式使得学习到的表示的内积恰好对应于一个目标条件的价值函数（goal-conditioned value function）。

- 我们用这个思路重新解释了一个已有的强化学习方法，其实质就是在做对比学习，并由此提出了一种更简单但效果相当的新方法。


## introduction

已有研究发现，有必要为强化学习算法配备感知相关的损失函数【31, 43, 70, 74, 89, 91, 101, 140】或数据增强技术【72, 74, 117, 136】，从而在实际中将“表示学习问题”与“强化学习问题”分离开来。

考虑到已有文献在函数逼近和状态混叠（state aliasing）条件下对强化学习的分析结果【2, 134, 138】，端到端学习表示之所以脆弱也就不足为奇了【72, 136】：算法需要好的表示才能驱动强化学习的学习过程，而强化学习过程又需要学习好的表示

因此，我们提出一个问题：我们是否可以设计出一种不依赖于额外感知损失函数的强化学习算法，同时仍然能够学到良好的表示？

与其使用强化学习算法来“兼顾”表示学习任务，我们选择反其道而行之：使用一个表示学习算法来同时解决某类强化学习问题，尤其是目标条件强化学习（goal-conditioned RL）问题。

目标条件强化学习是一个研究广泛的课题【6, 14, 21, 62, 80, 119】，从表示学习的角度来看，它具有很大吸引力，因为它可以完全在自监督（self-supervised）方式下进行，不需要人工指定的奖励函数。

**对比学习（contrastive learning）方法，借助同一条轨迹中的观察值（如【95, 109】中所做）进行学习，同时将动作作为附加输入考虑在内（见图 1）**


**直观上来看，对比学习在这种设定下就类似于一个目标条件的价值函数：相近的状态会有相似的表示，不可达的状态则有不同的表示。**

==我们在本文中展示了如何使用对比表示学习来完成目标条件强化学习。我们在形式上将所学表示与最大化奖励的目标关联起来，表明表示之间的内积就对应于一个价值函数。==





# Answers

## GPT_Answers


## DS_Answers


## Other_Answers


# Codes

```python

```


# FootNotes
