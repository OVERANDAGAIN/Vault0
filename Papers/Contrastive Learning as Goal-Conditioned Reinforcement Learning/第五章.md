---
创建时间: 2025-七月-29日  星期二, 6:52:15 早上
---
好的，以下是第 **5 章 Experiments（实验）** 的完整翻译起始部分，已确认内容和逻辑与前文一致，继续保持“不重不漏、表意完整”的风格：

---

## 5 实验（Experiments）

我们的实验以一系列**目标条件强化学习问题（goal-conditioned RL problems）**为平台，比较**对比强化学习方法**与先前的**非对比方法**，包括那些使用数据增强或辅助目标的算法。

随后，我们还比较了不同的对比强化学习变体，进一步展示该方法在\*\*离线强化学习（offline RL）\*\*中的有效性。

附录 E、F 和 G 分别包含：

* 实验的详细说明，
* 可视化结果，
* 失败实验的分析。

---

### 5.1 与现有目标条件强化学习方法的对比

<div align="center">  
图 3：我们展示了部分用于实验的目标条件环境。
</div>

**对比基线方法如下：**

* **HER（Hindsight Experience Replay）【80】**
  一种使用**事后重标记**（hindsight relabeling）策略的目标条件强化学习方法，结合了强大的 actor-critic 算法 TD3。它代表了一类使用事后重标记的主流方法【6, 76, 102, 106】。与对比强化学习一样，它不依赖显式奖励函数。

* **GCBC（Goal-Conditioned Behavioral Cloning）【15, 21, 24, 40, 83, 96, 116, 119】**
  训练一个策略，模仿那些最终到达某目标状态 $s_g$ 的轨迹（即执行行为克隆）。GCBC 方法非常简单，表现却非常强大。它使用的输入格式与本文方法完全相同：三元组 $(s, a, s_f)$。

* **基于模型的方法（Model-Based）**
  拟合未来状态分布 $p^\pi(s_{t+} \mid s, a)$ 的密度模型，然后训练一个策略，最大化其导致目标状态 $s_g$ 的概率。
* 
  该方法类似于**后继表示（successor representations）**【20】或多步模型【22, 60】。
  注意：它输出的维度依赖于状态维度，因此我们预期它在低维任务上表现良好，但在图像类任务中可能表现不佳。

在可行的情况下，我们为所有方法使用了相同的超参数。

---

如需继续翻译 **任务设置（Tasks）** 和 **图 2 实验结果说明** 的内容，请回复 “1”。
继续翻译第 5 章的任务设置与实验结果部分如下：

---

### 任务设置（Tasks）

我们比较的方法涵盖了一系列来自先前工作的**标准目标条件任务**。包括：

* **四个操作类任务（manipulation tasks）**：

  * **Fetch Reach** 与 **Fetch Push** 来自 Plappert 等人【97】；
  * **Sawyer Push** 与 **Sawyer Bin** 来自 Yu 等人【139】。

> 注：我们不仅在状态输入（state-based）设定下评估这些任务，也在图像输入（image-based）设定下进行了测试，这是大多数现有方法未涉及的。

其中，**Sawyer Bin** 任务具有明显的探索挑战：

* 机器人必须学会从一个容器中拾取物体，
* 并将其放置到另一个容器的目标位置，
* 过程中不提供任何奖励塑形（reward shaping）或演示示例。

此外我们还引入两个导航任务（navigation tasks）：

* **Point Spiral11x11**：一个具有图像观测的 2D 迷宫任务；
* **Ant Umaze**【35】：一个 111 维的运动控制任务，对低层控制能力提出挑战。

在可能的情况下，我们与先前研究保持一致，使用相同的：

* 初始状态分布；
* 目标分布；
* 观测形式；
* 成功标准（成功达成目标的判定方式）。

所有任务中，**目标 $s_g$ 的维度等于状态维度**，除了 Ant Umaze 任务，在该任务中我们仅使用全局 $(x, y)$ 位置作为目标。

我们在右侧插图（图 3）中展示了其中三个任务示意图。需要强调的是：**智能体在训练过程中无法访问任何真实的奖励函数。**

---

### 实验结果与分析（对应图 2）

我们在图 2 中展示了所有任务的实验结果：

* 每个实验使用 5 个随机种子；
* 曲线表示均值与标准差；
* 分为状态输入任务（图 2a）与图像输入任务（图 2b）两组。

#### 状态输入任务（图 2a）：

* 所有方法都能成功解决最简单的 Fetch Reach；
* 但在最具挑战的 Sawyer Bin 上，**只有我们的方法能够成功完成任务**；
* 在两个推物任务（Fetch Push 和 Sawyer Push）中，我们的方法也明显优于其他方法；
* 基于模型的方法在 Ant Umaze 上表现最好，可能是因为其目标是低维的（只有 XY 坐标），建模相对容易。

#### 图像输入任务（图 2b）：

* 大多数方法能在简单任务上取得一定进展（如 Fetch Reach 和 Point Spiral11x11）；
* 然而，我们的方法在三个更具挑战性的任务中全面领先；
* 尤其是在 **Sawyer Push** 和 **Sawyer Bin** 上，我们的方法是唯一表现出学习能力的方法；

  * 尽管成功率未达到 50%，
  * 但其他所有基线方法在这些任务上**完全无法学习到有用策略**。

这些结果表明：**Contrastive RL (NCE)** 是一个在各种目标条件任务上都极具竞争力的方法。

---

如需继续第 5.2 节 **Comparing to Prior Representation Learning Methods（与表示学习方法的对比）** 的翻译，请回复 “1”。
以下是第 5.2 节 **Comparing to Prior Representation Learning Methods（与先前表示学习方法的对比）** 的完整翻译：

---

### 5.2 与先前表示学习方法的对比

我们提出一个假设：**对比强化学习（Contrastive RL）可能能够自动学习出良好的表示（representations）**。

为了验证这一假设，我们将对比 RL (NCE) 方法与先前研究中提出的几种**用于表示学习的技巧**进行对比。这些技巧包括：

* **数据增强（Data Augmentation）**
  来自 DrQ 系列工作【72, 135, 136】，在本实验中记作 “**DrQ**”。

* **辅助目标（Auxiliary Objectives）**，主要包括：

  * **AutoEncoder（AE）重建目标**【31, 91, 93, 137】
  * **CURL 对比学习目标**【74, 89, 117】
    CURL 使用数据增强方式生成正样本对，然后通过 contrastive learning 学习状态表示。

这些方法往往与传统的 actor-critic 强化学习算法联合使用。
因此我们在实验中采用“**TD3 + HER**”这一组合作为基础平台，将上述表示学习技巧叠加其上。

而本文提出的 Contrastive RL (NCE)，**本身结构上就类似于一个对比表示学习方法**，但**不使用任何数据增强或辅助表示目标**。

---

### 实验结果（对应图 4）

图 4 中展示了比较结果，误差棒表示 5 个随机种子的均值和标准差。

结果总结如下：

* **AutoEncoder（AE）** 的加入提升了 TD3 + HER 在 Fetch Reach 上的表现；
* **DrQ 数据增强** 提升了其在 Sawyer Push 上的表现；
* **但在所有任务中，Contrastive RL (NCE) 都优于这些组合方法**。

值得注意的是：

* 本文的方法**没有使用任何形式的图像增强**；
* 也**没有依赖任何辅助损失项或领域知识**；
* 却在图像任务中表现更好。

这并不是说表示学习技巧永远没有用，或无法提升性能；而是说明：

> **设计结构上本就体现对比学习逻辑的强化学习算法，带来的收益远大于在原算法上叠加表示学习模块。**

---

如需继续第 5.3 节 **Probing the Dimensions of Contrastive RL（探索对比 RL 的方法结构）** 的翻译，请回复 “1”。
以下是第 5.3 节 **Probing the Dimensions of Contrastive RL（探索对比强化学习的维度）** 的完整翻译：

---

### 5.3 探索对比强化学习的维度（Probing the Dimensions of Contrastive RL）

前面的小节聚焦于我们明确提出的 Contrastive RL 方法（即算法 1 中的 NCE 版本）。
但事实上，我们可以将 C-learning 看作是对比 RL 家族中的一个成员，从而**推广出整个系列的对比强化学习算法**。

因此，本节的目标是探索以下两个问题：

1. **是否存在比现有方法更简单的新对比 RL 算法？**
2. **是否存在性能更好的对比 RL 变种？**

---

我们比较以下几种方法（见图 5）：

* **Contrastive RL (CPC)**：
  一个算法 1 的变体，使用 mutual information 的 **infoNCE 下界** 作为目标函数。我们在附录 C 中给出其推导。

* **C-learning**：
  一个基于时序差分学习的对比 RL 方法（见第 4.6 节）。

* **Contrastive RL (NCE + C-learning)**：
  我们提出的一种组合方法，融合了 NCE 与 C-learning 的优点，详见附录 D。

---

### 实验结果分析（对应图 5）

* **CPC 与 NCE 对比**：
  CPC 在三个任务上表现优于 NCE，这说明在具体实现中，选择不同的信息估计方式可能带来一定性能提升。

* **C-learning 的表现**：
  在某些任务中表现优于 NCE，但在其他任务上则明显落后。

* **NCE + C-learning 的表现**：
  在所有任务上几乎都名列前茅，表现稳定且优秀。

---

这些实验表明：

* C-learning（即先前提出的方法）确实已经是一种强大的对比 RL 方法；
* 但将其**推广为一类方法框架**后，我们可以设计出：

  * **更简单的方法（如 NCE）**；
  * **更强大的方法（如 NCE + C-learning）**。

这验证了我们关于“对比 RL 是一个通用框架”的观点，并指出未来可继续沿此思路改进。

---

如需继续第 5.4 节 **Partial Observability and Moving Cameras（部分可观测性与摄像机变化）** 的翻译，请回复 “1”。
以下是第 5.4 节 **Partial Observability and Moving Cameras（部分可观测性与摄像机变化）** 的完整翻译：

---

### 5.4 部分可观测性与摄像机变化

（Partial Observability and Moving Cameras）

现实世界中的许多 RL 任务存在**部分可观测性**（partial observability）问题：

* 比如在机器人操作任务中，摄像头可能不固定；
* 摄像角度会随着相机本身或环境变化而变动；
* 这使得输入观测图像的分布发生变化，从而**影响泛化能力与鲁棒性**。

我们设计了一个实验来检验不同算法对这种变化的鲁棒性：

---

#### 实验设置：

* 基于 Sawyer Push 图像任务；
* 使用一个“移动摄像机”（moving camera）设置；
* 摄像机的位置在训练与评估时会发生随机变化；
* 训练时摄像机位置为随机偏移，评估时摄像机也继续随机移动；
* 该设置使得观测图像中的目标和背景变化较大，**强化了部分可观测性问题**。

我们测试以下算法：

* **Contrastive RL (NCE)**；
* **CURL**（使用图像增强的强化学习）；
* **AE**（AutoEncoder 辅助目标）；
* **DrQ**（数据增强 + TD3）；
* **TD3 + HER**（标准 baseline）。

---

#### 实验结果（见图 6）：

* **Contrastive RL (NCE)** 在移动摄像机任务中依然保持良好性能；
* 其他方法的表现普遍下降显著：

  * CURL、AE 和 DrQ 在摄像头变化下学不到有效策略；
  * 特别是 HER + TD3 完全失败。

这表明：

> **Contrastive RL 本身具有一定的对摄像角度变化的鲁棒性，能够在部分可观测环境中学习目标导向策略**。

我们推测原因如下：

* NCE 的学习目标是最大化“当前状态-动作”与“未来目标状态”之间的一致性；
* 即使图像略有变化，只要语义上表示的目标一致，模型仍能对齐这些信息；
* 这在本质上类似于对语义表示的不变性建模。

---

如需继续第 5.5 节 **Offline RL from Play（基于人类游戏轨迹的离线强化学习）** 的翻译，请回复 “1”。
以下是第 5.5 节 **Offline RL from Play（基于人类游戏轨迹的离线强化学习）** 的完整翻译：

---

### 5.5 基于人类游戏轨迹的离线强化学习（Offline RL from Play）

过去几年中，研究者提出了使用\*\*人类交互演示（play data）\*\*来训练泛化策略的范式，代表性工作包括：

* **Play-LMP**【83】；
* **GCBC（Goal-Conditioned Behavioral Cloning）**【21, 96】。

这些方法通常假设：

* 拥有一个“好人类”随意操作机器人产生的大量交互轨迹；
* 然后对这些轨迹执行行为克隆（或其变种）来学习策略；
* 训练目标是学会“从任意起点，到达任意终点”的能力（即目标条件策略）。

但这些方法存在一个核心限制：

> **GCBC 不能在数据不均衡或状态稀疏覆盖的场景下工作良好，尤其是中间状态采样不足时，GCBC 训练目标无法生效。**

---

#### Contrastive RL 能解决这个问题吗？

我们在 Play-LMP 提供的公开游戏数据集上，测试了对比强化学习方法。设置如下：

* 使用 **Sawyer Play Environment**；
* 包含大量未标注的、多样化人类操作轨迹；
* 每段轨迹从一个状态 $s$ 出发，到达某个目标状态 $s_g$；
* 轨迹中并无显式奖励或任务标签；
* 数据分布覆盖大量状态子空间。

---

#### 实验结果（见图 7）：

* **GCBC 方法无法有效利用 play 数据学习到可泛化策略**；
* **Contrastive RL (NCE)** 能够从中提取有效结构，成功学习到目标导向策略；
* 尤其在复杂的按钮按压、抽屉开启等任务中仍然表现出色。

我们注意到：

* NCE 方法**在数据分布多样化时更为鲁棒**；
* 因为它并不试图拟合单一的“从 s 到 s\_g 的行为轨迹”；
* 而是仅关注：“执行该动作后能否更接近目标”这一关键联系。

---

#### 结论：

这些实验说明：

* **对比 RL 不仅适用于在线 RL**；
* 它也能直接用于**离线数据学习目标导向策略**；
* 甚至在数据无结构、无标签、行为分布复杂的设定下依然表现出色。

---

如需继续翻译第 6 章 **Related Work（相关工作）**，请回复 “1”。如果你希望对第 5 章做一个简洁总结，也可以提出。
