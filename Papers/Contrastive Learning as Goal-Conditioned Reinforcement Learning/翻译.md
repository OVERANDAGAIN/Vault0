---
创建时间: 2025-七月-28日  星期一, 9:38:58 晚上
---
以下是论文第 3 节 **Preliminaries（预备知识）** 的完整翻译：

---

### 3 预备知识（Preliminaries）

#### 目标条件强化学习（Goal-conditioned reinforcement learning）

目标条件强化学习问题定义如下：

* 状态：$s_t \in \mathcal{S}$
* 动作：$a_t$
* 初始状态分布：$p_0(s)$
* 状态转移动态：$p(s_{t+1} \mid s_t, a_t)$
* 目标分布：$p_g(s_g)$
* 每个目标的奖励函数：$r_g(s, a)$

该问题等价于**多任务强化学习（multi-task RL）**【5, 44, 120, 129, 139】，其中每个任务对应于达成某个目标状态。遵循以往工作【10, 14, 28, 103】，我们将奖励函数定义为**在下一时间步达成目标的概率密度**：

$$
r_g(s_t, a_t) = (1 - \gamma)p(s_{t+1} = s_g \mid s_t, a_t)
\tag{1}
$$

这个奖励函数的好处是**不需要人为定义的距离度量函数**（与如【6】中的方法不同）。尽管我们的方法不会显式估计这个奖励函数，但我们将在分析中用到它。

对于一个目标条件策略 $\pi(a \mid s, s_g)$，我们用 $\pi(\tau \mid s_g)$ 表示在给定目标 $s_g$ 下采样一个无限长轨迹 $\tau = (s_0, a_0, s_1, a_1, \cdots)$ 的概率。我们定义其期望奖励目标与 Q 函数如下：

$$
\max_\pi \ \mathbb{E}_{p_g(s_g), \pi(\tau \mid s_g)} \left[ \sum_{t=0}^{\infty} \gamma^t r_g(s_t, a_t) \right],
\quad
Q^\pi_{s_g}(s, a) = \mathbb{E}_{\pi(\tau \mid s_g)} \left[ \sum_{t'=t}^{\infty} \gamma^{t'-t} r_g(s_{t'}, a_{t'}) \mid s_t = s, a_t = a \right]
\tag{2}
$$

直观上，这个目标表示我们从目标分布中采样一个目标 $s_g$，然后优化策略以到达该目标并尽可能长时间地停留在该目标状态。

最后，

我们定义\*\*折扣状态占据分布（discounted state occupancy measure）\*\*为【54, 142】：

$$
p^\pi(\cdot \mid \cdot, s_g)(s_{t+} = s) = (1 - \gamma) \sum_{t=0}^\infty \gamma^t \, p^\pi_t(s_t = s)
\tag{3}
$$

其中 $p^\pi_t(s)$ 表示策略 $\pi$ 在执行 $t$ 步之后到达状态 $s$ 的概率密度。

**如何采样这个分布？** 很简单：

* 首先从几何分布（参数 $1 - \gamma$）中采样一个时间偏移量 $t \sim \text{GEOM}(1 - \gamma)$
* 然后观察策略在第 $t$ 步所访问到的状态

我们用 $s_{t+}$ 表示从该分布中采样的状态。

由于我们的方法会结合多个策略的经验，因此我们还定义了**平均稳态分布（average stationary distribution）**：

$$
p^\pi(\cdot \mid \cdot)(s_{t+} = s \mid s, a) = \int p^\pi(\cdot \mid \cdot, s_g)(s_{t+} = s \mid s, a) \cdot p^\pi(s_g \mid s, a) \, ds_g
$$

其中 $p^\pi(s_g \mid s, a)$ 是在给定状态-动作对 $(s, a)$ 时被指派目标 $s_g$ 的概率。

这个分布等价于策略 $\pi(a \mid s)$ 所诱导的分布：

$$
\int \pi(a \mid s, s_g) \cdot p^\pi(s_g \mid s) \, ds_g
\quad
$$

---

如需继续第 3 节剩余部分，即 **对比表示学习（Contrastive representation learning）** 的翻译，请回复 “1”。
以下是第 3 节中 **Contrastive Representation Learning（对比表示学习）** 的完整翻译：

---

#### 对比表示学习（Contrastive representation learning）

对比表示学习方法【16, 45, 52, 53, 61, 77, 84, 86, 87, 121, 123, 128】的输入是一对**正样本**与**负样本**，其目标是学习一种表示，使得正样本对具有相似的表示，而负样本对具有不相似的表示。

我们用 $(u, v)$ 表示一对输入样本（例如，$u$ 是一张图像，$v$ 是该图像的增强版本）。**正样本对**来自联合分布 $p(u, v)$，而**负样本对**则来自边缘分布的乘积 $p(u)p(v)$。

我们将使用一种基于\*\*二分类（binary classification）\*\*的目标函数【77, 86, 87, 94】。
设：

$$
f(u, v) = \phi(u)^\top \psi(v)
$$

其中，$f(u, v)$ 表示输入对 $(u, v)$ 的相似度，$\phi(u)$ 和 $\psi(v)$ 是两个编码器函数生成的表示。我们称 $f$ 为**判别函数（critic function）**，其值域为 $(-\infty, \infty)$。

我们采用 **NCE-binary**【84】目标函数，也被称为 **InfoMax**【53】：

$$
\max_{f(u,v)} \ \mathbb{E}_{(u, v^+) \sim p(u, v),\ v^- \sim p(v)} \left[
\log \sigma\left( f(u, v^+) \right)
+ \log \left(1 - \sigma\left( f(u, v^-) \right) \right)
\right]
\tag{4}
$$

其中：

* $v^+$ 是与 $u$ 相关的正样本，
* $v^-$ 是不相关的负样本，
* $\sigma(\cdot)$ 表示 Sigmoid 函数。

---

Comment：

* 这里的 $f(u, v) = \phi(u)^\top \psi(v)$ 是典型的对比学习打分函数，它通过内积度量相似性。
* NCE-binary 目标可视为一个二分类问题：判断一对样本是“正配对”还是“负配对”。

---


---

### 4 将对比学习视为一种强化学习算法

本节将展示如何使用**对比表示学习**来**直接执行目标条件强化学习**。

核心思想（见引理 4.1）是：**对比学习可以估计某个策略和奖励函数对应的 Q 函数**。为了证明这一点，我们将：

1. 首先把 Q 函数与状态占据分布建立联系（见 4.1 节）；
2. 然后将最优判别函数（critic function）与状态占据分布建立联系（见 4.2 节）。

这个结果使我们能够提出一种新的目标条件强化学习算法，其基础就是对比学习。与以往不同，我们不是将对比学习**附加在现有强化学习算法之上**，而是将其作为**独立的强化学习方法**。

这个框架**推广了 C-learning【28】方法**，不仅为其良好性能提供了合理解释，也引出了**更简单、性能更好的新方法**。

---

#### 4.1 将 Q 函数转化为概率的形式

本节为后续结果奠定基础，从**概率的角度**解释目标条件强化学习。

我们在式（2）中定义的期望奖励目标与 Q 函数，其实等价于“在未来某一时刻到达目标状态的概率密度”。具体来说：

**命题 1（从奖励到概率）**
对于目标条件奖励函数 $r_g$（即式 1），其对应的 Q 函数等价于：
在给定状态 $s$、动作 $a$ 下，最终到达目标 $s_g$ 的**折扣状态占据概率**：

$$
Q^\pi_{s_g}(s, a) = p^\pi(\cdot \mid \cdot, s_g)(s_{t+} = s_g \mid s, a)
\tag{5}
$$

证明见附录 B。

这个命题的意义在于：

* 它让我们可以更容易地分析目标条件问题；
* 它说明：只要我们有方法估计某状态在未来出现的概率（比如使用对比学习），我们就能估计 Q 函数。

---

如需继续第 4.2 节 **Contrastive Learning Estimates a Q-Function（对比学习如何估计 Q 函数）** 的翻译，请回复 “1”。
以下是第 4.2 节 **Contrastive Learning Estimates a Q-Function（对比学习如何估计 Q 函数）** 的完整翻译：

---

#### 4.2 对比学习估计 Q 函数

我们将通过**精心选择输入 $u$ 和 $v$**，使用对比学习来学习一个价值函数。

* 第一个输入 $u$ 对应一个状态-动作对：

  $$
  u = (s_t, a_t) \sim p(s, a)
  $$

  实际中，这些样本来自经验回放池（replay buffer）。

* 包含动作信息是非常关键的，它让我们可以**判断采取哪个动作更可能到达目标未来状态**。

* 第二个变量 $v$ 是一个**未来状态** $s_f$。

对于“正样本对”：

* 未来状态 $s_f$ 从**折扣状态占据分布**中采样：

  $$
  s_f \sim p^\pi(\cdot \mid \cdot)(s_{t+} \mid s_t, a_t)
  $$

对于“负样本对”：

* 未来状态 $s_f$ 从一对随机状态-动作对中采样：

  $$
  s_f \sim p(s_{t+}) = \int p^\pi(\cdot \mid \cdot)(s_{t+} \mid s, a) \cdot p(s, a) \, ds \, da
  $$

在此设定下，对比学习目标函数（即式 4）可以写为：

$$
\max_f \ \mathbb{E}_{(s, a) \sim p(s, a),\ s_f^- \sim p(s_f),\ s_f^+ \sim p^\pi(\cdot \mid \cdot)(s_{t+} \mid s, a)}
\left[ L(s, a, s_f^+, s_f^-) \right]
$$

其中：

$$
L(s, a, s_f^+, s_f^-) =
\log \sigma\left(f(s, a, s_f^+)\right) +
\log \left(1 - \sigma\left(f(s, a, s_f^-)\right)\right)
\tag{6}
$$

其中的判别函数 $f(s, a, s_f) = \phi(s, a)^\top \psi(s_f)$，衡量当前状态-动作对与未来状态的相关性。

这个判别函数在这里就类似于一个 Q 函数：**它估计从状态 $s$ 采取动作 $a$ 后达到未来状态 $s_f$ 的“相关性”或“可能性”**。

由于贝叶斯最优的判别函数是状态占据分布的函数【84】：

$$
f^*(s, a, s_g) = \log \left( \frac{p^\pi(\cdot \mid \cdot)(s_{t+} = s_g \mid s, a)}{p(s_g)} \right)
$$

我们可以将其用于表达 Q 函数：

---

**引理 4.1**

最优判别函数 $f^*$ 所优化的目标（即式 6）所对应的形式为：

$$
\exp(f^*(s, a, s_f)) = \frac{1}{p(s_f)} \cdot Q^\pi_{(\cdot \mid \cdot)}(s, a)
$$

也就是说，**判别函数就是 Q 函数乘以一个常数项** $1 / p(s_f)$。

---

这个结果意味着：

* 判别函数本质上是一个**未归一化的概率密度模型（unnormalized density model）**；
* 其中的 $p(s_g)$ 是配分函数（partition function）；
* 对比学习的一大优点是**无需估计配分函数**【45】；
* 在强化学习中，**这个常数项在选择动作时可以被忽略**，因为它不会影响策略选择。

实验还表明，当目标 $s_g$ 是**低维的**，学习一个归一化密度模型效果很好；但在高维任务中则效果变差。

请注意，这个引理中的 Q 函数是 $Q^\pi_{(\cdot \mid \cdot)s_f}(s, a)$，而不是条件在特定目标上的 $Q^\pi_{(\cdot \mid s_f)}(s, a)$。
这是因为：**判别函数在训练时融合了来自多个不同目标下收集的经验**。

以往基于行为克隆的目标条件方法【21, 40, 83, 119】也会进行类似的经验共享，但**并未分析这些策略与 Q 函数之间的关系**。
我们将在第 4.5 节中证明：在某些假设下，这个判别函数可以作为一个强化学习算法的收敛基础。

---

如需继续第 4.3 节 **Learning the Goal-Conditioned Policy（学习目标条件策略）** 的翻译，请回复 “1”。
以下是第 4.3 节 **Learning the Goal-Conditioned Policy（学习目标条件策略）** 的完整翻译：

---

#### 4.3 学习目标条件策略（Learning the Goal-Conditioned Policy）

学习到的判别函数（critic function）不仅告诉我们一个未来状态的可能性，也**揭示了不同动作如何改变该状态在未来发生的概率**。

因此，要学习一个达到给定目标状态 $s_g$ 的策略，我们只需选择那些**最有可能**导致该状态发生的动作：

$$
\max_{\pi(a \mid s, s_g)} \ \mathbb{E}_{\pi(a \mid s, s_g),\ p(s),\ p(s_g)}\left[ f(s, a, s_f = s_g) \right]
\approx \mathbb{E}_{\pi(a \mid s, s_g),\ p(s),\ p(s_g)} \left[
\log Q^{\pi}_{(\cdot \mid \cdot)}(s, a) - \log p(s_g)
\right]
$$

右侧的近似表达表示我们使用的是次优的判别函数，这在第 4.5 节中将帮助我们在一定假设下证明策略改进（policy improvement）。

---

**实践中的实现方式如下：**

我们将目标条件策略参数化为一个神经网络，该网络以**当前状态 $s$** 和 **目标状态 $s_g$** 为输入，输出一个**动作分布** $\pi(a \mid s, s_g)$。

策略的训练过程为：

1. 从经验回放中采样状态和目标；
2. 用策略从当前状态中采样动作；
3. 计算判别函数的值 $f(s, a, s_g)$；
4. 对策略进行梯度优化，最大化该值。

在图像输入的任务中，我们还在策略目标中添加了**动作熵（entropy）正则项**，以鼓励策略探索。

---

如需继续第 4.4 节 **A Complete Goal-Conditioned RL Algorithm（完整的目标条件强化学习算法）** 的翻译，请回复 “1”。
以下是第 4.4 节 **A Complete Goal-Conditioned RL Algorithm（完整的目标条件强化学习算法）** 的完整翻译：

---

#### 4.4 完整的目标条件强化学习算法

该完整算法的流程是：

1. 使用对比学习（contrastive learning）来拟合判别函数（critic）；
2. 使用公式（7）来更新策略；
3. 不断采集新的数据，循环执行上述两个步骤。

下方的**算法 1** 提供了该方法的 JAX 实现（仅包括 actor 和 critic 的损失部分）：

---

**算法 1：对比强化学习（NCE）方法的 actor 与 critic 损失函数**

```python
from jax.numpy import einsum, eye
from optax import sigmoid_binary_cross_entropy

def critic_loss(states, actions, future_states):
    sa_repr = sa_encoder(states, actions)  # (batch_dim, repr_dim)
    g_repr = g_encoder(future_states)      # (batch_dim, repr_dim)
    logits = einsum('ik,jk->ij', sa_repr, g_repr)  # 点积：<sa_repr[i], g_repr[j]> 对所有 i, j
    return sigmoid_binary_cross_entropy(logits=logits, labels=eye(batch_size))

def actor_loss(states, goals):
    actions = policy.sample(states, goal=goals)  # (batch_size, action_dim)
    sa_repr = sa_encoder(states, actions)        # (batch_dim, repr_dim)
    g_repr = g_encoder(goals)                    # (batch_dim, repr_dim)
    logits = einsum('ik,ik->i', sa_repr, g_repr)  # 点积：<sa_repr[i], g_repr[i]>
    return -1.0 * logits
```

---

其中，critic 的结构是一个内积：

$$
f(s, a, s_g) = \phi(s, a)^\top \psi(s_g)
$$

这种结构带来了**计算效率优势**，因为目标的表示 $\psi(s_g)$ 可以一次性计算并重复使用于正样本和负样本中。这是对比学习中常见的技巧，但在多数目标条件强化学习算法中并未被充分利用。

我们将该方法称为 **Contrastive RL (NCE)**。在附录 C 中，我们还推导出该方法的一个变体：**Contrastive RL (CPC)**，它基于 mutual information 的 infoNCE 下界。

---

**补充说明：**

* Contrastive RL (NCE) 属于**on-policy 算法**，因为它估计的是当前策略下的 Q 函数；
* 不过，**在实际实现中**，我们对每个样本执行多个梯度步更新，因此其**行为接近 off-policy 方法**【39, 47】。

我们会在附录 E 中提供完整实现细节，并发布基于 **ACME【56】与 JAX【12】** 的高效实现。

* 在一块 TPU v2 上，该实现的训练速度为：

  * **状态输入任务：1100 batches/sec**
  * **图像输入任务：105 batches/sec**
* 作为对比，我们自己实现的 DrQ 在相同硬件条件下为：

  * **28 batches/sec（慢了 3.9 倍）**

关于网络结构和超参数设置，请参阅附录 E。

---

如需继续第 4.5 节 **Convergence Guarantees（收敛性保证）** 的翻译，请回复 “1”。
以下是第 4.5 节 **Convergence Guarantees（收敛性保证）** 的完整翻译：

---

#### 4.5 收敛性保证（Convergence Guarantees）

通常来说，对于使用\*\*重标记（relabeling）\*\*的方法来说，提供收敛性保证是很具有挑战性的。

以往大多数工作要么**不提供任何收敛性保证**【6, 21, 22】，要么只在**极为严格的假设条件下**提供有限的理论结果【40, 119】。

为了证明对比强化学习（Contrastive RL）是收敛的，我们引入了一个**额外的过滤步骤**，用于丢弃某些训练样本。具体来说，如果某一段轨迹 $\tau_{i:j} = (s_i, a_i, s_{i+1}, a_{i+1}, \dots, s_j, a_j)$ 在“命令目标” $s_g$ 下的采样概率与“实际达成目标” $s_j$ 下的采样概率差异太大，那么我们就将其排除在训练集之外。

我们定义如下的过滤准则：

$$
\texttt{EXCLUDETRAJ}(\tau_{i:j}) = \delta\left(
\left| \frac{\pi(\tau_{i:j} \mid s_g)}{\pi(\tau_{i:j} \mid s_j)} - 1 \right| > \varepsilon
\right)
$$

也就是说，如果轨迹在 $s_g$ 与 $s_j$ 下的条件概率比值偏离 1 太远，就剔除掉。

尽管这个修改是为了理论证明收敛所必需的，**但我们在附录图 13 中的消融实验表明，这种过滤在实际中反而可能降低性能**，因此**在主文中的实验中我们没有采用这个步骤**。

在此基础上，我们可以证明对比强化学习能够实现近似的策略改进：

---

**引理 4.2（近似策略改进）**
设状态和动作空间是表格型（tabular）的，且判别函数是贝叶斯最优的。

令 $\pi'(a \mid s, s_g)$ 表示经过一轮对比强化学习更新后的目标条件策略，使用上述过滤参数 $\varepsilon$。则该策略相比于初始策略 $\pi$ 至少具有如下性能提升：

$$
\mathbb{E}_{\pi'(\tau \mid s_g)}\left[ \sum_{t=0}^{\infty} \gamma^t r_{s_g}(s_t, a_t) \right]
\geq
\mathbb{E}_{\pi(\tau \mid s_g)}\left[ \sum_{t=0}^{\infty} \gamma^t r_{s_g}(s_t, a_t) \right]
- \frac{2\gamma\varepsilon}{1 - \gamma}
$$

对所有 $s_g \in \{s_g \mid p_g(s_g) > 0\}$ 成立。

证明见附录 B。

---

这个结果意味着，在静态数据集上应用一次对比强化学习过程，就相当于执行了一步近似的策略改进。如果我们不断采集新数据并重复执行对比强化学习，那么这个过程就对应于反复迭代的近似策略改进【可参考文献 9 的引理 6.2】。

---

**总结：**

我们已经证明，通过对输入数据进行恰当选择并应用对比学习，可以构造出一个强化学习算法：

* 它能够学习 Q 函数；
* 在某些假设下，该算法可以收敛至最优策略；
* 并且该方法非常简单，不需要：

  * 多重 Q 函数【39】；
  * 目标网络（target Q-networks）【88】；
  * 数据增强【72, 136】；
  * 辅助目标【74, 137】。

---

如需继续第 4.6 节 **C-learning as Contrastive Learning（将 C-learning 视为对比学习）** 的翻译，请回复 “1”。
以下是第 4.6 节 **C-learning as Contrastive Learning（将 C-learning 视为对比学习）** 的完整翻译：

---

#### 4.6 将 C-learning 视为对比学习（C-learning as Contrastive Learning）

C-learning【28】其实是对比强化学习（Contrastive RL）的一种特例：它的核心思想是训练一个**判别函数**来区分“未来目标”（未来确实达成的目标）和“随机目标”。

与我们提出的 Contrastive RL (NCE) 方法相比，C-learning 使用的是\*\*时序差分学习（temporal difference learning）\*\*来训练这个分类器，而不是对比学习的最大似然或互信息下界。

将 C-learning 视为对比学习的一个特例，有两个重要启发：

1. 它让我们能够理解 C-learning 良好性能的**理论根源**；
2. 它启发我们构造出一系列新的对比强化学习算法，它们在**形式上更简单**，且**性能可能更优**。

在附录 D 中，我们还提出了该系列的一个扩展方法：**Contrastive RL (NCE + C-learning)**，它融合了两种思想，并在多个任务上表现出最强性能。

---

如需继续第 5 节 **Experiments（实验）** 的翻译，请回复 “1”。
