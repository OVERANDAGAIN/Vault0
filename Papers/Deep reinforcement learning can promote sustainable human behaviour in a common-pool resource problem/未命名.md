---
created: 2024-12-18T20
updated: ...
---
[[DRL common-pool resource problem]]

# Questions

- [?] 


# Answers
# Fig.1
![[Pasted image 20250520014050.png]]
Fig. 1 是这篇论文的核心图，结构清晰，涵盖了实验设计、机制流程、对比实验和行为差异等关键信息。下面我将逐部分详细解释，帮助你全面理解：

---

## 🧩 **A 部分：机制运作示意图（左上）**

这是整个游戏的基本流程图，演示了系统机制在一轮游戏中如何分配资源，以及人类玩家如何做出“回馈”。

### 关键要素解析：

* **机制（Mechanism）**：AI 或手动设计的资源分配者，决定每轮给每位玩家多少资源 $e_i$（蓝色花朵）。
* **资源池 $R_t$**：最多为200，初始资源池；如果资源归零就无法恢复。
* **玩家行为**：

  * 每人收到资源 $e_i$；
  * 自主决定回馈 $c_i$，放回资源池；
  * 剩下的部分为私有收益（Surplus，黄色金币）。
* **回馈收益放大因子**：回馈部分被放大1.4倍再注入资源池，模拟“投资回报”机制。

**👉 启发**：这个机制本质上是一个“动态重复信任博弈 + 公共池更新”系统。

---

## 🧠 **B 部分：实验流程设计图（右上）**

展示了论文的完整研究路径与阶段。

### 流程解释：

1. **收集人类数据**：在不同 $w$ 值机制下进行实验，记录行为数据。
2. **行为克隆模型**：使用 imitation learning 训练出能“模仿人类”的虚拟玩家（行为克隆）。
3. **RL 训练 M1 机制**：用这些克隆来训练 RL 社会机制。
4. **可解释机制设计**：提炼 RL 策略，形成“插值机制”。
   5–8. **多个实验验证**：用真人对 RL 机制、比例机制、插值机制进行对比。

**👉 启发**：整个设计是“模拟 ➜ 优化 ➜ 简化 ➜ 对比验证”的循环，兼顾性能与可用性。

---

## 📈 **C 部分：不同机制下的行为对比（下方）**

每列代表一个机制类型（Equal、Proportional、RL Agent），每行是不同信息的可视化。

### 第一行：资源池和奖励曲线（Top row in each panel）

* **蓝线 Pool**：当前资源池 $R_t$ 的大小；
* **红线 Reward**：每轮所有人的总剩余收益（Surplus）。

### 第二到第五行：每位玩家的资源“分配 vs 回馈”轨迹

* **实线 Offers**：收到的资源；
* **虚线 Reciprocation**：回馈给池子的数量；
* **灰区**：玩家被分配为0资源时的回合（代表被“排除在外”）。

---

### 三种机制表现差异解读：

| 机制类型                  | 特征          | 资源池趋势         | 玩家行为趋势                  |
| --------------------- | ----------- | ------------- | ----------------------- |
| Equal Baseline        | 所有人每轮平均分配资源 | 快速枯竭          | 多人长期不回馈（free-rider）导致崩盘 |
| Proportional Baseline | 完全依赖上轮回馈比例  | 资源池长时间存续，但不平等 | 少数人持续贡献，其他人被“永久排除”      |
| RL Agent (M1)         | RL学出的策略     | 高资源池+高奖励      | 动态分配，适度惩罚但会再纳入，保持群体活跃   |

**👉 启发**：RL Agent 学会了更精妙的分配策略——既能激励回馈，又能避免极端不平等和“永久放逐”。

---





# Codes

```python

```


# FootNotes
