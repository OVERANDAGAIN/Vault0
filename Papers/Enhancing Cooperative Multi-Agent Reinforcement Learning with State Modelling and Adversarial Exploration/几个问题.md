---
创建时间: 2025-十一月-25日  星期二, 3:17:50 下午
---
[[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration]]

# Questions

- [?] 

## 第一个问题：关于 $z^{i}$

在文章中的说：

$$
z^{i}\enspace{\sim}\enspace p_{\omega_{i}}(\cdot|o^{i})
$$
但是这里的 $z^{i}$ 实际上是 对 $o^{-i}$ 的表征。(更准确地说，应该是加权后的、全掉冗余信息的 $o^{-i}$ ，即 $\Longrightarrow$ $w^{i}\cdot o^{-i}$)
但是论文中又说 $z^{i}$ 应该是从自己的观测 $o^{i}$ 对去全局状态 `state` 的把握，那么在多个智能体的情况，这里的公式怎么去计算呢？
>也就是 $o^{-i}$ 不只是一个人的观测时，怎么去输入和验证呢？
>是 `concat` ? 还是其他的操作？

### 第二个问题：关于 `loss` 中的ELBO

>In doing so, $SMPE^2$ entails enhanced performance of the joint policy by finding state beliefs $z^i$ w.r.t maximizing the value function. This comes in contrast to other approaches (e.g., (Papoudakis et al., 2021; Papoudakis & Albrecht, 2020; Nguyen et al., 2023; Hernandez-Leal et al., 2019; Fu et al., 2022)) ==**which may suffer from distribution mismatch (Ma et al.), as $z_i$ is disconnected from the policy.**==

>原文中说这句话的意思是什么？
>为什么可以说“本文的方法把中间的表征与policy联系起来了？只是因为在loss中把总体value和ELBO训练目标联系起来了吗？


### 第三个问题：关于第一个问题的解释？
>(a) With it, **==although the target of ED grows linearly with the number of agents,==** only features that can be inferred through $z_i$ remain as part of other agents’ observations in the reconstruction loss.

>原文中说，`ED` 会随着玩家的数目增长，那么是不是说对于每一个`opponent`都有一个ED？
>（原文中已经很清楚地说明了，对于每一个`opponent`都有一个 `AM Filter` )

>那么这里的 `ED` 和` AM Filter` 是一一对应的吗？如果不是，是如何对应的？

### 第四个问题：原文的表述
>To make the embeddings zi variational (and thus probabilistic as in the definition of state modelling), **==following the problem in (2),==** we add the standard KL divergence regularization term, as follows:

>这里所说的 (2) 的问题是指什么？

### 第五个问题：关于新添加的critic如何处理state
>where $\hat{s}= o^i⊕(w^i·o^{−i})$ is the filtered state from the view of agent i’s modelling, with ⊕ meaning **==vector concatenation,==** and superscript for target network.

>这里的 concat 操作具体是怎么做的？举个例子。

### 第六个问题：关于总体训练的loss理解
![[Pasted image 20251125161012.png]]

图片中：
$L_{encodings}$ :
1. $L_{critic}^{w}$ 是 新增的 critic ,目的是使 $w^{i}$ 与policy产生联系
2. $L_{rec}$ 是 监督训练的 $w^{i}$ 
3. $L_{norm}$ 是 防止 $w^{i}$ 训练到0（因为 $L_{rec}$ 中当 $w^{i}$ 都为0时，也是一种解，但是无意义）
4. $L_{KL}$ 是 干啥的？**==这是第四个问题需要先解决的，也就是为什么要引入这个loss==**

$L_{actor}$ :
	**==这里的为什么采取这种样子？需要解释一下。==**

$L_{critic}$ :
	就是policy训练的loss  即 $\xi$ 参数化的

### 第七个问题：原文中的 Adversarial Exploration 的 Intrinsic Reward

![[Pasted image 20251125162005.png]]

>由于这部分小节是在前面那一大段的loss介绍之后的，所以这里的总体reward是怎么使用的？是直接替换掉原始环境的reward吗？


### 第八个问题：关于confidence interval

>Results are averaged over six random seeds, and the primary metric is the unnormalized average episodic reward. **==We report 25-75% confidence intervals==** (as in (Zheng et al., 2021)). Following (Papoudakis et al., 2020), we set the total number of steps to 10M for MPE and LBF, and 40M for RWARE.

>原文中的这一句话是什么意思？

### 第九个问题：ED与AE
原文中提到的ED这样的encoder-decoder架构，和AE和关系吗？

### 第十个问题：single controller
>a single learnable controller setting,

文中经常提到的这个词是什么意思？

### 第十一个问题：MPE-spread 中的离散动作

>Action space: The action space is discrete and involves 5 actions: standing still, moving right, left, up and down.

>为什么在MPE那样看似连续的环境会是采用5个离散动作呢？难道也是网格那样的图吗？


### 第十二个问题：文中的实验环境所示的 observation

文中的实验环境所示的 observation都是一维的张量表示，如何理解？
比如：
1. MPE: $[24,]$
2. LBF: $[11,]$
3. RWARE: $[71,]$



### 第十三个问题
>文中关于这部分的解释很令人费解，描述的也不清楚


![[Pasted image 20251126103031.png]]


### 第十四个问题：
>图片中蓝色的这些有什么区别，是干什么的？

![[Pasted image 20251126104033.png]]

# Answers

## GPT_Answers


## DS_Answers


## Other_Answers


# Codes

```python

```


# FootNotes
