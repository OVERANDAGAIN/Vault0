---
创建时间: 2025-十一月-25日  星期二, 3:17:50 下午
---
[[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration]]

# Questions

- [?] 

## 第一个问题：关于 $z^{i}$

在文章中的说：

$$
z^{i}\enspace{\sim}\enspace p_{\omega_{i}}(\cdot|o^{i})
$$
但是这里的 $z^{i}$ 实际上是 对 $o^{-i}$ 的表征。(更准确地说，应该是加权后的、全掉冗余信息的 $o^{-i}$ ，即 $\Longrightarrow$ $w^{i}\cdot o^{-i}$)
但是论文中又说 $z^{i}$ 应该是从自己的观测 $o^{i}$ 对去全局状态 `state` 的把握，那么在多个智能体的情况，这里的公式怎么去计算呢？
>也就是 $o^{-i}$ 不只是一个人的观测时，怎么去输入和验证呢？
>是 `concat` ? 还是其他的操作？

### 第二个问题：关于 `loss` 中的ELBO

>In doing so, $SMPE^2$ entails enhanced performance of the joint policy by finding state beliefs $z^i$ w.r.t maximizing the value function. This comes in contrast to other approaches (e.g., (Papoudakis et al., 2021; Papoudakis & Albrecht, 2020; Nguyen et al., 2023; Hernandez-Leal et al., 2019; Fu et al., 2022)) ==**which may suffer from distribution mismatch (Ma et al.), as $z_i$ is disconnected from the policy.**==

>原文中说这句话的意思是什么？
>为什么可以说“本文的方法把中间的表征与policy联系起来了？只是因为在loss中把总体value和ELBO训练目标联系起来了吗？


### 第三个问题：关于第一个问题的解释？
>(a) With it, **==although the target of ED grows linearly with the number of agents,==** only features that can be inferred through $z_i$ remain as part of other agents’ observations in the reconstruction loss.

>原文中说，`ED` 会随着玩家的数目增长，那么是不是说对于每一个`opponent`都有一个ED？
>（原文中已经很清楚地说明了，对于每一个`opponent`都有一个 `AM Filter` )

>那么这里的 `ED` 和` AM Filter` 是一一对应的吗？如果不是，是如何对应的？


# Answers

## GPT_Answers


## DS_Answers


## Other_Answers


# Codes

```python

```


# FootNotes
