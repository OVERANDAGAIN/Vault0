---
åˆ›å»ºæ—¶é—´: 2025-åä¸€æœˆ-25æ—¥  æ˜ŸæœŸäºŒ, 3:17:50 ä¸‹åˆ
---
[[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration]]

# Questions

- [?] 

### ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šå…³äº $z^{i}$

åœ¨æ–‡ç« ä¸­çš„è¯´ï¼š

$$
z^{i}\enspace{\sim}\enspace p_{\omega_{i}}(\cdot|o^{i})
$$
ä½†æ˜¯è¿™é‡Œçš„ $z^{i}$ å®é™…ä¸Šæ˜¯ å¯¹ $o^{-i}$ çš„è¡¨å¾ã€‚(æ›´å‡†ç¡®åœ°è¯´ï¼Œåº”è¯¥æ˜¯åŠ æƒåçš„ã€å…¨æ‰å†—ä½™ä¿¡æ¯çš„ $o^{-i}$ ï¼Œå³ $\Longrightarrow$ $w^{i}\cdot o^{-i}$)
ä½†æ˜¯è®ºæ–‡ä¸­åˆè¯´ $z^{i}$ åº”è¯¥æ˜¯ä»è‡ªå·±çš„è§‚æµ‹ $o^{i}$ å¯¹å»å…¨å±€çŠ¶æ€ `state` çš„æŠŠæ¡ï¼Œé‚£ä¹ˆåœ¨å¤šä¸ªæ™ºèƒ½ä½“çš„æƒ…å†µï¼Œè¿™é‡Œçš„å…¬å¼æ€ä¹ˆå»è®¡ç®—å‘¢ï¼Ÿ
>ä¹Ÿå°±æ˜¯ $o^{-i}$ ä¸åªæ˜¯ä¸€ä¸ªäººçš„è§‚æµ‹æ—¶ï¼Œæ€ä¹ˆå»è¾“å…¥å’ŒéªŒè¯å‘¢ï¼Ÿ
>æ˜¯ `concat` ? è¿˜æ˜¯å…¶ä»–çš„æ“ä½œï¼Ÿ


```ad-note

è®ºæ–‡æ˜ç¡®å†™äº†ï¼š

> **the encoder-decoder reconstructs the concatenation of other agentsâ€™ observations.**
> ï¼ˆè§åŸæ–‡ï¼šED grows linearly with number of agentsï¼‰

### æ‰€ä»¥ï¼š

### **å¯¹äº N ä¸ªå…¶ä»–æ™ºèƒ½ä½“ï¼š**

$$
o^{-i} = o^1 \oplus o^2 \oplus \dots \oplus o^{i-1} \oplus o^{i+1} \oplus\dots \oplus o^n
$$

**é€ä¸ªæ‹¼æ¥ï¼ˆvector concatenation âŠ•ï¼‰ï¼Œä¸æ˜¯å¹³å‡ã€ä¸æ˜¯åŠ æƒã€‚**
ä¹‹åå†ç”± **AM Filter** $w^i$ å¯¹ *æ¯ä¸€ç»´* åš soft selectionã€‚

---

### **æœ€ç»ˆ reconstruction loss çš„è¾“å…¥æ˜¯ï¼š**

$$
L_{\text{rec}} = | w_i^{e} \cdot o^{-i} - w_i \cdot \hat{o}^{-i}|^2
$$

```
### ç¬¬äºŒä¸ªé—®é¢˜ï¼šå…³äº `loss` ä¸­çš„ELBO

>In doing so, $SMPE^2$ entails enhanced performance of the joint policy by finding state beliefs $z^i$ w.r.t maximizing the value function. This comes in contrast to other approaches (e.g., (Papoudakis et al., 2021; Papoudakis & Albrecht, 2020; Nguyen et al., 2023; Hernandez-Leal et al., 2019; Fu et al., 2022)) ==**which may suffer from distribution mismatch (Ma et al.), as $z_i$ is disconnected from the policy.**==

>åŸæ–‡ä¸­è¯´è¿™å¥è¯çš„æ„æ€æ˜¯ä»€ä¹ˆï¼Ÿ
>ä¸ºä»€ä¹ˆå¯ä»¥è¯´â€œæœ¬æ–‡çš„æ–¹æ³•æŠŠä¸­é—´çš„è¡¨å¾ä¸policyè”ç³»èµ·æ¥äº†ï¼Ÿåªæ˜¯å› ä¸ºåœ¨lossä¸­æŠŠæ€»ä½“valueå’ŒELBOè®­ç»ƒç›®æ ‡è”ç³»èµ·æ¥äº†å—ï¼Ÿ

```ad-note
è®ºæ–‡ä¸­æ–°å¢ï¼š

### **State Modeling Criticï¼š**

$$
V_k(\hat{s})
$$

å…¶ TD error ç›´æ¥ä½œç”¨äº encoder / filter çš„å‚æ•°ï¼š

$$
L_{w\text{-critic}}=|r_t+\gamma V_{k'}(\hat s_{t+1})-V_k(\hat s_t)|^2
$$

### ğŸ”¥ **å…³é”®ç‚¹ï¼š$z^i$ å‚ä¸ $\hat s$ çš„æ„é€ ï¼Œå› æ­¤ TD loss åå‘ä¼ æ’­åˆ° encoderã€‚**

**å› æ­¤ï¼š$z^i$ å­¦åˆ°çš„æ˜¯ â€œå¯¹æé«˜ value function æœ‰ç”¨çš„ç‰¹å¾â€ã€‚**

è¿™æ˜¯è®ºæ–‡ä¸­å¼ºè°ƒçš„â€œrepresentation ä¸ policy å¼ºè€¦åˆâ€ã€‚
```
### ç¬¬ä¸‰ä¸ªé—®é¢˜ï¼šå…³äºç¬¬ä¸€ä¸ªé—®é¢˜çš„è§£é‡Šï¼Ÿ
>(a) With it, **==although the target of ED grows linearly with the number of agents,==** only features that can be inferred through $z_i$ remain as part of other agentsâ€™ observations in the reconstruction loss.

>åŸæ–‡ä¸­è¯´ï¼Œ`ED` ä¼šéšç€ç©å®¶çš„æ•°ç›®å¢é•¿ï¼Œé‚£ä¹ˆæ˜¯ä¸æ˜¯è¯´å¯¹äºæ¯ä¸€ä¸ª`opponent`éƒ½æœ‰ä¸€ä¸ªEDï¼Ÿ
>ï¼ˆåŸæ–‡ä¸­å·²ç»å¾ˆæ¸…æ¥šåœ°è¯´æ˜äº†ï¼Œå¯¹äºæ¯ä¸€ä¸ª`opponent`éƒ½æœ‰ä¸€ä¸ª `AM Filter` )

>é‚£ä¹ˆè¿™é‡Œçš„ `ED` å’Œ` AM Filter` æ˜¯ä¸€ä¸€å¯¹åº”çš„å—ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œæ˜¯å¦‚ä½•å¯¹åº”çš„ï¼Ÿ

```ad-note
åªæœ‰ä¸€å¥— decoderï¼Œä¸æ˜¯ä¸€ä¸ª opponent ä¸€ä¸ª decoderã€‚

ç”±äº decoder çš„è¾“å…¥è¾“å‡ºç»´åº¦éš agent æ•°é‡å¢åŠ è€Œå¢åŠ ï¼ˆå› ä¸º $o^{-i}$ æ˜¯ concatï¼‰ï¼Œæ‰€ä»¥ï¼š

âœ” **ED çš„é‡æ„ç›®æ ‡ç»´åº¦éšç©å®¶æ•°é‡çº¿æ€§å¢é•¿**
âœ– **ä½†å¹¶ä¸å­˜åœ¨ â€œæ¯ä¸ªå¯¹æ‰‹ä¸€ä¸ª decoderâ€**
```
### ç¬¬å››ä¸ªé—®é¢˜ï¼šåŸæ–‡çš„è¡¨è¿°
>To make the embeddings zi variational (and thus probabilistic as in the definition of state modelling), **==following the problem in (2),==** we add the standard KL divergence regularization term, as follows:

>è¿™é‡Œæ‰€è¯´çš„ (2) çš„é—®é¢˜æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ
```ad-note
è®ºæ–‡ä¸­â€œ(2)â€æŒ‡çš„æ˜¯ï¼š

### **ï¼ˆ2ï¼‰å¼ï¼šState Modelling çš„ optimize objective**

$$
V_{SM}=\max_{\omega,\psi} \mathbb{E}_{p\omega}\big[\mathbb{E}_{\pi\psi}\sum_t \gamma^t r_t \big]
$$

ä¹Ÿå°±æ˜¯è¯´ï¼š

* æ—¢ç„¶ $z^i$ æ˜¯ä½œä¸º â€œstate beliefâ€ çš„æ¦‚ç‡åˆ†å¸ƒ
* å°±å¿…é¡»ä¿è¯å®ƒæ˜¯ **variational / probabilistic** çš„
* å› æ­¤åŠ  KL regularizationï¼Œä½¿å…¶ç¬¦åˆ VAE ç»“æ„ä¸­çš„å˜åˆ†æ¨æ–­è¦æ±‚
```
### ç¬¬äº”ä¸ªé—®é¢˜ï¼šå…³äºæ–°æ·»åŠ çš„criticå¦‚ä½•å¤„ç†state
>where $\hat{s}= o^iâŠ•(w^iÂ·o^{âˆ’i})$ is the filtered state from the view of agent iâ€™s modelling, with âŠ• meaning **==vector concatenation,==** and superscript for target network.

>è¿™é‡Œçš„ concat æ“ä½œå…·ä½“æ˜¯æ€ä¹ˆåšçš„ï¼Ÿä¸¾ä¸ªä¾‹å­ã€‚

### ç¬¬å…­ä¸ªé—®é¢˜ï¼šå…³äºæ€»ä½“è®­ç»ƒçš„lossç†è§£
![[Pasted image 20251125161012.png]]

å›¾ç‰‡ä¸­ï¼š
$L_{encodings}$ :
1. $L_{critic}^{w}$ æ˜¯ æ–°å¢çš„ critic ,ç›®çš„æ˜¯ä½¿ $w^{i}$ ä¸policyäº§ç”Ÿè”ç³»
2. $L_{rec}$ æ˜¯ ç›‘ç£è®­ç»ƒçš„ $w^{i}$ 
3. $L_{norm}$ æ˜¯ é˜²æ­¢ $w^{i}$ è®­ç»ƒåˆ°0ï¼ˆå› ä¸º $L_{rec}$ ä¸­å½“ $w^{i}$ éƒ½ä¸º0æ—¶ï¼Œä¹Ÿæ˜¯ä¸€ç§è§£ï¼Œä½†æ˜¯æ— æ„ä¹‰ï¼‰
4. $L_{KL}$ æ˜¯ å¹²å•¥çš„ï¼Ÿ**==è¿™æ˜¯ç¬¬å››ä¸ªé—®é¢˜éœ€è¦å…ˆè§£å†³çš„ï¼Œä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆè¦å¼•å…¥è¿™ä¸ªloss==**

$L_{actor}$ :
	**==è¿™é‡Œçš„ä¸ºä»€ä¹ˆé‡‡å–è¿™ç§æ ·å­ï¼Ÿéœ€è¦è§£é‡Šä¸€ä¸‹ã€‚==**

$L_{critic}$ :
	å°±æ˜¯policyè®­ç»ƒçš„loss  å³ $\xi$ å‚æ•°åŒ–çš„

### ç¬¬ä¸ƒä¸ªé—®é¢˜ï¼šåŸæ–‡ä¸­çš„ Adversarial Exploration çš„ Intrinsic Reward

![[Pasted image 20251125162005.png]]

>ç”±äºè¿™éƒ¨åˆ†å°èŠ‚æ˜¯åœ¨å‰é¢é‚£ä¸€å¤§æ®µçš„lossä»‹ç»ä¹‹åçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ€»ä½“rewardæ˜¯æ€ä¹ˆä½¿ç”¨çš„ï¼Ÿæ˜¯ç›´æ¥æ›¿æ¢æ‰åŸå§‹ç¯å¢ƒçš„rewardå—ï¼Ÿ


### ç¬¬å…«ä¸ªé—®é¢˜ï¼šå…³äºconfidence interval

>Results are averaged over six random seeds, and the primary metric is the unnormalized average episodic reward. **==We report 25-75% confidence intervals==** (as in (Zheng et al., 2021)). Following (Papoudakis et al., 2020), we set the total number of steps to 10M for MPE and LBF, and 40M for RWARE.

>åŸæ–‡ä¸­çš„è¿™ä¸€å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

### ç¬¬ä¹ä¸ªé—®é¢˜ï¼šEDä¸AE
åŸæ–‡ä¸­æåˆ°çš„EDè¿™æ ·çš„encoder-decoderæ¶æ„ï¼Œå’ŒAEå’Œå…³ç³»å—ï¼Ÿ

### ç¬¬åä¸ªé—®é¢˜ï¼šsingle controller
>a single learnable controller setting,

æ–‡ä¸­ç»å¸¸æåˆ°çš„è¿™ä¸ªè¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

### ç¬¬åä¸€ä¸ªé—®é¢˜ï¼šMPE-spread ä¸­çš„ç¦»æ•£åŠ¨ä½œ

>Action space: The action space is discrete and involves 5 actions: standing still, moving right, left, up and down.

>ä¸ºä»€ä¹ˆåœ¨MPEé‚£æ ·çœ‹ä¼¼è¿ç»­çš„ç¯å¢ƒä¼šæ˜¯é‡‡ç”¨5ä¸ªç¦»æ•£åŠ¨ä½œå‘¢ï¼Ÿéš¾é“ä¹Ÿæ˜¯ç½‘æ ¼é‚£æ ·çš„å›¾å—ï¼Ÿ


### ç¬¬åäºŒä¸ªé—®é¢˜ï¼šæ–‡ä¸­çš„å®éªŒç¯å¢ƒæ‰€ç¤ºçš„ observation

æ–‡ä¸­çš„å®éªŒç¯å¢ƒæ‰€ç¤ºçš„ observationéƒ½æ˜¯ä¸€ç»´çš„å¼ é‡è¡¨ç¤ºï¼Œå¦‚ä½•ç†è§£ï¼Ÿ
æ¯”å¦‚ï¼š
1. MPE: $[24,]$
2. LBF: $[11,]$
3. RWARE: $[71,]$



### ç¬¬åä¸‰ä¸ªé—®é¢˜
>æ–‡ä¸­å…³äºè¿™éƒ¨åˆ†çš„è§£é‡Šå¾ˆä»¤äººè´¹è§£ï¼Œæè¿°çš„ä¹Ÿä¸æ¸…æ¥š


![[Pasted image 20251126103031.png]]


### ç¬¬åå››ä¸ªé—®é¢˜ï¼š
>å›¾ç‰‡ä¸­è“è‰²çš„è¿™äº›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œæ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ

![[Pasted image 20251126104033.png]]

# Answers

## GPT_Answers


## DS_Answers


## Other_Answers


# Codes

```python

```


# FootNotes
