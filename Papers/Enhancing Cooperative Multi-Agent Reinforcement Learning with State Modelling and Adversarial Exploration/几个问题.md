---
åˆ›å»ºæ—¶é—´: 2025-åä¸€æœˆ-25æ—¥  æ˜ŸæœŸäºŒ, 3:17:50 ä¸‹åˆ
---
[[Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration]]

# Questions

- [?] 

## ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šå…³äº $z^{i}$

åœ¨æ–‡ç« ä¸­çš„è¯´ï¼š

$$
z^{i}\enspace{\sim}\enspace p_{\omega_{i}}(\cdot|o^{i})
$$
ä½†æ˜¯è¿™é‡Œçš„ $z^{i}$ å®é™…ä¸Šæ˜¯ å¯¹ $o^{-i}$ çš„è¡¨å¾ã€‚(æ›´å‡†ç¡®åœ°è¯´ï¼Œåº”è¯¥æ˜¯åŠ æƒåçš„ã€å…¨æ‰å†—ä½™ä¿¡æ¯çš„ $o^{-i}$ ï¼Œå³ $\Longrightarrow$ $w^{i}\cdot o^{-i}$)
ä½†æ˜¯è®ºæ–‡ä¸­åˆè¯´ $z^{i}$ åº”è¯¥æ˜¯ä»è‡ªå·±çš„è§‚æµ‹ $o^{i}$ å¯¹å»å…¨å±€çŠ¶æ€ `state` çš„æŠŠæ¡ï¼Œé‚£ä¹ˆåœ¨å¤šä¸ªæ™ºèƒ½ä½“çš„æƒ…å†µï¼Œè¿™é‡Œçš„å…¬å¼æ€ä¹ˆå»è®¡ç®—å‘¢ï¼Ÿ
>ä¹Ÿå°±æ˜¯ $o^{-i}$ ä¸åªæ˜¯ä¸€ä¸ªäººçš„è§‚æµ‹æ—¶ï¼Œæ€ä¹ˆå»è¾“å…¥å’ŒéªŒè¯å‘¢ï¼Ÿ
>æ˜¯ `concat` ? è¿˜æ˜¯å…¶ä»–çš„æ“ä½œï¼Ÿ


```ad-note

è®ºæ–‡æ˜ç¡®å†™äº†ï¼š

> **the encoder-decoder reconstructs the concatenation of other agentsâ€™ observations.**
> ï¼ˆè§åŸæ–‡ï¼šED grows linearly with number of agentsï¼‰

### æ‰€ä»¥ï¼š

### **å¯¹äº N ä¸ªå…¶ä»–æ™ºèƒ½ä½“ï¼š**

$$
o^{-i} = o^1 \oplus o^2 \oplus \dots \oplus o^{i-1} \oplus o^{i+1} \oplus\dots \oplus o^n
$$

**é€ä¸ªæ‹¼æ¥ï¼ˆvector concatenation âŠ•ï¼‰ï¼Œä¸æ˜¯å¹³å‡ã€ä¸æ˜¯åŠ æƒã€‚**
ä¹‹åå†ç”± **AM Filter** $w^i$ å¯¹ *æ¯ä¸€ç»´* åš soft selectionã€‚

---

### **æœ€ç»ˆ reconstruction loss çš„è¾“å…¥æ˜¯ï¼š**

$$
L_{\text{rec}} = | w_i^{e} \cdot o^{-i} - w_i \cdot \hat{o}^{-i}|^2
$$

```
## ç¬¬äºŒä¸ªé—®é¢˜ï¼šå…³äº `loss` ä¸­çš„ELBO

>In doing so, $SMPE^2$ entails enhanced performance of the joint policy by finding state beliefs $z^i$ w.r.t maximizing the value function. This comes in contrast to other approaches (e.g., (Papoudakis et al., 2021; Papoudakis & Albrecht, 2020; Nguyen et al., 2023; Hernandez-Leal et al., 2019; Fu et al., 2022)) ==**which may suffer from distribution mismatch (Ma et al.), as $z_i$ is disconnected from the policy.**==

>åŸæ–‡ä¸­è¯´è¿™å¥è¯çš„æ„æ€æ˜¯ä»€ä¹ˆï¼Ÿ
>ä¸ºä»€ä¹ˆå¯ä»¥è¯´â€œæœ¬æ–‡çš„æ–¹æ³•æŠŠä¸­é—´çš„è¡¨å¾ä¸policyè”ç³»èµ·æ¥äº†ï¼Ÿåªæ˜¯å› ä¸ºåœ¨lossä¸­æŠŠæ€»ä½“valueå’ŒELBOè®­ç»ƒç›®æ ‡è”ç³»èµ·æ¥äº†å—ï¼Ÿ

```ad-note
è®ºæ–‡ä¸­æ–°å¢ï¼š

### **State Modeling Criticï¼š**

$$
V_k(\hat{s})
$$

å…¶ TD error ç›´æ¥ä½œç”¨äº encoder / filter çš„å‚æ•°ï¼š

$$
L_{w\text{-critic}}=|r_t+\gamma V_{k'}(\hat s_{t+1})-V_k(\hat s_t)|^2
$$

###  **$z^i$ å‚ä¸ $\hat s$ çš„æ„é€ ï¼Œå› æ­¤ TD loss åå‘ä¼ æ’­åˆ° encoderã€‚**

**å› æ­¤ï¼š$z^i$ å­¦åˆ°çš„æ˜¯ â€œå¯¹æé«˜ value function æœ‰ç”¨çš„ç‰¹å¾â€ã€‚**

è¿™æ˜¯è®ºæ–‡ä¸­å¼ºè°ƒçš„â€œrepresentation ä¸ policy å¼ºè€¦åˆâ€ã€‚
```
## ç¬¬ä¸‰ä¸ªé—®é¢˜ï¼šå…³äºç¬¬ä¸€ä¸ªé—®é¢˜çš„è§£é‡Šï¼Ÿ
>(a) With it, **==although the target of ED grows linearly with the number of agents,==** only features that can be inferred through $z_i$ remain as part of other agentsâ€™ observations in the reconstruction loss.

>åŸæ–‡ä¸­è¯´ï¼Œ`ED` ä¼šéšç€ç©å®¶çš„æ•°ç›®å¢é•¿ï¼Œé‚£ä¹ˆæ˜¯ä¸æ˜¯è¯´å¯¹äºæ¯ä¸€ä¸ª`opponent`éƒ½æœ‰ä¸€ä¸ªEDï¼Ÿ
>ï¼ˆåŸæ–‡ä¸­å·²ç»å¾ˆæ¸…æ¥šåœ°è¯´æ˜äº†ï¼Œå¯¹äºæ¯ä¸€ä¸ª`opponent`éƒ½æœ‰ä¸€ä¸ª `AM Filter` )

>é‚£ä¹ˆè¿™é‡Œçš„ `ED` å’Œ` AM Filter` æ˜¯ä¸€ä¸€å¯¹åº”çš„å—ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œæ˜¯å¦‚ä½•å¯¹åº”çš„ï¼Ÿ

```ad-note
åªæœ‰ä¸€å¥— decoderï¼Œä¸æ˜¯ä¸€ä¸ª opponent ä¸€ä¸ª decoderã€‚

ç”±äº decoder çš„è¾“å…¥è¾“å‡ºç»´åº¦éš agent æ•°é‡å¢åŠ è€Œå¢åŠ ï¼ˆå› ä¸º $o^{-i}$ æ˜¯ concatï¼‰ï¼Œæ‰€ä»¥ï¼š

âœ” **ED çš„é‡æ„ç›®æ ‡ç»´åº¦éšç©å®¶æ•°é‡çº¿æ€§å¢é•¿**
âœ– **ä½†å¹¶ä¸å­˜åœ¨ â€œæ¯ä¸ªå¯¹æ‰‹ä¸€ä¸ª decoderâ€**
```
## ç¬¬å››ä¸ªé—®é¢˜ï¼šåŸæ–‡çš„è¡¨è¿°
>To make the embeddings zi variational (and thus probabilistic as in the definition of state modelling), **==following the problem in (2),==** we add the standard KL divergence regularization term, as follows:

>è¿™é‡Œæ‰€è¯´çš„ (2) çš„é—®é¢˜æ˜¯æŒ‡ä»€ä¹ˆï¼Ÿ
```ad-note
è®ºæ–‡ä¸­â€œ(2)â€æŒ‡çš„æ˜¯ï¼š

### **ï¼ˆ2ï¼‰å¼ï¼šState Modelling çš„ optimize objective**

$$
V_{SM}=\max_{\omega,\psi} \mathbb{E}_{p\omega}\big[\mathbb{E}_{\pi\psi}\sum_t \gamma^t r_t \big]
$$

ä¹Ÿå°±æ˜¯è¯´ï¼š

* æ—¢ç„¶ $z^i$ æ˜¯ä½œä¸º â€œstate beliefâ€ çš„æ¦‚ç‡åˆ†å¸ƒ
* å°±å¿…é¡»ä¿è¯å®ƒæ˜¯ **variational / probabilistic** çš„
* å› æ­¤åŠ  KL regularizationï¼Œä½¿å…¶ç¬¦åˆ VAE ç»“æ„ä¸­çš„å˜åˆ†æ¨æ–­è¦æ±‚
```
## ç¬¬äº”ä¸ªé—®é¢˜ï¼šå…³äºæ–°æ·»åŠ çš„criticå¦‚ä½•å¤„ç†state
>where $\hat{s}= o^iâŠ•(w^iÂ·o^{âˆ’i})$ is the filtered state from the view of agent iâ€™s modelling, with âŠ• meaning **==vector concatenation,==** and superscript for target network.

>è¿™é‡Œçš„ concat æ“ä½œå…·ä½“æ˜¯æ€ä¹ˆåšçš„ï¼Ÿä¸¾ä¸ªä¾‹å­ã€‚

## ç¬¬å…­ä¸ªé—®é¢˜ï¼šå…³äºæ€»ä½“è®­ç»ƒçš„lossç†è§£
![[Pasted image 20251125161012.png]]

å›¾ç‰‡ä¸­ï¼š
$L_{encodings}$ :
1. $L_{critic}^{w}$ æ˜¯ æ–°å¢çš„ critic ,ç›®çš„æ˜¯ä½¿ $w^{i}$ ä¸policyäº§ç”Ÿè”ç³»
2. $L_{rec}$ æ˜¯ ç›‘ç£è®­ç»ƒçš„ $w^{i}$ 
3. $L_{norm}$ æ˜¯ é˜²æ­¢ $w^{i}$ è®­ç»ƒåˆ°0ï¼ˆå› ä¸º $L_{rec}$ ä¸­å½“ $w^{i}$ éƒ½ä¸º0æ—¶ï¼Œä¹Ÿæ˜¯ä¸€ç§è§£ï¼Œä½†æ˜¯æ— æ„ä¹‰ï¼‰
4. $L_{KL}$ æ˜¯ å¹²å•¥çš„ï¼Ÿ**==è¿™æ˜¯ç¬¬å››ä¸ªé—®é¢˜éœ€è¦å…ˆè§£å†³çš„ï¼Œä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆè¦å¼•å…¥è¿™ä¸ªloss==**
```ad-note

### ğŸŸ¢ **ç»“è®ºï¼š KL loss çš„ä½œç”¨æ€»ç»“**

| ä½œç”¨                            | æ˜¯å¦å…³é”®                             |
| ----------------------------- | -------------------------------- |
| ä¿è¯ $z^i$ æ˜¯ probability belief | âœ” å¿…è¦ï¼ˆstate modelling definitionï¼‰ |
| ç»´æŒ embedding ç©ºé—´ç¨³å®š             | âœ” ç¨³å®šè®­ç»ƒ                           |
| é¿å…è¿‡æ‹Ÿåˆå’Œ posterior collapse     | âœ”                                |
| ç¨³å®š SimHash æ¢ç´¢å¥–åŠ±               | âœ” è®ºæ–‡å®éªŒä¸­éå¸¸é‡è¦                      |

```
$L_{actor}$ :
	**==è¿™é‡Œçš„ä¸ºä»€ä¹ˆé‡‡å–è¿™ç§æ ·å­ï¼Ÿéœ€è¦è§£é‡Šä¸€ä¸‹ã€‚==**
```ad-note
åŸæ–‡å…¬å¼ï¼š

$$
L_{\text{actor}}(\psi_i)
=-\beta_H \cdot H(\pi_{\psi_i}(a_t^i | h_t^i, z_t^i))
-\log \pi_{\psi_i}(a_t^i,|,h_t^i, z_t^i)\cdot (r_t + V_\xi^\pi(s_{t+1}) - V_\xi^\pi(s_t))
$$

è¿™æ˜¯ä¸€ä¸ª **æ ‡å‡† A2C / Advantage Actor-Critic çš„ actor loss**ï¼Œå…¶æ¥æºæ˜¯ï¼š

### **ğŸ“Œ Actor loss = - log Ï€(a|s)  Â· Advantage**

$$
L_{\text{policy-gradient}}
= -\log\pi(a_t|s_t)\cdot A_t
$$

åœ¨ SMPEÂ² ä¸­ï¼Œ
$s_t$ æ›¿æ¢æˆ agent çš„ belief + RNN çŠ¶æ€ï¼š$(h_t^i, z_t^i)$

è€Œ advantage ç”¨ TD(0)ï¼š

$$
A_t = r_t + V_\xi(s_{t+1}) - V_\xi(s_t)
$$

æ‰€ä»¥ actor loss å®Œæ•´å†™æ³•æ˜¯ï¼š

$$
L_{\text{actor}} = -\mathbb{E}[\log \pi(a_t|h^i_t,z^i_t)\cdot A_t] - \beta_H H(\pi)
$$

---

## ğŸ§  ä¸ºä»€ä¹ˆè¦æœ‰ç†µé¡¹ $H(\pi)$ï¼Ÿ

ä¸ºäº† encourage explorationï¼Œåœ¨ actor-critic ä½“ç³»ä¸­æ˜¯æ ‡å‡†åšæ³•ï¼š

* é˜²æ­¢ç­–ç•¥è¿‡æ—© collapse
* ä¿æŒè¡Œä¸ºå¤šæ ·æ€§

è®ºæ–‡ä¸­ä½¿ç”¨çš„ $\beta_H$ æ­£æ˜¯ entropy coefficientã€‚

```
$L_{critic}$ :
	å°±æ˜¯policyè®­ç»ƒçš„loss  å³ $\xi$ å‚æ•°åŒ–çš„

## ç¬¬ä¸ƒä¸ªé—®é¢˜ï¼šåŸæ–‡ä¸­çš„ Adversarial Exploration çš„ Intrinsic Reward

![[Pasted image 20251125162005.png]]

>ç”±äºè¿™éƒ¨åˆ†å°èŠ‚æ˜¯åœ¨å‰é¢é‚£ä¸€å¤§æ®µçš„lossä»‹ç»ä¹‹åçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ€»ä½“rewardæ˜¯æ€ä¹ˆä½¿ç”¨çš„ï¼Ÿæ˜¯ç›´æ¥æ›¿æ¢æ‰åŸå§‹ç¯å¢ƒçš„rewardå—ï¼Ÿ

```ad-note

åŸæ–‡ï¼š

> Agent *i* uses the modified reward
> â€ƒ$\tilde r_t^i = (r_t^i + \beta \hat r_t^i)$

è¿™é‡Œçš„æ„æ€éå¸¸æ˜ç¡®ï¼š

### ğŸ‘‰ **ä¸æ˜¯æ›¿ä»£åŸ rewardï¼Œè€Œæ˜¯ â€œåŠ æƒåŠ ä¸Šâ€ intrinsic rewardã€‚**

ä¹Ÿå°±æ˜¯è¯´ï¼š

* ç¯å¢ƒçœŸæ­£çš„ rewardï¼š$r_t^i$
* SMPEÂ² çš„æ¢ç´¢å¥–åŠ±ï¼ˆSimHash-based novelty rewardï¼‰ï¼š$\hat r_t^i$
* æœ€ç»ˆ RL ç”¨æ¥è®­ç»ƒ actor/critic çš„ reward æ˜¯å®ƒä»¬çš„å’Œã€‚

**è¿™æ˜¯ standard exploration bonus ç»“æ„**ï¼Œç±»ä¼¼ï¼š

* curiosity (ICM/NDIGO)
* RND
* count-based exploration
* EMCã€EOI ç­‰æ–¹æ³•

```
## ç¬¬å…«ä¸ªé—®é¢˜ï¼šå…³äºconfidence interval

>Results are averaged over six random seeds, and the primary metric is the unnormalized average episodic reward. **==We report 25-75% confidence intervals==** (as in (Zheng et al., 2021)). Following (Papoudakis et al., 2020), we set the total number of steps to 10M for MPE and LBF, and 40M for RWARE.

>åŸæ–‡ä¸­çš„è¿™ä¸€å¥è¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

## ç¬¬ä¹ä¸ªé—®é¢˜ï¼šEDä¸AE
åŸæ–‡ä¸­æåˆ°çš„EDè¿™æ ·çš„encoder-decoderæ¶æ„ï¼Œå’ŒAEå’Œå…³ç³»å—ï¼Ÿ
```ad-note

### ğŸ”´ ä¸åŒç‚¹ï¼ˆæœ¬æ–‡ ED = æ›´å¤æ‚çš„ AEï¼‰

| é¡¹              | ä¼ ç»Ÿ AE         | æœ¬æ–‡ ED                          |
| -------------- | ------------- | ------------------------------ |
| é‡æ„ç›®æ ‡           | é‡æ„è‡ªå·±çš„è¾“å…¥ $o^i$ | é‡æ„ **å…¶ä»– agent çš„è§‚æµ‹** $o^{-i}$   |
| Encoder è¾“å…¥     | $o^i$         | $o^i$                          |
| Decoder è¾“å‡º     | $\hat o^i$    | $\hat o^{-i}$ï¼ˆconcat + filterï¼‰ |
| æ˜¯å¦ variational | å¯é€‰            | âœ”å¿…é¡»ï¼ˆVAEï¼‰                       |
| æ˜¯å¦ä¸ RL è”åŠ¨      | å¦             | âœ”critic å¼•å¯¼è¡¨ç¤ºå­¦ä¹                  |
| æ˜¯å¦å¸¦ filter     | å¦             | âœ”AM Filter                     |

---

### ğŸ“Œ å› æ­¤ï¼š

> æœ¬æ–‡çš„ ED = **VAE + multi-target reconstruction + RL-driven gradient**
> æœ¬è´¨æ˜¯ä¸€ä¸ªå¸¦éšå˜é‡çš„ã€é¢å‘ MARL çŠ¶æ€å»ºæ¨¡çš„ Autoencoderã€‚

```
## ç¬¬åä¸ªé—®é¢˜ï¼šsingle controller
>a single learnable controller setting,

æ–‡ä¸­ç»å¸¸æåˆ°çš„è¿™ä¸ªè¯æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
```ad-note

æ–‡ä¸­å¤šæ¬¡è¯´åˆ°ï¼š

> a single learnable controller setting

å®ƒæŒ‡çš„æ˜¯ MARL ä¸­ä¸€ç§ **å‚æ•°å…±äº«ï¼ˆparameter sharingï¼‰** æˆ– **é›†ä¸­å¼æ§åˆ¶å™¨ï¼ˆsingle policy controlling all agentsï¼‰** çš„è®¾å®šã€‚

### ğŸ“Œ æ„æ€æ˜¯ï¼š

**æ‰€æœ‰ agent å…±äº«åŒä¸€ä¸ª policy ç½‘ç»œï¼ˆcontrollerï¼‰**ï¼Œè¾“å…¥ä¸åŒï¼Œè¾“å‡ºå„è‡ªåŠ¨ä½œã€‚

è¿™åœ¨ MPE/LBF/RWARE å¸¸ç”¨ï¼š

* ç¯å¢ƒæ˜¯å¯¹ç§°çš„
* æ¯ä¸ª agent çš„è§’è‰²ä¸€æ ·
* å‚æ•°å…±äº«æé«˜æ•ˆç‡ & ç¨³å®šæ€§

```
## ç¬¬åä¸€ä¸ªé—®é¢˜ï¼šMPE-spread ä¸­çš„ç¦»æ•£åŠ¨ä½œ

>Action space: The action space is discrete and involves 5 actions: standing still, moving right, left, up and down.

>ä¸ºä»€ä¹ˆåœ¨MPEé‚£æ ·çœ‹ä¼¼è¿ç»­çš„ç¯å¢ƒä¼šæ˜¯é‡‡ç”¨5ä¸ªç¦»æ•£åŠ¨ä½œå‘¢ï¼Ÿéš¾é“ä¹Ÿæ˜¯ç½‘æ ¼é‚£æ ·çš„å›¾å—ï¼Ÿ

```ad-note

MPEï¼ˆMulti-Agent Particle Environmentï¼‰ç¡®å®æ˜¯ä¸€ä¸ª **è¿ç»­ 2D ç©ºé—´**ã€‚

ä½†åŸå§‹ MPE å…è®¸ **ä¸¤ç§ action space è®¾è®¡**ï¼š

| Action type | æè¿°                                        |
| ----------- | ----------------------------------------- |
| continuous  | 2D åŠ›/é€Ÿåº¦æ§åˆ¶ï¼Œä¾‹å¦‚ (-1~1)^2                     |
| discrete    | 5 actionsï¼šstay / up / down / left / right |

è®ºæ–‡é€‰ç”¨çš„æ˜¯ï¼š

### ğŸ‘‰ **ç¦»æ•£åŠ¨ä½œç‰ˆæœ¬ï¼ˆæ ‡å‡† MARL benchmark ç‰ˆæœ¬ï¼‰**

åŸå› ï¼š

1. ç¦»æ•£åŠ¨ä½œæ›´é€‚åˆ A2C / COMA / QMIX ç­‰ baseline
2. é¿å…è¿ç»­æ§åˆ¶å¸¦æ¥çš„ policy variance
3. MPE çš„åä½œæ€§è´¨ä¸ä¾èµ–ç²¾ç»†åŠ›å­¦ï¼Œç¦»æ•£åŠ¨ä½œå·²è¶³å¤Ÿä½“ç° coordination

---

### ğŸ“Œ **MPE å¹¶ä¸æ˜¯ç½‘æ ¼ï¼Œåªæ˜¯ actions æ˜¯ç¦»æ•£æ–¹å‘ç§»åŠ¨ï¼Œæœ¬è´¨ä»æ˜¯è¿ç»­ 2D åæ ‡ç¯å¢ƒ**ã€‚

ä¾‹å¦‚ï¼šæ‰§è¡Œ â€œmove rightâ€ = ä»¥å›ºå®šé€Ÿåº¦å‘å³ç§»åŠ¨ Î”xã€‚

è¿™ä¸ªåŠ¨ä½œç©ºé—´è®¾è®¡æ˜¯æ–‡çŒ®ä¸­æœ€å¸¸ç”¨ç‰ˆæœ¬ã€‚
```
## ç¬¬åäºŒä¸ªé—®é¢˜ï¼šæ–‡ä¸­çš„å®éªŒç¯å¢ƒæ‰€ç¤ºçš„ observation

æ–‡ä¸­çš„å®éªŒç¯å¢ƒæ‰€ç¤ºçš„ observationéƒ½æ˜¯ä¸€ç»´çš„å¼ é‡è¡¨ç¤ºï¼Œå¦‚ä½•ç†è§£ï¼Ÿ
æ¯”å¦‚ï¼š
1. MPE: $[24,]$
2. LBF: $[11,]$
3. RWARE: $[71,]$

```ad-note

è¿™äº› observation æœ¬è´¨ä¸Šæ˜¯ï¼š

> æŠŠç¯å¢ƒä¸­å±€éƒ¨è§‚æµ‹ã€åœ°å›¾ patchã€ç‰©ä½“å±æ€§ã€é˜Ÿå‹å±æ€§å…¨éƒ¨ encode æˆä¸€ä¸ª flattened 1D å‘é‡ã€‚

**ä¸æ˜¯è¡¨æ ¼ã€ä¸æ˜¯æŒ‰ç»´åº¦åˆ†å—ï¼Œæ˜¯â€œåºåˆ—å¼çš„ feature vectorâ€ã€‚**

```

## ç¬¬åä¸‰ä¸ªé—®é¢˜: å¯è§†åŒ–å±•ç¤ºAM Filter
>æ–‡ä¸­å…³äºè¿™éƒ¨åˆ†çš„è§£é‡Šå¾ˆä»¤äººè´¹è§£ï¼Œæè¿°çš„ä¹Ÿä¸æ¸…æ¥š


![[Pasted image 20251126103031.png]]


## ç¬¬åå››ä¸ªé—®é¢˜ï¼šæ˜“æ··æ·†çš„å‚æ•°
>å›¾ç‰‡ä¸­è“è‰²çš„è¿™äº›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œæ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ

![[Pasted image 20251126104033.png]]

# Answers

## GPT_Answers


## DS_Answers


## Other_Answers


# Codes

```python

```


# FootNotes
