---
created: 2024-12-24T14:26
updated: 2024-12-24-15.
---
[[GAMMA]]

# Questions


# Answers
```ad-seealso
对于没接触过多重检验或统计知识的人来说，可以用以下简单的方式描述 **Holm-Bonferroni Correction**：

---

### **问题背景：为什么需要这个方法？**
假设你有多个问题需要同时判断真假，比如：
- $A$：这个药有没有效果？
- $B$：这个方法能不能提高效率？
- $C$：这个训练是不是有效？

每个问题都可能有 5% 的概率出错（即随机认为对的时候其实错了）。如果你同时检验 10 个问题，那么总出错的概率会大大增加（远远超过 5%），可能导致更多“假阳性”（即错把假的结果当成真的）。

**Holm-Bonferroni Correction** 就是用来降低这些出错概率的方法。

---

### **简单类比：Holm-Bonferroni Correction 像“分段门槛”**

1. **假设**你有几个问题（或结果），每个问题都有一个可能出错的概率 $p$-值：
   - 比如 $p_1 = 0.01, p_2 = 0.02, p_3 = 0.05$，这些值越小，表示结果越可能是真实的。

2. **按从小到大的顺序整理这些概率**：
   - 把所有 $p$-值排个队，最小的排在前面，比如：$p_1, p_2, p_3$。

3. **设置一个逐步提升的门槛**：
   - 第一个问题的门槛是 $\alpha / 3$（总允许出错概率除以剩下的未检查问题数量）；
   - 第二个问题的门槛是 $\alpha / 2$；
   - 第三个问题的门槛是 $\alpha / 1$。

4. **依次检查每个问题的概率是否低于门槛**：
   - 如果某个问题的 $p$-值低于当前门槛，就认为结果显著（拒绝原假设）。
   - 一旦遇到不通过门槛的问题，后面的问题都不再通过。

---

### **具体情境例子**
假设你同时检验 3 个问题，显著性水平（$\alpha$）设为 0.05，结果的 $p$-值是：
$$p_1 = 0.01, \; p_2 = 0.03, \; p_3 = 0.04.$$

**检查步骤：**
1. 排序：$p_1 = 0.01, p_2 = 0.03, p_3 = 0.04$。
2. 设置门槛：
   - 对第一个问题：门槛是 $0.05 / 3 = 0.0167$。
   - 对第二个问题：门槛是 $0.05 / 2 = 0.025$。
   - 对第三个问题：门槛是 $0.05 / 1 = 0.05$。
3. 比较：
   - $p_1 = 0.01 < 0.0167$：通过，认为显著。
   - $p_2 = 0.03 > 0.025$：不通过，停止检验。
   - $p_3$ 不再检查，直接不显著。

**结果：** 只有第一个问题显著，其他问题不显著。

---

### **通俗总结**
Holm-Bonferroni Correction 是一种“分段式”的方法：
1. 给每个问题设置一个不同的门槛（越往后越宽松）。
2. 从最小的 $p$-值开始依次检查，直到遇到不通过的为止。
3. 它可以在控制整体出错率的情况下，尽可能保留更多的显著结果。
```
## GPT_Answers
**Holm-Bonferroni Correction** 是一种用于控制多重比较中 **家庭错误率（Family-Wise Error Rate, FWER）** 的方法，比经典的 Bonferroni 修正更具统计效率。它在多重假设检验中通过一种逐步校正（stepwise correction）的方法减少过于保守的控制，同时仍然维持错误率的控制。

---

### **1. 为什么需要 Holm-Bonferroni Correction？**

在多重假设检验中（如同时对 $m$ 个假设进行显著性检验），每个假设都有一定的犯第一类错误（错误地拒绝 $H_0$）的概率。如果我们单独用显著性水平 $\alpha$ 进行检验，多个检验的总体错误率会显著上升。这被称为 **多重比较问题**。

- **Bonferroni Correction** 的解决方案：将显著性水平 $\alpha$ 均分给每个假设检验（$\alpha/m$）。然而，这种方法可能过于保守，导致假阳性率（Type I Error）降低，但假阴性率（Type II Error）提高。

- **Holm-Bonferroni Correction** 是一种更灵活的逐步方法，在保证控制错误率的同时，减少保守性。

---

### **2. Holm-Bonferroni Correction 的方法**

#### 输入：
- $m$：假设检验的数量；
- $p_1, p_2, \dots, p_m$：假设检验的 $p$-值。

#### 步骤：
1. **对 $p$-值排序**：从小到大排序，记为 $p_{(1)}, p_{(2)}, \dots, p_{(m)}$，对应的原假设为 $H_{(1)}, H_{(2)}, \dots, H_{(m)}$。

2. **设定修正阈值**：计算每个假设的修正显著性水平：
   $$\alpha_{(i)} = \frac{\alpha}{m - i + 1}, \quad i = 1, 2, \dots, m$$
   其中 $\alpha$ 是总的显著性水平（如 0.05）。

3. **逐步检验**：
   - 从最小的 $p$-值开始，比较 $p_{(i)}$ 与 $\alpha_{(i)}$：
     - 如果 $p_{(i)} \leq \alpha_{(i)}$，拒绝对应的原假设 $H_{(i)}$，继续检验下一个。
     - 如果 $p_{(i)} > \alpha_{(i)}$，停止检验，所有剩余假设不拒绝。

4. **输出结果**：已拒绝的假设为显著；未拒绝的假设为不显著。

---

### **3. 示例**

#### 假设：
- 5 个假设检验，显著性水平 $\alpha = 0.05$；
- $p$-值：$p_1 = 0.01, p_2 = 0.04, p_3 = 0.03, p_4 = 0.002, p_5 = 0.06$。

#### 步骤：
1. 对 $p$-值排序：
   $$p_{(1)} = 0.002, p_{(2)} = 0.01, p_{(3)} = 0.03, p_{(4)} = 0.04, p_{(5)} = 0.06$$

2. 修正显著性水平：
   - $\alpha_{(1)} = \frac{\alpha}{m} = \frac{0.05}{5} = 0.01$；
   - $\alpha_{(2)} = \frac{0.05}{4} = 0.0125$；
   - $\alpha_{(3)} = \frac{0.05}{3} = 0.0167$；
   - $\alpha_{(4)} = \frac{0.05}{2} = 0.025$；
   - $\alpha_{(5)} = \frac{0.05}{1} = 0.05$。

3. 比较 $p$-值与阈值：
   - $p_{(1)} = 0.002 \leq \alpha_{(1)} = 0.01$：拒绝 $H_{(1)}$；
   - $p_{(2)} = 0.01 \leq \alpha_{(2)} = 0.0125$：拒绝 $H_{(2)}$；
   - $p_{(3)} = 0.03 > \alpha_{(3)} = 0.0167$：停止检验。

4. 结果：
   - $H_{(1)}$ 和 $H_{(2)}$ 被拒绝；
   - $H_{(3)}, H_{(4)}, H_{(5)}$ 不被拒绝。

---

### **4. Holm-Bonferroni Correction 的优点**
1. **更高的统计效率**：相比 Bonferroni 方法，Holm-Bonferroni 方法对较小的 $p$-值更宽松（修正的阈值较大），从而减少假阴性。
2. **控制家庭错误率（FWER）**：保证对多重检验中整体显著性水平的严格控制。

---

### **5. Holm-Bonferroni 和其他方法的对比**

| 方法                      | 特点                                     | 适用场景                            |
|---------------------------|------------------------------------------|-------------------------------------|
| Bonferroni Correction     | 简单直接，但过于保守                    | 假设检验数量少，结果差异显著时      |
| Holm-Bonferroni Correction| 更灵活，不那么保守                      | 假设检验数量较多，需要提高统计效率  |
| Benjamini-Hochberg (BH)   | 控制假发现率（FDR），对统计效率更宽松    | 假阳性容忍度高且假设数量较多时      |

---

### **6. 总结**
Holm-Bonferroni Correction 是一种逐步校正方法，用于在多重假设检验中控制家庭错误率。它比 Bonferroni 更灵活，在确保错误率控制的同时提高了统计效率。适用于假设检验数量较多、结果差异可能不大的情境，是一种实用的多重比较校正技术。

## Other_Answers


# Codes

```python

```


# FootNotes
