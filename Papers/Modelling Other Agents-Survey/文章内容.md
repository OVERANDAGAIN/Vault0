---
created: 2024-12-18T20
updated: ...
---
[[Autonomous Agents Modelling Other Agents-A Comprehensive Survey and Open Problems]]

# Questions

- [?] 


# 重点摘抄
4.1 策略重建（Policy Reconstruction）
4.1.1 条件动作频率（Conditional Action Frequencies）
>但单一分布无法捕捉依赖交互历史的复杂行为。要提升此方法的能力，关键在于基于历史中的某些信息对动作分布进行条件化。

此类基于条件频率的方法也被用于**不完全信息的扩展形式博弈**（如扑克）：

* 由于玩家拥有私有信息（如手牌）和公共信息（如桌面牌），决策需在“信息集”中进行，即玩家无法区分的状态集合。
* 可以为每个信息集设立独立的 Dirichlet 分布（Southey 等, 2005），并在每次观察到对方动作后更新该分布；
* 也可以基于博弈的纳什均衡解初始化这些分布，然后逐步向实际观察到的动作频率调整（Ganzfried 和 Sandholm, 2011）；
* Billings 等（2004）提出方法：基于完整动作序列来学习频率，并通过一系列越来越粗的抽象级别实现泛化。此外，引入衰减因子，使得新近观察具有更大权重。


### 5.4 决策关联性（Integration with Decision-Making）

对其他智能体的建模并非孤立目的，**其最终作用在于改进自身的决策过程**。因此，我们需要考察建模如何融入智能体的决策模块。常见的集成方式有：

1. **策略调整（Policy Shaping）**：

   * 根据模型预测的对方行为，优化自身策略；
   * 例如预测协作伙伴行为以实现更高效的任务协同。

2. **博弈推理（Game-Theoretic Reasoning）**：

   * 将对手建模结果作为输入，参与博弈均衡分析；
   * 应用于安全博弈、机制设计、策略学习等任务。

3. **规划辅助（Planning Guidance）**：

   * 在路径规划或计划生成中，利用建模结果规避冲突或生成协同策略；
   * 例如在人群导航、多机器人路径协同中。

4. **多模型融合（Model Ensemble in Decision-Making）**：

   * 同时维护多个候选对手模型，根据不同模型结果做联合决策；
   * 可提升鲁棒性，适应对手行为变化。

5. **不确定性评估（Uncertainty-Aware Action Selection）**：

   * 在模型置信度低时选择保守策略；
   * 或通过探索行为主动获取更多观察信息。


### 5.6 方法组合与集成建模（Combinations of Methods and Integrated Modelling）

单一建模方法往往难以应对复杂的现实场景，因此越来越多研究探索方法间的**组合使用**，主要形式包括：

* **并行组合**：同时使用多种方法进行建模，如策略重建 + 类型识别，用于多尺度、多视角建模；
* **层次组合**：将方法按层次组织，如先识别类型，再重建策略；
* **互补组合**：一个方法作为另一方法的组件，如使用递归推理输出作为图模型输入；
* **集成学习**：采用集成学习思想，维护多个模型，对结果加权输出；
* **知识迁移与共享建模**：从一个任务中学习得到的模型被迁移到另一个智能体或任务上。

研究表明，**组合模型在精度、适应性与鲁棒性上明显优于单模型方法**，但也增加了系统复杂性与训练成本。

---

### 5.7 主动建模（Active Modelling）

传统建模方法多为被动建模，即从观测数据中被动学习模型。然而，**主动建模**强调智能体应通过**有策略的行为干预**来提升建模效率与准确性。

关键机制包括：

* **策略性探测**：智能体选择能最大化建模信息增益的动作（如最易区分不同对手类型的测试动作）；
* **主动提问**：在人机交互中，系统主动向人类提问以获取偏好或意图；
* **最小交互学习**（few-shot learning）：通过少量交互快速推断对方策略；
* **博弈实验设计**：通过设计具区分性的任务诱导对方暴露策略。

主动建模可以有效提升建模速度与精度，特别适用于对手类型不明、交互成本高的场景。但也存在探索-利用权衡的问题：太激进的探测可能会破坏任务目标或合作关系。



# Codes

```python

```


# FootNotes
