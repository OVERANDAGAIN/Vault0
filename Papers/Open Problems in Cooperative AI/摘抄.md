---
created: 2024-12-18T20
updated: ...
---
[[Open Problems in Cooperative AI]]

# Questions

- [?] 


# Answers

### 3.3 个体视角与规划者视角（The Individual Perspective and the Planner Perspective）

在合作问题中，我们可以根据关注的对象是谁来区分两种主要视角：

* **个体视角（Individual Perspective）**：聚焦于某个个体如何在合作环境中达成其目标，通常关注提升该个体的合作能力。本节中所讨论的“理解”“沟通”“承诺”等主题多属于这一视角。

* **规划者视角（Planner Perspective）**：聚焦于整个群体的社会福利，思考如何通过政策、制度或集中机制干预群体行为，从而提升整体合作水平。这种视角通常对应于“制度（Institutions）”章节的内容。

个体视角通常关注**机器代理体自身的行为学习与优化**，但也可以扩展为**面向人类的 AI 辅助系统**。规划者视角则更关注**人类群体**，但同样适用于涉及机器的混合群体。

两种视角的选择取决于我们希望解决的问题类型与所处的机会空间：

* 如果我们的任务是帮助某个个体提升其合作能力，那就应当采纳个体视角；
* 如果我们能影响规范、政策、中介机制或制度等“行为塑造因素”，那么就应考虑规划者视角。

==实际上，大多数合作问题**从两种视角都能获益**：==

* 一个**合格的社会规划者**应当理解个体的利益和合作能力，才能更有效地设计干预机制，促进合作；
* 同样，一个**具备合作能力的智能体**也需要具备“社会规划者”的能力，理解整体合作形势，识别哪些群体级别的变化有助于推动所有个体朝向帕累托最优（Pareto frontier）。

> 例如，联合国前秘书长达格·哈马舍尔德曾说：“每一位参与国际事务的人，特别是在联合国工作的官员，都应为‘全人类’发声，而不仅仅从本国利益出发。”【GRW19】

两种视角在**研究方法上也存在差异**：

* **个体视角**更常涉及：代理体优化自身策略、对局部环境建模、预测其他智能体的反应；
* **规划者视角**尤其在面对大规模群体时，更侧重研究**群体行为的平衡与涌现机制**，其方法类似于统计物理中的分析框架【Kra97】：例如研究哪些结构性“推动”可以使系统涌现出有利的均衡。

---


以下是第四章第 4.1 小节 **Understanding（理解）** 的完整中文翻译：

---

### 4.1.1 理解世界（The World）

对于单一智能体而言，首要问题是预测自己行动的结果。这可以通过：

* **无模型方式**：如直接学习最优策略或价值函数；
* **有模型方式**：构建世界模型（model）、环境状态表示等。

强化学习的核心任务即是“理解世界”。

在多智能体环境中，世界的复杂性显著上升。特别是，智能体的行为会暴露其“私有信息”——例如它如何观察环境、拥有何种能力。一个代理体可以通过观察他人的行动，**间接推断对方掌握的世界状态信息**，这种能力对合作而言非常关键。

例如，一个投资人和一位创业者是否共同投资取决于他们对市场前景的理解。如果双方都犹豫不决，可以通过**共享私有信息**来判断是否值得合作。这种合作需要双方能够可信传达信息。

* 若利益足够一致 + 存在沟通渠道 ⇒ 容易达成合作；
* 若缺乏沟通渠道 ⇒ 必须依赖**间接推理**；
* 若缺乏共同利益 ⇒ 即使沟通也可能失真 ⇒ 需要**代价信号（costly signals）**。

==**代价信号经典例子**：学生在高难课程中取得好成绩，这种行为太难伪造，因此能有效传递“我很有能力”的信息【Spe73, Zah77, Bar13】。==

AI 工具可以用于构建“可信的世界理解辅助系统”来帮助人类更好达成合作，如：

* 理解因果关系的推荐系统；
* 基于隐私保护的联合学习系统；
* 多方协同决策中的数据共享平台。

---

### 4.1.2 理解行为（Behaviour）

相比单体环境，在多智能体场景下，一个个体能否预判他人的行为至关重要。

在合作任务中，成功往往取决于对“他人策略”的理解：

* 合作是否值得做（你要相信别人也会合作）；
* 背叛是否可惩罚（你要让别人知道你会回应背叛）。

这种理解在博弈论中以“纳什均衡”的形式被建模：每个智能体的策略都是对其他策略的最优回应。更进一步的建模（如贝叶斯纳什均衡）还需要智能体的信念系统彼此一致。

此外，还有一些比纳什更“合作化”的均衡概念，如：

* 超理性（superrationality）【Hof08, FRD15】；
* 程序均衡（program equilibrium）【Ten04, BCF+14】；
* 凯尔多-希克斯效率（Kaldor-Hicks efficiency）【Hic39, Kal39】：允许通过“假想转移”来达成帕累托改进。

举例：**重复囚徒困境**中，策略如“以牙还牙”（tit-for-tat）就表现出对对方行为的良好理解：

* 初始友好 ⇒ 建立互信；
* 偶尔原谅 ⇒ 宽容失误；
* 可惩罚 ⇒ 威慑背叛。

这一策略之所以有效，关键在于其“可理解性”高，便于对方理解其意图【Axe84】。

理解行为的方式可以划分为：

* **经验性学习**（如进化、自适应）：策略通过不断交互慢慢调整；
* **理论性推理**（如建模、搜索）：通过内部模型对未来进行模拟推演。

例如：

* 一支足球队经过大量训练后形成默契协调；
* “换哪边开车”这种社会规范，最初是自发形成的“协定”，后被法律正式采纳【You96】；
* 更高层次的策略协同，需要对可能策略空间的深入理解，这就涉及建模误差、计算成本等问题。

结论：行为的理解对于**维持**合作均衡来说是基本要求，而**实现**合作均衡则常常需要更强的理论建模与对行为背后驱动因素的推理。

---

### 4.1.3 理解偏好（Preferences）

在很多情况下，预测行为的关键在于理解他人的偏好或目标。这些偏好可能是：

* 显式的（如收益函数）；
* 隐含的（如价值观、欲望、长期目标等）。

如前所述的“单边保证博弈”中，如果 Row 不理解 Column 的偏好，会导致合作失败。

然而，获取他人偏好并不容易：

* 即便在纯共同利益下，偏好表达也可能**计算上难以完成**；
* 人类往往**难以完整描述自己的偏好**【Bos14, Rus19】；
* 在动机冲突下，代理体还有动机**故意误导他人**。

这也是为何\*\*偏好学习（preference learning）\*\*近年来成为 AI 安全与对齐领域的重点研究方向【RDT15, CLB+17】。常用方法包括：

* **逆强化学习（IRL）**：从行为反推偏好【NR+00】；
* **协作式 IRL / 助理游戏**：交互式教学与主动演示【HMRAD16, SFA+20】；
* **偏好比较、目标示例、行为注释等多模态监督方式**【BHL+19a, JMD20】。

值得一提的是：\*\*AI 对齐（Alignment）\*\*研究旨在处理“人机纵向协调”，而 Cooperative AI 主要面向“多方横向协调”。前者关注代理体是否忠诚地执行主人的目标，后者关注各方之间如何达成公平合作。

---

### 4.1.4 理解递归信念（Recursive Beliefs）

到目前为止，我们主要讨论的是“一阶理解”——即关于行为、偏好与信念的直接建模。但在多智能体博弈中，**递归建模**（即“我知道你知道我知道……”）是不可或缺的。

这种能力也被称为**递归心智建模（recursive mind-reading）**，在社交复杂环境中非常关键。例如：

* 实验显示：人类最多可进行 7 层递归思维建模【OKSSP15】；
* AI 研究中，递归建模已在谈判博弈、Hanabi 等任务中有所应用【dWVV15, BFC+20】。

当一个命题被多个智能体共享，且他们都知道这个命题成立，且知道彼此知道它成立，如此无限下去时，我们称之为**共同知识（common knowledge）**。

这种机制在很多现实合作场景中发挥着关键作用，如：

* 金钱系统【Har15, Sea95】；
* 社会动员与革命【Loh94】；
* 近年 AI 研究也在探索如何建立“近似共同知识”【FdWF+18】。

但要注意：递归建模能力本身并非总是有利于合作。

例如：

* 在**有限重复囚徒困境**中，如果双方都“理性”且知道博弈时长，那么理性推理将倒退至首轮互相背叛；而若他们缺乏足够高阶的“互知”，反而可能实现合作。
* 实验发现，在公共品博弈中，强迫参与者快速决策反而更容易合作，说明“本能”可能是合作，而“理性思考”可能导致背叛【RGN12, RPKT+14】。





# Codes

```python

```


# FootNotes
