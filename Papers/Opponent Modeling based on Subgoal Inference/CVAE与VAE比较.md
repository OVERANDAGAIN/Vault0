---
创建时间: 2024-十二月-31日  星期二, 4:04:57 下午
---
[[OMG]]
[[VAE-ELBO loss]][^1]

# Questions

- [?] 


# Answers
## Overall_Answers
### VAE:
$$L^V = -D_{KL}(q(z|x) \| p(z)) + \beta \cdot \mathbb{E}_{q(z|x)}(\log(p(x|z))).$$
隐空间z中采样，生成x $\Longrightarrow$ 参数为 $\theta$
z:单位高斯分布
q:把x和z对应 x $\Longrightarrow$ z     参数为$\phi$


### CVAE:
不是从z采样了，从z|y中采样
$$L^V = -D_{KL}(q(z|x,y) \| p(z|y)) + \mathbb{E}_{q(z|x,y)} (\log p(x|z, y))$$


知乎分析VAE:[^2]
[Site Unreachable](https://zhuanlan.zhihu.com/p/344546057)

以下是图片中的文字转写：

---

### 使用场景

众所周知，VAE 和 CVAE 都是 **生成模型**。从字面上就能看出来，VAE 是纯粹的生成模型，也就是说不能控制它生成什么东西，而 CVAE 可以根据给定的标签进行生成。

比如说同样在手写数字 **MNIST** 数据集上，VAE 是乱来的，它生成的只能是根据单位高斯中采样的隐变量 $z$ 生成的东西。也就是说只能保证生成的是手写数字，但是不能保证生成的是多少。但是 CVAE 就接受输入标签，比如说可以用希望输入的数字作为标签，如果给定标签 6，那么 CVAE 可以生成各种各样的 **6666666**。

这个大概就是 VAE 和 CVAE 之间的区别。

---

## VAE_Answers


---
$p-\theta\;;q- \phi$

整个 VAE 是基于“概率模型”的，我们希望生成的是 $x \in X$，最直接的想法就是直接从 $P(X)$ 中采样。但是问题是我们并不知道 $P(X)$ 的分布是什么样的。所以我们退一步，从一个已知分布的 **隐空间** $Z$ 中进行采样，然后进行生成。生成器是一个神经网络，把它的参数记作 $\theta$。这里假设 $Z$ 服从 **单位高斯分布**：

$$P(X') = \int_Z P_\theta (X'|Z) P(Z)$$

我们需要保证，对于根据 $Z$ 生成出来的结果 $X'$，仍然服从分布 $P(X)$

这个其实很容易实现，对于一个采样出来的隐变量 $z_i$ 和它的对应输入 $x_i$，我们只需要==对生成模型的输出 $\hat{x}_i$ 和 $x_i$ 做 $L2$ Loss 就可以==。那么现在的问题是：

什么是L2 Loss?[^3]

````ad-help
### **什么是 L2 Loss？**

L2 Loss（也称为 **Mean Squared Error (MSE)** 或 **平方误差损失**）是机器学习和深度学习中最常用的回归损失函数之一。它通过计算预测值和目标值之间的平方误差来衡量模型的预测误差。

---

### **公式**

假设有 $n$ 个样本，预测值为 $\hat{y}_i$，真实值为 $y_i$，则 L2 Loss 定义为：

$$L_{2} = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$$

### **公式解释**
1. **误差**：$(\hat{y}_i - y_i)$ 表示预测值和真实值之间的差距。
2. **平方**：平方操作可以放大大的误差，同时避免正负误差相互抵消。
3. **求和并取均值**：将所有样本的误差平方加总，并取平均值，表示整体误差。

---

### **特点**
1. **平滑性**：L2 Loss 对误差进行平方，使其具有平滑的导数性质，在优化时收敛较稳定。
2. **惩罚大误差**：由于平方项的存在，大的误差会被更严重地惩罚，因此 L2 Loss 更关注离群点。
3. **计算简单**：计算过程简单直观，且易于通过梯度下降优化。

---

### **应用场景**
- **回归问题**：用于衡量模型预测的连续值（如房价预测、气温预测）与真实值之间的偏差。
- **生成模型**：如 VAE 或 CVAE 中，用于衡量生成样本与目标样本之间的相似性。

---

### **优缺点**
#### **优点**：
- **数学性质良好**：平滑且连续，有助于模型的优化。
- **常用且直观**：适合大多数回归任务。

#### **缺点**：
- **对离群点敏感**：大误差被平方后会对损失值产生过大的影响，可能导致模型偏向于极端数据点。

---

总结：L2 Loss 是一种简单高效的损失函数，适合大多数回归和生成任务，但在异常值较多的场景中可能需要更鲁棒的损失函数（如 L1 Loss）。
`````


---

如何==把 $z_i$ 和 $x_i$ 进行配对==？？？** 对于训练集中数据 $x_i$，必须找到相应的 $z_i$，才能进行训练。

---

答案仍然是使用神经网络进行 **映射**。这里的映射有一个限制，就是我们之前的假设 $P(Z)$ 服从 $\mathcal{N}(0, 1)$。

对于每一个数据 $x_i$，我们都希望它能够找到对应的 $z_i$，也就是说我们希望推断网络代表的分布 $q_\phi(Z|X)$ 去学习 $P(Z|X)$。这里的学习方法，就是和 **变分推断** 有关。

---

$$L = \log p(x)$$

$$L = \sum_{z} q(z|x) \log p(x)$$

$$= \sum_{z} q(z|x) \log \left( \frac{p(z, x)}{p(z|x)} \right)$$

$$= \sum_{z} q(z|x) \log \left( \frac{p(z, x) q(z|x)}{q(z|x) p(z|x)} \right)$$

$$= \sum_{z} q(z|x) \log \left( \frac{p(z, x)}{q(z|x)} \right) + \sum_{z} q(z|x) \log \left( \frac{q(z|x)}{p(z|x)} \right)$$

$$L = L^V + D_{\text{KL}}(q(z|x) \| p(z|x))$$

---

可以看出 $L$ 显然是一个定值，那么如果想要最小化第二项的 KL 散度，需要通过最大化 $L^V$ 来实现。

---
**其中：**

$$L^V = \sum_{z} q(z|x) \log \left( \frac{p(z, x)}{q(z|x)} \right)$$

$$L^V = \sum_{z} q(z|x) \log \left( \frac{p(x|z)p(z)}{q(z|x)} \right)$$

$$L^V = \sum_{z} q(z|x) \log \left( \frac{p(z)}{q(z|x)} \right) + \sum_{z} q(z|x) \log p(x|z)$$

$$L^V = -D_{\text{KL}}(q(z|x) \| p(z)) + \mathbb{E}_{q(z|x)}[\log p(x|z)]$$

--- 

我们的假设是 $P(Z)$ 服从标准高斯分布，$P(Z|X) = \mathcal{N}(Z|\mu(x), \sigma^2(x) \ast I)$ 服从高斯分布。推断网络的输出是 $\mu, \sigma$，因此可以利用高斯分布的 KL 散度的计算公式直接计算。后面一部分的损失求解需要用蒙特卡罗方法进行计算，但是这里我们可以用上文中说的 $L_2$ Loss 直接进行优化。

再次看这个计算 Loss 的公式，我们可以发现，第一项的 **KL 散度** 是用来保证隐变量 $Z$ 的总体是服从单位高斯分布的，后一项是保证生成器可以很好地从隐变量还原到数据。

需要注意的是，虽然 $Z$ 的总体是服从单位高斯分布的，但是分布 $P(Z|X)$ 只是普通的高斯分布。对于一个特定的训练数据 $x_i$，整个 Loss 函数会让它尽量向单位高斯靠拢，但是又同时需要它保证自己的特异性，是一个 trade-off。因此之后会出现 $\beta$-VAE。也就是说将 Loss 函数改成了：

$$L^V = -D_{KL}(q(z|x) \| p(z)) + \beta \cdot \mathbb{E}_{q(z|x)}(\log(p(x|z))).$$

其中的 $\beta$ 是一个超参数，可以调节模型的生成能力。

整个模型的精髓在于 **损失函数** 中的第一项里面的 KL 散度，引入了噪声，保证了模型的生成能力。

---


## CVAE_Answers

CVAE 中需要根据输入 $y$ 来进行输出。也就是说训练集中是数据对 $(x, y)$，$y$ 是输入，也就是 **condition**，$x$ 是我们期待的输出。在手写数字生成的例子中，$y$ 就是我想输出的数字，比如说 6；$x$ 就是最后输出的 6 的图片。

因此，我们采样的时候，不再是从 $P(Z)$ 中直接采样，而是从 $P(Z|Y)$ 中进行采样，因此假设变成了：
$$P(Z|Y) = \mathcal{N}(Z | \mu(Y), I)$$

相应的，整个损失函数变成了：
$$L^V = -D_{KL}(q(z|x,y) \| p(z|y)) + \mathbb{E}_{q(z|x,y)} (\log p(x|z, y))$$

条件 $y$ 要同时输入到 encoder 和 decoder 中。

这个改动还是很直观的，就是通过条件改变隐变量的均值，从而控制了隐变量采样的位置，控制最后的输出结果。

---

# Codes

```python

```


# FootNotes

[^1]: 不一样的证明方式_Jensen不等式与直接展开
[^2]: 知乎分析VAE链接
[^3]: 介绍L2 Loss