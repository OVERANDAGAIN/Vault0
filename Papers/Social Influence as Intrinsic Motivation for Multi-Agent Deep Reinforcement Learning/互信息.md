---
创建时间: 2024-十二月-19日  星期四, 7:48:23 晚上
created: 2024-12-19T19:48
updated: ...
---
[[Social Influence as Intrinstic Motivation]]

# Questions
1. 简单介绍一下互信息
2. 互信息、kl散度、熵的关系（交叉熵）
3. 互信息定义推导
4. 条件熵推导

# Answers

## 互信息
**互信息（Mutual Information, MI）**是信息论中的一个重要概念，用来衡量两个随机变量之间的**依赖关系**或**相关性**。它反映了一个随机变量包含了多少关于另一个随机变量的信息。

---

### **1. 互信息的定义**
对于两个随机变量 $X$ 和 $Y$，互信息定义为：

$$I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)},$$

- $p(x, y)$：$X$ 和 $Y$ 的联合概率分布。
- $p(x)$, $p(y)$：$X$ 和 $Y$ 的边缘概率分布。

**直观解释：**
- 如果 $X$ 和 $Y$ 完全独立，则 $p(x, y) = p(x)p(y)$，互信息为 0（两个变量无任何关联）。
- 如果 $X$ 和 $Y$ 完全依赖（高度相关），则 $p(x, y)$ 和 $p(x)p(y)$ 的差距最大，互信息值较大。

---

### **2. 互信息的含义**
互信息可以理解为：
- $X$ 和 $Y$ 之间的**信息共享量**。
- $X$ 提供了多少关于 $Y$ 的信息，或者反过来 $Y$ 提供了多少关于 $X$ 的信息。

#### **与熵的关系：**
互信息可以用熵来表示：[^2]
$$I(X; Y) = H(X) - H(X | Y)$$
- $H(X)$：$X$ 的熵，表示 $X$ 的不确定性。
- $H(X | Y)$：在知道 $Y$ 的情况下，$X$ 的条件熵，表示 $X$ 的剩余不确定性。

==从公式可以看出，互信息表示 $Y$ 减少了 $X$ 的不确定性有多少。==

---

### **3. 互信息的性质**
1. **非负性：** $I(X; Y) \geq 0$。互信息总是非负的。
2. **对称性：** $I(X; Y) = I(Y; X)$。互信息是对称的，表示 $X$ 提供给 $Y$ 的信息量与 $Y$ 提供给 $X$ 的信息量相同。
3. **独立性：** 如果 $X$ 和 $Y$ 独立，则 $I(X; Y) = 0$。


---

### **5. 应用**
互信息广泛应用于机器学习、数据科学和信息理论，主要包括：
1. **特征选择：** 衡量一个特征和目标变量的相关性，用于选择重要的特征。
2. **图像处理：** 用于配准两张图片，通过计算它们的互信息找到最佳对齐方式。
3. **聚类分析：** 衡量聚类结果和真实标签之间的依赖性。
4. **自然语言处理：** 评估词语之间的关联性。

---

### **6. 总结**
互信息衡量了两个变量之间的信息共享量，是判断变量之间相关性和依赖关系的重要工具。它可以看作一种更加通用的相关性度量，适用于非线性关系，而不像传统的皮尔森相关系数只适用于线性关系。

## 三者关系

**互信息（Mutual Information, MI）**、**KL散度（Kullback-Leibler divergence）** 和 **熵（Entropy）** 都是信息论中的核心概念，它们之间有紧密的联系。以下将从定义、公式推导和相互关系的角度进行介绍。

---

### **1. 熵（Entropy）**
熵是用来衡量一个随机变量的不确定性或信息量的指标。

#### **定义：**
对于一个离散随机变量 $X$ 及其概率分布 $p(x)$，熵定义为：

$$H(X) = -\sum_{x \in X} p(x) \log p(x)$$

#### **含义：**
- 熵越大，变量的不确定性越高（例如均匀分布的熵最大）。
- 熵可以看作是变量中包含的“信息量”或“复杂度”的度量。

---

### **2. KL散度（Kullback-Leibler Divergence, $D_{KL}$）**
KL散度衡量两个概率分布 $P$ 和 $Q$ 之间的“相对熵”或“差异”。

#### **定义：**
对于两个概率分布 $P(x)$ 和 $Q(x)$，KL散度定义为：

$$D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

#### **含义：**
- KL散度描述了从分布 $Q$ 到分布 $P$ 的信息损失。
- 如果 $P$ 和 $Q$ 完全相同，则 $D_{KL}(P \| Q) = 0$。

---

### **3. 互信息（Mutual Information, $I(X; Y)$）**
互信息衡量两个随机变量 $X$ 和 $Y$ 之间的关联程度，表示一个变量中包含了多少关于另一个变量的信息。

#### **定义：**
互信息的定义公式为：

$$I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}$$

#### **含义：**
- **互信息表示 $X$ 和 $Y$ 的联合分布 $p(x, y)$ 和独立分布 $p(x)p(y)$ 的差异**（即依赖关系的强弱）。
- 如果 $X$ 和 $Y$ 完全独立，则 $I(X; Y) = 0$。

---

### **4. 三者的联系**

#### **4.1 互信息与熵的关系**[^5]
互信息可以通过熵来表达：

$$I(X; Y) = H(X) - H(X | Y)$$

或对称地：

$$I(X; Y) = H(Y) - H(Y | X)$$

等价于：


$$I(X; Y) = H(X) + H(Y) - H(X, Y)$$

##### **解释：**
- $H(X)$：$X$ 的总信息量（不确定性）。
- $H(X | Y)$：在知道 $Y$ 的情况下，$X$ 的剩余不确定性。
- $I(X; Y)$：表示 $Y$ 减少了 $X$ 的不确定性有多少（或者反之亦然）。

#### **4.2 互信息与KL散度的关系**[^4]
互信息也可以用KL散度来表示：

$$I(X; Y) = D_{KL}(p(x, y) \| p(x)p(y))$$

note[^9]
````ad-help
这段公式的推导主要基于信息论中的 **互信息** 定义以及 **KL散度（Kullback-Leibler Divergence）** 的性质。以下是详细的推导步骤，逐步说明公式是如何得到的。

---

### **1. 互信息的定义**
互信息 $I(A^j; A^k | z)$ 描述了两个随机变量 $A^j$ 和 $A^k$ 在给定条件 $z$ 下的依赖关系。定义为：

$$I(A^j; A^k | z) = \sum_{a^j, a^k} p(a^j, a^k | z) \log \frac{p(a^j, a^k | z)}{p(a^j | z) p(a^k | z)},$$

#### **解释：**
- $p(a^j, a^k | z)$：给定条件 $z$ 下，$A^j$ 和 $A^k$ 的联合分布。
- $p(a^j | z)$, $p(a^k | z)$：给定 $z$ 下，$A^j$ 和 $A^k$ 的边缘分布。
- 分子 $p(a^j, a^k | z)$ 和分母 $p(a^j | z)p(a^k | z)$ 的比值描述了变量之间的依赖性。

此公式是互信息的基本定义。

---

### **2. 使用KL散度重写互信息**
KL散度的定义为：

$$D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}.$$

将互信息的定义与KL散度联系起来，可以看到互信息实际是在计算联合分布 $p(a^j, a^k | z)$ 与独立假设下的分布 $p(a^j | z)p(a^k | z)$ 的KL散度：

$$I(A^j; A^k | z) = D_{KL}(p(a^j, a^k | z) \| p(a^j | z)p(a^k | z)).$$

这是互信息与KL散度的关系。

---

### **3. 分解KL散度的形式**
进一步展开 KL 散度的公式：

$$D_{KL}(p(a^j, a^k | z) \| p(a^j | z)p(a^k | z)) = \sum_{a^j, a^k} p(a^j, a^k | z) \log \frac{p(a^j, a^k | z)}{p(a^j | z)p(a^k | z)}.$$

这一步直接回到了互信息的定义，但这里引出了KL散度的表达，目的是为了接下来的分解。

---

### **4. 引入条件概率的性质**
根据条件概率公式 $p(a^j, a^k | z) = p(a^k | z)p(a^j | a^k, z)$，我们可以把互信息公式中的联合分布 $p(a^j, a^k | z)$ 分解：

$$I(A^j; A^k | z) = \sum_{a^j, a^k} p(a^k | z)p(a^j | a^k, z) \log \frac{p(a^j | a^k, z)p(a^k | z)}{p(a^j | z)p(a^k | z)}.$$

将分子分母整理后：

$$I(A^j; A^k | z) = \sum_{a^j, a^k} p(a^k | z)p(a^j | a^k, z) \log \frac{p(a^j | a^k, z)}{p(a^j | z)}.$$

注意，这里利用了 $p(a^k | z)$ 出现在分子和分母中，可以直接抵消。

---

### **5. 提取 $p(a^k | z)$**
把 $p(a^k | z)$ 提取到外层求和后，可以写成如下形式：

$$I(A^j; A^k | z) = \sum_{a^k} p(a^k | z) \sum_{a^j} p(a^j | a^k, z) \log \frac{p(a^j | a^k, z)}{p(a^j | z)}.$$

此时，内层求和 $\sum_{a^j} p(a^j | a^k, z) \log \frac{p(a^j | a^k, z)}{p(a^j | z)}$ 实际上是一个KL散度的形式。

---

### **6. 用KL散度表示互信息**
根据KL散度的定义，上述内层部分可以写为：

$$D_{KL}(p(a^j | a^k, z) \| p(a^j | z)) = \sum_{a^j} p(a^j | a^k, z) \log \frac{p(a^j | a^k, z)}{p(a^j | z)}.$$

因此，互信息可以简化为：

$$I(A^j; A^k | z) = \sum_{a^k} p(a^k | z) D_{KL}(p(a^j | a^k, z) \| p(a^j | z)).$$

这就是论文中给出的公式：

$$I(A^j; A^k | z) = \sum_{a^k} p(a^k | z) D_{KL}[p(a^j | a^k, z) \| p(a^j | z)].$$
![[Pasted image 20241219210947.png]]
---

### **7. 总结**
推导过程的关键点在于：
1. 从互信息的定义出发，结合KL散度的定义，将互信息表示为两个分布之间的KL散度。
2. 使用条件概率 $p(a^j, a^k | z) = p(a^k | z)p(a^j | a^k, z)$ 将公式分解。
3. 提取 $p(a^k | z)$，将互信息分解为加权求和的形式，其中每个权重对应一个KL散度。

最终得到了论文中的公式形式，清晰地表示了互信息是通过 $a^k$ 的加权计算多个KL散度的结果。
````
##### **解释：**
- $I(X; Y)$ 表示 $p(x, y)$（联合分布）与 $p(x)p(y)$（独立分布假设）的差异。
- 当 $X$ 和 $Y$ 完全独立时，联合分布 $p(x, y) = p(x)p(y)$，此时 $D_{KL} = 0$，互信息为 0。

#### **4.3 KL散度与熵的关系**[^7]
KL散度本质上是熵的一个推广，表示“相对熵”：

- KL散度可以表示为：

$$D_{KL}(P \| Q) = H(P, Q) - H(P)$$

其中 $H(P, Q)$ 是交叉熵，表示分布 $P$ 和 $Q$ 的混合熵。[^6]
````ad-help
**交叉熵（Cross-Entropy）**是信息论和机器学习中的一个重要概念，用来衡量两个概率分布之间的差异。它通常用于分类任务中的损失函数，特别是在神经网络中，用来评估模型预测分布和真实分布之间的距离。

---

### **1. 什么是交叉熵？**

交叉熵衡量的是一个分布 $q(x)$ 用来表示另一个目标分布 $p(x)$ 的效率。定义如下：

#### **定义：**
给定两个概率分布 $p(x)$ 和 $q(x)$，交叉熵 $H(p, q)$ 定义为：

$$H(p, q) = -\sum_{x} p(x) \log q(x),$$

其中：
- $p(x)$：是真实分布（通常是目标分布）。
- $q(x)$：是模型预测分布。
- $x$：表示样本的可能取值。

#### **直观解释：**
- 交叉熵衡量了用分布 $q(x)$ 对分布 $p(x)$ 进行编码所需的平均位数。
- 如果 $q(x)$ 更接近 $p(x)$，交叉熵会更小；如果 $q(x)$ 偏离 $p(x)$，交叉熵会更大。

---

### **2. 交叉熵的来源：**

交叉熵可以看作是熵的推广，熵是描述单个概率分布的内部复杂性的指标，而交叉熵描述两个概率分布之间的匹配程度。

#### **熵的定义：**

熵 $H(p)$ 是一个分布自身的不确定性，定义为：

$$H(p) = -\sum_{x} p(x) \log p(x).$$

当我们希望用另一个分布 $q(x)$ 来表示 $p(x)$ 时，交叉熵 $H(p, q)$ 在此基础上扩展为：

$$H(p, q) = H(p) + D_{KL}(p \| q),$$

其中 $D_{KL}(p \| q)$ 是 KL 散度，表示 $p(x)$ 和 $q(x)$ 的差异。

因此：

- 当 $q(x) = p(x)$ 时，交叉熵 $H(p, q)$ 就等于熵 $H(p)$。
- 当 $q(x)$ 偏离 $p(x)$ 时，交叉熵会比熵 $H(p)$ 更大。

---

### **3. 为什么交叉熵被用作损失函数？**

在机器学习中，交叉熵损失函数常用于分类问题，特别是多分类任务。我们用一个具体的场景说明：

#### **分类问题中的应用：**

假设：
- $p(x)$：是真实分布（真实类别的概率分布），通常是独热编码（one-hot encoding）。
- $q(x)$：是模型预测的分布（通过 softmax 得到的概率分布）。

交叉熵损失函数为：

$$L = -\sum_{x} p(x) \log q(x).$$

对于单个样本：
- 如果真实类别是 $c$，则 $p(x)$ 在 $x = c$ 时为 1，在其他情况下为 0。
- 于是损失函数简化为：

$$L = -\log q(c),$$

即：交叉熵只关心模型预测在真实类别 $c$ 上的概率 $q(c)$。

#### **目标：**
通过最小化交叉熵损失，模型会尝试使预测分布 $q(x)$ 更接近真实分布 $p(x)$，从而提高分类性能。

---

### **4. 交叉熵与 KL 散度的关系**

交叉熵可以表示为熵和 KL 散度的和：

$$H(p, q) = H(p) + D_{KL}(p \| q),$$

其中：
- $H(p)$：是真实分布的熵，表示 $p(x)$ 的固有复杂性，是一个固定值（与 $q(x)$ 无关）。
- $D_{KL}(p \| q)$：是 KL 散度，表示 $q(x)$ 与 $p(x)$ 的差异。

因此，在优化模型时：
- 最小化交叉熵等价于最小化 KL 散度，因为 $H(p)$ 是固定的。

---

### **5. 交叉熵在机器学习中的应用**

#### **(1) 二分类任务：**

对于二分类问题（例如，预测正类或负类的概率），交叉熵损失函数为：

$$L = -\big( y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \big),$$

其中：
- $y$：是真实标签（0 或 1）。
- $\hat{y}$：是模型预测的正类概率。

#### **(2) 多分类任务：**

对于多分类问题，交叉熵损失函数为：

$$L = -\sum_{i} y_i \log \hat{y}_i,$$

其中：
- $y_i$：是类别 $i$ 的真实标签（独热编码形式）。
- $\hat{y}_i$：是模型预测类别 $i$ 的概率（通过 softmax 输出）。

#### **(3) 多标签分类：**

对于多标签分类（每个样本可能有多个类别标签），可以扩展二分类的交叉熵损失函数，将其应用到每个类别的预测上，最后求平均。

---

### **6. 直观理解**

交叉熵的核心思想是：如果我们希望模型的预测分布 $q(x)$ 能够尽可能接近真实分布 $p(x)$，那么交叉熵可以作为衡量这种接近程度的工具。交叉熵越小，说明预测分布和真实分布越相似。

- **极端情况：**
  - 如果模型预测正确（即 $q(x)$ 和 $p(x)$ 完全一致），交叉熵为 0。
  - 如果模型预测偏离真实分布，交叉熵会增大。

---

### **7. 总结**

- **定义：**交叉熵 $H(p, q) = -\sum_x p(x) \log q(x)$，衡量分布 $q(x)$ 对目标分布 $p(x)$ 的描述能力。
- **损失函数：** 在分类任务中，交叉熵通过惩罚错误的预测，使模型预测的分布逐渐接近真实分布。
- **与 KL 散度关系：**交叉熵等于熵 $H(p)$ 加上 KL 散度 $D_{KL}(p \| q)$，优化交叉熵的过程等价于最小化 KL 散度。
- **应用：**广泛用于二分类、多分类、多标签分类任务，特别是通过 softmax 或 sigmoid 函数生成预测概率时。
````
##### **条件熵与KL散度的联系：**[^8]
条件熵 $H(X | Y)$ 可以看作是 $X$ 和 $Y$ 的联合分布与边缘分布的差异（通过KL散度描述）：

$$H(X | Y) = H(X, Y) - H(Y)$$

---

### **5. 总结：互信息、KL散度与熵的关系**
1. **熵（H）：**
   - 测量单个随机变量的“自信息”或不确定性。

2. **KL散度（$D_{KL}$）：**
   - 衡量两个概率分布之间的“相对熵”或差异。

3. **互信息（I）：**
   - 测量两个随机变量之间的依赖性，可以通过熵或KL散度定义：
     - $I(X; Y) = H(X) - H(X | Y)$
     - $I(X; Y) = D_{KL}(p(x, y) \| p(x)p(y))$

三者的关系可以用一张图来总结：

- **熵**（单变量）：描述信息量。
- **KL散度**（两分布）：描述分布之间的差异。
- **互信息**（两变量）：描述变量之间的信息共享量，用KL散度计算联合分布和独立分布的差异。

# 互信息定义推导
我们从**互信息的定义**出发，逐步推导出互信息的公式 $I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}$，并解释每一步的推导逻辑。

---

### **1. 互信息的定义**
互信息 $I(X; Y)$ 表示两个随机变量 $X$ 和 $Y$ 之间的依赖程度，定义为：

$$I(X; Y) = H(X) - H(X|Y)$$

#### **解释：**
- $H(X)$ 是 $X$ 的熵，表示 $X$ 的不确定性。
- $H(X|Y)$ 是在已知 $Y$ 的情况下，$X$ 的条件熵，表示在给定 $Y$ 时 $X$ 的剩余不确定性。
- 因此，互信息 $I(X; Y)$ 表示 $Y$ 提供了多少信息以减少 $X$ 的不确定性。

---

### **2. 用熵的公式展开**
熵的公式为：

$$H(X) = -\sum_{x} p(x) \log p(x),$$
$$H(X|Y) = -\sum_{x, y} p(x, y) \log p(x|y),$$
---
==条件熵的推导==[^3]

---

将它们代入互信息的定义 $I(X; Y) = H(X) - H(X|Y)$，得到：

$$I(X; Y) = -\sum_{x} p(x) \log p(x) + \sum_{x, y} p(x, y) \log p(x|y).$$

---

### **3. 用条件概率公式替换**
条件概率公式为：

$$p(x|y) = \frac{p(x, y)}{p(y)}.$$

将其代入第二项 $\sum_{x, y} p(x, y) \log p(x|y)$，得到：

$$\sum_{x, y} p(x, y) \log p(x|y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(y)}.$$

因此，互信息变为：

$$I(X; Y) = -\sum_{x} p(x) \log p(x) + \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(y)}.$$

---

### **4. 将 $p(x)$ 的定义代入**
边缘概率 $p(x)$ 的定义为：

$$p(x) = \sum_{y} p(x, y).$$
````ad-help
边缘概率 $p(x)$ 的定义 $p(x) = \sum_{y} p(x, y)$ 是概率论中的基本性质，来源于联合概率分布和边缘概率分布之间的关系。以下是详细的推导和解释：

---

### **1. 联合概率的定义**

联合概率 $p(x, y)$ 描述了两个随机变量 $X$ 和 $Y$ 的联合发生概率。例如，假设 $X$ 和 $Y$ 是两个随机变量，则 $p(x, y)$ 表示 $X = x$ 且 $Y = y$ 同时发生的概率。

---

### **2. 边缘概率的定义**

边缘概率 $p(x)$ 是指随机变量 $X$ 的概率分布，不关心随机变量 $Y$ 的具体取值，也就是对 $Y$ 的所有可能值进行“求和”或“积分”（如果是连续随机变量的话）。因此，$p(x)$ 可以通过联合概率 $p(x, y)$ 对 $Y$ 求和得到：

$$p(x) = \sum_{y} p(x, y).$$

#### **解释：**
- $\sum_{y}$：表示对所有可能的 $Y = y$ 的取值求和。
- 本质上，边缘概率就是联合概率中将 $Y$ 的影响“边缘化”掉，仅关注 $X$ 的概率分布。

---

### **3. 直观理解**

#### **例子 1（骰子和硬币）：**
假设我们有一个随机事件，掷一次骰子并抛一次硬币：
- $X$ 表示骰子的点数，取值范围是 $\{1, 2, 3, 4, 5, 6\}$。
- $Y$ 表示硬币的结果，取值范围是 $\{\text{正面}, \text{反面}\}$。

联合概率 $p(x, y)$ 是骰子点数和硬币结果的联合概率分布。例如：
- $p(3, \text{正面})$：表示骰子掷出 $3$ 且硬币为正面的概率。

边缘概率 $p(x)$ 是骰子点数的概率（忽略硬币结果），可以通过对所有硬币结果的联合概率求和得到：
$$p(x = 3) = p(3, \text{正面}) + p(3, \text{反面}).$$

#### **例子 2（数据表）：**
如果我们有一个联合概率表，比如 $X$ 表示性别（男、女），$Y$ 表示职业（学生、教师、工程师），那么：
- $p(\text{男}, \text{学生})$：表示联合概率，即性别为男且职业为学生的概率。
- $p(\text{男})$：边缘概率，即性别为男的概率，可以通过对职业（$Y$）的所有可能值求和得到：
  $$p(\text{男}) = p(\text{男}, \text{学生}) + p(\text{男}, \text{教师}) + p(\text{男}, \text{工程师}).$$

---

### **4. 数学推导**

#### **联合概率到边缘概率的推导：**
假设 $X$ 和 $Y$ 是离散随机变量，联合概率分布定义为 $p(x, y)$，表示 $X = x$ 且 $Y = y$ 同时发生的概率。根据全概率公式：
$$p(x) = \sum_{y} p(x, y).$$

#### **连续情形：**
如果 $X$ 和 $Y$ 是连续随机变量，联合概率密度函数为 $p(x, y)$，边缘概率密度函数 $p(x)$ 定义为：
$$p(x) = \int_{y} p(x, y) \, dy,$$
表示对联合概率密度函数 $p(x, y)$ 在 $y$ 的维度上积分，消去 $y$ 的影响。

---

### **5. 结合互信息的背景**

在互信息的背景中，边缘概率 $p(x)$ 的定义是为了从联合概率分布 $p(x, y)$ 中提取单个随机变量的概率分布。我们通过以下步骤使用边缘概率：

1. $p(x, y)$ 是联合概率分布，表示 $X = x$ 和 $Y = y$ 同时发生的概率。
2. $p(x) = \sum_{y} p(x, y)$ 用来表示随机变量 $X$ 的边缘概率。
3. 同理，$p(y) = \sum_{x} p(x, y)$ 表示随机变量 $Y$ 的边缘概率。

这样可以从联合分布中提取出每个变量的单独分布，用于计算互信息中的对数项 $\log \frac{p(x, y)}{p(x)p(y)}$。

---

### **6. 总结**

边缘概率的定义 $p(x) = \sum_{y} p(x, y)$ 来自概率论的基本规则：
- 边缘概率描述了单个随机变量的概率分布。
- 它通过联合概率分布对其他变量（在这里是 $Y$）求和（或积分）得到。
- 在互信息推导中，边缘概率用于刻画单个变量的分布，以便将联合分布和独立分布（假设独立时的联合分布）进行对比。

````

代入 $-\sum_{x} p(x) \log p(x)$，并展开后与第二项组合：

$$I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(y)} + \sum_{x, y} p(x, y) \log \frac{1}{p(x)}.$$

将两部分合并：

$$I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}.$$

---

### **5. 最终公式**
得到了互信息的最终公式：

$$I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}.$$

---

### **公式的直观解释**
- 分子 $p(x, y)$：表示 $X$ 和 $Y$ 的联合分布。
- 分母 $p(x)p(y)$：表示 $X$ 和 $Y$ 在独立情况下的联合分布。
- $\log \frac{p(x, y)}{p(x)p(y)}$：表示实际联合分布与独立假设的偏差。
- 权重 $p(x, y)$：保证我们只关注联合分布中可能发生的事件。

当 $X$ 和 $Y$ 完全独立时，$p(x, y) = p(x)p(y)$，此时 $\log \frac{p(x, y)}{p(x)p(y)} = 0$，互信息为 0。

---

### **总结**
互信息公式 $I(X; Y) = \sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}$ 是从互信息的定义 $I(X; Y) = H(X) - H(X|Y)$ 出发，通过熵和条件概率的公式逐步推导得出的，直观地衡量了两个变量之间的统计相关性。

# 条件熵推导
要理解 $H(X | Y)$ 是如何得到 $-\sum_{x, y} p(x, y) \log p(x|y)$ 的，可以从**条件熵的定义**和条件概率的基本公式出发推导。

---

### **1. 条件熵的定义**
条件熵 $H(X|Y)$ 的定义是：在已知随机变量 $Y$ 的情况下，随机变量 $X$ 的不确定性。其数学定义为：

$$H(X | Y) = \mathbb{E}_{Y} \big[ H(X | Y = y) \big],$$

- 这里 $H(X | Y = y)$ 表示当 $Y = y$ 时，$X$ 的熵。
- 条件熵 $H(X | Y)$ 是对 $Y$ 的所有可能值加权后的期望。

展开条件熵的定义，我们有：

$$H(X | Y) = \sum_{y} p(y) H(X | Y = y).$$

---

### **2. 单个条件熵的公式**
在固定 $Y = y$ 时，$X$ 的熵为：

$$H(X | Y = y) = -\sum_{x} p(x | y) \log p(x | y),$$

其中：
- $p(x | y)$ 是给定 $Y = y$ 时，$X$ 的条件概率。

因此，总的条件熵为：

$$H(X | Y) = \sum_{y} p(y) \bigg[ -\sum_{x} p(x | y) \log p(x | y) \bigg].$$

---

### **3. 将条件概率的权重合并**
在上式中，注意到 $p(x | y)$ 和 $p(y)$ 可以通过联合分布 $p(x, y)$ 表示：

$$p(x, y) = p(x | y)p(y),$$

这意味着：

$$p(x | y)p(y) = p(x, y).$$

将其代入条件熵公式中：

$$H(X | Y) = -\sum_{y} \sum_{x} p(x | y)p(y) \log p(x | y),$$

因为 $p(x | y)p(y) = p(x, y)$，我们进一步简化得到：[^1]

$$H(X | Y) = -\sum_{x, y} p(x, y) \log p(x | y).$$

---

### **4. 最终公式**
条件熵的最终公式为：

$$H(X | Y) = -\sum_{x, y} p(x, y) \log p(x | y).$$

# Codes

```python

```


# FootNotes

[^2]: [[#互信息定义推导]]
[^5]: 互信息与熵的关系
[^4]: KL散度和互信息的关系
[^7]: KL散度与熵的关系
[^6]: 什么是交叉熵
[^8]: 条件熵与KL散度的关系
[^3]: [[互信息#条件熵推导]]
[^1]: [[求和的等价]]
[^9]: 互信息、KL散度、与文章中的“因果影响”的关系