---

mindmap-plugin: basic

---
# 单智能体强化学习

## 基础方法
### [Temporal Difference (TD)](https://link.springer.com/article/10.1007/BF00992696) ; Sutton, 1988
### [Q-Learning](https://link.springer.com/article/10.1007/BF00992698) ; Watkins & Dayan, 1992
### [Dyna-Q](https://www.sciencedirect.com/science/article/abs/pii/000437029390005J) ; Sutton, 1990

## 深度价值方法
### [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013
#### 改进：稳定性与收敛性
##### [Double DQN](https://arxiv.org/abs/1509.06461) ; Hasselt et al., 2015
##### [Dueling DQN](https://arxiv.org/abs/1511.06581) ; Wang et al., 2016
#### 改进：数据效率
##### [Prioritized Replay](https://arxiv.org/abs/1511.05952) ; Schaul et al., 2015

## 策略梯度与Actor-Critic
### [Policy Gradient (REINFORCE)](https://link.springer.com/article/10.1007/BF00992696) ; Williams, 1992
#### 改进：高方差问题
##### [Actor-Critic](https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html) ; Sutton et al., 1999
#### 改进：收敛与约束
##### [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015
##### [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017

## 深度连续控制
### [DDPG](https://arxiv.org/abs/1509.02971) ; Lillicrap et al., 2015
#### 改进：稳定性与探索
##### [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018


# 单智能体强化学习（前沿）

## 模仿学习
### 基础方法
#### [Behavior Cloning (BC)](https://link.springer.com/article/10.1007/BF00992696) ; Pomerleau, 1989
#### [Inverse Reinforcement Learning (IRL)](https://dl.acm.org/doi/10.1145/1015330.1015430) ; Ng & Russell, 2000
### 改进：对抗生成
#### [GAIL](https://arxiv.org/abs/1606.03476) ; Ho & Ermon, 2016

## 基于模型方法
### 经典框架
#### [Model Predictive Control (MPC)](https://www.sciencedirect.com/science/article/pii/S0005109804002766) ; Mayne, 1960/2005
#### [PILCO](https://proceedings.mlr.press/v28/deisenroth13.html) ; Deisenroth & Rasmussen, 2013
### 改进：深度模型预测
#### [MBPO](https://arxiv.org/abs/1906.08253) ; Janner et al., 2019

## 离线强化学习
### 基础：分布约束
#### [BCQ](https://arxiv.org/abs/1812.02900) ; Fujimoto et al., 2019
### 改进：保守估计
#### [CQL](https://arxiv.org/abs/2006.04779) ; Kumar et al., 2020
### 改进：隐式策略学习
#### [IQL](https://arxiv.org/abs/2110.06169) ; Kostrikov et al., 2021

## 目标导向与泛化
### 基础：目标条件化
#### [UVFA](https://arxiv.org/abs/1802.09464) ; Schaul et al., 2015
#### [HER](https://arxiv.org/abs/1707.01495) ; Andrychowicz et al., 2017
### 改进：跨任务泛化
#### [Meta-RL (MAML)](https://arxiv.org/abs/1703.03400) ; Finn et al., 2017
