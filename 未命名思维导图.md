---

mindmap-plugin: basic

---

# MAS





## 单智能体

* 理论框架

  * 形式化单智能体决策与部分可观测

    * [MDP](https://link.springer.com/article/10.1007/BF00992696) ; Sutton, 1988
	    * 状态转移依赖 Markov 性
		* 目标：找到最优策略 $\pi^*$
    * [POMDP](https://www.sciencedirect.com/science/article/pii/S000437029800023X) ; Cassandra, 1998
	    * 仅能观测部分信息，需引入观察函数
		* 适用于带噪声/不完全信息的环境
* 价值函数方法

  * 通过近似最优 $Q$ 函数导出策略

    * [Q-Learning](https://link.springer.com/article/10.1007/BF00992698) ; Watkins & Dayan, 1992
	    * off-policy，更新公式收敛性有保证
    * [SARSA](https://dl.acm.org/doi/10.5555/645529.657617) ; Singh et al., 2000
		* on-policy，有限状态空间收敛证明
	* 函数逼近解决连续状态
		* [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013/2015
	
* 基于策略的方法

  * 动机：直接优化参数化策略，缓解值函数逼近限制

    * [REINFORCE](https://dl.acm.org/doi/10.1145/138243.138273) ; Williams, 1992

      * 改进方向：降低方差、提升收敛

        * [Actor–Critic](https://papers.nips.cc/paper/1786-convergence-properties-of-policy-iteration) ; Konda & Tsitsiklis, 1999
      * 改进方向：稳定更新过程（信任域/近端）

        * [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015
        * [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017
	        * TRPO 的简化高效实现
    * [Deterministic Policy Gradient](https://proceedings.mlr.press/v32/silver14.pdf) ; Silver et al., 2014
		* 适用于连续动作空间
    * [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018
	    * 最大熵框架，稳定高效

---

## 多智能体

* 理论框架

  * 动机：扩展到多主体交互与部分可观测

    * [Markov Game](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
    * Partially Observable Markov Game (POMG)
* 设置分类

  * 动机：按效用结构划分（合作 / 竞争 / 混合动机）

    * Cooperative

      * [Team MDP](https://link.springer.com/chapter/10.1007/3-540-61380-2_18) ; Boutilier, 1996
      * [Independent Q-learning](https://link.springer.com/chapter/10.1007/3-540-45545-0_14) ; Lauer & Riedmiller, 2000
    * Competitive（零和）

      * [Minimax-Q](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
      * [AlphaGo Zero](https://www.nature.com/articles/nature24270) ; Silver et al., 2016
      * [OpenAI Five](https://arxiv.org/abs/1912.06680) ; OpenAI, 2019
    * Mixed-motive（一般和）

      * [Nash Q-learning](https://dl.acm.org/doi/10.1145/502512.502549) ; Hu & Wellman, 2003
      * [Policy Hill-Climbing](https://www.sciencedirect.com/science/article/pii/S0004370201001250) ; Littman, 2001
      * 团队内部协作 + 团队间对抗（如 AlphaStar, Jaderberg et al., 2019）
* 混合动机下的合作
	
	* 社会困境建模
	
	  * 个体理性与群体理性的张力（矩阵博弈：R, P, T, S 及社会困境不等式）
	
	    * 互惠优于互害：$R>P$；互惠优于被占便宜：$R>S$；互惠优于“掷硬币”：$2R>T+S$；恐惧/贪婪：$P>S$ 或 $T>R$
	  * 时序扩展与部分可观测（从一次性博弈到马尔可夫博弈）
	
	    * [Sequential Social Dilemmas (SSD)](https://arxiv.org/abs/1702.03037) ; Leibo et al., 2017
	
	      * 用马尔可夫博弈与策略集 $\Pi_C,\Pi_D$ 定义 R/P/T/S 的长期回报
	  * 交互外部性与协调难度刻画
	
	    * Schelling 图（最小可行协作规模、外部性随协作者数量的变化）
	  * 非对称困境
	
	    * 奖励、能力、观测异质带来的角色分化与“贸易”式合作机会
	
	* 策略优化方法（CTDE/分布式策略）
	
	  * 中心化评论家 + 去中心化执行者（可处理协作/竞争/混合）
	
	    * [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017
	
	      * 每个体独立 critic；可推断他人动作以缓解信息缺失
	    * [MAPPO](https://arxiv.org/abs/2103.01955) ; Yu et al., 2022
	
	      * PPO 的多智能体 CTDE 版本；参数共享，解 Dec-POMDP
	    * [HAPPO](https://arxiv.org/abs/2109.11251) / [HATRPO](https://arxiv.org/abs/2109.11251) ; Kuba et al., 2021
	
	      * 多智能体优势分解理论，避免参数共享
	
	  * 信用分配与改进基线（仅在解释后给改进）
	
	    * [COMA](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018
	
	      * 反事实基线：边际化单体动作以度量其贡献
	    * [LIIR](https://arxiv.org/abs/1906.10129) ; Du et al., 2019
	
	      * 个体内在奖励 + 元梯度双层优化，提高个体可辨识度
	
	* 值函数方法（集中训练的价值分解）
	
	  * 从个体 $Q_i$ 到全局 $Q_{\text{tot}}$ 的可分解与 IGM 约束
	
	    * [VDN](https://arxiv.org/abs/1706.05296) ; Sunehag et al., 2018
	
	      * 线性求和分解，简单可扩展
	    * [QMIX](https://arxiv.org/abs/1803.11485) ; Rashid et al., 2018
	
	      * 单调性混合网络，表达力更强
	    * [QTRAN](https://arxiv.org/abs/1905.05408) ; Son et al., 2019
	
	      * 更弱假设，广义可分解
	    * [Qatten](https://arxiv.org/abs/2002.03939) ; Yang et al., 2020
	
	      * 多头注意力混合，显式度量个体贡献
	    * [QPLEX](https://arxiv.org/abs/2008.01062) ; Wang et al., 2020
	
	      * dueling 风格分解，引入 advantage-based IGM
	
	* 通信机制（静态/网络化/注意力）
	
	  * 固定或可学习的通道与拓扑
	
	    * [RIAL/DIAL](https://arxiv.org/abs/1605.06676) ; Foerster et al., 2016
	    * [CommNet](https://arxiv.org/abs/1605.07736) ; Sukhbaatar & Fergus, 2016
	    * [ATOC](https://arxiv.org/abs/1802.07420) ; Jiang & Lu, 2018
	    * [DGN](https://arxiv.org/abs/2002.03939) ; Jiang et al., 2020
	    * 注意力路由与时序调度
	
	      * [VAIN](https://arxiv.org/abs/1707.06315) ; Hoshen, 2017
	      * [TarMAC](https://arxiv.org/abs/1810.11187) ; Das et al., 2019
	      * [IC3Net](https://arxiv.org/abs/1810.02885) ; Singh et al., 2019
	      * [SchedNet](https://arxiv.org/abs/1903.06350) ; Kim et al., 2019
	
	* 社会偏好与利他激励（Other-regarding preferences）
	
	  * 在外在回报上加入内在项以鼓励群体最优
	
	    * [Prosocial RL](https://arxiv.org/abs/1707.01066) ; Peysakhovich & Lerer, 2017
	
	      * 将群体平均收益并入个体回报
	    * [Inequity Aversion](https://arxiv.org/abs/1803.08884) ; Hughes et al., 2018
	
	      * 忌不公平（嫉妒/内疚）项，稳定合作
	    * [Social Value Orientation (SVO)](https://arxiv.org/abs/2006.04152) ; McKee et al., 2020
	
	      * 用角度偏好刻画“自利—亲社会”连续体
	
	* 行为影响与对手塑形（Co-player shaping）
	
	  * 显式建模他人学习/响应，超越“天真学习者”
	
	    * [LOLA](https://arxiv.org/abs/1709.04326) ; Foerster et al., 2018
	
	      * 优化 $V(\theta_1,\theta_2+\Delta\theta_2)$，学会互惠/惩罚
	    * [SOS](https://arxiv.org/abs/1811.08469) ; Letcher et al., 2019
	
	      * 稳定对手塑形，提升鲁棒性
	    * [Social Influence](https://arxiv.org/abs/1810.08647) ; Jaques et al., 2019
	
	      * 以对他人策略的 KL 影响作为内在奖励
	    * [LIO](https://arxiv.org/abs/2006.03409) ; Yang et al., 2020
	
	      * 学习化的“奖励转移”激励器，直接塑形他人
	
	* 声誉与规范（Reputation & Norms）
	
	  * 通过可识别性/制裁形成群体规范并对齐个体行为
	
	    * [Competitive Altruism](https://arxiv.org/abs/2106.07864) ; McKee et al., 2021
	
	      * 以“贡献水平”偏差为内在项，复现人类在 Cleanup 的合作/匿名效应
	    * [Classifier Norm Model (CNM)](https://arxiv.org/abs/2306.16614) ; Vinitsky et al., 2023
	
	      * 公开制裁数据训练“规范分类器”，去中心化涌现群体规范
	
	* 合同与奖励交换（机制扩展）
	
	  * 通过承诺/侧付改变激励结构，降低背叛收益
	
	    * [Contracts](https://arxiv.org/abs/2005.04819) ; Hughes et al., 2020
	
	      * 提议并达成联合动作的绑定合同
	    * [Reward Exchange](https://arxiv.org/abs/2306.03461) ; Willis & Luck, 2023
	
	      * 跨回合的配额式奖励互换，促成合作政策



## 评测与基准

* 完全合作任务

  * 协同行军与微操（分散观测、集中评测）

    * [SMAC（StarCraft Multi-Agent Challenge）](https://arxiv.org/abs/1902.04043) ; Samvelyan et al., 2019

      * 小队协作、胜率/回合回报为主
  * 双人高协作烹饪（异质搭档与零样本协作）

    * [Overcooked-AI](https://arxiv.org/abs/1910.05789) ; Carroll et al., 2019/2020

      * 搭档更替、策略协调与可泛化性
  * 连续物理 2v2 足球（连续控制与协作涌现）

    * [MuJoCo Soccer](https://arxiv.org/abs/1902.07151) ; Liu et al., 2019

      * 自组织协作、长时序博弈
  * 逼真规则完备足球（复杂战术与多样对手）

    * [Google Research Football](https://arxiv.org/pdf/1907.11180) ; Kurach et al., 2019/2020

      * 规则全面、难度可控、对手库丰富

* 混合动机 / 社会困境任务

  * 公地清理与公共品供给

    * [Clean-up](https://arxiv.org/abs/1803.08884) ; Hughes et al., 2018

      * 清污–采集权衡与群体可持续性
  * 资源共用–再生（过度采集与再生动态）

    * [Commons / Harvest](https://arxiv.org/abs/1707.06600) ; Perolat et al., 2017（后续命名为 Harvest, Hughes et al., 2018）

      * 过度采集—长期枯竭的顺序困境
  * 彩币博弈（短期诱惑与长期收益权衡）

    * [Coin Game](https://arxiv.org/abs/1707.01068) ; Lerer & Peysakhovich, 2017

      * 颜色约束下的协作/背叛选择
  * 等级协作采集（能力门槛与分工协同）

    * [Level-Based Foraging（基准综述）](https://arxiv.org/abs/2006.07869) ; Papoudakis et al., 2021

      * 等级合力采集、可设部分可观测
  * 泛化评测套件（陌生伙伴与新组合）

    * [Melting Pot 1.0](https://proceedings.mlr.press/v139/leibo21a/leibo21a.pdf) ; Leibo et al., 2021

      * 多子环境统一评测、跨伙伴适配
    * [Melting Pot 2.0](https://arxiv.org/abs/2211.02856) ; Agapiou et al., 2022

      * 子环境与场景规模扩展、社会动力学更丰富

* 评价指标

  * 团队任务（完全合作）

    * 公共回报/胜率（统一团队回报）
  * 混合动机（一般和）

    * 集体效用：$R_C=\sum_i R_i$（总回合回报）
    * 可持续性：回报产生时间的平均（Perolat et al., 2017）
    * 平等性：Gini 系数与 $E=1-G$（Hughes et al., 2018）
  * 机制导向指标（对应方法内在机制）

    * 赠与/转移频率： [Gifting](https://cdn.aaai.org/ojs/7208/7208-13-10437-1-10-20200526.pdf) ; Lupu & Precup, 2020
    * 激励强度与成本： [LIO](https://dl.acm.org/doi/abs/10.5555/3495724.3496999) ; Yang et al., 2020
    * 制裁/声誉演化： [CNM](https://arxiv.org/abs/2211.02856)（套件含规范评测场景）
    * 社会影响互信息： [Social Influence](https://arxiv.org/abs/1810.08647) ; Jaques et al., 2019
  * 任务导向指标（按环境自定义）

    * 清污量/苹果再生率（Clean-up / Harvest）
    * 他人硬币收集比例（Coin Game）
    * 分工与占位、路径冲突率（Overcooked / LBF）

* 评测协议与可重复性（建议放入方法学节点）

  * 标准化训练时长/超参与不确定性度量

    * [Reproducibility in MARL Evaluation](https://arxiv.org/abs/2209.10485) ; Gorsane et al., 2022

      * 建议报告失败案例、区分算法方差与环境方差
