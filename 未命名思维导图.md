---

mindmap-plugin: basic

---

# 单智能体强化学习（整理版）

## 价值方法 · 函数逼近

### 动机：高维状态下近似Q值

#### [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013

##### 改进 · 稳定性

* [Double DQN](https://arxiv.org/abs/1509.06461) ; van Hasselt et al., 2015
* [Dueling DQN](https://arxiv.org/abs/1511.06581) ; Wang et al., 2016

##### 改进 · 数据效率

* [Prioritized Replay](https://arxiv.org/abs/1511.05952) ; Schaul et al., 2015

## 策略梯度 · 显式策略

### 动机：直接学习参数化策略

#### [REINFORCE](https://link.springer.com/article/10.1007/BF00992696) ; Williams, 1992

##### 改进 · 降方差与收敛性

* [Actor–Critic](https://papers.nips.cc/paper_files/paper/2000/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html) ; Konda & Tsitsiklis, 2000
* [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015
* [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017

## 连续控制 · 离线策略

### 动机：连续动作空间 + 提升采样效率

#### [DDPG](https://arxiv.org/abs/1509.02971) ; Lillicrap et al., 2015

##### 改进 · 稳定性与鲁棒性

* [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018

## 模仿学习 · 无需奖励

### 动机：稀疏或缺失奖励场景

#### [Behavior Cloning](https://dl.acm.org/doi/10.1145/97243.97245) ; Pomerleau, 1989

#### [GAIL](https://arxiv.org/abs/1606.03476) ; Ho & Ermon, 2016

## 基于模型 · 训练提效

### 动机：学习环境模型，减少真实交互

#### [MPC](https://www.sciencedirect.com/science/article/pii/S0005109804002766) ; Mayne, 2005

#### [PETS](https://arxiv.org/abs/1805.12114) ; Chua et al., 2018

#### [MBPO](https://arxiv.org/abs/1906.08253) ; Janner et al., 2019

## 离线强化学习 · 固定数据

### 动机：不允许或不安全的在线交互

#### [BCQ](https://arxiv.org/abs/1812.02900) ; Fujimoto et al., 2019

#### [CQL](https://arxiv.org/abs/2006.04779) ; Kumar et al., 2020

#### [IQL](https://arxiv.org/abs/2110.06169) ; Kostrikov et al., 2021

## 目标导向 · 条件化策略

### 动机：跨任务泛化 & 多目标控制

#### [UVFA](https://arxiv.org/abs/1802.09464) ; Schaul et al., 2015

#### [HER](https://arxiv.org/abs/1707.01495) ; Andrychowicz et al., 2017

#### [Meta-RL (MAML)](https://arxiv.org/abs/1703.03400) ; Finn et al., 2017

