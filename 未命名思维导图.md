---

mindmap-plugin: basic

---

# MAS



## 多智能体合作学习
- 合作范式
    - 动机：多智能体目标关系不同
        - 纯合作（Pure Cooperation）
            - 环境：simple-spread 等
        - 混合动机（Mixed-motivation）
            - [Sequential Social Dilemmas (SSD)](https://arxiv.org/abs/1702.03037) ; Leibo et al., 2017
- 架构与训练方式
    - 集中训练，分布式执行（CTDE）
        - [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017
        - [QMIX](https://arxiv.org/abs/1803.11485) ; Rashid et al., 2018
        - [QTRAN](https://arxiv.org/abs/1905.05408) ; Son et al., 2019
        - [COMA](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018
- 通信与信息共享
    - 动机：通过通信提升协调性
        - [DIAL](https://arxiv.org/abs/1605.06676) ; Foerster et al., 2016
        - [CommNet](https://arxiv.org/abs/1605.07736) ; Sukhbaatar et al., 2016
        - [IC3Net](https://arxiv.org/abs/1810.03916) ; Singh et al., 2018
- 信号归因（Credit Assignment）
    - 动机：解决个体贡献难以分辨的问题
        - [Counterfactual Multi-Agent Policy Gradients (COMA)](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018
- 伙伴建模与适应
    - 动机：预测/适应他人策略
        - [LOLA](https://arxiv.org/abs/1709.04326) ; Foerster et al., 2017
        - Bayesian Theory of Mind (ToM-based MARL)
- 激励与社会机制
    - 动机：促进合作与公平
        - [LIO (Learning with Intrinsic Social Influence)](https://arxiv.org/abs/1810.08647) ; Yang et al., 2018
        - [Social Influence](https://arxiv.org/abs/1810.08647) ; Jaques et al., 2019
        - Prosocial RL / Inequity Aversion（参见人类合作建模文献）
- 契约与规范
    - 动机：通过机制设计约束个体
        - Contract-based MARL (早期经济学模型)
        - Norm-based Cooperation (CNM, 未形成单一论文代表)





## 单智能体强化学习

### 理论框架

#### [MDP](https://link.springer.com/article/10.1007/BF00992696) ; Sutton, 1988

* 状态转移依赖 Markov 性
* 目标：找到最优策略 $\pi^*$

#### [POMDP](https://www.sciencedirect.com/science/article/pii/S000437029800023X) ; Cassandra, 1998

* 仅能观测部分信息，需引入观察函数
* 适用于带噪声/不完全信息的环境

### 价值函数方法

#### [Q-Learning](https://link.springer.com/article/10.1007/BF00992698) ; Watkins & Dayan, 1992

* off-policy，更新公式收敛性有保证

#### [SARSA](https://dl.acm.org/doi/10.5555/645529.657617) ; Singh et al., 2000

* on-policy，有限状态空间收敛证明

#### [Deep Q-Network (DQN)](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013/2015

* 函数逼近解决连续状态
* 人类水平 Atari 表现

### 策略优化方法

#### [Policy Gradient / REINFORCE](https://dl.acm.org/doi/10.1145/138243.138273) ; Williams, 1992

* 直接优化参数化策略
* 高方差，需要改进

#### [Actor–Critic](https://papers.nips.cc/paper/1786-convergence-properties-of-policy-iteration) ; Konda & Tsitsiklis, 1999

* 策略+价值并行学习，降低方差

#### [Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) ; Silver et al., 2014

* 适用于连续动作空间

#### [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015

* 信任域保证策略更新单调改进

#### [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017

* TRPO 的简化高效实现

#### [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018

* 最大熵框架，稳定高效

---

## 多智能体学习

### 理论框架

#### [Markov Game](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994

* 扩展 MDP 到多主体交互
* 包含 Cooperative / Competitive / Mixed

#### \[Partially Observable Markov Game (POMG)]

* 多智能体部分可观测场景

### 设置分类

#### Cooperative

* [Team MDP](https://link.springer.com/chapter/10.1007/3-540-61380-2_18) ; Boutilier, 1996
* [Independent Q-learning](https://link.springer.com/chapter/10.1007/3-540-45545-0_14) ; Lauer & Riedmiller, 2000

#### Competitive（零和）

* [Minimax-Q](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
* [AlphaGo Zero](https://www.nature.com/articles/nature24270) ; Silver et al., 2016
* [OpenAI Five](https://arxiv.org/abs/1912.06680) ; OpenAI, 2019

#### Mixed-motive（一般和）

* [Nash Q-learning](https://dl.acm.org/doi/10.1145/502512.502549) ; Hu & Wellman, 2003
* [Policy Hill-Climbing](https://www.sciencedirect.com/science/article/pii/S0004370201001250) ; Littman, 2001
* 团队内部协作 + 团队间对抗（如 AlphaStar, Jaderberg et al., 2019）