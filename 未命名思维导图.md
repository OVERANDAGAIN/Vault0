---
mindmap-plugin: basic
display-mode: outline
---

# MAS

## 多智能体
- 理论框架
   - 扩展到多主体交互与部分可观测
      - [Markov Game](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
      - Partially Observable Markov Game (POMG)
- Settings
   - 按效用结构划分（合作 / 竞争 / 混合动机）
      - Cooperative（纯合）
         - [Team MDP](https://link.springer.com/chapter/10.1007/3-540-61380-2_18) ; Boutilier, 1996
         - [Independent Q-learning](https://link.springer.com/chapter/10.1007/3-540-45545-0_14) ; Lauer & Riedmiller, 2000
      - Competitive（零和）
         - [Minimax-Q](https://www.jmlr.org/papers/volume4/littman03a/littman03a.pdf) ; Littman, 1994
         - [AlphaGo Zero](https://www.nature.com/articles/nature24270) ; Silver et al., 2016
         - [OpenAI Five](https://arxiv.org/abs/1912.06680) ; OpenAI, 2019
      - Mixed-motive（一般和）
         - [Nash Q-learning](https://dl.acm.org/doi/10.1145/502512.502549) ; Hu & Wellman, 2003
         - [Policy Hill-Climbing](https://www.sciencedirect.com/science/article/pii/S0004370201001250) ; Littman, 2001
         - 团队内部协作 + 团队间对抗
- 混合动机下的合作
   - 社会困境建模
      - 个体理性与群体理性的张力（矩阵博弈：R, P, T, S 及社会困境不等式）
         - 互惠优于互害：$R>P$；互惠优于被占便宜：$R>S$；互惠优于“掷硬币”：$2R>T+S$；恐惧/贪婪：$P>S$ 或 $T>R$
      - 时序扩展与部分可观测（从一次性博弈到马尔可夫博弈）
         - [Sequential Social Dilemmas (SSD)](https://arxiv.org/abs/1702.03037) ; Leibo et al., 2017
            - 用马尔可夫博弈与策略集 $\Pi_C,\Pi_D$ 定义 R/P/T/S 的长期回报
      - 交互外部性与协调难度刻画
         - Schelling 图（最小可行协作规模、外部性随协作者数量的变化）
-
-
      - 非对称困境
         - 奖励、能力、观测异质带来的角色分化与“贸易”式合作机会
   - 策略优化方法（CTDE/分布式策略）
      - CTDE
         - [MADDPG](https://arxiv.org/abs/1706.02275) ; Lowe et al., 2017
            - 每个体独立 critic；可推断他人动作以缓解信息缺失
         - [MAPPO](https://arxiv.org/abs/2103.01955) ; Yu et al., 2022
            - PPO 的多智能体 CTDE 版本；参数共享，解 Dec-POMDP
         - [HAPPO](https://arxiv.org/abs/2109.11251) / [HATRPO](https://arxiv.org/abs/2109.11251) ; Kuba et al., 2021
            - 多智能体优势分解理论，避免参数共享
      - 信用分配与改进基线（仅在解释后给改进）
         - [COMA](https://arxiv.org/abs/1705.08926) ; Foerster et al., 2018
            - 反事实基线：边际化单体动作以度量其贡献
         - [LIIR](https://arxiv.org/abs/1906.10129) ; Du et al., 2019
            - 个体内在奖励 + 元梯度双层优化，提高个体可辨识度
   - Other-regarding Preferences
      - 在奖励函数中引入社会偏好（prosociality、altruism、inequity aversion）
         - [Prosocial RL](https://arxiv.org/abs/1707.02341) ; Peysakhovich & Lerer, 2018a
         - [Gifting Mechanism](https://arxiv.org/abs/2006.12044) ; Lupu & Precup, 2020
   - Opponent Modeling / Shaping
      - LOLA及其扩展
         - 通过“显式考虑对手的学习更新”来影响其未来策略；为此需要能对期望回报在“对手一次或多次学习步”上求导
            - [LOLA](https://arxiv.org/abs/1709.04326) ; Foerster et al., 2018
               - 改进/扩展：用通用的高阶梯度估计器稳定实现“对手学习可微化”
                  - [DiCE](https://arxiv.org/abs/1802.05098) ; Foerster et al., 2018
      - 稳定性与一致性（避免“自作聪明”的不收敛、假设不一致等）
         - 在对手塑形的同时，保证局部收敛/不落入鞍点，并消除“我假设对手是天真学习者”的不一致
            - [Stable Opponent Shaping (SOS)](https://arxiv.org/abs/1811.08469) ; Letcher et al., 2019
               - 改进/扩展：两方都做塑形时学到**一致**的更新规则
                  - [COLA](https://arxiv.org/abs/2209.07125) ; Willi et al., 2022
               - 改进/扩展：用**近端**约束替代原始一阶近似，使更新对参数化不敏感
                  - [POLA](https://openreview.net/forum?id=sq3jtWc2O1n) ; Zhao et al., 2022
      - 直接激励（显式给予“奖励/惩罚”信号来塑形对手）
         - 将“激励函数”与策略解耦，奖励由施加者学习并直接进入受体的回报，避免仅靠环境动态间接影响
            - [LIO](https://openreview.net/forum?id=j9kqa82Yqfm) ; Yang et al., 2020
      - 元学习/无模型塑形（长视角、多步塑形，无需白盒对手）
         - 把“与会学习的对手交互”提升为**元博弈**：一次元步=一整局游戏，学会跨多局逐步塑形
            - [M-FOS](https://openreview.net/forum?id=naY7Qqg8mO) ; Lu et al., 2022
               - 改进/扩展：在高维、长时序任务中扩展可塑形能力（结合ES/RNN样式的跨局信息整合）
                  - [Shaper](https://arxiv.org/abs/2402.01068) ; Khan et al., 2024
               - 改进/扩展：把“会学习的对手”正式化为**元POMDP**并给出高效PG算法
                  - [COALA_PG](https://arxiv.org/abs/2406.04378) ; Bertrand et al., 2024
               - 改进/扩展：以**元价值函数**显式建模“当前更新的长远效应”，从而给出方向
                  - [MeVa](https://arxiv.org/abs/2306.02338) ; Cooijmans et al., 2023
      - 最优响应视角与可扩展替代
         - 通过**近似/摊销的最优响应**来塑形，避免高阶梯度和白盒假设
            - [Best Response Shaping](https://arxiv.org/abs/2404.06519) ; Aghajohari et al., 2024
      - 面向对手的值函数/算法特化
         - 直接**控制对手的Q值或学习信号**来塑形，而非只对环境动作施加影响
            - [LOQA: Learning with Opponent Q-Learning Awareness](https://arxiv.org/abs/2406.02920) ; Aghajohari et al., 2024
      - 表征驱动的对手影响
         - 学习低维**对手行为表征**并通过自身策略**可控地影响**该表征的演化，绕开深层递归推理的计算爆炸
            - [Learning Latent Representations to Influence Multi-Agent Interaction](https://proceedings.mlr.press/v164/xie22a.html) ; Xie et al., 2021
- 评测与基准
   - 协同行军与微操（部分观测、集中评测）
      - [SMAC（StarCraft Multi-Agent Challenge）](https://arxiv.org/abs/1902.04043) ; Samvelyan et al., 2019
   - 双人高协作烹饪（异质搭档与零样本协作）
      - [Overcooked-AI](https://arxiv.org/abs/1910.05789) ; Carroll et al., 2019/2020
   - 连续物理 2v2 足球（连续控制与协作涌现）
      - [MuJoCo Soccer](https://arxiv.org/abs/1902.07151) ; Liu et al., 2019
   - 逼真规则完备足球
      - [Google Research Football](https://arxiv.org/pdf/1907.11180) ; Kurach et al., 2019/2020
   - 公地清理与公共品供给
      - [Clean-up](https://arxiv.org/abs/1803.08884) ; Hughes et al., 2018
   - 资源共用–再生（过度采集与再生动态）
      - [Commons / Harvest](https://arxiv.org/abs/1707.06600) ; Perolat et al., 2017（后续命名为 Harvest, Hughes et al., 2018）
   - 彩币博弈（短期诱惑与长期收益权衡）
      - [Coin Game](https://arxiv.org/abs/1707.01068) ; Lerer & Peysakhovich, 2017
   - 等级协作采集（能力门槛与分工协同）
      - [Level-Based Foraging（基准综述）](https://arxiv.org/abs/2006.07869) ; Papoudakis et al., 2021
   - 泛化评测套件（陌生伙伴与新组合）
      - [Melting Pot 1.0](https://proceedings.mlr.press/v139/leibo21a/leibo21a.pdf) ; Leibo et al., 2021
      - [Melting Pot 2.0](https://arxiv.org/abs/2211.02856) ; Agapiou et al., 2022

## 单智能体
- 理论框架
   - 形式化单智能体决策与部分可观测
      - [MDP](https://link.springer.com/article/10.1007/BF00992696) ; Sutton, 1988
         - 状态转移依赖 Markov 性
         - 目标：找到最优策略 $\pi^*$
      - [POMDP](https://www.sciencedirect.com/science/article/pii/S000437029800023X) ; Cassandra, 1998
         - 仅能观测部分信息，需引入观察函数
         - 适用于带噪声/不完全信息的环境
- 价值函数方法
   - 通过近似最优 $Q$ 函数导出策略
      - [Q-Learning](https://link.springer.com/article/10.1007/BF00992698) ; Watkins & Dayan, 1992
         - off-policy，更新公式收敛性有保证
      - [SARSA](https://dl.acm.org/doi/10.5555/645529.657617) ; Singh et al., 2000
         - on-policy，有限状态空间收敛证明
      - 函数逼近解决连续状态
         - [DQN](https://arxiv.org/abs/1312.5602) ; Mnih et al., 2013/2015
- 基于策略的方法
   - 直接优化参数化策略，缓解值函数逼近限制
      - [REINFORCE](https://dl.acm.org/doi/10.1145/138243.138273) ; Williams, 1992
         - 改进方向：降低方差、提升收敛
            - [Actor–Critic](https://papers.nips.cc/paper/1786-convergence-properties-of-policy-iteration) ; Konda & Tsitsiklis, 1999
         - 改进方向：稳定更新过程（信任域/近端）
            - [TRPO](https://arxiv.org/abs/1502.05477) ; Schulman et al., 2015
            - [PPO](https://arxiv.org/abs/1707.06347) ; Schulman et al., 2017
               - TRPO 的简化高效实现
      - [Deterministic Policy Gradient](https://proceedings.mlr.press/v32/silver14.pdf) ; Silver et al., 2014
         - 适用于连续动作空间
      - [SAC](https://arxiv.org/abs/1801.01290) ; Haarnoja et al., 2018
         - 最大熵框架，稳定高效